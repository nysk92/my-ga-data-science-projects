{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ed9621",
   "metadata": {},
   "source": [
    "# Data Cleaning and EDA\n",
    "\n",
    "The project task is to build a model that can classify between posts of different subreddits.\n",
    "\n",
    "Scraped raw data from reddit can be found in the [scraped data folder](../data/scraped).\n",
    "\n",
    "This notebook covers cleaning the scraped data and saving a cleaned dataset, doing EDA on the datasets, and trying out some light models to get a sense of how the subreddits differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812155a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import regex as re\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d49b34",
   "metadata": {},
   "source": [
    "## Data Cleaning of Scraped Reddit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d017e81",
   "metadata": {},
   "source": [
    "### Cleaning of the Language Technology Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d650f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Language Technology dataset\n",
    "lt = pd.read_csv('../data/scraped/lt_data_1_160621.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1fbfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = lt.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f8772b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c430053",
   "metadata": {},
   "source": [
    "Drop irrelevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4192b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = lt[['title', 'selftext', 'url', 'subreddit', 'name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a9424c",
   "metadata": {},
   "source": [
    "Check whether sensible to dedupe only based on name field for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ccf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lt[lt.duplicated(subset=['title','name'], keep=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b0a64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lt[lt.duplicated(subset=['name'], keep=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5256e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = lt.drop_duplicates('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22660cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae5de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4583cc",
   "metadata": {},
   "source": [
    "There are many entries with no selftext due to it being a shared embedded video. Theese entries with no selftext but with title are still useful as the titles are quite descriptive.\n",
    "\n",
    "Combine the title and selftext to a new field called 'content'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9e38b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lt.loc[:,'selftext'] = lt['selftext'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3319032",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d9c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt['content'] = lt['title'] + ' ' + lt['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c156abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ca9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = lt.drop_duplicates('content')\n",
    "lt.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d7b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b403dc",
   "metadata": {},
   "source": [
    "Get a sense of how the posts vary by length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e229854e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lt['len'] = lt['content'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece9626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lt['len'].hist()\n",
    "plt.title('Distribution of Post Character Length')\n",
    "plt.xlabel('Character Length of Language Tech Posts')\n",
    "plt.ylabel('Number of Posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt[lt['len'] > 10000]['len'].hist()\n",
    "plt.title('Posts Above 10000 Characters')\n",
    "plt.xlabel('Character Length of Language Tech Posts')\n",
    "plt.ylabel('Number of Posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264f96c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt[lt['len'] < 1000]['len'].hist()\n",
    "plt.title('Posts Below 10000 Characters')\n",
    "plt.xlabel('Character Length of Language Tech Posts')\n",
    "plt.ylabel('Number of Posts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53f59b0",
   "metadata": {},
   "source": [
    "As expected, there will be many posts that are very short, especially given that around 200 posts had words in the title only and not in selftext."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b290cae1",
   "metadata": {},
   "source": [
    "Next, some entries from title and content are printed to get a sense of any weird characters or patterns to look our for when cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bcca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in lt['title'][:100]:\n",
    "    print(t, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b0891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sqb(text):\n",
    "    \"\"\"\n",
    "    Removes square brackets\n",
    "    and text between them from a string of text.\n",
    "    Allows for larger window within a url-like pattern.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\[.{1,20}\\]|\\[.{1,20}\\..{1,8}/.{0,50}\\]', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410ab01",
   "metadata": {},
   "source": [
    "From the titles, one pattern we way want to remove that does not preserve information is words in square brackets, such as `[d]` and `[Video]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abffc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in lt['selftext'][:10]:\n",
    "    print(repr(t), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26726c1e",
   "metadata": {},
   "source": [
    "Looking through the content, one observation is that there are a lot of urls, but the urls are not totally useless information. It is worth getting part of the substrings in the url such as 'github' or the contituent words in 'facebook-ai-open-source-the-flores-101-data-set-for-better-translation-systems-around-the-world'. \n",
    "\n",
    "Perhaps the urls can be broken down instead of totally removed. To address this, we can remove all slashes and dashes from the texts, and remove http/https and common top level domain names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c70c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_urls(text):\n",
    "    \"\"\"\n",
    "    Breaks down urls in text\n",
    "    into constituent information.\n",
    "    \"\"\"\n",
    "    return re.sub(r'/|-|_|http:|https:|html|en|www|google|facebook|reddit|\\.com|\\.co|\\.net|\\.info|\\.org|\\.us|\\.uk|\\.eu|\\.ru|\\.de|\\.fr|\\.au|\\.cn|\\.in|\\.jp|\\.ca|\\.tk|\\.ly|\\.io', ' ',\n",
    "                  text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5929cc36",
   "metadata": {},
   "source": [
    "Note that long token strings in urls will be filtered out by the min_df in the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8508c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(text):\n",
    "    pstem = PorterStemmer()\n",
    "    return ' '.join([pstem.stem(w) for w in text.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d8c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stpwrds = stopwords.words('english')):\n",
    "    return ' '.join([w for w in text.split(' ') if w not in stpwrds])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f00edb9",
   "metadata": {},
   "source": [
    "Numbers will also be removed at the preprocessing as previous iterations of CV vocab shows numbers to be unhelpful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f27ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(raw_text):\n",
    "    processed = re.sub(r'\\n|\\t', ' ', raw_text)\n",
    "    processed = melt_urls(clean_sqb(processed))\n",
    "    processed = re.sub(r'\\d+', '', processed)\n",
    "    processed = processed.lower()\n",
    "    processed = remove_stopwords(processed)\n",
    "    return stem(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdae01f",
   "metadata": {},
   "source": [
    "### Visualising Vocab of the Language Technology Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15283568",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(\n",
    "    preprocessor = preproc,\n",
    "    ngram_range = (1,4),\n",
    "    min_df = 2,\n",
    "    max_df = 0.95,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80853ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_cvec = cvec.fit(lt['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4eecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_cvec_tr = lt_cvec.transform(lt['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f47115",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_vocab = lt_cvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e67d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_counts = np.asarray(lt_cvec_tr.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f441c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_word_counts_dict = dict(sorted(zip(lt_vocab, lt_counts[0,:]), key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af648a1f",
   "metadata": {},
   "source": [
    "The vocab dictionary ordered by count gives a sense of the most common words and phrases found in the Language Technology corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c262dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_word_counts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e64ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lt.to_csv('../data/clean/lt_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6009a403",
   "metadata": {},
   "source": [
    "### Cleaning of the Neuro Linguistic Programming Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1447feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the NeuroLingPro dataset\n",
    "nl = pd.read_csv('../data/scraped/ud_nl_data_1_160621.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1c4b2f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5d8ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98768768",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = nl[['title', 'selftext', 'url', 'subreddit', 'name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8efcda",
   "metadata": {},
   "source": [
    "Check whether sensible to dedupe only based on name field for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15e07ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nl[nl.duplicated(subset=['title','name'], keep=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nl[nl.duplicated(subset=['name'], keep=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c36db9d",
   "metadata": {},
   "source": [
    "All the duplicates by title are same as duplicates by name. The data can be deduped by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = nl.drop_duplicates('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca6462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44263cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb5074e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nl[nl.selftext.isna()][10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdc2c02",
   "metadata": {},
   "source": [
    "Although there are quite a lot of entries without selftext, the title contains quite a lot of descriptive vocabulary that could be used to train the model. This is data we should preserve.\n",
    "\n",
    "Combine the title and selftext to a new field called 'content'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0266d38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nl.loc[:,'selftext'] = nl['selftext'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427ff941",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57fb39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl['content'] = nl['title'] + ' ' + nl['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12778a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ac8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f28fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl.drop_duplicates('content').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630f0a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = nl.drop_duplicates('content')\n",
    "nl.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e2866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3614565b",
   "metadata": {},
   "source": [
    "Get a sense of how the posts vary by length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab80ba44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nl['len'] = nl['content'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40c1da3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nl['len'].hist()\n",
    "plt.title('Distribution of NLP Posts by Character Length')\n",
    "plt.xlabel('Character Length of NLP Posts')\n",
    "plt.ylabel('Number of Posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c891fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl[nl['len'] > 10000]['len'].hist()\n",
    "plt.title('Posts Above 10000 Characters')\n",
    "plt.xlabel('Character Length of NLP Posts')\n",
    "plt.ylabel('Number of Posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e43f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl[nl['len'] < 1000]['len'].hist()\n",
    "plt.title('Posts Below 1000 Characters')\n",
    "plt.xlabel('Character Length of NLP Posts')\n",
    "plt.ylabel('Number of Posts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92ee994",
   "metadata": {},
   "source": [
    "There are only 2 posts above 10,000 characters long in this dataset, and most of the posts are under 1000 characters. A large majority are between 0-100 characters, as there are the posts where the title makes up the most of the text content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3700503f",
   "metadata": {},
   "source": [
    "Next, samples of the text will be looked through to see if there are any patterns to clean that the above cleaning functions do not take care of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfee8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in nl['title'][:100]:\n",
    "    print(t, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8474ebe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for t in nl[nl['selftext']!='']['selftext'][:10]:\n",
    "    print(repr(t), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecf1b16",
   "metadata": {},
   "source": [
    "There seems to be numbers and words in square bracket that require cleaning, which the above functions will take care of. urls do not seem to be as prevalent in this dataset. Nonetheless, they will be broken down as with the first dataset, as and when they occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_cvec = CountVectorizer(\n",
    "    preprocessor = preproc,\n",
    "    ngram_range = (1,4),\n",
    "    min_df = 2,\n",
    "    max_df = 0.95,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd4a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_cvec.fit(nl['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d890312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_cvec_tr = nl_cvec.transform(nl['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cdea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_vocab = nl_cvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45daeaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_counts = np.asarray(nl_cvec_tr.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_word_counts_dict = dict(sorted(zip(nl_vocab, nl_counts[0,:]), key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddb2d49",
   "metadata": {},
   "source": [
    "Next, the top vocabulary counts will be inspected for anything that requires further cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d4efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_word_counts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92534d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nl.to_csv('../data/clean/nl_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeb2ba6",
   "metadata": {},
   "source": [
    "The vocabulary output generally looks clean for now, enough to move on to combine the datasets and train a prototype model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddcd4c1",
   "metadata": {},
   "source": [
    "### Combine the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f1bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned data\n",
    "# nl = pd.read_csv('../data/clean/nl_data.csv')\n",
    "# lt = pd.read_csv('../data/clean/lt_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d492761",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([nl, lt], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fadf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['content', 'len', 'subreddit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112f47cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681aa1e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9311faef",
   "metadata": {},
   "source": [
    "### Compare Vocabulary\n",
    "\n",
    "A Count Vectorizer will be used on the combined data to get the word counts for each feature. After that, the resultant matrix will be converted to a dataframe, with some aggregations and transformations made to it so that the feature counts for the two topics can be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db0563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_cvec = CountVectorizer(\n",
    "    preprocessor=preproc,\n",
    "    ngram_range = (1,4),\n",
    "    min_df = 2,\n",
    "    max_df = 0.95,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f46fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_cvec_matrix = combined_cvec.fit_transform(df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8931a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_df = pd.DataFrame(combined_cvec_matrix.todense(), columns=combined_cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353e87ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = pd.concat([df.drop(columns='len'), cvec_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = count_df.groupby('subreddit').sum().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb85155",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df['total count'] = count_df['LanguageTechnology'] + count_df['NLP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac1f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df['diff'] = np.abs(count_df['LanguageTechnology'] - count_df['NLP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a22da",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df.rename_axis('feature', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01134b96",
   "metadata": {},
   "source": [
    "Get the top occuring words/phrases in the Language Technology data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9ef707",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df.sort_values(by='LanguageTechnology', ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d64e797",
   "metadata": {},
   "source": [
    "Get the top occuring words/phrases in the Neuro Linguistic Programming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df.sort_values(by='NLP', ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef71132",
   "metadata": {},
   "source": [
    "Examine top words that occur in one topic but not the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df[count_df['NLP']==0].sort_values(by='LanguageTechnology', ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c4be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df[count_df['LanguageTechnology']==0].sort_values(by='NLP', ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a3f7b0",
   "metadata": {},
   "source": [
    "Examine words that occur a similar amount of times between both topics' datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d979f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df[(count_df['LanguageTechnology'] > 30)\n",
    "         &(count_df['NLP'] > 30)\n",
    "         &(count_df['diff'] < 10)\n",
    "         &(count_df['total count'] < 800)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966c5cbb",
   "metadata": {},
   "source": [
    "Examine words that differ largely in the number of counts between topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0cfed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df.sort_values(by='diff', ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a31f319",
   "metadata": {},
   "source": [
    "Based on background knowledge of the topics, some word distributions between the two topics can be visualised. Some words that one could guess will be prominent across both topics, or distinct to one can be used to subset the dataframe and have their distributions visualised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bb819b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = count_df.loc[['nlp', 'program', 'languag', 'learn', 'cours', 'linguist', 'search', \n",
    "              'expert', 'coach', 'practition', 'scam', 'scienc', 'python', 'gpt'],\n",
    "             ['LanguageTechnology','NLP']].plot(kind='barh',\n",
    "                                                title='Word Count Comparison',\n",
    "                                                figsize=(8,12),\n",
    "                                                fontsize=14,\n",
    "                                                ylabel='Word Count Log Scale',\n",
    "                                                #xlim=(0,100),\n",
    "                                                logx=True\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1307bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = ax.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8161d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.savefig('../img/word_dist_barh.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1dbb6c",
   "metadata": {},
   "source": [
    "Next, get an overall sense of how many terms are unique to each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27238ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aca23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df[(count_df['LanguageTechnology'] > 0)\n",
    "         &(count_df['NLP'] > 0)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b342627",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df[(count_df['LanguageTechnology'] == 0)\n",
    "         &(count_df['NLP'] != 0)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcbc6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df[(count_df['LanguageTechnology'] != 0)\n",
    "         &(count_df['NLP'] == 0)].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc4fc6",
   "metadata": {},
   "source": [
    "Although there are quite a lot of commonly occurding words/phrases between the topics (5350), there are also many words/phrases that occur in one topic's data and not the other (about 10000). This shows that the classification task to separate the two has a lot of potential to be a successful one as there are many potential distinguishing features.\n",
    "\n",
    "Next, some further clean up will be done and the first model prototype will be tried."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c140efa0",
   "metadata": {},
   "source": [
    "## Train a Prototype Model\n",
    "\n",
    "Now, the combined dataset will be used to build a prototype model to get a better feel of the classification task.\n",
    "\n",
    "The main goal is to get a rough sense of how succesful this classification task will be, and what sort of further data cleaning needs to be done before the data moves to the next stage where many different models and hyper params will be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512af64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eabda2",
   "metadata": {},
   "source": [
    "There is a close to 50/50 split. The benchmark accuracy is about 51%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd55545a",
   "metadata": {},
   "source": [
    "Next, the subreddit will be mapped to numberic class labels.\n",
    "The positive label (1) will be the Language Technology label, and 'NLP' (the Neuro Linguistic Programming) subreddit will be the negative label (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LT'] = df['subreddit'].map(lambda x: 0 if x == 'NLP' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c11899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['LT'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158df837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('../data/clean/nl_lt_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a091b1bc",
   "metadata": {},
   "source": [
    "### Separate Data into Predictors and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c932da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is a series as it will be featurised with a Count Vectorizer later\n",
    "X = df['content']\n",
    "y = df['LT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e97a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b56c62",
   "metadata": {},
   "source": [
    "### Removing Obvious Vocabulary from the Features\n",
    "The model will not be useful if it simply learns the subreddit name or synonyms for the subreddit name.\n",
    "These will be removed from featurisation by being added to stopwords. \n",
    "\n",
    "However, note that for the task of classifying between Language Technology and Neuro Linguistic Programming posts, the term 'NLP' will not be removed from the features as it is highly prevalent in both topics ('NLP' also stands for 'Natural Language Processing' which is a popular synonym for Language Technology). Hence, 'NLP' should not be a distinguishing feature between the two topics, and in reality it is better to leave that term there precisely because these two topics confusingly use the term 'NLP' to refer to two different things, and the utlity of the classifier will be to distinguish between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15641b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vocab(wc_dict, substr):\n",
    "    \"\"\"\n",
    "    Check for a certain term in the\n",
    "    word count dictionary \n",
    "    based on a substring match.\n",
    "    \"\"\"\n",
    "    return {k:v for k,v in wc_dict.items() if substr in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c765a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_vocab(lt_word_counts_dict, 'program')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38d6e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_vocab(nl_word_counts_dict, 'neurolinguist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcd5f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not removing 'nlp', 'linguist', programming' as they are commonly used terms in both sets\n",
    "stpwrds_lt_nl = stopwords.words('english') + ['language', 'technology',\n",
    "                                              'natural', 'processing',\n",
    "                                              'neuro', 'neurolinguist', 'neurolinguistic']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967a84e8",
   "metadata": {},
   "source": [
    "Uodate the preprecessing function to account for extended stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ffe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_lt_nl(raw_text):\n",
    "    processed = re.sub(r'\\n|\\t', ' ', raw_text)\n",
    "    processed = melt_urls(clean_sqb(processed))\n",
    "    processed = re.sub(r'\\d+', '', processed)\n",
    "    processed = processed.lower()\n",
    "    processed = remove_stopwords(processed, stpwrds=stpwrds_lt_nl)\n",
    "    return stem(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a90e481",
   "metadata": {},
   "source": [
    "### Create the Count Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75769b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_nl_cvec = CountVectorizer(\n",
    "    preprocessor = preproc_lt_nl,\n",
    "    ngram_range = (1,4),\n",
    "    min_df = 2,\n",
    "    max_df = 0.95,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867372cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_nl_cvec.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f05e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_nl_cvec_tr = lt_nl_cvec.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877443b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_nl_vocab = lt_nl_cvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83058e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_nl_counts = np.asarray(lt_nl_cvec_tr.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_dict = dict(sorted(zip(lt_nl_vocab, lt_nl_counts[0,:]), key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb6fd65",
   "metadata": {},
   "source": [
    "Inspect the combined vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d8618",
   "metadata": {},
   "source": [
    "### Train a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e74002",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('cvec', CountVectorizer(\n",
    "    preprocessor = preproc_lt_nl,\n",
    "    ngram_range = (1,4),\n",
    "    min_df = 2,\n",
    "    max_df = 0.95,)),\n",
    "                 ('logreg', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f14b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb39cd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca22f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9595543f",
   "metadata": {},
   "source": [
    "A simple logistic regression classifier seems to work remarkably well in distinguishing between the two topics.\n",
    "The features' coefficients will be inspected to see if the prominent features seem sensible in the sense that it would generalise to more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3054670f",
   "metadata": {},
   "source": [
    "### Inspect Coefficients for Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24b530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(sorted(zip(model['cvec'].vocabulary_.keys(), np.exp(model['logreg'].coef_[0])), key=lambda x:np.abs(x[1]), reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc73ff",
   "metadata": {},
   "source": [
    "Some names are surfacing in the vocabulary. Some investigation is warranted to ensure these are not reddit users, but actual people associated with the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c4539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_pattern(phrase, text):\n",
    "    if phrase in text.lower():\n",
    "        print(re.findall(r'.{1,20}' + phrase.replace(' ', r'\\s') + r'.{1,20}', text, flags=re.I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ad2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in nl['content']:\n",
    "    inspect_pattern('james pesch', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0485b39a",
   "metadata": {},
   "source": [
    "A google search shows that James Pesch is some seemingly popular personality in Neuro Linguistic Programming, so the name would be a sensible feature to retain to distinguish between the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82d748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in nl['content']:\n",
    "    inspect_pattern('blow', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fdc2a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for text in lt['content']:\n",
    "    inspect_pattern('specif', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14dca53",
   "metadata": {},
   "source": [
    "It seems weird that 'blow' is a significant feature to distinguish between the two topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c1c78",
   "metadata": {},
   "source": [
    "Next, to further test the robustness of this preliminary classifier, it will be evaluated on a more difficult task - being able to distinguish between closely related topics to Language Technology and Neuro Linguistic Programming. These closely related topics are Deep Learning (similar topic to Language Technology) and Hypnosis (similar topic to Neuro Linguistic Programming)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b083f29f",
   "metadata": {},
   "source": [
    "## Evaluation Using Related Topics \n",
    "### Deep Learning and Hypnosis Data\n",
    "This data of related topics will be used to see if the model can generalise well to unseen data of closely related topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26cfcb6",
   "metadata": {},
   "source": [
    "### Load and Clean the Deep Learning and Hypnosis Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31021922",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = pd.read_csv('../data/scraped/ud_dl_data_1_160621.csv')\n",
    "hy = pd.read_csv('../data/scraped/ud_hy_data_1_160621.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b53784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52564b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hy.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_dedupe(data):\n",
    "    \"\"\"\n",
    "    Converts raw scraped data into \n",
    "    format ready to be featurised.\n",
    "    Follows the dataframe transformations \n",
    "    in the top half of this nb.\n",
    "    \"\"\"\n",
    "    data = data[['name', 'title', 'selftext', 'url', 'subreddit']]\n",
    "    data = data.drop_duplicates('name')\n",
    "    data['selftext'] = data['selftext'].fillna('')\n",
    "    data['content'] = data['title'] + ' ' + data['selftext']\n",
    "    data = data.drop_duplicates('content')\n",
    "    data.reset_index(inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8ce720",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = clean_and_dedupe(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7618be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab6850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hy = clean_and_dedupe(hy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001f5fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c64f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "hy_dl = pd.concat([hy,dl], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22620b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hy_dl = hy_dl[['content', 'subreddit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5085116",
   "metadata": {},
   "outputs": [],
   "source": [
    "hy_dl['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b5b9ba",
   "metadata": {},
   "source": [
    "Ensure that the positive label (1) in this set lines up with the analogous positive label of the model training data. So deep learning posts will be labelled '1', as they are the analogous posts to the language technology posts labelled '1' above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9da205",
   "metadata": {},
   "outputs": [],
   "source": [
    "hy_dl['DL'] = hy_dl['subreddit'].map(lambda x: 1 if x=='deeplearning' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111a4fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hy_dl['DL'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8f9339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hy_dl.to_csv('../data/clean/hy_dl_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d37d8cd",
   "metadata": {},
   "source": [
    "### Evaluate Logistic Regression Model on Related Topics\n",
    "\n",
    "Now, with the data from the related topics, Hypnosis and Deep Learning, prepared for prediction, the logistic regression model above trained on the Neuro Linguistic Programming and Language Technology data will be evaluated for how well it can generalise to distinguish between closely related topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deae7bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dl = hy_dl['content']\n",
    "y_dl = hy_dl['DL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926de1d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.score(X_dl, y_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b96de2",
   "metadata": {},
   "source": [
    "The model trained on distinguishing Language Techonology posts from Neural Linguistic Programming posts generalises quite well to distinguish between Deep Learning and Hypnosis content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b32ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_dl, model.predict(X_dl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045645df",
   "metadata": {},
   "source": [
    "The model's perfomance on the Deep Learning and Hypnosis set is decent, however, some analysis can be done on where the model could improve.\n",
    "\n",
    "Firstly, the model suffers on precision (of 78%) for the negative label. Meaning for every 10 posts the model classifies as 'Hypnosis', about 2 are wrongly classified, and actually about 'Deep Learning'.\n",
    "\n",
    "The model has a relatively low recall rate (75%) for the positive model too. It cannot detect about one quarter of the posts on deep learning.\n",
    "\n",
    "There are several ways one could try to improve the score on unseen data. Different models or different regularization parameters can be tried, which will be done in the next notebook where there is a more thorough exploration of different models.\n",
    "\n",
    "Another method to try is to change the preprocessor to get better features. Above, stemming was tried, but not lemmatization, which wil be quickly tried next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e425b4bd",
   "metadata": {},
   "source": [
    "### Try Lemmatization\n",
    "Lemmatizaion will be tested to see if it can outperform stemming as a preprocessing step that can contribute to better model performance. In theory lemmatization should provide a better consolidation of different forms of words into its original form, thereby helping the model learn better with more distinct and consolidated word features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1921c9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(word):\n",
    "    pos_key = pos_tag([word])[0][1][0]\n",
    "    pos_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'R': wordnet.ADV,\n",
    "        'V': wordnet.VERB\n",
    "    }   \n",
    "    return pos_dict.get(pos_key, wordnet.NOUN)\n",
    "\n",
    "def lem(text):\n",
    "    lemm = WordNetLemmatizer()\n",
    "    return ' '.join([lemm.lemmatize(w, get_pos(w)) for w in text.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528b3a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_lem(raw_text):\n",
    "    processed = re.sub(r'\\n|\\t', ' ', raw_text)\n",
    "    processed = melt_urls(clean_sqb(processed))\n",
    "    processed = re.sub(r'\\d+', '', processed)\n",
    "    processed = processed.lower()\n",
    "    processed = remove_stopwords(processed, stpwrds=stpwrds_lt_nl)\n",
    "    return lem(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766594d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lem = Pipeline([('cvec', CountVectorizer(\n",
    "    preprocessor = preproc_lem,\n",
    "    ngram_range = (1,4),\n",
    "    min_df = 2,\n",
    "    max_df = 0.95,)),\n",
    "                 ('logreg', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed99f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lem.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c1a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lem.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320916e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lem.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ad418",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lem.score(X_dl, y_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c30daf0",
   "metadata": {},
   "source": [
    "Lemmatization does not seem to make a difference in the model accuracy on both the train and test sets. The model with lemmatization proprocessing also suffers a slightly lower score on the related topics. Considering lemmatization is also a more expensive technique than stemming, it will be disregarded in the subsequent model testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caf723c",
   "metadata": {},
   "source": [
    "## Next Notebook: Trying Different Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66292263",
   "metadata": {},
   "source": [
    "**In the [subsequent notebook](02_Models.ipynb), different featurisers and models will be evaluated for their performance on the original dataset and the related dataset.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
