{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a055da6d",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "Binary classification of posts that come from the Language Technology subreddit vs the Neuro Linguistic Programming subreddit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3ec328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb91c241",
   "metadata": {},
   "source": [
    "Important fields: title, selftext and subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75f0e5",
   "metadata": {},
   "source": [
    "## Scraping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71aa20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(url, subreddit, agent='Redditor 2.0', cycles=1):\n",
    "    after = after_store[subreddit]\n",
    "    posts = []\n",
    "    \n",
    "    for _ in range(cycles):\n",
    "        if after == None:\n",
    "            current_url = url\n",
    "        else:\n",
    "            current_url = url + '?after=' + after\n",
    "\n",
    "        res = requests.get(current_url, headers={'User-agent': agent})\n",
    "        if res.status_code != 200:\n",
    "            print('Status error', res.status_code, current_url)\n",
    "            break\n",
    "        print('Succesful: ' + current_url)\n",
    "\n",
    "        current_dict = res.json()\n",
    "        current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "        posts.extend(current_posts)\n",
    "        after = current_dict['data']['after']\n",
    "        after_store[subreddit] = after\n",
    "\n",
    "        sleep_duration = random.randint(1,30)\n",
    "        print('sleep duration: ' + str(sleep_duration))\n",
    "        time.sleep(sleep_duration)\n",
    "        \n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44931f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_posts(posts_list):\n",
    "    unique_posts = len(set([p['name'] for p in posts_list]))\n",
    "    print(f'Currently at {len(posts_list)} posts.')\n",
    "    print(f'{unique_posts} unique posts by name.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e104e1",
   "metadata": {},
   "source": [
    "Create some agent names to cycle through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = ['Redditor 1.0', 'Redditor 2.0', 'Redditor 5.0']\n",
    "# keep redefining list of agents as necessary in subsequent crawls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de50b0a7",
   "metadata": {},
   "source": [
    "### Storing After Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee61f6a9",
   "metadata": {},
   "source": [
    "Use a dictionary to store the reddit 'after' keys so that the crawls can be broken into different sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fa77d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_store = {\n",
    "    'lt': None,\n",
    "    'nlp': None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce6db9e",
   "metadata": {},
   "source": [
    "## Scrape the LanguageTechnology Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b57454",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.reddit.com/r/LanguageTechnology.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1037b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_posts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0f8814",
   "metadata": {},
   "source": [
    "Get first 50 posts (25 posts per cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b21e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_posts.extend(\n",
    "    crawl(\n",
    "        url,\n",
    "        'lt',\n",
    "        cycles=2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34a8d4d",
   "metadata": {},
   "source": [
    "Get 250 more lt posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a44e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_posts.extend(\n",
    "    crawl(\n",
    "        url,\n",
    "        'lt',\n",
    "        cycles=10\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6ed196",
   "metadata": {},
   "source": [
    "Get 500 more lt posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6ce10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_posts.extend(\n",
    "    crawl(\n",
    "        url,\n",
    "        'lt',\n",
    "        cycles=20,\n",
    "        agent = np.random.choice(agents)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27eada4",
   "metadata": {},
   "source": [
    "Get 500 more lt posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_posts.extend(\n",
    "    crawl(\n",
    "        url,\n",
    "        'lt',\n",
    "        cycles=20,\n",
    "        agent = np.random.choice(agents)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8548cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_posts(lt_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c253c9d",
   "metadata": {},
   "source": [
    "Starting to get duplicates...\n",
    "\n",
    "Try another 100 posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a038da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_posts.extend(\n",
    "    crawl(\n",
    "        url,\n",
    "        'lt',\n",
    "        cycles=4,\n",
    "        agent = np.random.choice(agents)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae2fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_posts(lt_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83d2e9a",
   "metadata": {},
   "source": [
    "Waiting 1 day does not improve the max unique posts.\n",
    "Day 2: try different endpoint type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd9d31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_store['lt_top'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_top = 'https://www.reddit.com/r/LanguageTechnology/top.json?t=all&limit=25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b81b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_top_posts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beaeb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_top_posts.extend(\n",
    "    crawl(\n",
    "        url_top,\n",
    "        'lt_top',\n",
    "        cycles=4,\n",
    "        agent = np.random.choice(agents)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5697ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_posts(lt_top_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e862d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_store['lt_contr'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc65203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_contr = 'https://www.reddit.com/r/LanguageTechnology/controversial.json?t=all&limit=25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0c9440",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_contr_posts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ea5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_contr_posts.extend(\n",
    "    crawl(\n",
    "        url_contr,\n",
    "        'lt_contr',\n",
    "        cycles=4,\n",
    "        agent = np.random.choice(agents)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ca133",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_posts(lt_contr_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98330d77",
   "metadata": {},
   "source": [
    "Tried different endpoints on different days. Different end points do not yield a lot more posts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47f98ab",
   "metadata": {},
   "source": [
    "### Inspect Collected Data in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7270a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(lt_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2dbc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b6774",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['textlen'] = df['selftext'].map(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dead6f1",
   "metadata": {},
   "source": [
    "Check for Dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33701644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7754df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['selftext']!=''].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f23b82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates('name').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a1b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates('title').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ee641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('data/scraped/lt_data_1_160621.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa88b81",
   "metadata": {},
   "source": [
    "## Scrape the Neuro-Linguistic Programming Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea98b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = 'https://www.reddit.com/r/NLP.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e6cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ed751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_posts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44148d1f",
   "metadata": {},
   "source": [
    "Get first 100 nl posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7685b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_posts.extend(\n",
    "    crawl(\n",
    "        url2,\n",
    "        'nlp',\n",
    "        cycles=4,\n",
    "        agent = np.random.choice(agents)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31694346",
   "metadata": {},
   "source": [
    "Get next 500 nl posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4383bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_posts.extend(\n",
    "    crawl(\n",
    "        url2,\n",
    "        'nlp',\n",
    "        cycles=20,\n",
    "        agent = np.random.choice(agents)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5358492",
   "metadata": {},
   "source": [
    "Get next 500 nl posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55971b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_posts.extend(\n",
    "    crawl(\n",
    "        url2,\n",
    "        'nlp',\n",
    "        cycles=20,\n",
    "        agent = np.random.choice(agents)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86df915",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_posts(nlp_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68de65d",
   "metadata": {},
   "source": [
    "Starting to face dupes, try for 200 more posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c847ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_posts.extend(\n",
    "    crawl(\n",
    "        url2,\n",
    "        'nlp',\n",
    "        cycles=8,\n",
    "        agent = np.random.choice(agents)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4301c67f",
   "metadata": {},
   "source": [
    "### Inspect Collected NL Data in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa18e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(nlp_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a17146",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe7fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce962a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['selftext']!=''].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ee0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['selftext']!=''][['selftext', 'title', 'url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4445c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.drop_duplicates('name').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34344d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop_duplicates('title').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b891076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.to_csv('data/scraped/ud_nl_data_1_160621.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5877a0e",
   "metadata": {},
   "source": [
    "## Get More Reddit Posts \n",
    "For verifying the model with adjacent topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_store.update({'dl':None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf28612",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dl = 'https://www.reddit.com/r/deeplearning.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d435c5c5",
   "metadata": {},
   "source": [
    "### Get DeepLearning Posts\n",
    "Related topic to Language Technology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e11a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_posts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9602ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 100 dl posts\n",
    "dl_posts.extend(\n",
    "    crawl(\n",
    "        url_dl,\n",
    "        'dl',\n",
    "        cycles=4,\n",
    "        agent = np.random.choice(agents)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6595b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 100 dl posts\n",
    "dl_posts.extend(\n",
    "    crawl(\n",
    "        url_dl,\n",
    "        'dl',\n",
    "        cycles=4,\n",
    "        agent = np.random.choice(agents)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b050590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 300 dl posts\n",
    "dl_posts.extend(\n",
    "    crawl(\n",
    "        url_dl,\n",
    "        'dl',\n",
    "        cycles=12,\n",
    "        agent = np.random.choice(agents)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16982711",
   "metadata": {},
   "source": [
    "Save to df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa8a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_df = pd.DataFrame(dl_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41af8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl_df.to_csv('data/scraped/ud_dl_data_1_160621.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986ac166",
   "metadata": {},
   "source": [
    "### Get Hypnosis Posts \n",
    "Related topic to neurolinguistic programming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_store.update({'hy':None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baf57f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_hy = 'https://www.reddit.com/r/hypnosis.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83480d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hy_posts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3c94b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 200 hy posts\n",
    "hy_posts.extend(\n",
    "    crawl(\n",
    "        url_hy,\n",
    "        'hy',\n",
    "        cycles=8,\n",
    "        agent = np.random.choice(agents)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048ba7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_posts(hy_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e473a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 200 hy posts\n",
    "hy_posts.extend(\n",
    "    crawl(\n",
    "        url_hy,\n",
    "        'hy',\n",
    "        cycles=8,\n",
    "        agent = np.random.choice(agents)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f18bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 200 hy posts\n",
    "hy_posts.extend(\n",
    "    crawl(\n",
    "        url_hy,\n",
    "        'hy',\n",
    "        cycles=8,\n",
    "        agent = np.random.choice(agents)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c7761",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get 200 hy posts\n",
    "hy_posts.extend(\n",
    "    crawl(\n",
    "        url_hy,\n",
    "        'hy',\n",
    "        cycles=8,\n",
    "        agent = np.random.choice(agents)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76fef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 200 hy posts\n",
    "hy_posts.extend(\n",
    "    crawl(\n",
    "        url_hy,\n",
    "        'hy',\n",
    "        cycles=8,\n",
    "        agent = np.random.choice(agents)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb78011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_posts(hy_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa0f022",
   "metadata": {},
   "source": [
    "Save to df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1432b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "hy_df = pd.DataFrame(hy_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be30759",
   "metadata": {},
   "outputs": [],
   "source": [
    "hy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed743a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hy_df.to_csv('data/scraped/ud_hy_data_1_160621.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
