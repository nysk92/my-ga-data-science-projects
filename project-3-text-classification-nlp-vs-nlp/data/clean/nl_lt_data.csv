content,len,subreddit,LT
"This subreddit is about the PSYCHOLOGICAL TECHNIQUES created by Richard Bandler and John Grinder. Are you interested in Natural Language Processing? Go to /r/LanguageTechnology.

Are you interested in machine learning applied to understanding language? Go to /r/LanguageTechnology.

Are you interested in Richard Bandler and John Grinder's approach to communication, personal development, and psychotherapy known as *Neuro-Linguistic Programming*? You're in the right place.",474,NLP,0
"Advertise here only This post is for those who wish to promote their books, services, etc.",90,NLP,0
Who's Beliefs are In Your Head? ,32,NLP,0
Are they all idiots...? ,24,NLP,0
"THAT'S IT! I'm over my NLP program I don't know if I need to rant or whatever but I'm over my NLP program and my coach.
I paid $2500 for what feels like nothing! I want to get over my depression and with 2 sessions left, nothing has changed. There is no life changing moment that was promised. 
I'm so mad at my coach, I felt that I have had no communication during this time. 

He's making me do this task that I write the perfect day for everyday. I write how I wake up in the morning feeling refreshed and blah blah blah. 350 minimum for the day. 

After I few days of doing this I get no replies from him tell me if I'm on the right track.
I've even wrote some ridiculous things in there to see if I get a response and there's nothing.

I have got to the point where I hate writing it and I've skipped days. I've had no emails asking why I haven't sent anything in. I email him and asked him if I'm on the right track and he replied that it was fine. I even replied that I hate doing this task and I got no reply. 

I've had more anxiety doing this program than I should have.

TLDR 
I'm having a winge because my NLP coach hasn't replied to my task emails.",1161,NLP,0
"How can i begin? How do you recommend me to start getting into the world of nlp?
I know nothing about, i understand the genreal idea but not in practicality and in theory.
What books,courses,videos etc... do you recommend me to begin learn nlp?",244,NLP,0
"Techniques to avoid answering questions Good Evening,

I’m an employer of a personal assistant. We are also friends. However, circumstances dictate that I need to keep my life private, while remaining affable, especially at work. 

What are some techniques and strategies for politely answering questions without giving away any details?

I’m pretty familiar with the Milton model, which could help.

What have you been up to? Oh my weekend was pleasant and a nice change.

But any specific techniques and strategies you could point me towards would be much appreciated.

Thanks",578,NLP,0
The NLP metamodel part 1 - Episode #8 of the free online ABC-NLP Practitioner ,78,NLP,0
Do This BEFORE You Change Limiting Beliefs ,43,NLP,0
"NLP's ""Sensory Acuity"" and SMEACs - is it all nonsense? ",56,NLP,0
Can you break down which NLP technique is this? Visualize and future pacing? ,77,NLP,0
"Installation Folks,

played a session of dhe 2000, I think session 5.

the next day my visual acuity had improved.

as well as my sense of smell. I went to lunch, ordered, smelled my food before I heard the kitchen ring a bell and a minute latter what I ordered appeared.

it got me thinking about installation.

is anyone aware of a resource that discusses what Richard installs and in what videos?

my very incomplete list is:

dhe 5 acuity, leave smoking behind.

dhe 7 rhythm

Dhe 8 tonality perception

dhe 11 integration of earlier dhe strategies

i didn’t make a list when I did nhr intro but I know there’s some in here

im working on persuasion engineering now

i think I’m right that dhe session 5 helps with acuity. I will probably rerun it and see if it improves again",780,NLP,0
Meta States | A Model of NLP for Emotional State Mastery ,57,NLP,0
Here is a short clip from Million Dollar Listing NY. Is the salesperson using NLP? Please explain? ,99,NLP,0
"Moving process to unconscious Folks, I was listening to a seminar and I was thinking about a new skill I’ve acquire.

it’s still very new and it’s still conscious. I was thinking just like learning to ride a motorcycle it’ll move itself to unconscious competenc  


and I was listening to Richard talk about doing stuff faster and easier.

how do you move new skills to just becoming built in. Do you run a propulsion system? It’s just applying a rule set. If x happens do y, else take no action. waiting and deciding if x has happened takes a lot of patience and energy.

thanks!",580,NLP,0
"Getting out of destructive behaviour and developing self discipline Hey Folks,

I'm posting this to seek out help from you guys.

I'm a Freelance Copywriter, so it goes without saying that I've to be very active with marketing myself and having discipline and consistency.

So the thing is, I read a lot. On mindset, productivity, psychology, marketing etc.

And I try to implement the things I've learned, but I fail.

I've figured out my limiting beliefs and stuff as well. (Awaken the giant within approach by Tony Robbins)

I am disciplined for 3 days to a week and then I'm back to old patterns - procrastination, resentment, frustration.

I don't know what's going in my head. 

It is weird because I feel like I know where I am going wrong and yet I can't find my way around it.

Can you please help me out with any advice?

Thank you.",842,NLP,0
"What are your questions around changing limiting beliefs? I will be delivering a course on how to change our limiting beliefs next month and I would love your input :)

What questions, challenges, issues would you love for me to cover in the course?   What are your top questions about belief change that you would love if I answered in the program?",349,NLP,0
Time lines and your thirty year plan. Episode #6 of the online ABC-NLP Practitioner ,84,NLP,0
"Question for a new joiner Hello everyone! I’m so excited to be here, I’m extremely enthused about NLP and I cannot wait to read everyone’s posts and learn as much as I can. 

I have a question - I’m currently in the process of implementing a project that combines mental health and NLP. In order to build my database, I have to maximise responses on a google form that I’ve made. 
Since I’m new here and I really don’t want to be breaking any rules I’ve come for advice - is it okay if I post the form here for responses? And are there any other recommended places where people wouldn’t mind me putting it up? 

I’m extremely passionate about this project and this subject, hence the long message - I apologise if I’ve flouted a rule. 

I hope everyone’s safe and has a good day!",779,NLP,0
"Logic Levels Exploration How can I use LL to clarify my thinking? 
The model is supposed to be a valuable tool for organizing our thinking, information gathering and communication.
It allows to focus on any ""particular level of organization.
What is the chunking process?
Tips and adivices? ?",292,NLP,0
The Most Important Belief to Install in Yourself... Hint: It's a Belief about Beliefs ,86,NLP,0
#2: NLP coach format. What to do as a NLP coach? Basic NLP techniques to teach your cliënt first. The online mentoring of real coach ,133,NLP,0
Limiting beliefs What are some common and deep limiting beliefs,63,NLP,0
"How could I go about using NLP to adjust my belief system in a way that I value hard work/suffering/effort? I'm thinking of people who enjoy working out on an empty stomach, because the struggle invigorates them, for example. Or in general, people that always take the hard way or the long way because they instinctively enjoy the prospect of reaping the rewards of hard work so much that the effort doesn't matter to them.

How can I use NLP to become like this?",463,NLP,0
"So I heard about NLP and asked myself... I heard NLP and it sounds very useful and some techniques are ethically legitimate for me but I asked myself if it is really useful and if it works in everyday life. Could someone with experience tell me? 

Thank you guys in advance and have a nice day.",294,NLP,0
"How to select an NLP Coaching course Most of the people here in this community would be NLP Practitioners, NLP Trainers etc., and some of the community members would have had great experience to learn from Richard Bandler, John Grinder, Robert Dilts, Judith DeLozier, Steve Andres and other co-developers of NLP in 1970s. 

Later on because of the fragmentation of NLP into different camps in 80s and 90s, lead to fragmentation of knowledge as well, and as such a number of courses came by, different models were brought into these courses, and unfortunately there was no central body to manage what was going on, hence, the quality of courses dropped. Trainers with half baked information started to train in NLP with their own map,  and that created a lot of issues that we see in NLP during present times. 

NLP based coaching is probably a subset of classic NLP Practitioner and Master Practitioner programs. While the coaching fraternity development and emerged alongside NLP, but coaching community eventually was limited to a few accreditation bodies, ICF and EMCC being the most prominent ones. 

A lot of NLP coaches have aligned with ICF, because ICF keeps a check on what is being offered, and under the ethics, standards and guidelines of coaching umbrella. 

Now if you are looking for an NLP course, but with more of Coaching flavor. Look for a coaching course delivered by an NLP Trainer who has learnt NLP from Richard Bandler, John Grinder, Robert Dilts, Judith, Steve, Michael Hall, Steve Gilligan, Joseph OC, and other known NLP trainers, and have specifically attended a coaching courses from Master Coach Trainers associated with different accreditation bodies like ICF, EMCC, or Meta Coaching. If you are looking to specifically looking to use NLP knowledge in Life Coaching[How to become a Life Coach](https://nlpcoach.in/how-to-become-a-life-coach-in-india/), Leadership Coaching, Wellness Coaching, then this approach of learning from someone who knows both NLP, and has good understanding of coaching helps. 

The blend of NLP and looking at the concepts of NLP in a coaching frame would enable a trainer to develop and deliver a world class NLP coaching course.",2188,NLP,0
"Hi, Reddit NLP community! I am Pedro, an NLP student from Brazil. Check the text so we can share information and becomer better users of NLP together! I Just got here on Reddit (and I´m stil kinda lost here). I wanted to share this post as a way of making contact with other NLP students around the world so we can share information and grow together.

&amp;#x200B;

Here in Brazil, we have many NLP schools that teach distorted information. It´s very rare to find people that can stick to the original NLP models or at least add more quality to them without taking any information away (when teaching). As a result, we have a very poor NLP community.  


I have the opportunity to learn from Dr. Nelson Spritzer, PhD, here in my hometown Porto Alegre (@drnelsonspritzer on IG). He learned NLP and then trained with Richard Bandler's supervision for some weeks, so his model is splendid. Besides that, he was a Cardiology researcher for many years, so he is absolutely careful with the information he shares and has very good criteria to make his choices. He really is an example of what he´s teaching: a model of wisdom, health and good choicemaking. 

&amp;#x200B;

I believe NLP can really impact the humanity in a good way. If people knew more about the way their unconscious mind works and the techniques to control it, we would really see our race have better results in many spheres: individually, colectively, with earth,... and others. Thats why I´m looking to put myself in good comunities with people that are like the idea of quality NLP being more accessible to the people.

&amp;#x200B;

Talk to me and let´s share information and grow together!   


English is not my natural language, so feel free to metamodel me, give me feedback or make any correction on my text ;)",1784,NLP,0
"NLP exercises I have a dumb question. How do folks remember to use the NLP techniques? I journaled and use alarms on my phone and watch, once I get distracted I forget.",168,NLP,0
"How to decorate a house using NLP idea's? I'm planning to decorate my house, any idea's how to let elements in NLP in the decor creating compassion, peace, love or any other interesting things in it?",199,NLP,0
Any ideas on propulsion system? How to create an obsession?,59,NLP,0
How do you use NLP to visualize your future? ,45,NLP,0
Alphabet therapists and their absurd beliefs about the human immune system and COVID19 ,87,NLP,0
#5 Get a better self image and more self love. Part #5 of the free online ABC-NLP Practitioner USA ,99,NLP,0
Do all songs have hypnotic effects on listeners? How careful should we be when listening to breakup or sad songs? ,114,NLP,0
How to Social Engineer ,23,NLP,0
"Archetypes: ""The Blasted Tower"" (Qabalistic Path of Peh) - or, When Life Just Starts Falling Apart. ",100,NLP,0
NLP 2.0 ,8,NLP,0
Hypnotic- Collection of confidence Dm if anyone is interested.,62,NLP,0
"Are there any NLP/HYPNOSIS Master Coaches or experienced practitioners in the building that would be interested in mentoring? If so, please send me a private message, would like to present an offer. ",199,NLP,0
"Good NLP exercises Are there NLP  exercise I can do at home, more mentally the actual application. I am new to NLP and am very immature, I need to step up my game.",163,NLP,0
David Snyders Sphere of Inluence Anyone tried David Snyders remote influence tactic from CPI?,93,NLP,0
Powerful Hypnotic Language Pattern Everyone Needs to Know ,58,NLP,0
Book Tips? Best book on Milton Model and its usage?,51,NLP,0
I'm not dead and NLP still wont cure cancer ,44,NLP,0
Trying to get a friend to cut down on drinking. Any suggestions? A close friend of mine is developing a drinking problem. I want to use NLP in our daily conversations to get her to cut back. Any suggestions?,207,NLP,0
How to make great decisions. Episode #4 of the international edition of the ABC-NLP Practitioner ,97,NLP,0
The VAKatrak instagram filter! ,31,NLP,0
A Simple Process to Bring More Meaning to Your Life ,52,NLP,0
"Hypnosis &amp; NLP Discord Community Idea Hi,

A few months back I tossed a idea around for a skype group, but I apologize I had COVID pretty bad, and my work/finances needed to come first at the time.

However, I currently have a lot of free time, and so I created a Hypnosis &amp; NLP discord.

The discord is only for practice/chat about NLP &amp; Hypnosis, and it is not for any promotion of products.

This is experimental, so come join and we can make it better together as we double our skill's :)

Invite: [https://discord.gg/2AdYXMFcXg](https://discord.gg/2AdYXMFcXg)",576,NLP,0
"Intro to Self-Actualization | What do you need as a human being? When we think of our ‘needs’ most of us initially think of our 'lower' needs.

Our need food, water, sleep, exercise, money, etc.

But as human beings we also have 'higher' needs.  Our need to actualize our potential, our need to become the best version of ourselves, to express ourselves fully, our need to continually improve....  

These needs are just as real as our need for food and water --- they just operates in a different way.

As a coach, leader and/or someone interested in developing your potential this is an important distinction to understand.

The most famous model of self-actualization is Abraham Maslow’s hierarchy of needs model where at the bottom of the pyramid are our survival needs, above that is security, and then social, self-esteem and at the top of the pyramid are these higher 'self-actualization' needs.

As human beings we start at the bottom of the pyramid at our survival needs and as we gratify those sufficiently we are freed to move up to the next level.

An example that I use to describe this is to imagine that you are under water holding your breath.

In the beginning it is likely that you can really enjoy being under water. You can relax, think about your goals, reflect on your life, and just enjoy yourself.

As time goes on however, as your lower need for oxygen remains ungratified, more and more of your attention gets pulled toward gratifying that need.

Eventually it can reach the point where someone could lose their sense of self, lose their values, and might even consciously or unconsciously drown another individual just to get some air because every single cell in one's body becomes devoted toward that one task — meeting that lower need. 

One of the amazing things about our lower needs is that the second that you get a deep breath of oxygen, air is the last thing on your mind. Almost immediately you are freed up to focus on other things in your life.

The lower needs work in this way. They unlock level upon level as you sufficiently gratify the level below: The less you have, the more you need them. The second you fulfill them, they completely drop out of consciousness.

This is not the same as how our self-actualization needs work. Perhaps surprisingly they work in quite the opposite way.  The more you get them the more you want them!

The more the painter paints, the more they want to paint.

The more the singer sings, the more they want to sing.

The more you express yourself fully the more you want to express yourself fully.

But there is a problem with this...

We are not able to visit in this 'higher' realm, let alone live there, until our lower needs are sufficiently gratified.

Every morning we wake up and we need to eat again. Every night we need to sleep again. Every month we need to pay our bills.

If you try to live at the peak without meeting your lower needs you will eventually burn out or need to divert your path. It is just not sustainable.

It is only when we have systems in place to consistently gratify our lower needs that we are able to free up our inner vitality to really focus on what it is that we are best at - what it is we are meant to do in this world.

Let's take a moment of self-reflection:

* How well are you meeting your lower needs on a consistent basis? Perhaps your focus could be on implementing strategies to meet these lower needs in a systematic way.

* Where would you say that you are currently living in terms of Maslow's pyramid? Are you more focused on survival, security, social, or at the top of the pyramid?   This self-assessment will give you a system for knowing what to prioritize for the next stage of your development.

* At what level are you focusing your vitality at this stage of development and is that aligned with your needs? For example if you are really focused on getting a relationship, but are unable to pay your bills you may want to reconsider your priorities.

In better understanding how our needs work we can gain perspective as to what our sticking points are and we can realign our priorities at each stage of our development to make ongoing progress towards unleashing our highest and best potential as a human being.",4247,NLP,0
Overcome Perfectionism and Move Towards Peak Performance ,57,NLP,0
"Best reading order of Bandler / Grinder for ""biggest change"" -&gt; ""fine-tune"" I'm embarking on the Bandler/Grinder rabbit-hole. I've identified some of their books which are recurrently recommended: _Structure of Magic I &amp; II_, _Patterns of .. Erickson_, _Trance-Formations_, _Persuasion Engineering_, _Reframing_, and _Frogs into Princes_. I've concluded that _Structure of Magic_ and _Patterns_ provide the most bang-for-buck to start (do you agree?). 

My goal is to tackle first the books which provide the most holistic, boulder-moving, powerful change; then move progressively into specific, fine-tuning material. I wonder which of _Structure_ vs _Patterns_ might be a better first-start, with that goal in mind? (Or maybe one of the others?). I've a hunch _Patterns_ is a better first, since it seems to cover more territory; where _Structure_ is fine-tuned on grammatical details.",893,NLP,0
Best book about nlp to start out with What is a good first book about NLP to start out with? Idk amything about nlp and I heard it can help me very much. Thanx,159,NLP,0
How to stop negative inner selftalk. Episode #3 of the ABC-NLP Practitioner ,76,NLP,0
Interview with Mariani Ng | Founder of METAMIND ,48,NLP,0
"[Casual-Chatting-Post] Getting started..., getting into the mood..., getting into the flow... aaaand how I get there TL;DR: Post is about getting back into the groove or starting new habits. The bump we ""have to get over"" when starting out, that feels awkward and how to tackle that. Not a guide, just talking bs about the topic. Coffeebreak-style.

&amp;#x200B;

Multiple endeavors I have are pretty easy and fulfilling once I get going and do it habitually for a few days. Then life gets in the way, I study for so long I forget what sunlights looks like and come out of my one-sided hermit-crab study-lifestyle after a few weeks - dumbfounded and confused.

&amp;#x200B;

* Today is the day I start fitness back up!
* Today is the day I continue my journal and goal-setting excercises!
* Today is the day I restart the meditation habit I had going!

But it feels weird and not at all as smooth as it used to be. Or when starting something new, it's a complex mess and I don't know where to start to get a pleasing, smooth and good result.

**What doesn't work at all** (I've tried) is working around the problem. I spent a good 30minutes thinking ""Ah, maybe I need more inspiration."" watching YouTube Videos on Journaling to get my creative juices going. And yet nothing. The perfect moment to start didn't come.Maybe a 20min. meditation session to get my mind off of things. So the fairy of inspiration can descent from the heavens ... nothing.

**What does work** ... at least the only solution ""to get back into the game"" I have found was to start where I am.

Starting up journaling or written excercises, I screw up the first page. Just writing it without coherent thought out plan or prettiness to it. Messy, unsorted and somewhat uncomfortable. Exactly how I feel towards the subject actually. Then out of that ... a more refined version emerges.

Same with fitness. Dragging my ass along the excercises thinking ""This is dumb and stupid"" evolves into liking it a little bit.

I haven't completly accepted that (it seems) the only way back into the habit/task is through the first messy minutes of it, though.",2119,NLP,0
Can you call yourself a therapist and start your own business with an NLP certificate from Kain Ramsey? ,104,NLP,0
"6 Models to Take Your NLP to the Next Level Want to go beyond the original NLP Models?  Here are 6 Neuro-Semantic models that will greatly expand your capabilities as a NLP Practitioner and Coach.

Model #1: The Meta-States Model - For Unconscious Communication
Beyond the traditional communication model of NLP the Meta-States model opens us to much more accurately track and transform the higher levels of mind, belief systems, inner dialogue, and all of the meta levels of the mind in a systemic and organized manner.

Acknowledged by INLPTA in 1995 as “the most significant contribution to NLP”, Meta-States model give us the ability to track back from someone’s behavior and representations up to the layers and layers of thoughts in the back of the mind that govern that experience all the way back down to our resulting behavior.

Model #2: The Axes of Change Model - For Change &amp; Learning
The Axis of Change incorporates the four key mechanisms for generative change – Motivation, Decision, Creation and Solidification. Not only does it create a structured framework for coaching conversations, but it will make your blind spots as a practitioner blatantly obvious and give you a clear path to expand your personal growth, coaching, and communication skills.

Designed for generative change, the Axes of Change Model results in change that is sustainable without the resistance and relapse features that characterize many other models designed for more remedial change.

This model dramatically transformed the way that I do coaching and is a powerful change model that I use in my sales process as well as to enhance all of my other NLP techniques.

Model #3: The Benchmarking Model – For Measurement &amp; Implementation
By its nature subjective experience is... subjective!

Would you like to learn a model that you can use to make intangible experiences more objective and measurable?

At its core the Neuro-Semantic Benchmarking model is based upon the ability to use the Meta-Model and to de-nominalize terms back down to their sensory based referents – but it takes this process so much farther.

In fact, this art has been refined over and over again in each competency based Meta-Coach Certification Training.

The practice of Neuro-Semantic benchmarking has massively transformed my ability to stretch my global-detail &amp; intuitor-sensor meta-programs with truly life changing consequences.

Applying this model allows us to take our outcomes, values, skills and experiences and make them measurable so we can better track the growth, learning and development of ourselves and those whom we work with.

Model #4:  The Self-Actualization Quadrants - For Self-Actualization
If you know me at all you know that in my opinion — Self-Actualization is the highest frame within which to practice NLP, Coaching, and Neuro-Semantics.  Self-Actualization is the larger frame from in which we can check the ecology of any system or change, and when we focus on unleashing the highest and best of ourselves and our clients everything else falls into place.

Model #5: The Matrix Model – For Systemic Thinking
The Matrix Model is designed as a systems model for thinking and working systemically with people, groups and organizations.  In Neuro-Semantics we use the Matrix Model for working with the multiple layers of meaning frames that create and describe the client’s world so that we can “follow the energy through the system”.

Starting with any external event, the matrix model allows us to track the process of embodying that information, processing that information, metabolizing that information into energy and embodying it into the state from which our behaviors flow out of.

We use the Matrix Model and its 7 matrices as a template for gathering and sorting information, for profiling, modeling, and working with the systemic complexity of human experience.

Model #6: The Facilitation Model – For Facilitating Change
When you coach/communicate with others (or yourself) do you tend to be too soft, empathetic and supportive?

Or perhaps you fall on the other side of the spectrum overly pushing and challenging in your communications.

The Neuro-Semantic Facilitation Model organizes the 7 core skills of Coaching/Communication onto the Meaning and Performance Axes so we can stretch ourselves as facilitators and take ourselves to the next level as agents of change.

Not only that, but it ‘facilitates’ you to find your perfect balance between being ruthlessly challenging and compassionately supportive.

When I first discovered this model I (like many other coaches) tended to be overly supportive in my coaching to the detriment of my clients.  Now I have the permission, capability, and experience of challenging my clients to take action and to stretch which has had dramatic effects in their (as well as my own) performance.

Any questions? Feel free to reach out and I am happy to support!",4925,NLP,0
"Psychopath Pattern (Created By Me) Anchor feelings of hate

Go into your timeline,

&amp;#x200B;

Go To a time when you were in a loving state as a kid and associate

fire off anchor

shoot through entire timeline until you come to present time then future pace

(Warning can create anti social tendancies)",306,NLP,0
"Doing NLP 'on' People vs. 'with' People As a NLP practitioner I was taught step-by-step processes/patterns that I could do ‘on’ somebody.

It was completely normal in that NLP culture for me to say something like “I just did NLP on him” or “She just did NLP on me”, or ""They just NLPed me"".

When I started learning/practicing Neuro-Semantic NLP one of the differences in culture was the shift from the focus on 'the process' to the focus on 'the person' and 'the relationship'.

Rather than practicing NLP 'on’ people, we practiced our NLP 'with’ people.

A slightly different way of framing it with a massive real-world impact in my life.

In terms of my day to day communications and relationships this re-frame supports me in building more authentic rapport with others. 

It supports me in building more genuinely caring relationships.  

It supports me in generating long term, sustainable, win-win relationships personally and professionally. 

This reframe supports me in more healthy and respectful communication patterns and it supports me in treating others as I would like to be treated.

In terms of coaching relationships this way of operating supports me in building collaborative relationships with my clients in service of them and their desired outcomes. 

It supports my clients to feel more empowered to take responsibility for their own results, increases their personal power, and it demonstrates as a role model a much more resourceful way of how to relate with other people.

With over ten years having passed since I shifted this way of thinking for myself I can say from my personal experience that doing NLP 'with people' has been much more powerful for me than doing NLP 'on people'.",1711,NLP,0
"I want to do both Design Human Engineering and Neuro-Hypnotic Repatterning. Which should ideally be done first? Title says it all. Sorry if this isn't the right sub, but I honestly don't know if any other sub fits the bill (since it's Bandler's stuff and all)",259,NLP,0
A Story about Learning How to Learn about Learning ,51,NLP,0
"(Some) coaches, (some) therapists and their ridiculous self-importance ",71,NLP,0
How to overcome adversity and take away worries about the future using visualization exercises. Episode #2 ABC-NLP Practitioner ,128,NLP,0
"Would love to hear your success stories. As the title says, I really want to hear from real people out there who did a 180 of their life by using NLP principles.",161,NLP,0
New Course: Resilience | Instantly Bouncing Back from Whatever Life Brings You ,79,NLP,0
Exercises to deal with congruence in borderline personality ,60,NLP,0
"How do you modify your own meta programs? Meta programs are described [here](https://sourcesofinsight.com/meta-programs-and-intrinsic-values-in-nlp/). I imagine they're difficult to shift considering the amount of impact they have on you and your reality? Even if it's difficult, though, I'd still like to learn how to do this.",327,NLP,0
Utilize Flow states Before Taking Breaks for Enhanced Productivity ,67,NLP,0
"Can you use NLP to not be self concious/shy about body parts? I feel very shy/self concious to be barefoot around others, I dont know why but I cannot be barefoot at anytime unless alone. Is there a way to make myself no longer feel naked when barefoot? To just feel the same way as I do about bare hands? Any links or help anyone can offer? Thank you.",352,NLP,0
A short NLP story ,18,NLP,0
Episode 1 of the online ABC-NLP Practitioner. ABC-NLP is the scientific version of NLP that Joost van der Leij is developing with the VU-university in Amsterdam ,161,NLP,0
The Two Types of Problems We Face as Human Beings ,50,NLP,0
"Book Recommendation Hi! I just started to get into NLP and got the book 'NLP: The essential guide' by Tom Hoobyar, I really like it so far, but I wanted to know if any of you had any recommendation on what to read next? I'm really curious on how to use it to help others.",271,NLP,0
AR filter for NLP eye accessing cues on Instagram ,50,NLP,0
NLP versus the Happy Brain method and Brainscape model ,55,NLP,0
"Is it possible to use NLP on yourself to make it so that you're more focused on the future instead of the past? Title says it all. I'm jealous of people who naturally look forward to the future instead of focusing on the past. One person described it to me as being totally indifferent to the content of their memories because ""it's just a memory"", something that allows them to quickly let go of any thoughts pertaining to said memories. I wonder if NLP would help me?",469,NLP,0
Client Interview with Perry Vessels of Argil Solutions ,55,NLP,0
"How I Built My Coaching Practice Are you starting your coaching practice?   Perhaps you have been coaching for some time now but still aren’t as confident in your skills as you would like to be, even though you are excited to share your tools with the world?

Recently I received a question from a client asking how I built my practice, and how I was able to build trust in myself and my skills.
Below is the first part of my reply...

To begin, I practiced at every opportunity I could. There are some skills where I needed to have someone’s permission and make it more formal of a coaching session, but there were also skills that could be practiced more informally and conversationally. 

For example, whenever I would be buying something I would practice basic rapport, listening, questioning with the person at the register. I would practice shifting the state of somebody sitting next to me on the bus, at the crosswalk or at the cafe. Any opportunity I got I would practice for free.

I asked anyone and everyone if I could practice X technique with them or do Y process to help them achieve an outcome.  

A basic script of what I would ask people was something along the lines of:
“Hi! I have been training as a coach and was wondering if you would be open to experiencing a 25 minute session with me. Worst case scenario things are exactly the same but we could potentially help you make a big impact in your life that otherwise wouldn’t have happened.  Would you be open to this?”

Or “Hey, I learned a process that could help you achieve X. Would you be open to experiencing this with me? It only takes 15-20 minutes. Worst case scenario you end up exactly the same but many people share that this has had a profound impact for them.  Would you be open to experience this with me?”

I found that having a time frame was important because you don’t want it to go on forever.  Whether it is 15 minutes, 30 minutes, or an hour, I would invite people to experience free sessions with me and at the end I would ask them for their feedback.

* What did I do that you loved in that session?
* What was the best skill I demonstrated?
* What could I have said, done, or asked that would have taken your experience to the next level?

In fact, after 10 years of experience as a coach I still ask these questions after many of my sessions.  Honest feedback is one of the best ways to increase your competence. And if they share that they enjoyed the session, you can always ask for a testimonial and/or referral. If you sense that they got a lot of value you can also invite them to work with you more formally. 

If you are afraid to ask people out of fear of making mistakes, or have a hard time separating yourself from your behavior these resources may help: 
* Embrace Your Fallibilities: https://www.youtube.com/watch?v=zXYUpGPDoa0
* There is no such thing as failure: https://www.youtube.com/watch?v=VwBbS5VNbrc
* The power of acceptance: https://www.youtube.com/watch?v=bur5JfnshtM
How to overcome perfectionism and take action: https://youtube.com/watch?v=QQdpHLoFrOQ

Feel free to leave your questions and comments below and if you like I can publish more on this topic. Let me know.",3194,NLP,0
"New Here. Some Questions... My first question would be how best to absorb the information in NLP. It's a lot, even going by what's in the NLP For Dummies book. I'm familiar with the hypnosis aspect, but the rest is a lot for me to take in.

Second question... given what little I do know about NLP, I've seen a lot of information and techniques from a man whose material was required reading for a course I was taking. His name is Dr. H. Norman Wright. I'm wondering if anyone has any information on his studying NLP. His model of the brain, advice on building rapport, and even some techniques he shows on reducing stress, seem like they're based on NLP (one of his techniques I recognized as hypnosis), and he recommends hypnotic training like EMDR.

Third question... I've noticed that there are various ways to group together Milton Model and hypnotic techniques phenomenologically, such as yes/no sets and pacing/leading as a following phenomenon. Has anyone else noticed this, and does it make for easier learning?

I look forward to any insight on these questions",1070,NLP,0
"Creating the Space for Your Coaching Clients to Speak Their Truth &amp; Open Up about their Real Issues I was recently asked by a client, “How do I build the rapport, safety and vulnerability in my coaching so a client can feel safe to speak their truths?”.

Beyond the basic NLP rapport techniques there are many coaching skills &amp; states that support this process which is so crucial to being effective as a coach.

AWARENESS OF THE CONTEXT My first consideration would be the context with which the person came to you. Did they come by choice? Were they forced to work with you? Did they come to you tentatively or with desire? Did they invest money to have this meeting? All of these factors will help determine the level of ‘rapport’ right out of the gate. The more desiring, willing, and invested they are as a general rule the more open they will be to share more deeply.

DEVELOPING YOUR SKILLS OF LISTENING AND SUPPORTING Most people are not aware of their ‘real’ problems’ or ‘real goals’ — or at least they aren’t very clear on how to communicate them. So they will test you with smaller problems/pseudo problems and see how you respond before bringing the ‘real’/’deeper’ issues to the table. How do you handle these?

Do you default quickly to giving advice? Or do you actively &amp; non-judgmentally listen, ask exploratory questions, acknowledge what you are hearing, check that you are understanding correctly, summarize what you have heard, etc. Do you separate out pseudo problems &amp; symptoms from the real issues and hear what isn’t being stated directly?

Do you call them out when you calibrate that they are being inauthentic, shallow, or playing it safe and invite them to get real? These skills will allow them to feel safe, supported, understood and build trust in your competence &amp; caring so they will be more comfortable to share their deeper, more meaningful issues.

DEVELOPING YOUR DEEP QUESTIONING SKILLS Ask deeper questions that pull them one level deeper than they have shared and see if you have resistance or if they follow your lead. In many contexts it is our job as a ‘coach’ to invite the deeper sharing rather than to ‘wait for it’.

DEVELOPING YOUR SKILL OF FRAMING Setting frames up front can help accelerate this process. Let them know that your conversations are confidential (because most of your clients refer their friends and colleagues to you or whatever reason is authentic to you). Let them know up front that you’re going to ask them deeper questions to get to the heart of what is most important. Let them know up front that the more real they are, the more real/authentic their results will be, etc.

What frames can you set at the beginning of your sessions/meetings that will allow them to feel more comfortable/help resolve some resistance they may be feeling to authentically share?

DEVELOPING YOUR COACHING STATE Lastly for now, how well do you unconditionally accept your clients? How well can you appreciate them as people and even appreciate their ‘problems’/parts of themselves that even they don’t appreciate about themselves yet? How much can you see them as a unique and sacred individual on a growth journey? How much do you put yourself aside and so they can sense it is all about them rather than about you, your ego, or you getting results. And again, how well can you ‘just listen’ and explore without the need to change anything. All of these things will support them in feeling safe and supported and accelerate the process of opening up. (All of these states can be cultivated more deeply in yourself with the tools in our State Elicitation course).

All of these skills and states will support you in your coaching to create the safe space your clients need to open up and discover the real and significant issues in their life that will create meaningful change to make your work so valuable they can’t help but actively promote and praise you and your work.",3946,NLP,0
A rant about NLPers &amp; life coaches and their attitudes to COVID19 ,70,NLP,0
"Active listening &amp; Rapport - any recommendations for books/videos on the topic? I’m currently putting alot of effort into bettering my active listening and rapport skills.

My repertoire is rather poor, though.
Currently I have Para-phrasing and mirroring bodylanguage 
It feels rather awkward and faked, but I get positive results so  ¯\_(ツ)_/¯

The best results I have with the Erickson Approach of self-hypnotizing myself and then talking in that state

Any idea on good books, links or videos to check out?",514,NLP,0
"What is your modality? https://strawpoll.com/51j1c111q

Taking a quick survey on which modality is the most common.

This is not necessarily how you learn, rather how you think about information.

If you plan to go on a date tomorrow with someone do you imagine the conversations you'll have (Auditory) how the restaurant and your date will look (Visual) or how you will *feel* during the date (Kinesthetic)?

While everyone uses a blend of these differing methods and no one is 100% one or 0% anther, please only select the option you are *most* comfortable with or find yourself considering the most situations with in your day to day life.

Here's the link again https://strawpoll.com/51j1c111q thanks for your time. Feel free to comment about your experiences thinking in your own head below.",796,NLP,0
"The Perform, Reflect, Prepare Loop ",35,NLP,0
"Can somebody learn NLP from home with free resources ? I'm a 24 year old male, who wishes to cure his ADHD naturally, without medication. I tried affirmations like I am focused, I am hardworking and so on. 
They work very conditionally and temporarily. My HR manager who had been suffering with dyslexia for years asked me to give NLP a shot as psychiatric medication do more harm than good. This gentle man is a highly energetic positive person who is excellent at his job. Just like Tony The nlp coach.

At the moment due to covid I cannot learn this in person, since the covid situation is very bad in my place. Can I learn NLP fully from home ? My goal is not just to cure my ADHD but also be very good at my job and develop people's skills.",745,NLP,0
Optimistic Nihilism: The inherent meaninglessness of life doesn't have to lead to despair. It can be freeing once you know the blank canvas is there for you to apply your meaning to: ,183,NLP,0
"A fascinating podcast with more details of how NLP was used by Nancy Salzman to coercively-control NXIVM members; featuring Susan Dones  Check out [this podcast](https://www.alittlebitculty.com/episodes/ep6)  with a member of the original NXIVM 9, Susan Dones, who discusses how  Nancy Salzman used NLP to coercively-control NXIVM members and also  describes a Red Flag to potential cult recruits with regard to NLP and  hypnosis use in Cults. Hope this helps!",460,NLP,0
The Best Leaders Keep their Authority Here... ,46,NLP,0
"Science What studies back up the use of imbedded commands?

For example the set of sentences:

 ""What do you think of adopting someone else's thinking process because one finds their own less worthy of less smart? Is it simply the logical thing to do?""


It seems like this kind of thing would be pretty testable.

(Edit clarity)",329,NLP,0
"Prac and Master Prac with ABNLP... Trainer with ITANLP? Hi, 

Just wondered if anyone knew if this was a possible route to NLP trainer?  

I was certified with ABNLP a few years ago and the trainer I had sadly passed away. Rather than try to find a different trainer to do trainer's training with, I'd rather do the training with John Grinder &amp; co if possible. 

Thank you.",377,NLP,0
How to effectively use sensory acuity in NLP | Learn N' Grow | Coach Me ,72,NLP,0
NLP for health In my culture masturbation is considered as sin and it is believed that it affects health badly. Well I am 22 and been doing it for almost 10 years. In my subconscious I still have these beleifs as it is sinful and bad for health. Medical sciences say it doesn't affecy health. I am skinny. Is it because of these beliefs? How can I reframe these beleifs?,370,NLP,0
NLP Surface &amp; Deep Structure - improved version ,52,NLP,0
"[Discussion] Working at old problems vs. building something new So me and my buddy have the following opposing views:

His:

""At the beginning you are an uninhibited lightbulb of childlike motivation, curiosity and pure energy. Then more or less layers of rules, habits and trauma are put on top. Dimming the level of energy, flexibility of actions and range of emotions. Peeling back those layers of indoctrination, bad habits and unsolved traumata to reveal the uninhibited you is the way to go. Once your free of the old ways, new ways form automatically.""  


vs.

  
Mine:

""You take your current level of extra time and energy, however little it may be and invest it into a new behaviour or thought. You are somewhat aware of what works against you and somewhat aware what does work for you. So you keep repeating what works for you, cause that will make you happier and more energetic. Being happier and more energetic you tend to make even more life affirming decisions. Building up a new way of life like a castle. Somehow forgetting the old behaviours and coping mechanisms you used to have, by finding new and better ways to live life.""  


Condendsed down it's ""Build the new, old gets forgotten on the way"" vs. ""Clean the old, the new comes automatically""  


All in all we both come to the same conclusion. By raising your well-being, life gets better. We just differ on the way to raise that well-being.

Paradoxically enough, he tends to implement new behaviours through discipline really easy, yet they have seldom much effect (think Jojo-Diet). While I can take his approach of ""cleaning up the past"" with ease and talk about traumatic events in such a positive manner, it's reworked into assets instead of hindrances, yet I see no benefit.

What are your thoughts from a personal or an NLP-Perspective?  
Digging, taking what's there, creating completly new, ...?",1882,NLP,0
"Instant rapport Hi,

What's the quickest and easiest way to establish rapport when you first meet someone? Most of my 1st time interactions are on the phone. TIA",161,NLP,0
Patterns are Bio-chemical in Nature! ,37,NLP,0
How to overcome your fear using NLP tools | Learn N Grow | Coach Me ,68,NLP,0
The Power of Disidentification ,31,NLP,0
"More Lofty Thoughts. Andrew T. Austin and Dov Meidan discuss NLP, timelines and perceptual positions ",101,NLP,0
"anyone done an NLP Practitioner training? please let us know your experience, positive or negative. esp if it's from a big name like Bandler, but any training is worth hearing about. please include the country.",210,NLP,0
"As If frames, NLP tools, and techniques ",40,NLP,0
"Changing core beliefs - NLP Hi all

I am after some advice. I get Anxiety over certain social situations (ie saying no to people that ask me to hang out - or when I feel its left to me to organise something - esp when I dont want to)...  How do I change this core belief in relation to an event or anticipated event? 

I never used to worry; or think about this.

Is a NLP practitioner/hypnotist the way to go?",410,NLP,0
NLP is NOT Psychotherapy ,25,NLP,0
"Question about learning NLP remotely I've visited several other threads in this sub asking for reviews on Udemy courses and those you you who have been in this for a long time have unanimously mentioned that this is not the best option.

However due to the current situation, and a learner's limit of time and money, what is the best way? Will it make sense to combine Udemy courses with Books/audio tapes etc? Are there any trainers who are specifically great in online mediums? Please be brutally honest in case you think someone can't learn it remotely at all.",563,NLP,0
"Before NXIVM there was another NLP-derived cult, Tony Quinn's EDUCO ",68,NLP,0
Meta program summaries! ,24,NLP,0
"Udemy course by Graham Nicholls Has anyone taken this course by Graham Nicholls? Have you found it worthy?

https://www.udemy.com/course/nlp-practitioner-master-practitioner-certification-course/",195,NLP,0
Where Do You Place Your Authority? NLP Internal vs. external authority ,71,NLP,0
"NLP Marin's Transformational NLP vs Regular NLP? Hi NLP sub-reddit –

NLP Marin markets their proprietary Transformational NLP is an improved, advanced, deeper, more impactful, more \[insert positive adjective here\], version of NLP. Few questions for the subreddit:

* Has anyone taken a traditional NLP program and the NLP Marin curriculum that can speak to the differences? 
* Can anyone can weigh in on the validity of Transformational NLP's purported improvements over NLP?",478,NLP,0
"Uncut session from the world of Faster EFT (if you're NLP based you'll get these suggestions but tapping might be new for you). About the content, there's heroin use, jail time, he attempted killing his father, the mother of his children died, incredibly painful stuff. Rapid ins &amp; outs of memories. ",304,NLP,0
"Anthony Robbins and NLP I read about NLP from Anthony Robbins and want to give it a go.
Just curious why does NLP seem to be tied with people that do hypnosis? I've done hypnosis sessions before and have some great downloads - but more interested in the NLP side. Do they go hand in hand? Anthony Robbins doesn't hypnotize his clients?
Appreciate any advice",357,NLP,0
"Practice Wanted to put it out there! Any new or experienced practitioners in NLP/Hypnosis to see if anyone would be interested in sharpening up their skills or help to impart their knowledge in some practice sessions where we can do a few exercises and test out a few interventions. It would be great for feedback, making a few tweaks where we can all really help each other out. I don't see how it could hurt! 

I currently want to find a group of guys or gals who are interested in partnering up to do some practice runs, the sky is the limit, it's all in the name of some fun and learning. 

I have a Hypnosis and NLP practitioner certificate, going for my master NLP practitioner certificate now and just looking to keep getting better and better and open to working with a few folks who are interested in meeting maybe a few times a month to get some practice in. 

Let me know what you guys think, and we can make it happen!",930,NLP,0
"ADHD/BiPolar Just curious to what everyone thinks here. Been off the meds for closer to 15 years total now. My mood has calmed down but I seem soo decidy. What I mean by that is originally I'm offered a job or a volunteer position I say yes originally and then I realize afterwards that was a bad decision and beat myself up over it. Is this a psychotherapy/NLP thing or a medicational thing? One day I seem a little bit down and depressed and the next I seem fine. I eat my feelings in the day usually too. I ended up with leaky gut over it too soo when I get these maniac episodes I tend to lean towards eating out which will just continue to make me sicker in the long run. Little bits and pieces of information in regards to whether medication and/or NLP or Psyotherapy in possibly a Naturopathic way would be appreciated.

Thanks,

Josh",841,NLP,0
"Talking about thoughts - Things you do in your head which sound crazy to non-NLP-folks So normally people I meet daily usually don't talk much about what they do in their head. Explaining the whole concept of sub-modalities to a non-NLP-indoctrinated leaves me struggling to find good explanations or just doesn't blow my audience out of the water.  
Some may have heard the Bandler story about Nikolai Tesla (scientist) and how he built an exact copy of his lab inside his head. Visualizing experiments during train rides, to later test them in his real workshops. His imaginary workshop coming rather close to the real thing.  


So this is what I wanna talk about. Being intrigued what you have to share.  


I'll give an example of something I can do, but have no idea if it is special or not, since I never told anyone.

I record voices with an eery accuracy in my head. Being able to play whole conversations or snippets that caught my interest back like a tape-recorder.   
It's normal memory so it quickly fades if it's non-impactful, but as memory goes can be excercised.

Also I can hear someones voice and basically let them say anything I want in my head, while also changing tonality.  
I have no idea what to do with this or how I could use this to improve myself, but it is kinda cool.  
So far I just dabble around and let voices from people I admire, love or like sometimes tell me I did good or that I look cute.",1430,NLP,0
"Creating a Better Representation With My Stomach and Chest to Relieve Claustrophobia I've used NLP and Tapping EFT to reduce my feelings of claustrophobia when I have allergy related asthma. I tend to identify my fear and emotional tension in my chest and upper stomach area. I've been looking for a technique or ""recipe"" to change the association I have to congestion and tension in those areas. 

Has anyone used a particular technique on claustrophobia as it relates to tension in the chest or stomach area? Ideas would be much appreciated.",543,NLP,0
"NLP and existential angst How do you see NLP practices capable of dealing with  the fear of dying and the possible abyss of nothing that awaits after someone passes away. I know it should help to have a vision/purpose for your life and preferably meaningful and satisfying relationships. 

I see myself as being lucky to know what motivates me and that I have intense meaningful relations with Friends, family and my partner. I still dread the idea of dying and the possible uselessness of my thoughts, feelings and personal development. 

I am very interested in your views..",576,NLP,0
"Is it all just about ourselves? Good day! I am no expert, I have so much to learn. The more life experiences I have the more I realize this NLP thing... well, I think it's more about ourselves. It's about what we do and how we respond to people. It's about being mindful at all times. It's about being able to step outside of ourselves and be a spectator to our interactions. What are your thoughts? I want to get better at NLP.",428,NLP,0
"Need help I was just wondering lately, can we put someone to trance under a minute using some conversation tactics.
Just curious 

Edit : I just delete an emoji",160,NLP,0
"NLP Pattern - Drug of Choice - Your experience? Hi,
in NLP is a pattern / hypnotic induction which is called Drug of Choice which should make it possible to reexperience every mental state produced by any substance. Who has tried it? 
I tested it and with my first attempt I had not much luck.
I used this description of technique:
http://www.altfeld.com/blog/drug-choice-technique-feeling-good-without-drug
Is anybody using this (or similiar) technique successful?",465,NLP,0
What is the link between NLP and Magick? ,41,NLP,0
"The stupid thing I do - if (Stress) {Smoke a cigarette} I reduced my smoking to about 9 cigarettes a week and also reduced the type of cigarette to the ""lightest""-Version i could find.Currently I try to switch to Sisha or e-Cigarette as well. Plus I sucessfully start to hate how cigarettes taste and make me feel afterwards, yet they are a very effective short-term tool for changing state. (Exchanging frustration state for feeling high blood pressure. Effective does not mean smart)

If I'm not stressed, I usually don't smoke at all. Altough studying and exam-phase, while juggling a project ... let's me be stressed very easily.

So while I shout at my study-buddy that sin(x)/x = 1 is a rule and his sin(x)/x=0 is bullshit the tension runs high and my benelovent mind comes up with my practiced solutions:

a) smoke a cigarette

b) take a nap

Alternatives just don't come to mind.

So far I've tried nothing and I'm all out of ideas. /jk

Joke aside, the problem is that in that moment of stress and frustration I can't remember other alternatives (like going for a jog)

Ideas?",1085,NLP,0
Is there a specific bias or heuristic that leads people to misread this? ,73,NLP,0
How to Avoid Burnout ,21,NLP,0
On the trail of the anchors Hey everyone. What feelings have you already successfully anchored with others? And how did you do it? Looking for ideas and inspiration as a beginner in NLP,185,NLP,0
"How do you start out your day? I've trying to build a morning routine that gets me ""up to speed"" so to say. Currently I drink my coffee write a bit of journal and get up to speed on my social interaction via WhatsApp Desktop-App.  
A list of ALL of my ToDos - including social, entertainment, holiday plans, career goals, skills to learn - to browse through is really helpfull to get the inner void of ""What am I gonna do today?"" going.  
I've found the juicier, prettier and more visually pleasing the lists are, the easier it is to get inspiriation going. (Altough finding inspiration can be a huge time sink and an ever growing ToDo-List while nothing gets crossed of is a trap by itself \* \*cough\* \* reddit save post/picture \* \*cough\* \*)  


Also a morning coffee.  
I should (we all know what that word means in NLP) try to implement some body based routine like stretching. My flesh vehicle gets neglected often enough.  


What are you working on/ has worked for you?",981,NLP,0
Interview with Madeline Robinson | NLP Trainer &amp; Coach ,59,NLP,0
NLP Techniques for Intensifying Your Positive Emotional States ,63,NLP,0
"Thoughts on on my self-NLP challenges.      I was certified as a practitioner in June of 2020 and I decided to take some time to work on myself before I start working with clients.  Despite my best efforts I have had a hard time making changes stick.  Frankly, I have reverted on 99% of the change work I have done.  The only thing I have made stick was silly.  I renewed my taste for an album that I played out more than twenty years ago by mapping it across to something new and exciting.  (Pretty Hate Machine by Nine Inch Nails for those who are curious.)  As a result I finally got around to playing with something that my trainer called tasking.
      I haven't seen this discussed anywhere so I'm posting it here in hopes that I  can help someone to help themselves.  It was taught to me that getting the client where they want to be while in the session is on the practitioner but once the client is back out in the world the responsibility for staying there is on them.  This is where tasking comes in.  Tasking is assigning the client (of in this case, yourself) homework designed to keep them in the effective mindset that was created during the session.  In my personal case, I have a list of statements of intent on my closet door that I read multiple times per day with the intent of taking the meaning of the statement into my current state.  It's kind of like an affirmation however it works better than affirmations alone.  In this case, I have created a change with NLP and am using the statement of intent to reaffirm the change that I made rather than trying to convince myself of something by repetition alone.
     The second thing that I want to stress is for those who lean more toward the kinesthetic end of things.  When I talk about the statements on my closet door I mean that I write them in a way that I will feel when I read them.  Then, As I read the statements of intent I take a second to really dwell in how the intent of the statement feels.  Over a short time you will be better able to sustain the feeling for longer and longer.
     Third, even though it is taught that when mapping across, you map across the visual submodalities don't be afraid to try mapping across auditory and kinesthetic also.  Mapping across kinesthetic submodalities is how I got myself to enjoy Pretty Hate Machine again.",2336,NLP,0
"Since this subreddit is dead, can you recommend others? Preferably smaller subreddits that aren't overrun like r/selfhelp  


What are your favorites?",150,NLP,0
Client Story: Applying NLP in Real-Time Events of 'Trauma' ,59,NLP,0
"Loneliness - What triggers makes us actually think our social needs are not satisfied and how to change all of it (Discussion) Inspiration was this video from kurzgesagt

[https://youtu.be/n3Xv\_g3g-mA](https://youtu.be/n3Xv_g3g-mA)

Short-Synopsis: ""Like hunger, loneliness is a indicating pain that some need is not met.""

Then it gets into how loneliness can warp the individuals perception of social interactions towards ""She didn't like me""/""I don't belong""

&amp;#x200B;

So given the mighty sword of ""Your brain can't differentiate between real experience and thought"" that's on the core of many NLP-Techniques, is it possible to satisfy our inner hunger for social acceptance through NLP?

I've been toying around with this a lot, cause (I'm crazy) ... my social skills are not the best. So I have built many behaviors around this.

\_\_\_\_\_\_

Structuring the inner world towards ""Nobody likes me"" - while searching for proof in the real world to justify that belief - is described in the aforementioned video.

Which is easy to do, since lots of social interactions can have something that can be misinterpreted and dwelled upon. Very easy to have resistance towards something and then gnaw on that one remark - that sounded like something your dad said to you when you were little - for the rest of the evening.

\_\_\_\_\_\_

Restructuring said world towards something more healthy like ""Some people like me, some love me and a few find me attractive"" is a very doable endeavor and affirmation that triggers happiness in me. (Worked a lot on this, so I can swallow it somewhat without resistance. Tweaking the audio-submodalities towards a believable, pleasant voice was good here)

\_\_\_\_\_\_

The questions would be:

To which degree should we built our inner world of thoughts towards something that pleases us vs. changing the reality for real?

What would be the best way of changing our inner world towards the better?

Is one ever satisfied with their social interactions and feels constantly like ""they belong"" and ""are loved"" or is one never to be satisfied no matter how much external validation is given?

&amp;#x200B;

&amp;#x200B;

\_\_\_\_\_\_ 

&amp;#x200B;

Meta-Commentary

    After this the post becomes more of a rant. The top part is the structured questions and talking topics I wanted to share.

&amp;#x200B;

&amp;#x200B;

My story here:

Thinking of loneliness as motivating social pain. Without it I wouldn't think much about widening or maintaining my social circle, since for me it's sometimes more work than fun. While I'm socially able and people want to hang out with me, it seldom gives me much. Which is also the main thing girlfriends have broken up with me over ""emotionally unavailable, like a robot.""

Makes sense, my dad has 0 friends - my mom likes the drama of people for entertainment and has a gaggle of girlfriends, but keeps them at arms length. They live for their business and only have each other (and their dog, which they adore). So my internal values are skewed towards the same ideals. Or as my first girlfriend described it ""Your family is the coldest I have ever met. Their not dementors in itself, but their presence sure makes the windows freeze over"" I did get two hugs from my mom.

So it's not completely frozen :D

&amp;#x200B;

&amp;#x200B;

Let's say the main hiccup with social interactions that leads to pain is resistance.

The breaking of rapport because the other has shown or said something that doesn't sit right with us.

Let's say my distant aunt goes on a rant about how ""Covid-Masks are bullshit""

I know she throws out this generalization of her beliefs, mainly because she can't just swallow the threat of covid. Covid restrictions destroyed her whole service industry business and she didn't have any income since 12.2019 and probably wont have any income till Summer. She's vicious and the tonality in her statement shows.

I would like to break rapport, cause simplifying it to ""Masks are bullshit"" doesn't sit right with me. I would like to get the statement right and bulldoze over her whole statement with ""the truth"" (my truth). But that doesn't get me anywhere in terms of rapport and social joy.

Leaving myself behind to go into her rabbit hole of frustration and anger would connect me with her, but I get a feeling of cringe and mini-vomit just thinking about it. Not really worth it

While I actually haven't talked to that aunt in 2 years. I have this exact problem in almost of all of my social interactions. Nobody fits perfect and my inability to be flexible, go into their world and be interested into going into their world causes me loneliness.

I have recognized that this is a problem in my life and are looking for ways to change this. Fiddling around with my inner world of thought and beliefs seems like a good start. Especially since forcing myself towards social groups is an ongoing theme in my life, which I dislike most of the time.

The only social situation where I listen like a hawk to what people are saying and are interested is the NLP Groups I managed in my city (Died due to covid :/). Luckily my job will give me enough financial freedom to travel to a bigger city and hopefully get a bigger NLP group going again after Covid subdues (2024 \*fingers crossed\*). So that's my current hope to get a social circle going that I share an interest with.

&amp;#x200B;

&amp;#x200B;

\_\_\_\_\_\_",5419,NLP,0
Better Marketing for Coaches (Posters and Postering) - get more clients with the cheapest methods ,98,NLP,0
Marketing for Coaches (knowing your customer) - brand identity versus personal identity ,88,NLP,0
"Any tips/tricks for distinguishing VKA? I'm talking about visual, auditory, kinesthetic, and I also subscribe to audio-digital as an option there. I've read a lot, but I haven't been able to pick up the knack of distinguishing these types of people and would like to hear from someone with real-world experience. Thanks",319,NLP,0
"Marketing for Coaches (2). How much to charge? (AKA ""fleecing the poor and needy."") ",84,NLP,0
"Marketing for Coaches (1). Status branding, grandiosity and lies. (Maybe try being honest?) ",92,NLP,0
Quick Synopsis of Primary Interests and how to apply this at work and parenthood [Here](https://www.thisspiritualfix.com/podcast/108-this-spiritual-fix-podcast-nlp-1).,167,NLP,0
"NLP Techniques to Cope with Grief, Loss &amp; Death ",52,NLP,0
Client Interview with Rick | Applying NLP in Sales for Closing the Knowing-Doing Gap ,85,NLP,0
"Dealing with narcissist/sadist I deal with a sadistic/narcissistic people (my manager), but I wish I could be a 'Robin Hood' and crush/punish them with psychology. Could you provide me with no-bullshit book/papers recommendations? I want to broadly expand my knowledge on this topic.

@edit
If someone is trying to build rapport is there a technique to either make that rapport work for you, or is there a way to stop someone in doing that?",440,NLP,0
How to Break Yourself Out of Negative Emotions with NLP Break States ,69,NLP,0
"Erotic Kinesthetic Anchors How plausible is it to kinesthetically anchor an erotic response or orgasm without any other techniques? If you squeeze somebody's knee or stroke their arm every time they have an orgasm, how well will that work (if at all), and how long would it take for that to trigger a full-blown orgasm or nearly that level of arousal?  


Are there any other things to strengthen the anchor to that state?",422,NLP,0
The Structure of Resilience | A Neuro-Semantic NLP Perspective ,63,NLP,0
What's with the weird gestures? ,32,NLP,0
A Tip to Better Enjoy the Journey from Where You are to Where You Want to Be ,77,NLP,0
"Skype/Discord Group Idea Hi,

I'd like to create a practice group where we can all talk about specific NLP/Hypnosis topics, and even practice with each other. 

I am not sure if we have a active discord here, but i'll go ahead and create a skype group for now, so go ahead and post if you'd be interested in joining, and we can get a nice practice group going.",360,NLP,0
"Book Suggestions I have heard of NLP before and understand the basic concept of it. So I am looking for a good book on Audible that will help me understand it a bit more fully and even give me something actable that I can use in my life. 

Also as a side note I was thinking of building my own mantras using NLP. Like different ones for different needs come up. Does anyone have any thoughts on the the subject?

Thanks for any help I get.",439,NLP,0
"NLP, Neuro-Semantics &amp; the Meta-States Model | Higher Governs Lower ",72,NLP,0
"Where's the advertising sticky post? (mentioned in the rules) Yay, for making this subreddit more active! I'm in :)   


Rules mention a sticky post for advertising - those can be good for luring in practitioners who wanna post their websites or books or something like that. Is the previous one old and achieved? Would it make sense to have a new one? And/or a sticky thread for youtube links, to guide the subreddit towards conversations and away from link dumps?",465,NLP,0
Interview with Ricard Olsson | NLP Trainer &amp; Coach in Sweden ,65,NLP,0
"How To Ask Questions For Sales/Coaching To Make The Other Person Emotional or Go Beyond Surface Level Answers? Hello there,  


I am new to NLP/Reddit. I am studying NLP to understand human behaviour as well as help people through coaching.  


I have read one book about the topic, however, I sometimes struggle to apply it in communication / coaching a friend since there are so much ""strategies"" in NLP. In 2018 I had a coaching/sales call with a personal development/NLP guru and it got me very emotional, I don't know what kind of techniques he used but it gave me so much relief.  


If someone can tell me how to apply it to a real-world situations or give me some practical steps, that would be very appreciated.",720,NLP,0
"NLP and Anchoring Resourceful States Internally - Books/Best Ways? Hey all,  
  
After a series of panic attacks over the weekend because of noisy neighbors(which led my heart to physically hurt while I tried to sleep), I need some help on triggering a resourceful state. There's a number of situations where I can see myself needing this:  
  
* After work (when I'm burnt out from doing sales all day)  
   
* When I'm getting in the zone to run my small businesses, where I require some extra motivation to do a variety of work activities(speaking with suppliers, cold-calling customers, admin. work, tech. support, etc.)  
  
* When I need to be assertive, deal with confrontational tasks, or things that provoke my anxiety  
  
* When I'm going through a ""destructive"" state, like the neighbors making too much noise but can't ask them to turn it down any lower, or when I'm feeling helpless and repeatedly more angry (as mentioned) 

As you can tell all of this stuff is related to me internally. I'm not looking to guide others to entering a more resourceful state, I really just want to command it within myself, so I can better tackle obstacles of my life.   
      
Keep in mind that, having been in the sales industry for about a decade of my life already, I probably already have a basic background of NLP. I also work with a therapist who has an NLP background and explained some of these things to me, like pattern interrupts.  

* Are there any good books for working on this sort of thing?  
  
* Since I'm looking specifically to work on things internally, do I even need to read any longer books? If not, what are some guides, websites, blogs, videos, etc. that can be just as helpful?  
  
* What are some ways where I can work on triggering a resourceful state?",1781,NLP,0
NLP Time Hack | Which Psychological Time Zone Do You Live In? ,62,NLP,0
"Discovering beliefs and testing them Hello,

I find it quite difficult to pin down what I believe in. When I try, I become doubtful and ambivalent. 

I would like to test my beliefs and explore why I have them.

I’d be grateful to learn of any techniques for drawing out beliefs. I understand the structure of beliefs, as per Sleight of Mouth.

Regards,

Andy",359,NLP,0
"Not caring what others think Hi,

This is a life long issue for me...caring too much what people think about me. 

Has anyone body else had this issue and what was your methodology in dissolving it?

When I think about it, it shouldn't be about getting to a place where I don't care whatsoever about what other people think (sociopath) - ""over-caring"", in the same way Bandler uses the term ""over-hesitating"" for people who never get stuff done. Rather it should be about getting to a place where i don't care enough for it stop me from doing what I want to do, expressing what I want to express without fear, and feeling what I want to feel. 

Any insights are appreciated!",674,NLP,0
"State management and submodalities issues Have you ever had an issue of not reacting to any of submodalities? Since I started NLP I had difficulties using submodalities on myself or controlling my state. During the courses, we were practicing some techniques in pairs but my colleagues often had problems trying it out on me, like swish, anchoring, circle of excellence or fast phobia cure. When its about recalling previous events, I just cant get my self into any of the states, positive or negative. I am still trying now and then, to slowly recall the images, smells, sounds or feeling but the more I try, the more it is for me to concetrate and recall any of those. In one moment, I was thinking I have some level of aphantasia. 

Maybe important to mention, there was also some traumatic events in the last few years, like deaths and accidents, but I didnt feel anything or had any phobias/traumas afterwards. 

Any advices? Has anybody tried [trick of the mind?](https://www.youtube.com/watch?v=js_unuMakWc)",1014,NLP,0
"Learn, Watch, Act for Integrating NLP Skills Naturally ",55,NLP,0
NLP old code vs new code ? Does anyone know the main differences between the two ? Which one is a better conduit toward change ? Can anyone recommend a good for new code if it is better than old code ?,201,NLP,0
The Problems with Advice ,25,NLP,0
AMSR Pain Meditation Dark Screen ,33,NLP,0
11 Must-haves for an Intelligent Virtual Assistant Platform in 2021 ,68,NLP,0
The NLP Fast Phobia cure ,25,NLP,0
"Phobias, anaphylaxis and NLP ",29,NLP,0
"Richard Bandler - How to Take Charge of Your Life - Questions I've just finished the audiobook. I don't know if this is normal but I don't seem to make images or create feelings that are strong enough to spin. I have a vague notion of these things. I'm not an excitable person, I'm very downbeat and I never overreact to anything. So I don't have a kind of thread to start pulling on or bold images to turn black and white.

Yes I've had bad and good things in my life, but when I recall them, they're all just vague. I'd imagine someone where has heard someone else describe their experience the same. If so what did you say or do to help them get a foothold, to allow them to start using what seem like useful techniques.",723,NLP,0
NLPers and their messiah complexes ,35,NLP,0
The 6 Levels of State Mastery ,30,NLP,0
New Course: NLP Anchoring | Instantly Access Your Desired Emotional States on Command ,86,NLP,0
NLP BACK TO BASICS (Eye Scan Patterns) ,39,NLP,0
Recommended reading ? I’ve watched all Richard Bandler videos and tapes from NLP eternal streaming platform. A lot of good stuff but I felt it was just a toe dip into NLP. A lot of NLP stuff (anything before get the life you want by Richard bandler) I was unable to find since most stuff it out of print. I’ve also read Richards newer book with the exception of teaching excellence and thinking on purpose. I’m looking to learn NLP to apply it to myself and was wondering if anyone has a recommended reading list of books that are available on the market today so that I can learn this field thoroughly. Thanks again !,618,NLP,0
EYE SCAN PATTERNS (Back to Basics) ,35,NLP,0
Reframing ,10,NLP,0
NLP for Losing Weight ,22,NLP,0
Are My Goals Realistic? ,24,NLP,0
FREE NLP Lesson BACK TO BASICS ,31,NLP,0
"Modeling a ""university math professor"" or learning math/physic etc. faster? Hello guys, I am reading ""Unlimited Power"" right now and it is the first NLP-related books - by the looks of it first of many.   
**I am about 1/4 into the book and again i am sorry if I ask the question too early or if it is dumb.**

I have read about the syntax of strategies to model someone, but what about a math professor, or what if you want to be very good at math but have hard time learning even though you love it?   
Is this something that has been modelled? if so are there specific books, videoes, etc. I can look into.",609,NLP,0
Implement New Communication Skills Without Fear ,48,NLP,0
NLPs vocabulary for more specifically understanding the structure of human consciousness ,89,NLP,0
"Munchausen's and Fake Therapists - a discussion on the chapter from my book titled, ""hysterical paralysis"" ",107,NLP,0
"Innervoice type based, or NLP 'based' or NA? Hi everyone, I was wondering based on a post about inner voices if people who don't have them are more inclined to be a specific enneagram type or if they cannot visualize and therefore not be able to do many types of NLP ""tricks"" (sorry couldn't find a better word for it). 

A friend I know says he has no inner voice. ""My voice is my voice?"" Pretty much and he's a type 6w5. Any NLP ""tricks"" I tried with/on him didn't work because he had a hard time visualizing and pretty much couldn't at all. 

So what do you think? Or can someone enlighten me on the subject?",611,NLP,0
"""There are no resistant clients, only inflexible therapists..."" (video) ",72,NLP,0
Interview with Marzuki Mohamed NLP Trainer based in Malaysia ,61,NLP,0
"Technique to manage a specific stressor. Looking for advice on a technique to manage my blood pressure when it's being tested. I don't have high blood pressure until it's being measured. Doctor has agreed that it's not high it's just the testing stressing me.
Any help would be greatly appreciated. Have limited knowledge of NLP.",329,NLP,0
"How do I remember my new habits when I'm angry or frustrated`? When I'm angry or frustrated I usually smoke a cigarette. Not the smartest move, I know.  


Where I'm currently dumbfounded is:  
When I'm angry the only thing I can think of as a relief is smoking. In those situation I am stubborn and dumb and not really open. Thinking is not my strong suit in those situations.  


So I'm trying to find a way to move me towards better habits. Like installing a ""reminder"" beforehand. Or a post-it on the cigarette package?",523,NLP,0
27. Right Man Syndrome (Narcissism) – (Tales from the Rainbow Machine by Andrew T. Austin) ,91,NLP,0
Happy New Year from the International Neuro-Semantic NLP Community! ,68,NLP,0
"Happy Cakeday, r/NLP! Today you're 12 Let's look back at some memorable moments and interesting insights from last year.

**Your top 10 posts:**

* ""[Request to change the name of the sub from NLP to Neuro linguistic programming](https://www.reddit.com/r/NLP/comments/fxaij1)"" by [u/Dankie001](https://www.reddit.com/user/Dankie001)
* ""[This subreddit is NOT about machine learning!](https://www.reddit.com/r/NLP/comments/jdof0k)"" by [u/sordidbear](https://www.reddit.com/user/sordidbear)
* ""[A commenter said “This is why machine will never replace humans”](https://www.reddit.com/r/NLP/comments/ey5lqq)"" by [u/allende1973](https://www.reddit.com/user/allende1973)
* ""[One thing this NLP sub taught me....](https://www.reddit.com/r/NLP/comments/jb2ij3)"" by [u/thefreshbraincompany](https://www.reddit.com/user/thefreshbraincompany)
* ""[High quality NLP ressources available for free for a limited time](https://www.reddit.com/r/NLP/comments/eya1au)"" by [u/mbmuenster](https://www.reddit.com/user/mbmuenster)
* ""[NLP expert needed: Is anyone watching HBO's ""The Vow"" about the NXIVM sex cult?](https://www.reddit.com/r/NLP/comments/iy5m16)"" by [u/Alternative\_Effort](https://www.reddit.com/user/Alternative_Effort)
* ""[NLP Fast Phobia Cure Without Words](https://www.reddit.com/r/NLP/comments/jk4n0w)"" by [u/thefreshbraincompany](https://www.reddit.com/user/thefreshbraincompany)
* ""[NLP Tools To Change Your Mood In Seconds | Heal Your Body Mind &amp; Soul | ...](https://www.reddit.com/r/NLP/comments/jxz04q)"" by [u/BiohakYourBody](https://www.reddit.com/user/BiohakYourBody)
* ""[This is a technique](https://www.reddit.com/r/NLP/comments/hty48h)"" by [u/walkingSideToSide](https://www.reddit.com/user/walkingSideToSide)
* ""[A group for learning NLP](https://www.reddit.com/r/NLP/comments/fh5eov)"" by [u/yasserfathelbab](https://www.reddit.com/user/yasserfathelbab)",1867,NLP,0
26. Are NLPers Scared of Schizophrenia? (Tales from the Rainbow Machine by Andrew T. Austin) ,93,NLP,0
How can I use NLP to forget and stop associating events in the past with current ones? In the past I used to hang out with a group who bullied me alot. Made fun of me and people even told me but I let it go. Eventually I removed them from my life. But each time some one in a group even remotely insults me the situation becomes exactly like what it was with my ex-friends. It sucks. It ruins so many fun moments. Banter is supposed to be fun but by connecting it with the times I was bullied in past makes me hate it.,518,NLP,0
"How to Respond When Your Coaching Client Says ""I Don't Know"" ",61,NLP,0
"NLP, voice hearing and internal dialogue ",41,NLP,0
"Agoraphobia Hello all. I am just learning nlp, and I wonder if are there any tehniques to overcome agoraphobia. I have a friend and his mother didn't leave the house for the last 5 years. 

Sorry for grammar mistakes, I am not fluent in English.",245,NLP,0
"Part 23 of the ""Tales from The Rainbow Machine"" series. This one focuses on the issue of NLPers thinking that they understand neurology. ",137,NLP,0
"""A Lifetime of Services Award to John Grinder and Richard Bandler, or not?"" ",76,NLP,0
A Powerful Tip for Making Better Decisions ,43,NLP,0
The 2 States of Consciousness You Flip Between All Day Long | NLP Uptime... ,76,NLP,0
"What are your go-to motivational techniques? (Here's mine) I'm a big ""once I've started, I can't stop""-guy. But to get over the hump or analyzing why I've got trouble getting over the hump, I usually do this:

\- Chunk down

\- Make a mental picture of the end result and dwell on it for 7 seconds (enhancing it via sub modalities till it gives of a blissfull vibe)

\- Stare at the beast from afar and let the built up associations/inner dialog go through yourself consciously. Just bare through it with a cup of tea in hand. (In this example I look at the basement cellar until my mind starts to sort it out mentally how I could get a handle on things)

\- Usually staying conscious and aware is what does the trick  


\_\_\_\_\_\_\_\_\_Cunk down\_\_\_\_\_\_\_\_\_\_

If I postpone or feel unmotivated:  
Mostly the task is to big in my mind an no clear path is set (Chunks are to big to comprehend/swallow)  
So I'll chunk it down and walk myself trough it on paper  


Example Task: Clean the basement

Internal Dialogue pre-technique:  
*""Jesus, where do I even begin. There so much to do and then the stuff I should sell on ebay"" - downward spiraling into demotivation.*

&amp;#x200B;

1. **Clean basement**

1.1 Take everything out of the room, starting with the one corner

1.2 Look at everything individually and sort it into categories

1.3 Take the easiest to fullfill category and complete it

1.4 Don't go into to much depth with big categories that feel like a rabbits hole. Just sort them aside for now (They have to be chunked down separately)

&amp;#x200B;

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

Most of the times there are good reasons why I feel unmotivated, but they are ""unconcious"" (Ignored and not really looked at eye value)",1777,NLP,0
"Anyone read really good books about NLP? Anyone know some books they read or heard to be good? I’d rather ask people here than paid articles on google.

I prefer practical methods with real examples rather than just theory but I want to know the theory behind too of course. I’m a beginner but I don’t mind advanced stuff",321,NLP,0
The FREE Model for applying NLP - What do you think? ,53,NLP,0
Submodalities and complex equivalencies ,40,NLP,0
How Good are You at Giving Feedback? ,37,NLP,0
NLP's TOTEs applied to OCD ,27,NLP,0
Interview with Rigo Ramirez | A Spiritual Teacher of an Ancient Congo Tr... ,76,NLP,0
An NLP Tip for Daily Interactions ,34,NLP,0
The Neuro-Semantics Crucible for Transformational Change ,57,NLP,0
A Tip for Higher Quality Coaching Clients &amp; Better Results ,63,NLP,0
"Practicing Eye Accessing Cues Alright folks, Who are some of your fevorite celebrities with obvious eye accessing cues?  I'm planning on watching video interviews and to get the mental ball rolling I want to start with people whose cues are super animated.",256,NLP,0
"Anyone have a good recommendation of how to deal with being on tilt? Whether you are playing a game, doing responsibilities, or just engaging in some activities, there are days where your model view of how to interact is insufficient in a way that’s not easy to see.  Like instead of finesse in the right direction, it happens to be in the exact wrong direction.  Emotion also plays a role and causes a sort of loop.  Maybe I don’t need to give an explanation, but you can get in sort of a negative loop, even if things are mostly working, or should be working, that are definitely not working as they should.  Is there anything NLP can offer to stop it, and prevent it?",670,NLP,0
"How do I do more of what I want and less of what I don’t? I used to be really disciplined, but it was mostly due to not having too many distractions. when coronavirus came around and the family is all at home I get side tracked either with the kids at home needed something (obviously I’ll have to prioritize them first) or my parents or brothers needing something. I basically got out of the groove of putting me first and growing as a person. Now I just spend my free time watching movies on Instagram or Facebook and watching meaningless YouTube stuff. I’m In the process on going back to college but I can’t for the life of me get myself to study and read educational stuff (the enjoyment is gone!) like I used to since I’m out of habit due to me being distracted with the family. So my question is how do I make what I’m doing now(tv, Netflix, YouTube, etc.) seem unattractive and make studying and learning more attractive so that I can get my life back on track.

TL;DR- been super unproductive, don’t want to be anymore. Are there any patterns or techniques to increase interest in things I want to pursue but now seem boring?",1134,NLP,0
Beliefs for Sale! How Good Are You at Buying? ,46,NLP,0
"Negative vs positive phrasing Am I right in thinking that saying ""don't worry about a thing..."" is the equivalent of saying ""do worry about a thing...""? Becauae ""don't"" is a contraction of ""do not"" and our brains have a hard time with that, don't they? 
So our brains would only hear the ""do"", right? Or am I just misunderstanding? 

Would a better way be to say ""relax about a thing..."", or whatever? 

Thanks.",411,NLP,0
"Video playlist for ""Tales from the Rainbow Machine"" - new videos added each week. ",82,NLP,0
Do you have a Plan for Your Year or Your Life? ,47,NLP,0
Stop 'Shoulding' on Yourself! ,30,NLP,0
Client interview with Stephan Iskander ,39,NLP,0
What is NLP in less than 60 seconds ,36,NLP,0
I am new here What exactly can i write or post about?,53,NLP,0
If You Want to Be Richer or More Skilled.. Beware! ,51,NLP,0
What if Goals Setting is NOT about Goal Achievement? ,53,NLP,0
"How can we selectively mentally associate/connect one thought with another? Hello, Im new here an was wondering, how can we connect one thought with another selectively?

I recall this is related to the subconscious but looking at it from a very practical perspective.

Allow me to describe what I mean by that question, Sometimes we might be in a place and hear something, the brain/mind will correlate that room or environment with the sound or/and the situation, the next time we revisit that room we recall the incident.

How do we do that in the first place? How would you go about selectively disconnecting and connecting them?

I have noticed this happens when we look at trauma, by observing we start to disconnect the negative emotion from the memory or similar.

I am trying to understand more in detail how this happens and how to willingly make it happen both ways to develop certain good habits and of course help with getting through trauma. Practically and in a relatable way if possible.

While at it, I know memes are somehow related to this dynamic.  
Its also noticed on how we correlate words (them being symbols) with experiences which in turn form a language, I am curious about this too.

Any information will be appreciated.

Thanks",1256,NLP,0
NLP Tools To Change Your Mood In Seconds | Heal Your Body Mind &amp; Soul | ... ,80,NLP,0
"What do you think is it ? Hi, So I am a person who had a difficult(parents ignored my feelings ) childhood. Have been rejected by every single love interest I ever had but I understand that since I wasn't a very interesting or fun person back then. I am glad those didn't work out. 

Now I am 27(f) have a great job, freelance work, I am liberated, I travel and do fun things everyday. I love everything about my life. I helped and still helping other people overcome their things but I still sometimes feel romantically lonely so much so that it makes me cry for 2-3 days now. I want to know what should I do to fix this and what it might be ?

For all the smart people, this is a throwaway account for  questions my friends and I can't ask, so the history won't do you much",775,NLP,0
3 Step Process for Overcoming Overwhelm and Taking Action with NLP ,67,NLP,0
"Drug of Choice and my ADHD Meds I'm waiting on my doctor to return my calls to get more Dextroamphetamine, so for now I am playing around with the Drug of Choice technique with some fairly good results. My only issue will be needing to fire this up far more frequently than taking a pill. Pharmacological intervention definitely helps me more than any other technique, but this is a good far down the stream technique that seems to be working. I actually fell back into it by mistake when describing to a friend how great my meds feel, how they help me, and recalling a day in a coding class when I first started taking meds--- my state shifted right into the medicated state so i decided to formally build upon it.",715,NLP,0
"How do I say this better? “I want more money.” This statement is true (I think) buts leaves me in perpetual ‘want and lack’.

While I’m here, what’s a useful approach to tackle my presupposition that ‘money is bad’?

Edit: ‘e’ to ‘me’",234,NLP,0
"I've started a new youtube videos series based on each of the chapter of ""The Rainbow Machine - Tales From a Neurolinguit's Journal"" (third video goes up on Friday) ",165,NLP,0
A Story of NLP &amp; Intuition ,31,NLP,0
NLP Certification There are alot of NLP trainings out in the world and I'm trying to find which one is the best to get certified under. Who would you recommend I get my certification through?,191,NLP,0
"NLP practitioner in the Maryland eastern shore and DMV area? Looking for an NLP practitioner, I found someone that charges $350 per session in Towson. Is average rate or too much? It feels like too much to me",208,NLP,0
"Can anyone recommend any good NLP books on the subject of “Framing”? By framing I mean the context within which one puts a situation. There can be internal framing, to change one’s own outlook on a situation or external framing, something that is often seen in Politics, law enforcement, legal matters and so forth.",315,NLP,0
"What does COVID mean for you? | NLP, COVID &amp; Personal Responsibility ",73,NLP,0
Complex Equivalences and Submodalities ,39,NLP,0
"Looking for someone to practice NLP/Hyposis with via video-call. Im preferably lookig for someone that knows how to apply hypnosis (a decent operator) or someone interested in interacting live, via video-call.

Im 28 and consider myself a decent operator, a bit rusty because of lack of practice so thats the reason why im looking for someone with which to enter trance simultaneously (Im planning to go into a trance too) and explore inner sensory exprience and so on (the mind), immerse in inner reality, and so on, and learn as we go.

Thanks.",546,NLP,0
NLP Fast Phobia Cure Without Words ,35,NLP,0
"First book I have been looking at titles within NLP and there are a lot to choose from. I was wondering what people suggested as a starting book.
My main area of focus overcoming fear of failure.
Thank you for all the help you can give.",236,NLP,0
Stepping Up to the Challenge of Running Your Own Mind | Neuro-Semantic NLP ,75,NLP,0
"From 7-Day NLP Practitioner Training to Therapist [https://23nlpeople.com/7-day-nlp-practitioner-training/](https://23nlpeople.com/7-day-nlp-practitioner-training/)

""I’ve seen it all too often. They go rushing out of the hall, clutching their 7-day certificate from the 7-day NLP practitioner training in their excitable little hands, eager to go out and change the world. They are also eager to go out and *make a lot of money* whilst changing this world. But how do they plan to do this?

Actually, many are keen to go out and make lots of money by taking on clients with psychological problems to whom they can charge a very high fee. After all, it’s all about knowing where to tap with the hammer, right? .......""",718,NLP,0
A Simple Practice that Will Change Your Life | Meta-Stating Acceptance ,71,NLP,0
"This subreddit is NOT about machine learning! Are you interested in Natural Language Processing? Go to /r/LanguageTechnology.

Are you interested in machine learning applied to understanding language? Go to /r/LanguageTechnology.

Are you interested in Richard Bandler and John Grinder's approach to communication, personal development, and psychotherapy known as *Neuro-Linguistic Programming*? You're in the right place.",422,NLP,0
"Perceptual positions Has anybody played around with Perceptual positions? Specifically in relation to aligning perceptual positions work done by Connirae Andreas. It was interesting work, specifically imagining a relationship between you and another person. And making adjustments to the submodalities. And within the framework of you and that person aligning and 1st 2nd and 3rd positions. So that they are each clean so to speak and noticing how the perception of that relationship changes. Or if anybody has any other comments on interesting use of perceptual positions please feel free to comment. Can elaborate more on her method and my experience with it if anybody is interested.",686,NLP,0
HOW FAST IS HMS Therapy ? ( PTSD WAR Veteran) ,46,NLP,0
Recursive Frame Analysis | One Flew Over The Cuckoo’s Nest ,59,NLP,0
Anchoring - Putting a bit of neuroscience and magick into a good ol’ bit of anchoring. ,87,NLP,0
How do I fix depression that is anchored to me lying in bed? I feel bad around the house but it feels worse when I lie in bed to chill out.  It’s probably anchored to my room too but the bed is the worst.  It’s not really an option to rearrange my room as I don’t have the energy.,280,NLP,0
One thing this NLP sub taught me.... ...is that the \`natural language processing\` people really don't pay attention.,118,NLP,0
Marketing NLP and the problem of the rocketry metaphor ,55,NLP,0
"T.O.T.E.s, OCD &amp; NLP - The Neurobiology of Obsessive Compulsive Disorder (OCD) - ",85,NLP,0
Linguistic Determinism Limits ,30,NLP,0
The Amygdala and When to Swish ,31,NLP,0
"Visual Imagery, NLP and Depression - How to Put Brightness and Colour into Mental Pictures ",91,NLP,0
NLP Modeling and Strategy Elicitation for Master Practitioners INTERVIEW... ,76,NLP,0
Sleight Of Mouth Patterns and Communication Patterns in Psychiatric Settings ,77,NLP,0
"Who regulates NLP in the UK?   

I have cause for concern regarding the toxic side of the self-development industry, its marketing towards children, families with sick children, and children with mental health problems. It’s application for ‘self-development’ that is being targeted towards children, as well as its use to treat medical conditions in children by those with no medical training or education; and alarmingly its encroachment into the school and classroom environments for both.

My hope is that this email will start a conversation with those who are best equipped to have it, so that the right action can be taken to ensure the welfare of all children.

There are a lot of people in this world selling snake oil, “cure all’s”, one in particular is N.L.P. 

N.L.P. is part of the self-development industry which is estimated to be worth globally $15 billion with nearly 6% average yearly gains. To put it lightly, people are making serious money from self-development. The selling of books, seminars online programmes, podcasts, blogs, live speaking events, and coaching are some of the mediums used.

This is potentially problematic because, put simply, people are more profitable to the self-development industry if they are unhappy and discontent, than they would be if they are happy and fulfilled.

In order to successfully market any self-development product, a sense of self dissatisfaction needs to be created in the prospective market or customer. In order to pull them in this is achieved by projecting a feeling of inadequacy. In turn, each piece of self-development material consumed, be it a video, book, seminar, webinar, or 1 on 1 session with a life coach, will give the consumer a sense of wellbeing, a small high. This high is caused by a surge in dopamine levels that rise in the brain.

This is incredibly dangerous because it can create an addictive cycle of behaviour. 

Self-development will make you feel good. There's no doubt about it. There's a feeling of satisfaction that anyone will have after finishing a self-development book, or after attending a seminar or coaching session, because these products are designed to give you a sense of motivation and accomplishment, especially in a group context where group social biases can make people vulnerable to the perceived inadequacies and insecurities upon which they are marketed to resolve. But that feeling of accomplishing something isn’t real because you haven’t actually accomplished anything other than consume a product that is designed to make you feel as though you have. Furthermore, that feeling of motivation, that bubble may burst due to external forces outside a person’s control, leading them to feel personally responsible for not having the success that was marketed to them, because they didn’t put in ‘enough hard work and commitment’ that you were told they needed to put in while their brain was flooded with dopamine. (Imagine this if you are suffering from depression, I’ll come to that later) Due to an increased sense of inadequacy this leaves people needing more and so the cycle of the consumption of more self-development material begins again.

The reason anything becomes addictive is not because of the thing itself. The reason someone becomes addicted to cocaine is not cocaine; it's what the cocaine does to the brain. It’s the rush of dopamine caused by cocaine that gets a user hooked. It’s the rush of dopamine that causes addiction. N.L.P. and other self-development products can cause pathological behaviour cycles by creating a dependency on the feeling of wellbeing triggered by dopamine. It does so by using social context and behaviour to turn people into a consumer of its products by playing inadequacy against success to create a false sense accomplishment and motivation that facilitates a dopamine rush, the high. N.L.P and other forms of self-development stimulate the same neurological and psychological mechanisms as cocaine.

In order to become an N.L.P. life coach or self-development guru I would suggest that you would have fallen foul to this kind of predatory marketing and in turn, in some way become addicted to its products and ideas. 

There are many N.L.P. organisations. I will focus solely on the ones I have seen that are aimed towards children that I believe to be toxic.

Many N.L.P. organisations and self-development gurus market the idea to potential prospective N.L.P. practitioners of being your own boss (are you sick of the 9 to 5?), financial freedom (do you want less stress?), success (do you need people to think highly of you?), all whilst helping children. You can do this after attending a course lasting only 4 days. Some even with one of those days spent on teaching someone how to set up a children’s therapy practice, and how to market their particular brand of N.L.P. as a franchise. Shockingly but not unsurprisingly there are optional add-ons, so after attending and successfully passing your short course, you can pass on your new N.L.P. skills to parents, and teachers.

Some of these N.L.P. and self-development practitioners take the opportunity to present themselves as a guru with promotional video testimonials from their ‘licenced practitioners’ praising them for their support and helping them achieve success, and finish as all new wave internet gurus do with “make the commitment”, “hard work”, “earn money” and “success”, with the offering of monthly private webinars and closed forums of N.L.P. and self-development content and not failing to subtly mention the financial commitment to your new guru to ensure your skills are up to date so you can acquire your next annual franchise licence for your successful N.L.P. business in what, on the face of it, appears in some instances to be a pyramid scheme.

Some of these N.L.P. and self-development organisations seem to blur the distinction between a medical licence and a franchise licence and sell the idea of helping children with medical conditions to potential practitioners who may then go on to think that because they are a ‘licenced N.L.P. practitioner’ and may already be a registered child minder, and have a clean police check; they can treat the symptoms of a number of medical conditions that could have their roots in all kinds of serious conditions that these practitioners would lack the competencies to diagnose.

One video testimonial I have seen, the new practitioner states they felt dissatisfied with the corporate world, so signed up to become a child therapist, and so applied to attend and after a 4 day course was a child therapist. 

In short you can quit your job at McDonald’s on Monday and by Friday be under the illusion that you are a ‘fully registered and licenced’ child therapist.

I struggle to reconcile the notion that it is possible to become a professional child therapist, regardless of whether or not you then operate in schools, but especially if you do, after completing a course lasting less than a week. I further struggle to believe that you can tell people that you are a licenced child therapist after completing one of these courses. I struggle more to believe that people who don’t have a degree in psychology or any medical qualification who are not doctors or professors are allowed to teach people to become a child therapist and give them the illusion that can work with children and in schools. 

It is my belief that this appears to actually be happening.

Schools, while having checks in place may fall short, because their child mentors and class support workers have a clean police check and are registered child minders and when they say they are a ‘licenced practitioner’, do schools realise this isn’t from a recognised medical training scheme and in actual fact issued by someone selling ’self-development’ franchises on the internet in an unregulated industry that the scientific community has dismissed due to its numerous factual errors and its lack of empirical evidence for effectiveness and recognises as pseudoscience. 

When it comes to diagnosing medical conditions, a medically trained professional, a Doctor, will, after many years of study, approach any patient with a degree of scepticism, and after rigorous investigation and the process of elimination rule out what things are definitely not, in order to find out what things may be, before narrowing down a diagnosis and then deciding on the correct course of treatment for a given condition. N.L.P. practitioners after a 3 or 4 day course won’t be able to do this, and would, with an unscientific mind, approach a patient with any given condition with flow chart diagnostics. They will look for certain symptoms that they could potentially treat with N.L.P. Once confirmation bias has falsely validated a condition, they would then apply their N.L.P. techniques to treat what they believe can be treated with N.L.P. 

Should they have accurately diagnosed a condition, blind luck will be interpreted as success.

Anxiety, O.C.D., A.D.H.D., Anger, Depression, , could all have social triggers, but are also symptomatic of other very serious conditions that N.L.P. practitioners would not be able to recognise with flow chart diagnostics let alone effectively diagnose or realise they should even be aware of after attending a 4 day seminar. As such, this could cause significant problems further down the line for a child with autoimmune disease, or perhaps a genetic condition like hemochromatosis, McLeod Neuroacanthocytosis Syndrome, Cushing's Syndrome (the list can go on) or in the worst case some types of Cancer. 

The symptoms of conditions these practitioners are treating could also be early onset schizophrenia. Schizophrenia causes all sorts of social and psychological symptoms because it is fundamentally a language disorder that in part robs the patient of the capacity to distinguish between their internal thoughts and the real world through something called ‘concrete language’. If you apply some one size fits all thinking techniques to a person (a child in this instance) suffering from this form of psychosis, the damage that could be done is catastrophic and potentially puts people, (children), in serious danger.

As a more frightening example, Depression (in its truest most destructive sense) is a genetic neurochemical disorder, that is as biological as diabetes, which through the impact of life experience influences, or perhaps just any way robs people at a biological level of the ability to experience joy. (Consider here what I mentioned above - that sense of personal responsibility for failure) Combine all this with people’s inability to distinguish the difference between being depressed and having depression due to semantic similarities and the social stigmatisms that further impact the psyche of someone who is already withdrawn and you have a cocktail for disaster. You can’t just sit them down and go through some thinking or social behavioural techniques and expect it to improve. This is because Depression, a neurochemical disorder, is in most cases a serotonin deficiency. This is analogous to sitting a diabetic down and telling them to snap out of it and hoping that in so doing their pancreas starts producing insulin. 

Depression is treated with professional therapy in conjunction with prescribed S.S.R.I. medication to trigger the production of serotonin in the brain. N.L.P. and other self-development products release dopamine. Neurochemically speaking this is comparable to treating depression with a cocaine habit. Children will report that they will feel better. Neither these practitioners, children, nor parents would be aware that they are actually not better and have in actual fact substituted a lack of self-worth, with a short term high, just like a ‘fix’. The damage that could be done here could result in suicide. This is because a patient with depression is at their most vulnerable from suicide when they are going through what appears to be a recovery; when everyone around them thinks they are doing well; after a period of psychomotor retardation, as they now have energy to take action on their self-destructiveness and self-harming thoughts.

I would suggest, the positive results that these organisations are reporting for treating Depression (as well as a number of other conditions) are in actual fact the result of a fallacy of composition biased by false validation because N.L.P. makes people ‘feel good’. In some cases children, sick children, who are potentially becoming hooked on the dopamine rush from consuming self-development products from an unregulated industry that has found its way into the classroom. I would further suggest that these statistics are in actual fact an alarm bell that should be seen as the number of children, potentially sick children, who are becoming self-development product addicts.

While the above may be extreme worst case scenarios, of which there could be more. Where more unnoticed damage could be done is in the realm of personality disorders and pathological behaviour.

Brain chemistry plays a huge role in personality disorders. ‘Messenger chemicals’ such as the previously mentioned serotonin are used in the brain to transmit signals between brain cells. Altered levels of serotonin have been linked to depression, aggression and difficulty controlling destructive urges. All of which some N.L.P. organisations wrongly target with their N.L.P. therapy which triggers dopamine.

In addition to this, if a person is overly sensitive to negative emotions and is neurotic, N.L.P. practitioners conditioning a child to avoid circumstances that may trigger negative feelings or reconditioning their thinking about these circumstances with repetitive self-affirmations reinforced with dopamine, could lead the child to develop a pathological narcissistic personality disorder simply because practitioner, parent, teacher or child wouldn’t be able to determine the difference between an angry child feeling better due to dopamine, and creating pathological behaviour models that will become malignant over time.

Are parents made fully aware that their child’s mentor, classroom assistant or even therapist; are offering or passively using potentially addictive pseudoscience from an unregulated industry for personal development? Furthermore do they realise in some cases offering this as a treatment for what they don’t realise is more often than not symptoms of autoimmune diseases or genetic disorder that require proper medical treatment? While these practitioners may have passed a police background check, and who are ‘fully licenced’, do the parents realise that these practitioners in actual fact have no medical training or education? Do parents realise they in fact only have a franchise licence from an unregulated industry issued after a 4 day course by a person; who in one instance that I have seen, who also has no formal medical training or education and holds only a certification in nursery nursing?

N.L.P. used by a psychiatrist with an advanced degree and understanding of psychology I am sure presents no problem what so ever. In fact there are numerous success stories about it positively changing people’s lives all of which are highly commendable but in these instances N.L.P. was applied by a trained any qualified psychotherapist.

N.L.P. in the wrong hands, like someone with no medical background having completed only a 3 or 4 day course is in my opinion reckless and incredibly dangerous. It is then being used to treat children’s medical conditions by people who are not qualified to know what they are doing. As well as being used in the application of self-development by exposing children to a potentially addictive cycle of consumption, that in the short term may make a child feel good, but in the long term may have catastrophic consequences.

The self-development industry could be seen as a community of individuals and entrepreneurs looking to better themselves. These life coaches could be seen as providing value by trying to make people’s lives better. I am sure some of them are. So where’s the harm? This is naive thinking when you're dealing with an unregulated industry that is worth billions that relies upon creating a sense of vulnerability and inadequacy to snare its practitioners that can then potentially be projected onto others to make a profit on potentially addictive products especially when they are marketed to children with medical conditions. In seems to me to be a pyramid scheme glued together by projecting social inadequacy onto the next person, so they can return a sense of affirmation.

It is my belief that there are people in The U.K. and quite likely elsewhere probably globally who have no medical qualifications, using potentially addictive N.L.P. techniques on children in schools. They have no education in psychology, they are not psychiatrists, and some have no medical qualifications. Some have only done a course only a few days long. Some of these people are treating psychological problems in children in schools.

I want to be clear, I am making no accusation of abuse against any individual, or organisation. There is no conspiracy, there is no malice. It is only my opinion that this is something that could potentially be harmful to children being committed naively by those who probably have the best of intentions, who are ignorant in their actions. This in my opinion has been allowed to take place due to an institutional failure that has let the potentially toxic side of an unregulated industry slip through the net and find its way into the classroom.

Please approach this with due diligence and care, children are involved, practitioners may have children, who could be exposed to backlash from sensationalist reactions. 

The self-development industry is unregulated, and that needs to change. Step one, get it out of the classroom.  


Should N.L.P. be regulated?

[View Poll](https://www.reddit.com/poll/j9t5az)",18007,NLP,0
The Unintentional Delivery of Post Hypnotic Suggestion ,55,NLP,0
The science of Innuendos and Double Meanings . The usage of these are sublime to humand mind ,93,NLP,0
Submodalities as expressed in language. ,40,NLP,0
Let go of REVENGE and Let Karma take care of it. (MEDITATION) ,62,NLP,0
"Go grab a coffee and some biscuits. You'll need it for this one: Michael Carroll NLP. NLP. The NLP Leadership Summit, The NLP Academy, New Code NLP, Andrew T Austin, NLP Integrity ",180,NLP,0
A Paradoxical First Step Towards Lasting Change ,48,NLP,0
SELF FORGIVENESS (guided meditation) ,37,NLP,0
Which is that one book which opened up your concept of NLP as a practitioner. Especially for therapeutic purposes. ,115,NLP,0
Mental images How many different images can you hold on your visual mental screen at the same time?,99,NLP,0
"Control my pounding heart I used an NLP technique to overcome a feeling of anxiety, the feeling in my gut is no more. But my heart pounds, can I control it using NLP?",166,NLP,0
Career Fear Release (Guided Meditation) ,40,NLP,0
"Six-Step Reframing Hey everyone.

I want to do the Six-Step Reframing on myself.

But I have a doubt. 

When contacting the ""creative"" part should I just relax and not think on anything, and wait for the signal? 

Or should I rationalize on thinking the new behaviors?

Basicly, what I'm not understanding is :

On step 4 ( **Create new behaviors using the creative part** ) should the mind be ""blank"" with no self-talk or should have self-talk?",445,NLP,0
NLP Hypnotic Language Patterns and Mental Self Defense ,55,NLP,0
Resilience training with NLP trainer Richard Bolstad ,53,NLP,0
"Eeeew = E.U. NLP in british politics The EEC was fine, but then they changed it to the E.U. / EU which sounds like Eeeew! Not specially in scottish or irish, mostly in midlands english... Eeew/ EU... A common expression of something icky. So im thinking the british were hypnotised by the word EU to a small degree, enough to vote in a referendum and win the brexit vote. You wouldnt call a shop the EUshop but a WhoopeeShop would fare a lot better. Am i imagining it? Im convinced the uk were NLP'd by saying Eeew in reference to Europe.",538,NLP,0
"Bootlegging NLP scam. To help get the video noticed, do please click like on the video, leave a comment, and share it to any other content creator who is affected. Google has already actioned half the URLs i submitted within the hour, so is faster than mentioned in the video. ",277,NLP,0
"NLP expert needed: Is anyone watching HBO's ""The Vow"" about the NXIVM sex cult? NXIVM is the sex cult famous for branding women with the leader's initials (along with those of Allison Mack).     The leader is a big believer in NLP, his #2 studied under Bandler.     

Midway through the 5th episode, we see an exchange between the leader Raniere and his girlfriend-follower Kristin Keefe.  In it, Raniere seems to be trying to do some NLP.    He starts with what I can only describe as ""don't think about a white bear""  statement. 

We over at /r/theNXIVMcase would really love expert opinions from NLP practioners.  

* What is he trying to?
* Where did he 'learn' it?  what sources does he seem to be inspired by?",715,NLP,0
The Key to Maintaining Commitment to Your Goals &amp; Resolutions ,66,NLP,0
"That never happened vs I didn't do that. Not really body language but which one is more likely to be a lie

In a scenario where I tell a person you did that (because he said so in the past) but this time he says that never happened. What do you think is true ?

Any other place youd recommend me to ask this question",316,NLP,0
"Class of a Master - worth it? I'm just getting back into Bandler after having been away for about 20 years. Yes, most of his stuff is exaggerated (DHE, cough, cough), but I found value in a lot of the mindest tools and submodailty stuff. Curious if anyone can recommend the Class Of A Master. Thanks for your opinions!",318,NLP,0
Where is your focus? ,21,NLP,0
How Effective Are You as a Coach at Receiving Feedback ,55,NLP,0
"Guys i need your help so much ;( Hi, i have a problem with my approach o nofap. Basically, Every thought about doing this blocks my throat chakra. If you dont believe in chakras, i can also say that - i was doing nofap, because i wasnt condifent enough around people - i made so strong connection in my mind that equals nofap with running from my authenticity.

I tried to think about this in other ways, but Still my brain sees nofap as = i am not condifent. 

How can i change this? Its kinda destructive to me ;(

Do you know some NLP technique for seeing things like that in the other way, like, im confident on basis (because now i am lol) and I Just do it for only having more energy for example? ",703,NLP,0
How might we use the Meta-model when faced with thought-terminating cliches? https://rationalwiki.org/wiki/Thought-terminating_cliché,133,NLP,0
"How do you sort your life and decide on what to work on? (Things to do, Soft-Skills to implement, new techniques to train, emotional hurdles to work through, ...) TL;DR: Metaphorically NLP opens up the inner and outer machinations into processes that can be changed. Given an adult human has a myriad of conceivable behavioral and thought patterns running that could be changed to something better, while simultaneously only having a limited time to change them and skill in NLP to change them (Megalomaniac beginner: ""I've read the first chapter of 'Frogs to princes' let's change my whole personality, it will only take a weekend""). How do you sort/decide what to focus on?

... I guess I said everything in the TL;DR. I'm a bit overwhelmed atm. 3rd Semester Studies, new girlfriend, new side-business, new hobby and some periodical success with cutting time-consuming monsters in my life like Netflix and Social Media.

Dream would be waking up with so much energy that I just blast through my ToDo-Lists. But I wake up rather groggy 5/7 Days of the week. Now balancing energy, motivation and time to becoming that energetic version of myself, while 'balancing the plates' of my worldy duties like making money, being a friend/boyfriend, cooking and studying.

Time is not the issue, energy though seems limited.

&amp;#x200B;

Any thoughts?",1344,NLP,0
Dimensionality reduction techniques ,36,NLP,0
"NLP group in Telegram. This is a community of NLP practitioners/Master practitioners and Hypnotherapists for mutual discussion and support. If you think that you can add value to this community with your contribution along with getting the same from others then do join. 

[https://t.me/joinchat/CXXCih14hZbr4hWZkpJlqw](https://t.me/joinchat/CXXCih14hZbr4hWZkpJlqw)",365,NLP,0
"Critique of NLP?! Hi all, I am very interested in doing a NLP practitioner course, also as an auxiliary to my meditation practice and a way into hypnosis (not with the goal of influencing other people but freeing my mind and supporting others to do the same). 

Of course I have come across various sources stating how NLP is a mere pseudoscience and has not been proven in many scientific studies. To be honest those critiques sound plausible.

I am now just looking for some experiences with NLP, both good and bad. Personal anecdotes, how did it actually change your life and help you overcome phobias, fears or for example anxiety.",635,NLP,0
"Technique to make uncomfortable situation enjoyable? This is work-related, and the discomfort comes mostly from the fact that I have to rather heavily apply my mental faculty while not enjoying the whole process all that much. Visualizing the payoff hasn't helped so far, and I am otherwise interested in this subject and can see myself doing it, even find it somewhat thrilling, and feel like it will get much easier when I've gotten better at it",447,NLP,0
Mapping Across Resources using Neuro-Linguistic Programming (NLP) ,66,NLP,0
"Wanting to see if anyone knows this study. I was going over a Montreal Seminar video of Richard Bandler's and he mentioned there was a 10 year government study that basically showed the more education a therapist had the lower the client success rate was.

I was curious if anyone has heard/ can find that or of any other similar studies that may have been touted off throughout the years.",389,NLP,0
"looking for a good primer about online coaching I imagine, there are many tips and tricks and tools that could be useful if I want to coach people online (via zoom, skype or something similar). And I can imagine some people already wrote down some of their experiences, for example how they work with spatial anchors and creative methods via the internet/webcam. where should I start reading more about it? if possible something NLP related. (and if possible something that covers team coaching as well as coaching individuals.) do you have some good recommendations for me?",574,NLP,0
(new code NLP) - The Many Lies of Michel Carroll ,49,NLP,0
"How to test the accuracy of sentiment classification model with un-labelled, unseen dataset? I am working on sentiment classification in a low-resource language using Weka. My dataset consisted of 300 instances, 150 positive 150 negative. Firstly I trained the machine with this dataset and built a model. Then I tested the accuracy of this model with a labelled testing-set consisting of 50+ and 50- instances. 

But now I want to use my model for practical application, like sentiment classification for an unlabelled dataset e.g a dataset consisting of reviews taken from amazon. How do I do this? 

If it's not possible to test machine with an unlabelled dataset after it has been trained and tested on labelled data then what does the field of Sentiment Classification bring to the table if it cannot be used for real-life applications?

About me: Linguistics undergrad, who is interested in the field of Computational Linguistics. My post might seem stupid to you, forgive me for that but I a noob in CL and ML. I am doing all this research on my own without any guidance.",1078,NLP,0
NLP for sadness I lost money unexpectedly and this saddened me. What does NLP suggest?,86,NLP,0
"What are you building an Image or an Identity? “What are you building, an Image or an Identity?” by Vikram Dhar https://link.medium.com/m2h2JW3qq9",146,NLP,0
Failure to Feedback technique Can anyone simplify the procedure for the failure to feedback technique in NLP?,109,NLP,0
"NLP State Elicitation Applied to Coaching, Communication, Leadership... and Yourself! ",86,NLP,0
"Changing beliefs Good day ! I’ve been dabbling a bit with NLP but I am trying to learn to make serious change. I want to learn how to change my beliefs. I’ve read thinking on purpose by Richard bandler but he only touches upon it briefly and doesn’t really go in depth on how to change beliefs. Can anyone recommend any resources, vids, audio programs to achieve this ?",369,NLP,0
Update on the great big NLP bootlegging scandal. Just what do personal development trainers do all day? ,104,NLP,0
"Best open source NLP technology Hi. I am joining a new multiyear R&amp;D project with following broad objectives:

• Summarizing key content across a range of sources or in a single document
• Capturing document-germane sentiment, assessing the tone, intent, and social content
• Determining the reasons for themed statements
• Identifying relationships among themes
Effectively parsing and combining findings, such as aggregate results by service, occupation,
or other demographics. where possible
•
Accommodating the plethora of nomenclature, jargon, andacronyms

What open source NLP technology would you recommend.",626,NLP,0
"Looking for VAK word lists Hi all,

Does anyone have links to comprehensive VAK word lists?

Thanks",99,NLP,0
How to learn NLP on your own ,29,NLP,0
"META-QUESTIONS, META-MODEL, MILTON MODEL, SEMANTIC REACTIONS &amp; TRANCE...... ",80,NLP,0
"Loosening a Limitation with Advanced Language Patterns Listen to this and deconstruct a limitation with NLP language patterns.

[https://youtu.be/oE3zJu3IXhM](https://youtu.be/oE3zJu3IXhM)",188,NLP,0
The NLP Bootlegging Scandal - the story so far - more to follow... ,67,NLP,0
On the Beach with Alan Whitton and Andrew T. Austin ,52,NLP,0
lurking this subreddit... I fully expect to come by one day for my usual lurking around only to discover that it has become a computer programming subreddit :D,159,NLP,0
NLP and Meta NLP has become a game changer for Coaches ,55,NLP,0
"Is this normal post session... Had my 3rd or 4th NLP session today, and it brought up a lot... although I am happy with what we discussed and my homework, and it made me look at things in a different way, I've come out with my mind spinning and very overwhelmed by how much it brought up... is this normal?",306,NLP,0
"NLP Rapport Techniques Applied to Coaching, Sales &amp; Leadership ",67,NLP,0
"How do you make sub-modality changes stick? I find that it takes a lot of concentration for me to change the sub-modalities of internal imagery. In the moment when I'm really trying I can feel better, however the next time I think of the image it's right back to the original giving me the same feelings etc. Is it a case of repeating the changes many times? If so, how many?  Because I've been trying to scratch out/blow out/whiteout certain memories or thoughts for years without success.",490,NLP,0
FREE LIVE ADVANCED NLP TRAINING - For all levels - https://www.nlpwithintelligence.com/freeNLPworkshop ,103,NLP,0
When Being ‘Authentic’ Becomes an Excuse ,41,NLP,0
"Yesterday I Learned the depths of my programming Just as the header describes.. I learned the depths of my social programming when I couldnt enjoy the rain with my children. 

I have my 3 young ones ranging 14, 11, and 1. They love to play in the rain. In my quest for infinite self. I have started adopting this philosophy. I have ventured out into the rain randomly and stood in it a few times. Doing my best to relax and realize my connection with everything... But yesterday imo was one of the most eye opening experiences so far..

My children were outside playing in the rain. They invited me to come outside to join them. Of course i agreed. However, as the water hit me, I tensed up.. As every drop hit me, i stayed tensed as if i was gonna be in trouble for relaxing. I watched my children play and enjoyed watching them play. When the urge got too great, I would go back under the awning of our house and watch. A couple times the toddler would walk over to me and pull me back out into the rain. To which I followed, stomped in the puddles or let the rain hit me again for a little while. Only to return to the safety of the house after giving in to my subconscious. 

When I realized i was as soaked as i could stand myself to be.. I went inside and dried off.. Both feeling negative for not being able to stay outside longer and for not enjoying being outside with my children. I immediately start probing myself for the source(s) of my feelings and resistance to enjoy myself in such a simple situation. I  finally come to a concluding idea that Bc I dont deserve to enjoy the experience bc im not making the money or in the position I would prefer to be in to ""have fun"" Immediately getting upset with myself for this idea only to come to the realization that i have not yet changed this underlying belief about money. And Its a matter of learning how exactly to access and change this ugly belief i have in order to move up and onward. 

what are some things i can do to get past this?",2001,NLP,0
"Beginners Course In search of a online course to start my NLP certification. If that's even a thing.

Also looking for a list of books to buy.",142,NLP,0
"Alternatives to common phrases I’ve noticed that greetings on private messages can become very dull, so I’m looking for alternatives and perhaps some reading on how different kinds of people communicate depending on education, age, gender.

For example, “hi, how are you?” is pretty standard. Recently, I’ve replaced it with “Hello, I hope this message finds you well?

And then ending informally tends to be “thanks.” It’s a bit dull, so maybe “let’s keep in touch.” Or something similarly open ended.

I look forward to hearing some ideas",540,NLP,0
Inoculate Yourself from Fear of Failure | Presupposition of Neuro-Semant... ,76,NLP,0
"Programming a more relaxed default mode Hi everyone,

I have a question regarding NLP in combination with mindfulness. I wish to learn (and teach) how to get better att staying present and mindful in daily life. Would NLP be an approach to use? And do you have any thoughts on how to go about it?

Grateful for any thoughts!",324,NLP,0
Success stories of NLP for insecurity Looking for some success stories. Thanks xx,81,NLP,0
"Examples of conversational anchoring? As you might know, anchoring is a powerful tool to help people to get in a resourceful state and anchor it!

Can you show some examples so we can broaden our learning field? I’m talking about regular conversations, during a presentation or interview form.",293,NLP,0
"BusinessCoach - Girlfriend is unhappy with her job - Want to do a coaching session with her - looking for ideas and good questions TL;DR Her job is amazing and very secure (Medical Studies), she ""hates"" it, but knows the benefits. i believe she has a bit of ""the grass is greener on the other side"", wanting to do something like a secretary job at a guitar shop or orchestra

**Her Job:**

*Her job - the best I can describe it - is looking through medical studies, foldering them into an everchanging sorting system, taking online-courses how the client want their studies to be sorted.*

*She has no medical degree or real understanding what she's sorting. (Not completly, but it suffices as explanation)*

*Then she hands it of and never hears from the study again. (She factually knows that her work helps or could help people, giving them hope to find a cure. But she usually never finds out if the medical study she worked on bore fruits)*

&amp;#x200B;

**What this post is about:**

We scheduled a ""Lets sit down and talk everything out"" for Saturday.I've noticed there are a lot of ""inner parts"" that built up animosity towards her job. Here are some I've isolated:

1. I don't see what I'm doing - (lacking the bigger picture or meaning)
2. The rules are always changing - Its frustrating - I need a job I can grasp fully
3. I feel like I'm not good at it
4. Maybe I'm not meant for it"" pointing towards her ADHD-like personality

She gets happy thinking about quitting her job, but knows a new job wouldn't hold up her current lifestyle money wise (Nice Apartment, money to spoil her cats) and ""It's a job you can get old in or get pregnant without a care""

She has a 4 day job and works as a guitarist for weddings on the side (which fulfills her, but she sorta knows doing that fulltime would mean stress)

&amp;#x200B;

**What kinda worked so far:**

""So the best would be having the same kind of money, but a simple job that has something to do with what you like .....\*looking for reaction\* .... or not working at all and still get money \*laughing and relief\*""

""Haha, yes the last part""

&amp;#x200B;

**What I tend to achieve:**

1. Disentangle the chaos of emotionally laden parts that argue against each other in her
2. Give her some relief with those emotions
3. Entwining stuff that has to do with her job ""It's complicated and frustrating"" vs. stuff she is projecting on the job ""I'm not worthy""
4. Laying it all out on paper. Making it comprehensible
5. Get boyfriend points :D /joke

&amp;#x200B;

Any advice?

The disentanglement of the different parts into graspable personalities is the hardest for me.

I'm thinking of naming them and getting them physical spaces/bodies:

*""So the part 'that gets frustrated at new sorting systems' is the salt shaker now, he has a right to talk and give his point of view""*",2841,NLP,0
"The Scarlett Letters Scarlett had gone through a lot in her life, and sometimes that happens just by being born in the wrong family. The utter nonsense that is spread in the personal development field, that we create our own destiny is more hurtful for people like Scarlett than it is for others. Yet like everything, utter nonsense might trigger negative emotions in us, like the guilt of not doing enough which leads us to do something and look for answers. That’s what happened with Scarlett.

After being mentally, emotionally, physically, sexually, and spiritually abused by people as close as the first-degree family, and as far as a taxi driver she met for the first time, or maybe by life just being mean because that is life sometimes, shit just plain happens. You can call it bad luck, and you can add to that the lack of opportunity to access a good support system that helps her create boundaries to protect herself.

We met for a couple of sessions, and her main answers to most of my questions were the following two answers:

· “I don’t know “
· “I am not able to”

So, I looked at her and asked the ever-annoying question: “ what's stopping you from knowing and doing whatever the hell you want to do?”

“Well, most of those who traumatized me are either dead, abroad, and in jail. Those who I still have access to trigger me so much I cannot even talk to them. And I have so much to say…”

“Alright then, you got to say them, Scarlett.”

""But how?""

“Well, you open Microsoft word, write a letter to each one of them. Then you open your email and send them to attached me. After you do that you delete the mail from your sent and trash folder. I read them and analyze them to know what we need to work on for you to start feeling better. When we start getting results, I will resend them to you so that you can read them from another perspective”

“But…. I can’t…”

""Why?""

“I harbor so many negative emotions, I might say a lot of mean and destructive things.”

“You do realize that you and I are the only ones that will read them, right?”

“That’s what I am worried about, you changing your mind and not helping me because of what I might say.”

“Yes, my feelings will be hurt when you express yours towards strangers, I never met that hurt you in ways beyond imagination.”

She cracked a smirk, and said: “Ok, I will try…. How much time do I have?”

“Can you send them tonight?” I said in a serious tone

“No way, I can’t, I need at least 10 days, I don’t know what to say, this is the hardest thing anyone ever asked of me…”

Interruptingly “10 days it is, to the hour, I have set a reminder to keep you committed to the second.”

She paid the coaching fee and left. She texted me a lot of questions and objections during the next few days, I did not respond. ( I later discovered from Andrew T. Austin’s work on the patterns of Chronicity, that this correlates to the first pattern that he calls “The 3 stage Ab-reaction”, and the best way to respond to a client’s negative emotion is not to allow them to bend your will with and stay on track you had in mind.)

One minute after the deadline, I receive the email from Scarlett with 15 attachments.

Without opening any of them, I delete the email from my inbox and trash, because that’s between Scarlett and them, and is of no business to me. From the following session, we started to get some improvement and result as she stopped carrying the unwanted weight of all the unsaid conversations to these 15 people.

Oh — I told her that I deleted the emails a year down the line, and she said laughingly “You asshole, but I wanted to see my drama, I was so looking forward to it, but I guess what’s in the past is in the past…”

NB: The stories I am sharing here are not prescriptions or advice in any way, they are just stories of what worked and did not work for me in specific contexts with specific people.",3881,NLP,0
"How do I write an effective persuasive email to someone who can mentor me? I'd like to email someone quite influential, who's offering a free weekly coaching session, he's a consultant and has done lots of seminars in different companies. I'd like to grow with him in influence, networking, and a lot of other stuff that he can help me with. The thing is I'm only 20 years old, dropped out, started my business early and failed a lot, how can I use that to my advantage?

These are the things that he's asking for to be answered in email:

1. Who you are and what your situation is.

2. Why you think coaching will help you.

3. What you'd like to get from the session.

So how do I persuade him? What should be the length, presentation, style of writing, usage of words, and etc.? What topics should I put, should I give a background of a bit of my ""sad"" history? What's the best way to answer his questions? How do I make it so that my email presents me as somebody who's the perfect candidate, an eccentric guy?",1014,NLP,0
"NLP practitioners and masters, do you practice NLP daily and how? What are the places and situations you can use and train NLP on daily basis? Besides training meta model on reddit ;)

What I use on daily basis is building rapport, reframing and agreement challenge while talking to colleagues on work. Sometimes meta or milton model if I recognize the opportunity but that seems rarely, especially because my workplace doesn't include working with people. Also I would like to try submodalities and anchors more often. So tell us, where is your playground?",557,NLP,0
Body Language is an Expression of Someone's Matrix | Every Frame has a G... ,76,NLP,0
"PRESTO!   

Late 2011 I met Janet. Social Media had become a thing at the time, and this was in the days pre-NLP and Coaching as a profession, yet I had started reading and looking around the field. Janet and I became close, and she later introduced me to her best friend Mary one day, Mary was cool but I never saw her again for a while. 

One year down the line, Janet and I stopped talking for various reasons. A while later, Mary reaches out to me asking for something and a long friendship began. 

In 2017 Mary was meeting her fiancée at a Café and she told me to join. As we were there, Janet calls her and says she wants to join, since she hasn’t seen her for a while. It had been around 5 years since I last saw her, and I had been through the career change and had been coaching and training for 4 years. 

Janet shows up, surprised to see me, sat down and there was general chat for a while. She then looks at me and asks “Did I ever tell you that I have an extreme fear of Pressure Pots?”, I said no, but maybe someday we can talk about it more...

&amp;#x200B;

&amp;#x200B;

[Commonly called Presto in my country as for the brand of Pressure Pots](https://preview.redd.it/0fqito9bb3d51.jpg?width=400&amp;format=pjpg&amp;auto=webp&amp;s=917e0f421b163e24c051915846a375324509e068)

Twenty minutes later, for a reason still unknown to me to this day,  I look at Janet and make the Pressure Pot sound: “Psssssssssss-PSHSHSHSH.” She looks startled and scared – side note, when Janet was young there was a civil war in her country, and the sounds of missiles were daily, somehow my mind connected that the sound of the missile is very close to the sound of the pressure pot (Presto) – I then tell her “Janet, a pressure pot is not a missile. It really isn’t.”

She laughs, looks at me confused, and said you know this might be the connection, and then proceeded to tell a story where she heard missiles on a specific day that left a specific impact on her. 

Two days later, I get a video on WhatsApp from Janet of her in the kitchen next to a pressure pot smiling. 

What I said could have been what really was the connection, or simply it was just a counterexample to what the fear meant to her up to that moment. Whatever it was – it worked.

**NB: The stories I am sharing here are not prescriptions or advice in any way, they are just stories of what worked and did not work for me in specific contexts with specific people.**",2438,NLP,0
"The Boy and the Nun   

Peter was 13. The school psychiatrist suggested to his mom that Peter starts on anti-psychotic medication. For any mom, hearing that was unacceptable and wanted to find other answers, so the psychiatrist gave the mom two months to find an answer to Peter’s problem or he will have to change schools because his reactions are influencing his classmates negatively.

Peter’s problem was that he was hallucinating a nun, visually and auditorily, VALAK from the movie The Conjuring, that caused him to have Panic Attacks, scream, and freeze. This had been going for 2 years on a daily basis, since his parents started the process of divorce. 

&amp;#x200B;

[Valak from the Conjuring or as Peter said from the Greek Bible as he never saw the movie.](https://preview.redd.it/znvn3qfstuc51.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=527cae99095e9506591531cab76eec170803d49d)

Peter’s mom, lets call her Julia had a friend who knew me, who knows I had the knack with experimenting with challenging cases. Although I am not a therapist, as a Coach and NLP Trainer, I have worked with challenging cases in collaboration with therapists for the better part of the last seven years, and got some interesting results as my frame of mind was different than theirs. 

Discovery Session, Peter and Julia come together. He shares his story and how long it had been happening. I deliberately misunderstood the name of the nun, and called her the name of a cheese “Kashkaval”, this is a PCW (Provocative Change Works) stance that helps in lightening up the mood and helping the client get out of his state a bit.

We did 4 sessions with intervals of between 2 weeks to 4 months in between and here is a summary.

Session One: We worked on changing the connection of the emotion to certain images in his mind. 

Result: He started seeing the Nun once every 2 days instead of daily

Session Two: Using the fast phobia model – we worked on changing a specific scene in his mind that started with the moment he started getting scared, and ended at the peak, towards starting way before the fear started where he was safe, and until he reached safety again after the peak.

Result: He started seeing the nun once every two weeks, but still believed it was out there not a figment of his imagination. 

Session Three: Mostly was a provocative conversation, getting him to think of different things in different ways. Two main points played a role here. His biggest trigger was when he was studying Maths and reached a point where he could not understand anymore. He was afraid to fail. So, we did a how to study Maths session a week later that was not related to change work, just a mental strategy session on how to solve problems. The second point was a question I asked “Have you ever taken a picture of “Crispy”?” The nun’s new name because her skin reminded him of crispy chicken. His answer was no.

Result: Two weeks later Peter sends me 3 Whatsapp Pictures telling me, that’s her, can you see, can you see? I asked where? Can you mark her for me so I can see her too? And the more he tried, the more it disappeared. And I did not hear from Peter or Julia for a while.

Session Four: Three months down the line, Julia calls me and says that Peter really wants to see you. We agreed on a date. Peter had grown in length, lost weight, and had asked his mom to change schools as he was not comfortable in the environment any more. When I asked about “Crispy”, his reply was: “Well I only saw her once, I was camping with my friends, and she showed up in the forest far away, waving her hand as if she was saying goodbye…”

That’s when I said goodbye to Peter, knowing that the best way to help someone change something is listening to how they create and maintain the problem, as PCW creator Nick Kemp says, and when heard well, change happens much quicker than expected and in much simpler more straightforward ways.",3929,NLP,0
NLP and learning a new foreign language Anyone have insights into how NLP can be used to develop an improved capacity for linguistic skills?,140,NLP,0
"What do you do when the bantering/compliance-testing never stops? When I try to make conversations/group conversations, there's lots of people trying to poke at me, trying to make fun of me, trying to tease me and etc.

I'm usually a good sport, and I've already become an agree and amplify master. I could crack a joke or two back and everyone laughs, the thing is this happens most of the time, and it's like most of the conversation is just filled with banter, not that it's not fun. I'm just concerned on how I would appear on the social ladder.

Maybe I should execute more pressure flip type of answers? Even still it would just be some back and forth thing, you hit me, I hit you. It's quite tiring, though I wouldn't want to eliminate this as it really can help my status as well.

I think I've become known as the type who would react like this in order to be funny for the whole group, it seems to be a reactive thing now, and I would say that they're doing it because they know I'm such an expert at this, and we'd all get a laugh, or maybe because they also get some dominance points?

Does this make me look bad or does this elevate me? I heard that passing these shit tests are my chances of appearing higher status, but if it's backfiring...

How do I reduce this? Should I set the frame and control the conversation?",1332,NLP,0
A Misconception around Preferred Sensory Modalities in NLP ,59,NLP,0
"NLP Group Practice Hey everyone,

The best way to continue mastering your skills is to PRACTICE, especially with other people.

So, I am starting a online hangout for us to double our skills.

If you're someone who is wanting to put in time for practice,  comment below or private message me, and i'll share the skype group with you.",333,NLP,0
"Not liking this type of duality.  [https://clyp.it/byjezd2j?token=f88092cbc3ddbfbf01db870fc2a63d2b](https://clyp.it/byjezd2j?token=f88092cbc3ddbfbf01db870fc2a63d2b)   
Says 2 things.  
Let us send our deepest thanks to our wonderful veterans, law enforcement, first responders, and the doctors, nurses, and scientist working hard to kill the virus

\[Let us\] send our \[deepest\] thanks to our wonderful veterans, law enforcement, first responders, *and* \[the doctors, nurses, and scientist working hard to **KILL.\]**      ...the virus  


original audio from [https://youtu.be/mXD4zPY4Ai0?t=60](https://youtu.be/mXD4zPY4Ai0?t=60)  
I hate listening to this guy and hear him using would could easily be dark psychology all the time. Too consistently.",753,NLP,0
This is a technique ,20,NLP,0
"NLP Practice: Applying concepts in day to day life, I'm a total beginner so bear with me So I work full time in customer service so most of my day is spent standing up and talking to customers. The interactions tend to be shallow and brief but I am trying to think of ways to build rapport and mindfully be aware of NLP related concepts around body language and patterns of speech. I'm curious how I can better practice and integrate certain things. Would love to hear from experienced NLP practitioners how they would recommend utilizing my time at work:

1. Body language: mirroring and matching. Assuming I'm standing, they're standing, and I have time to adjust my own posture as they input information into the credit card reader or sign, to what degree should I match (general upper body focus with limbs or very specific like, their ring finger is against counter so I match that)? when is it appropriate to match vs mirror? certain length of time to pause before matching etc?)
2. What are some examples of less obvious VAKOG expressions that I can notice and respond to in a complimentary way?
3. to what degree can I really practice this with limited time/openness from a customer? Many people will respond to questions and attempts to build rapport with silence or 'yeah'. Is this a 'lost cause' or are there subtle things I can pick up on to try to quickly engage? 

&amp;#x200B;

any resources you might offer would be greatly appreciated ! Also any practicing coaches who want to work with someone, I'd love to see some concepts in action in exchange for feedback/testimonial!

&amp;#x200B;

**An exercise to practice with: (going to try this out on my day off)**

Ask someone to think about someone they love, watch result, break state

Ask someone to think about someone they hate, watch result, break state

Ask them to think about either one, but don't tell you.

See what you see, say what you see, see if you were right!",1940,NLP,0
"Is anyone else... Starting this NLP journey completely alone?... I discovered this practice as a result of wanting to find better ways to communicate with the troubling people in my life. As a result, I became truly questioning myself and my behaviors in regards to inevitably EVERYTHING.. But Its been difficult practicing the techniques and drills on my own. Is anyone else working with this type of situation?",412,NLP,0
"How do you explain what NLP is? I recently started learning about NLP and enjoying it so far! The other day a friend of mine asked me what it was and I was completely stumped on a simple way to explain it to her. 

I'm still learning so maybe that's why I find it difficult right now, but would love to know how everyone else describes it to people who've never experienced it?",377,NLP,0
Feelings Vs. Emotions in NLP | What is the difference? ,55,NLP,0
"Does it work on people who don’t know you’re doing it? Something creepy just happened to my girlfriend, and I’m wondering if NLP could be used to make someone do something they wouldn’t want to do. Anyone know if this can be done?",230,NLP,0
NLP and decision making Wondering if NLP (natural language process) is capable of analyzing a context and make decisions on it? Like comparing two supersport cars in an article and pick the winner out of them.,209,NLP,0
"(Looking for ) Programs recommendation about getting hypnotize voice 
Hi i am looking for programs recommendations for getting control of my voice in order to get people attentive. And use voice to convey emotions to people by storytelling...",242,NLP,0
Why People Don’t Usually Follow Your Advice... ,47,NLP,0
"Book Suggestions Please!! Heya, 
Starting my coaching and NLP course in less then a month and was looking for any reccomend material on the subject? Favourite reads with anything to do with coaching not specifically NLP.

Thanks for your time and effort :)",256,NLP,0
How core elements of NLP hang together ,39,NLP,0
"NVC vs NLP Round 1: ""Person X has been confirmed as having COVID-19"" I'm a member of a facebook group where I read the following:

&gt; Person X has been confirmed as having COVID-19. 

And then I saw 114 comments, all of which presumed that COVID-19 was a real virus and you could in fact contract it.

I feel like a tiny ant against tidal wave of hypnotised, drugged out fools (sorry to use such a strong term. I'm aware of the use of it, but dont think sugar-coating my thoughts is going to help me grow in understanding of this approach. If I have a personality defect, let's fix it.).

I want to post something to the contrary but:

1. what good is it going to do?
1. i might get banned from the group for expressing a counterview.
1. What is my (uncensored) report about the situation saying about how I see the world?
1. How can I get these people to look at a site like [Questioning Covid](https://questioningcovid.com/) and realize there may not even be a corona virus?

# Also note

For a long time, I've been wondering about how NVC and NLP approach the same issue and which one I should pursue. That's why I plan to post this same question in 
reddit NVC and compare the responses.

## NVC thread
https://www.reddit.com/r/NVC/comments/hjbif4/nvc_vs_nlp_round_1_person_x_has_been_confirmed_as/",1304,NLP,0
Modeling Excellence with NLP | The Secret to Mastery ,53,NLP,0
"4 metaphors videos  

I've added four new videos to the Metaphors Mastery site.  These are open and available for non-members 'til later this afternoon.

The Supports: https://metaphorsmastery.org/824-2/

Anthropomorphications: https://metaphorsmastery.org/anthropomorphications-2/

Immersion and Confession: https://metaphorsmastery.org/immersion-and-confession/

Immersion Metaphor – Jonah, Noah &amp; Jesus too:

https://metaphorsmastery.org/immersion-metaphor-jonah-noah-jesus-too/",485,NLP,0
What are some great NLP products that enriched your life? ,58,NLP,0
World peace with NLP? | Levels of Rapport ,42,NLP,0
Pt 8. (More eBay) Is this the biggest bootlegging operation in personal development and NLP? ,93,NLP,0
Pt 7. (eBay) Is this the biggest bootlegging operation in personal development and NLP? ,88,NLP,0
Pt 6. (Updates) Is this the biggest bootlegging operation in personal development and NLP? ,91,NLP,0
Converting Your Communications to Your Listener's 'Operating System' ,69,NLP,0
"Good Introductory book for NLP - Not too boring but interesting Hello I picked up a book on how to develop rapport with NLP and was pretty impressed after trying a few basic techniques. Now I would like to learn more about NLP (for more than just rapport) and the full scope of what is possible with NLP. How about recommending a book to read? I do not like books with a lot of fluff. At this time I just want to know what is possible with NLP that is not boring. I don't want a bible but more of a overview of what is possible and what people use NLP for (other than rapport)?

Thank you!",589,NLP,0
NLP Presupposition: Resistance Indicates a Lack of Rapport ,59,NLP,0
"If you were to squeeze years of studying psychology and neuroscience down to a reddit reply, what would you reply with? Looking to get into the study of NLP but I’m not sure of where to start. Thought reddit would be great place to! Also, if you guys could sight people I should be watching lectures of and books I should be reading up on, that’d be great too.",360,NLP,0
"Statement on Michael Carroll – NLP Academy, East Croydon, London ",65,NLP,0
"Presentation about Submodalities in Language Class (German) TL;DR: Holding a 20min presentation about ""How experiences are safed to our memories and invoke emotions and therefore actions, and how emotions/memorys color our actions"", needing inspiration since my two coworkers left ship and now I have no introduction I can build upon

*Fluff/Background text, no need to read:*

*We were supposed to hold 60min. Presentation in July. Due to Covid we are no longer obligated to hold the presentation, but I still wanna hold it cause my grade is garbage. But I'm missing some inspiration, since I haven't touched a NLP book in months and my original plan was to work closely with my co-presentations-people. Well, they've sailed ship and left me.*

*The class is a electrical engineering class. So I can basically make stuff up, since no one knows about NLP. The only thing I have to watch is that my political leaning is towards the teacher cause he's a bitch and wants ""All people happy together, corporations bad, grow your own vegetables, read books"". He's known for giving pupils bad grades if they hold different political views. Most hated teacher in the entire school.*

*- end of fluff*

&amp;#x200B;

The whole presentation should have been

Guy #01: Media - Wording and language in media and how it makes impressions on us. Keyword associations

Guy #02: Media - Manipulation through advertisment, tools to make a brand look appealing. Keyword framing

Me: Taking there stuff and pointing out tonality and auditive markers and working in how submodalities work

The full ""My presentation will be about""-guideline I gave to the teacher was:Submodalities - saving, retrieve, emotions, action- tonalities and auditive markers- submodalities and inner structure of memories

\- Meta-Models - Meaning of ""They are **all** idiots""

\- How arguments that are generalized ""All cats are nice"" mean ""The person believes &gt;50% of cats are nice, due to my experience"" and saying ""I know one cat that is nice, so your argument is invalid"" doesn't serve the argument. Which I take as a segway to ""What do you actually want out of an argument? Proving you're right and they're wrong or something else""

Okay, writing this out helped a bit. If this random word salad triggered some ""He could read about xyz, that was inspirational to me"" or ""Look at this youtube video to get back into the NLP headspace"", I would be thankful

Thanks for reading.",2440,NLP,0
How to convince anyone to buy a product? what are the best methods?,67,NLP,0
"Looking for psychological techniques on how to deduce properly, thank you. ",75,NLP,0
Shamanism NLP Has anyone attended Richard Bandler’s shamanism NLP he conducted this seminar long ago if anyone can share their experiences and content of the seminar ? How is it different from regularNLP?,204,NLP,0
Lofty Thoughts Pt. 1 with Andrew T. Austin and Dov Meidan ,58,NLP,0
"""Moving Mountains"" Metaphor Analysis with Andrew T. Austin ",59,NLP,0
"how much NLP exists in ""How to Win Friends and Influence People"" by Dale Carnegie? ",83,NLP,0
"2 Of 5 Pillars Of Empowerment: Perception Is Projection - Part 1 In this blog, we 're going to look at the 2nd empowerment pillar I presented to you in my blog about how to get empowered through NLP. This [pillar is called projection.](https://matthewtweediehypnosis.com.au/nlp-news/2-of-5-pillars-of-empowerment-perception-is-projection-part-1)

&amp;#x200B;

https://preview.redd.it/dibyndtw1u151.jpg?width=750&amp;format=pjpg&amp;auto=webp&amp;s=6d93e5373208c48d9a6e45de6f67f01a8e47043a",489,NLP,0
How does NLP work | The Neuro-Linguistic Programming Communication Model ,73,NLP,0
"Learn ACTUAL NLP hi I'm just starting to study NLP, I know there's some powerful stuff out there but everything I've found is ""life"" coaches on old HTML websites selling a $47 ebook. I'm mostly interested in the persuasion side but the self-improvement stuff sounds good too.  What are the best no BS resources? all help is much-appreciated thanks!",348,NLP,0
Best entry level Audiobooks for NLP? Looking for a book on NLP thats on Audible that's entry level but gives a good all round overview and introduction to NLP.   Any suggestions?,178,NLP,0
Why are some people more prone to NLP? I discovered I'm very susceptible to NLP and I dont know why. Any insight into this? Is it poor discipline during childhood or being impulsive?,182,NLP,0
"How to get a solid understanding of NLP There are a lot of great books out there on the subject of NLP including hundreds of adaptations for every area of your life from dating to your career. Some popular books include: 

""Unlimited Power"" and ""Awaken the Giant Within"" by Tony Robbins - who has created a new technology out of NLP I think it's called Neuro-Associate Programming...

""How to Take Charge of Your Life"" by Richard Bandler - a somewhat easier to read, modernized version of ""Frogs Into Princes"" by Bandler and Grinder (founders of NLP)

""The Origins of Neuro Linguistic Programming"" by John Grinder

These are wonderful books that provide a great education on both NLP and self-improvement.

But...

As someone who went through a Richard Bandler certified Master Practitioner program, here are some of the things you will miss out on if all you do is read about NLP. And it is even worse if you are mislead by ""bad NLP"" information.

* You can miss experience practicing and drilling. Good NLP seminars implement the important factor of modeling and action. The original materials and people who contributed to NLP were constantly meeting up and documenting and sharing their experiences. Those experiences are much different than reading a book and testing out ""anchoring"" a couple days later at work. And it is best to practice after you've watched an expert put it to work.

** You won't learn to be dynamic nature of NLP and create unique patterns. There are a few very comprehensive and reputable NLP books like ""The Big Book of NLP"" by Shlomo Vlaknin that give you a good sense of how to apply NLP beyond anchoring, mirroring, and VAK. But NLP is hugely dynamic. And if you only read the basic books or reference random websites, you may not come to realize the full potential you have to implement it.

*** You'll miss out on the power NLP can have to affect people. I grew up with applying the scientific method to everything I did and I would say that made me a little too skeptical about most things. Or I would expect to see PhD level scientific research and data before I trusted something ""would work."" You can hear about how NLP can change people from books. But for me, there was nothing like seeing an expert NLP coach working one on one with someone and the effect it can have.

**** You only get a taste from reading. Reading is great and there are dozens of valuable books on the subject. Most of them go over the same or similar concepts, but rarely do the books go in depth about how to apply NLP to effectively help people.

NLP at its core is a communication and personal development tool. Communication requires people, and personal improvement is more effective when working with experts. 

Therefore, if you are fortunate enough to have an opportunity... 

If you want get a very strong understanding of NLP...

If you want to apply NLP beyond the basics to communicate better and help others... Practicing with people, with the guidance of an expert will be of huge value to you whether that is through a live training, seminar, or combination of digital and in person.

(If you thought this was helpful, check out my newsletter for more ideas and tools: https://mailchi.mp/emmettferguson/mindsetmattersfirstnewsletter)",3261,NLP,0
"Mirroring for conference call interview I have an interview over Skype/Zoom. Often times I copy posture during the first minutes of a conversation. Is there anything to mirror via call? E.g., besides voice tone?",211,NLP,0
"Emotional Detachment Hey everyone,

&amp;#x200B;

I was wondering if there's anything in NLP that works with emotional detachment. I'm kinda fishing for a solution to feel any positive emotions. My baseline emotion is flat and basically only experience the negative emotions and occassionally a laugh here and there. I've got some physical stuff that I thought might contribute to a lack of physical sensations of emotions, but then I thought that I could sense the negative ones so if I can physically experience negative emotions than I should be able to experience the positive ones as well right? I'm not depressed or anything, but there's just no positive emotions.

Anyways, its a lovely puzzle that keeps me going for one thing to another and repeat. So curious to know if there's anything on emotional detachment so I can see if that does anything :)

Thanks!",867,NLP,0
Pt.4 (Complicity) Is this the biggest scam/bootlegging operation in personal development and NLP? ,98,NLP,0
NLP &amp; Corona Virus - Day 21 - Moving from Victim to Victor ,63,NLP,0
Fear &amp; Anxiety That Your Children Are Going to DIE ,55,NLP,0
NLP COACHING QUESTIONS LESSON 2 ,32,NLP,0
NLP Coaching Questions Lesson 1 ,32,NLP,0
Presuppositions explained by set theory ,40,NLP,0
Fear of a Conspiracy Therapy ,29,NLP,0
"Examples of mapping out charting processes, goal setting toting, diagramming etc how to do this? I hope the question makes sense

Ive started a self coaching project and i need a way to give form to it , to represent the goals, things holding back, proceses to target,        What must be so,  what am i doing that needs to stop. What is required etc etc

In a way that i can easily refer to it see what im doing and where im at

I need a way to give form to it through symbols diagram or mindmap it - so that i in a few glances can refer to it

It just doesnt work otherwise. In writing. Its not about the content perce,   But how do i organise and represent that in a way that i can  easily view and get it
been doing alot of tracking my patterns,  ive been writing out goals, ive been figuring out doing thinking n writing about what  has to happen, what us required wtc
What processes i want to target


But i just dont know how to properly give form to it externally

I found 1 good diagram witch il post in the reply that i can modell to represent a big chunk overview understandings

Im asking because hypnosis and nlp tends to prettu good at charting out models, using the nlp tote    To map out processes 

But i cant find good examples and where can you learn how to do this?
All mindmap  resources ive checked out - are basicly just how to brainstorm a topic, or mindmap a flow of toughts -   Thats not the same

Any and all ideas are welcome

Thanks for your help",1475,NLP,0
How Design Human Engineering or DHE differs from NLP ,53,NLP,0
"Guide to TRANCE formation by Richard Bandler I was just ignoring this recommendation back in college,read it and wasn't really into it before.

But now re reading it in [www.openlibrary.org](https://www.openlibrary.org)   and listening to it while cooking and cleaning,  a lot of it is clicking.   Reading this book is like re discovering how Richard Bandler  has uncovered the magic of NLP, before they were peddled as get rich quick motivation scheme by the modern day hacks.  I have forgotten how reader friendly it is.    I have to have this book on paperback.",564,NLP,0
Fear of Losing Your Job Survival Programme P3 ,46,NLP,0
What is the process called when you heal traumatic memories by changing attributes of the memory? I am looking for the name of the process used to heal traumatic memories.. and also ask if there is a script for it. Thanks in advance,232,NLP,0
"Seeking advice: Help make Ericksonian language patterns sound and feel normal. Hello!

By nature I speak in a very blunt, direct manner.

Ericksonian language patterns, I understand the purpose is to be permissive, soft, suggestive yet when I speak in this way there's a sense of disharmony as its so foreign to me. 

Any advice on how to make Ericksonian language patterns to feel as comfortable as my natural tendencies?",422,NLP,0
What NLP techniques do you guys use for motivation? ,52,NLP,0
NLP &amp; Corona Virus Day 20 - the effect of the relationship sort metaprogram on your experience of lockdown. ,112,NLP,0
Irritating anchoring How is it that there are activities from which we think we like them but at the end it's not fun at all because we were just thinking that we like it?,171,NLP,0
NLP &amp; Corona Virus Day 19 - Challenges with people in lockdown - Ho'o Pono Pono download in comments ,105,NLP,0
"Stimulating the left hemisphere of mu brain for creativity, co-ordination and peace of mind by some simple tasks ",113,NLP,0
NLP &amp; Corona Virus - Day 18 - neutralising negative thinking and self talk ,79,NLP,0
John McAfee on Twitter: “A tutorial… On how the Mainstream Media… Manipulates you.” ,84,NLP,0
How to Let Go of Anxiety &amp; Fear 5 Mins ,43,NLP,0
Reputable NLP Courses and Accreditation I'm looking to take a an NLP course on Udemy.  Many seems to offer accreditation or certification of some kind. Which associations are recognized as being reputable and legitimate?,220,NLP,0
NLP &amp; Corona Virus - Day 17 - using NeuroDrivers to remove negative anchors and limiting beliefs ,101,NLP,0
LET GO OF GUILT THERAPY (2 Mins) ,33,NLP,0
Are there any good apps for nlp that walk you through the processes? I have downloaded a couple free ones...but figured the group here is likely to know if there are better ones,177,NLP,0
"Asking for help. I’m a people pleaser, how can I change the desire I have to please ppl to a desire to do the things I need to do for me? ",138,NLP,0
"Best nlp processes to work with current physical discomfort I am sure there are lots of processes I am unaware of, but looking for something that works on current physical discomfort.",183,NLP,0
"Can you guys suggest rare anchors that I can use, that aren't regularly used or encountered during the day? I want to create two anchors for fear and happiness, and I don't want them to trigger off at unwanted moments.",218,NLP,0
What are some of the most important/transformative nlp techniques? I bought a course for nlp on Udemy and it didnt have the phobia cure...I am wondering what else I am missing out on.,183,NLP,0
Are there any free resources that teach the nlp phobia cure? I just learned of this today and would like to investigate it.,123,NLP,0
NLP &amp; Corona Virus - Day 15 - Managing negative anchors in lockdown. ,73,NLP,0
What are the best nlp processes for panic attacks and also for generalized anxiety? I am looking for processes to apply for both of these. Generalized anxiety means it is pretty much constant.,192,NLP,0
"If we tend to look to our right when remembering things, &amp; to our left when constructing ideas, along with the different positions for different senses, if we developed bad habits like consistently looking up or down in a certain way, couldn't this be reverse-engineered and used to improve recall? So for example, look in a certain eye position when try to study and create a mental visualization with sound in order to strengthen the visualization. Or look to the left, in order to make the process of memory creation smoother.",533,NLP,0
"Is there a basic link between meditation and NLP? Can anybody explain if there is a similarity with how the two techniques can be used to achieve a restful mind?

&amp;#x200B;

&amp;#x200B;",189,NLP,0
Key Workers Stress &amp; Anxiety Buster ,40,NLP,0
Request to change the name of the sub from NLP to Neuro linguistic programming For every one Neurolinguistic post there's atleast 3 Natural language processing posts on this sub. So in order to avoid confusion it's probably better to address the sub with its full name? Idk if this is an appropriate way to convey this message but I feel this is necessary in order to maintain the integrity of the community and stop those annoying deep learning python posts flooding the sub.,476,NLP,0
How To Read Minds In Under 10 Minutes ,38,NLP,0
NLP &amp; Corona Virus - Day 10 - a physical workout program for you. ,70,NLP,0
NLP &amp; Corona Virus - Day 9 - Removing overwhelm whilst achieving your biggest goals ,88,NLP,0
NLP &amp; Corona - Day 8 - Belief Busters Questions to remove limiting beliefs ,79,NLP,0
NLP &amp; Corona Virus - Day 7 - A technique to discover what limiting beliefs are getting in the way of your goals. ,117,NLP,0
"Made breakthrough in procrastination, found strange physiological response. I’m usually responsible and to finally figure out and change what internal representation was stopping me from being responsible in another area of my life was amazing.  The only thing is that now, since this task I am no longer postponing starting today, is that earlier I felt something weird as I was about to finish it.  Getting close to done, the idea of doing the last parts started to make me feel flush in some places.  Like I was on the verge of a hot flash.  It was weird, like my body was going into some uncharted territory.  Has anyone else experienced this?  Is this something that more NLP should be used to fix or will this go away automatically?",738,NLP,0
NLP &amp; Corona Virus - Day 6 - Revised NLP 5 Principles of Success ,69,NLP,0
Broken Container (Metaphor Analysis ) by Andrew T. Austin (Metaphors of Movement) ,82,NLP,0
NLP &amp; Corona Virus - Day 5 - 2 important metaprograms in lock down and how to change them. Link in comments ,112,NLP,0
SCARED OF BEING ALONE (Therapy Meditation) ,43,NLP,0
"NLP &amp; Corona Day 4 - A fantastic Havening technique for removing anxiety, fear and panic from the past ",107,NLP,0
"Looking for advice on NLP coaching I'm an entrepreneur looking to break down the barriers that are preventing me from taking myself and my business to the next level.
Through conversations with colleagues getting an NLP coach was recommended. I know next to nothing about NLP.

What should I look for?  

Does anyone have suggestions? 

thanks",343,NLP,0
Increased Sales for Coaching ,29,NLP,0
The Last Ever Ross Jeffries Interview [Ice White] ,50,NLP,0
NLP &amp; Corona Virus - Day 3 - A technique to release anxiety from the future ,80,NLP,0
New NLP &amp; Corona Virus Pandemic - Day 2 ,44,NLP,0
Stuck in a Pit (Metaphors Analysis) with Andrew T Austin (Metaphors of Movement) ,81,NLP,0
New NLP &amp; Corona Virus Pandemic - Day 1 ,44,NLP,0
"Reframing the Coronavirus [https://nlppod.com/the-reframe-we-all-need-to-get-through-coronavirus/](https://nlppod.com/the-reframe-we-all-need-to-get-through-coronavirus/)

Good idea. That's similar to how some friends in Taipei and Hong Kong thought and behaved all the time when I visited them a few years ago... be careful with what you could do to the other people around you.",379,NLP,0
"""Nothing to Stand For"" (Metaphor Analysis) by Andrew T Austin ",62,NLP,0
Hand of God (Metaphors Analysis) with Andrew T. Austin | Metaphors of Movement ,79,NLP,0
Anxiety Removal Questions 2 x Minutes ,38,NLP,0
Coronavirus and A plague of life Coaches and NLPers. ,53,NLP,0
"Grown of Iris? Iris dilation is a fair indicator of a trance state, and access to complex thought. What would people's reactions be if I tell them they will get grown of iris?",175,NLP,0
"NLP and Huna for Mental, Emotional and Physical well being during the Corona Virus Pandemic Day 6 ",98,NLP,0
Using NLP &amp; Huna to maintain our wellbeing during the corona virus pandemic Day 5 ,86,NLP,0
Using NLP &amp; Huna to maintain our wellbeing during the corona virus pandemic Day 4 ,86,NLP,0
Using NLP &amp; Huna for wellbeing during the Corona virus pandemic Day 3 ,74,NLP,0
"Using NLP, Time Line Therapy, MER and Huna to maintain well being during the Corona Virus pandemic. ",100,NLP,0
"Help needed in abstractive summarisation. I'm working on a project on abstractive summarisation. I need some.help with the coding part of it. Please share some good github repos for the same. Please share some research papers too.

Thanks",238,NLP,0
In a Spin (Metaphor Analysis) - Metaphors of Movement with Andrew T. Austin ,76,NLP,0
"Using NLP, Time Line Therapy, MER and Huna to overcome the emotions around the Corona Virus Day 1 ",98,NLP,0
"Corona Virus and using NLP to stay safe. With everything that’s happening in the World right now there is obviously huge uncertainty and with that comes fear and anxiety. Even panic. 
We are worried about our family, loved ones, health, finances, jobs, and businesses. 
As a result I am doing a FB Live everyday for the foreseeable future. I’ll share really practical ways you can use NLP, Time Line Therapy, Mental Emotional Release and Huna to maintain your mental, emotional and physical well being. 
We can do this!
Get stronger. 
We can get through this together. 
We start tomorrow at 0800 GMT/UTC
If you can’t join me live the video will be available here on my FB profile, pages and groups. 
Stay safe my friends. 
Best Wishes Always
David
P.S. Please share this with anyone and everyone who might benefit.",814,NLP,0
Foggy - (Metaphor Analysis) ,28,NLP,0
Nightmares and IEMT ,20,NLP,0
"NLP and the A/V Sales funnel It's been years since I've visited the world of NLP. Back in my 20's I listened to audio tapes. I wanted to develop that instant rapport. It worked to an extent as I didn't stick to the training like I should have. However, id did give me a good framework for job interviews, etc. 

Now, in 2020 with a family, career, and the ambition to STOP working for the man by building an online business, I'm hoping those of you with more experience could give some guidance.

I'd like to use NLP on my YouTube videos to nudge people into clicking a link and hopefully buy the product I'm promoting.

My site's niche is personal growth (spiritual, fitness, and financial). My format will just be images/video clips with a text overlay, narration, and background music.

Besides the obvious ""click the link in the description"" statement, I wonder what techniques could be  used as far as text/visuals to condition the person watching to ""opt in"" internally?",976,NLP,0
"A group for learning NLP Hi, 

As I researched, NLP is best done in a group. That way we can collaborate in learning and share our experience with different techniques, learn from each other and ask each other. 

If anyone is interested in joining the group hit me a message. \\ (•◡•) /",286,NLP,0
The ulitmate introduction to NLP - How to build a successful life by Ric... ,76,NLP,0
"""Untethered"" (Metaphors Analysis) ",34,NLP,0
The Red Balloon and The Hands (Metaphor Analysis) ,50,NLP,0
"Is there a fairly widely accepted visualization for NLP? I am brand new to even learning this world exists, but I was curious if there was any visualization to this yet? If not, what is it that you use? I would love to collaborate with a more talented artist than I on something like this if interested. Thanks!",311,NLP,0
"People describe their world - not reality (not mine) TL;DR: I try to have more of an ""he talks about himself, no reason to defend my own world""-attitude in conversations. Lets explore their world

&amp;#x200B;

Example

Joe explains how every girl is a bitch. His emotions and therefore reality is true and talking against it is cumbersome and useless.

Metamodelling in normal conversations like ""Which girl exactly is a bitch?"" does work, but it gets the conversation on a logical boring level and denies the emotions of the other person. Sometimes they just want to vent.

&amp;#x200B;

I'm using an extreme example here. In my circumstances its way more subtle.

 I have trouble accepting their world, cause in my consciousness floats the in-congruence with my world. ""Linda is not a bitch""

How do I forget my world and be more open to theirs?

Any ideas? Been gnawing on this for a week now.",897,NLP,0
"CR-OH!-NO!-Virus is a word which induces panic. What do do about it? Just as ""new-direction"" transfers hidden meaning, so does the word ""coronavirus"". I think that the subtler meaning of the word is helping to whip the media and the markets into panic. Am I wrong?",264,NLP,0
What are some great volunteering jobs to practice NLP coaching skills? ,71,NLP,0
"Change your neuronal pattern sooner when you use Marijuana, is it really a shortcut?  

Hi,

My question might be more close to NLP but It might be more than that. That is why I make my question on a broader topic to see the validity of this statement.

By reading a lot of books in this field, I know almost 99% of the therapist, psychologist believes the neuro pattern shaps personality and you or try to make that change that personality in your self with beleive. It isn't true that it is a shortcut to get the same result faster when you are using a THC-dominate?

What I mean by that is, when you get high many thoughts are not disturbing and you can make a pure concentration in what you want and ignore everything else and not only making that person or abilities but believe in that person and get more in the details. Does it really have some impact on self-concerns or is this just a placebo or does not have effect

For example, When I get high I try to make my next version who is more listener and don't show a fast reaction to people. Then it starts in pure imagination and gets to the details without any thoughts from a different area of my life disturbing me.

\- I repeat this method a few times, and I have seen when I am not high at all. I am more calm and a listener.

\- Or I made a person who always made himself a breakfast or anything you want to be, and when I'm not high at all, I make myself breakfast,

I want to see is it true or fake statements?

Thanks,

Iman",1492,NLP,0
"[Question] How do you change from a source/spirituality level? So, I came across the logical levels by a guy talking about permanent change on from a identity level instead of a behavior level. And it made total sense. 

However, I noticed that their is one beneath the identity level that he did not talk about which was source/spirituality. 

This is how the pyramid was explained to me.

Environment
Behavior 
Skills
Beliefs
Identity
Source/spirituality 


Do you happen to know why wouldn’t someone talk about change at a source/spirituality level? 


And how does one go about change at a source/spirituality level?


Lastly, Any other tips you can give to help with the topic of permanent change? 


Thank you in advance.",727,NLP,0
Metaphors of Movement Q&amp;A with Andrew T. Austin (March 2020) ,65,NLP,0
"This guy is talking about how NLP changed his world and life, very honest ",74,NLP,0
WTF is NLP?! ,13,NLP,0
"Any books recommendations? So I've finally decided on a path in life. 

I've been trying to improve, but I'm struggling a lot with failure as well as doubt on that path that arises mostly from the failures.

So I want to cultivate a mindset that is not swayed by failure, bad luck, setbacks etc that cause me frustration which then leads to doubt and loss of motivation.

I want to be able to deal with all that shit and keep moving with a clear focus.

I would be grateful for any recommendations on books that can possibly help me cultivate a strong mindset that can keep me on the right track?

Or better any good books on NLP with practical advice, simple and to the point?

Thanks",685,NLP,0
"DHE and NHR without formal NLP training Before he quits,  I want to see Richard Bandler and train DHE and NHR.  He says that these technologies can be learned well enough without formal NLP training. I have a bit of informal NLP training from 20 yrs of off and on reading, watching seminars and listening to audiobooks. My understanding is for sure deficient.   


I've also have been listening to the Bandler DHE 2000 CDs off and on for what feels like 20 years. I actually prefer this approach far more than 'traditional' NLP.  I also like the NHR stuff that I've seen and worked with informally.   


For a person who is only interested in working on themselves, is DHE/NHR enough on their own bypassing NLP?   


I like the more generative and improvise type of feeling that DHE/NHR seem  to provide. For me, it seems more playful, creative, build on the fly and that it can be used for anything.   


What do you all think?",928,NLP,0
"Please Give Ideas To Make Me Want To Do Grunt Work! I've been working on this with success, but i have a lot of repetitive/ assignment work that I dread doing and procrastinate.   


Please give me your best techniques for reframing the work.  
Please give me ideas to create a powerful state for doing this work.   
Please give me ideas for creating a ritual to do before this work.   


I have a lot of resistance towards this work, although I am very good at it. I put it off and off and off.  But i have also have had some homerun days where I have outperformed everyone who does this type of work, it just requires a lot of effort that then drains me.   


One thing that I have considered that because a lot of this work pays per assignment, that  I can represent blocks of assignments as what it truly represent in my world-- Food, bills, mortgage payment, vacation money, rewards, hobby money, savings, investing money. So, I am not about to do an hour of assignments, I'm about to earn x% of my mortgage and so on..   


Thanks.   
Do not be stingy please gimme the good stuff.",1086,NLP,0
"Your favorite storys in the history of NLP (Funny Storys) Hi there,

i'm holding a presentation in english (not my first language) about NLP. It's supposed to be entertaining so I'm thinking about putting in lots of the storys Richard Bandler told about the ""crazy days"".

For example, when a patient believed to the core he was Jesus Christ and Richard dressed himself and a bunch of football linebackers up as Roman Legionares. ""You are Jesus Christ?"" ""Yes, I am."" Well, today is your day of Crucifixion. Please stand next to the wooden cross""

Or the  psych ward patient who watched sermons on TV and hallucinated the preacher crawl out of the TV and annoy him all day long. So Richard suggested changing the channel to PlayboyMansionTV and let the girls crawl out.

So what are some of the stories you know, that I can implement into my presentation?

Greetings",865,NLP,0
"Can I learn to use NLP on myself with only books and videos? 
HI everyone, I'm a bit of a lurker here.

I've heard good thing about nlp and how useful it can be(bandler, nlp gym and Steve Andreas 

My main goal is to learn state management and become more of an action taker. 

I've tried visualizations and affirmations with mixed success. 

I'm not trying to learn any persuasion techniques so far just techniques to induce change in myself. 

Especially when it comes to getting myself to do something that will benefit me. 
I usually do nothing no matter how simple, easy and safe the action can be. 

Ie make a phone call to the bank to see if my account is still working, calling my old boss for some information. 


Now this is the main thing I have to figure out and I have failed at it most of the time for the last 10 Years of adult life. 

I'm 29 and not exactly cash flush so I wonder if practicing on myself is enough to get me moving at least. 


I bought a couple books like core transformations by Andreas, get the life you want by bandler and a few others.

I have 8 hours of free time and a few books.

Is reading and practicing on myself enough to learn how to learn state management?

I understand coaching is usually better but it's far out of my means right now.",1284,NLP,0
"Tell me if I am getting NLP right Ok so, this is what I understand after studying NLP. Tell me if I am getting it right.

&amp;#x200B;

NLP, basically, is a method of leading a subject to an understanding, by leading them through a journey which demonstrates the situation of a topic to the subject, through a medium of storytelling. NLP uses the mechanic of subjects living vicariously through the story, whether through video, audio, book format or what have you, to have the subject to arrive at a conclusion that might otherwise trigger the subject against it if they were to have things explained normally.

&amp;#x200B;

Trying not to get too wordy, do I get the gist or is there more to it?",697,NLP,0
"Any help is appreciated I have to say I've been struggling lately because I get very anxious when I'm not productive and lately I've been very dissatisfied with myself and it became hard to be grateful for the good things I already do or have done. This has affected my mental health greatly. Now, is [https://www.nlpco.com](https://www.nlpco.com) program a good start to learning how to be productive and feeling good about yourself so you can actually be productive and eliminate bad thoughts that stops you from living normally? or it doesn't help have to learn it through a psychiatric? 

Sorry for the unnecessary information.",631,NLP,0
Looking for NLP coach I had a severe anxiety attack that had intrusive thoughts 6 months ago. This made me quit drinking (drank a 6 pack every other day for 10 years) I still get bad anxiety and the looping intrusive thoughts. Looking for someone to work with online with NLP or hypnosis or any other suggestions.,313,NLP,0
"PTSD | Intro to EMDR, NLP, &amp; Havening Technique | This was 3.0 CEUs for Mental Health FREE! ",96,NLP,0
"Looking for a Coach (email / Skype)  Hi there,

I (m32) am looking for NLP Coaching. Starting amount of around 4-8 hours. I had professional NLP training as practitioner (70h course), so I should be easy to work with.

The problem I want to work on is an addiction to pornography and cigarettes (3 cigarette per week, porn 3 times a week, normal mainstream stuff). They're linked together, since I only smoke while watching porn. (Think fat rich guy with monocle in a leather highchair puffing his cigar bellowing: ""Bring in the girls. Maybe one will tickle my fancy."" I do basically that :D )

The reason I want to get rid of this, is that I'm having minor heart palpitations.  The doctor gave me the green light and said it is a normal amount, but smoking certainly doesnt help. It also heavily drains my energy and motivation. But its highly enjoyable ... so the inner fight goes back and forth.

It’s a bothersome struggle. Working on it all by myself has been tried and is failing for years now.

Let me know if your interested and your hourly wages.

Greetings from Germany,

shibiku\_",1091,NLP,0
"What is the most practical book on social NLP? I have read and seen videos about NLP and think it is pretty cool and impressive, what is a good book that is practical to start out with?",185,NLP,0
"48hour program? Hello all! Today i came across a NLP program which claims to be a ""48hr life transforming program. It is designed to enable you to get inspired, achieve your goals, build lasting relationship through power of rapport, overcome fears, increase self confidence, achieve peak performance,"" and also gives a certificate which will enable me to coach/help others and myself in future.

&amp;#x200B;

Do you guys think a 48hr program can do something like this? I don't know much about NLP but I know our mind is capable of doing miracles. What is your opinion?",571,NLP,0
"Is This Related to NLP? I saw a video online about something called NBT or I could be mistaking, maybe you can help me understand the connection if any

The video showed gentleman sitting and repeating positive suggestions to himself as he tapped different points on his face, as he reiterated the mantra.

Points: under the nose, temple, corner of mouth and he also touched his solar plexus. All the while ""I will create a positive..."" 

Thank you.
Aidan from Ireland",468,NLP,0
"NLP with no emotions Hello, ill start a practitioner course soon but im slightly worried I wont benefit optimally because I dont really experience anypositive emotions. Im not depressed or on medication for anything but I Just dont experience those positive emotions. Being emotionally flat would probably be a good description. Anyways over tried the submodalities to create an emotion but I cant really seem to.get IT to work. It either doesn't work or a very very tiny feeling in my body happens. 
Also I literally cant.think of any emotional experience I can reference.

Ive consulted psychologists and done lots of work to change my mental states and I cant really figure out anything that's in the way. 

Does anyone think nlp would still beneficial or maybe even necessary. Maybe any clues for exercises?

Thank you :)",825,NLP,0
"High quality NLP ressources available for free for a limited time I just found the news in my email inbox:

Andreas NLP relaunched their website and to celebrate it they will give another free NLP gift for 7 days to everybody who subscribes to their newsletter:  


* 3rd Feb.: Connirae and Tamara Andreas. Core Tranformation. (ebook)
* 4th Feb.: Mark Andreas. Sweet Fruit from the Bitter Tree: 61 Stories of Creative and Compassionate Ways out of Conflict. (ebook)
* .... to be continued. 

All you have to do is to open their website, scroll down to the bottom and register for their newsletter:

[https://andreasnlp.com](https://andreasnlp.com)

Each gift is available for one day only so sign up now.

Enjoy!",712,NLP,0
Andreas Nlp is giving away free gifts for a week! ,50,NLP,0
A commenter said “This is why machine will never replace humans” ,65,NLP,0
NLP &amp; Hypnosis Podcast Episode ,35,NLP,0
Pepsi Zero Superbowl Commercial Swiiiiiiiiiiishhhhhhhhhhhhhhhhhh.,65,NLP,0
BBNLP The British Board of NLP - The British NLP Board That Never Existed ,74,NLP,0
An important aspect of PTSD (video clip from the NLP advanced mastery training) ,80,NLP,0
Are therapists really this stupid? Surely not....? ,51,NLP,0
Spinning Technique It's an interesting sounding technique but I've never noticed my feelings spinning. What's the therapist's workaround for that? Thanks.,154,NLP,0
"Whatever you think you are, you are that and more. Calibrate on behavior, you are not your behavior. Accept the person, change the behavior. ",141,NLP,0
NLP PACING AND LEADING | LINGUISTIC SKILL FOR SALES | PERSONAL &amp; PROFESSIONAL INFLUENCE LIFE COACH | I couldn't find this in the history and was asked to post again in the feed. ,182,NLP,0
CORP TRAINING | NLP SENSORY ACUITY | EMOTIONAL IQ | MIRRORING BODY LANGUAGE | EYE ACCESSING CUES ,97,NLP,0
Day 7 of The 7 Night Imagination Challenge ,43,NLP,0
Why did Richard Bandler &amp; John Grinder stop working together? ,66,NLP,0
Day 6 of The 7 Night Imagination Challenge ,43,NLP,0
Day 5 of The 7 Night Imagination Challenge ,43,NLP,0
Day 4 of The 7 Night Imagination Challenge ,43,NLP,0
Day +3 of The 7 Night Imagination Challenge ,44,NLP,0
Day +2 of The 7 Night Imagination Challenge ,44,NLP,0
Day one of the Seven Night Imagination Challenge ,49,NLP,0
The 7 Night Imagination Challenge! (Utilizes Vakogizing!) ,58,NLP,0
"Exercise for motivation Guys.

I’m not motivated to do anything. What NLP exercises would put that proverbial rocket back up my ass?",132,NLP,0
"Timeline therapy Hello everyone.
I have a question about timeline therapy.
In timeline therapy by tad James he says go above your timeline and see your timeline. 
I don’t see my timeline, i visualize it (i mean i made it up). and my visualization is not good. I mean it’s too vague with little to no specific.
When he says do you see any blank or holes in your timeline, i don’t see if there is any or not.
I think so should be any and i visualize there is.
(My problem is i don’t know it is true or i just made it up in my head.)

My ability to recall memories is very low. 
My ability to visualization is very low too.

What should i do ?

And by the way i’m scared of darkness and i can’t remember the memory (imprint) for change what should i do about this too?

I appreciate your help.",790,NLP,0
Catastrophisation and Delusions of Grandeur ,44,NLP,0
Against belief! Ep 2 on Shifting Realities with Thomas Bjorge • A podcast on Anchor ,84,NLP,0
IEMT in a client with bilateral esotropia ,42,NLP,0
Shifting Realities with Thomas Bjorge • A podcast on Anchor ,60,NLP,0
"Can you recommend literature about working with parts, inner team, etc.? HI there. Not sure if I should better ask here or in the hypnosis reddit. 

I got to know the idea of working with parts in NLP, and also know the social panorama model. Now,  I'd like to know more about working with parts - mainly for coaching, as I am not a therapist. 

Should I try the book by Roy Hunter on ""inner conflict resolution""? Or would you recommend some other literature on this topic?",473,NLP,0
"Case Studies / Documentation of Modelling Sessions Greetings. Are there any resources that publish the results of NLP Practitioners who have modelled accomplished, exellent indivuduals worthy of emulation? Seems this has to be available somewhere. Thank you much.",263,NLP,0
A Guide for NLP Practitioners Working with Adult Sexual Trauma and Rape ,72,NLP,0
"NLP trainers with martial arts backgrounds I've noticed several NLP Trainers tout their martial arts prowess by stating their rank.  I have many years of training and was taught to never say or ask what degree black belt you or another person is, but it's fine to tell how many years you've been practicing. Could this be an indication of insecurity?",350,NLP,0
"Help Hey guys,

I have a problem I need to overcome, I've fallen head over heels for a woman which just can not be. I need to put out the fire while maintaining the friendship that we currently have. 

She triggered me by saying something which i most likely took the wrong way but I figured as it was instant I could overcome it by using a slightly modified version of the fast phobia cure. It did not work.

Any recommendations on a technique to use for this would be much appreciated. Cheers,",495,NLP,0
"It's in Discord  

Found  this hypnosis and Neuro-programming community server it is also full of  subliminals and some powerful programs and researches  sounds fun you  might find this useful gg.

[https://discord.gg/gpRBjHs](https://discord.gg/gpRBjHs)",254,NLP,0
"Will I get anything from this course or is it a sham? Did anyone take this course on Udemy, will I get anything from it or is it just another sham?

https://www.udemy.com/course/nlp-practitioner-neuro-linguistic-programming-online-course/",238,NLP,0
Best/favorite NLP techniques for creating more focus and drive? ,64,NLP,0
How To Understand Another Person's Perspective: The Definitive Guide ,69,NLP,0
New year tips? What do you think of this? Curious to hear what you guys think. ,79,NLP,0
"Voice rolling I just knew that there is a technique called voice rolling which you speak words at a rate similar to heartbeats. So what exactly is meant by *a rate similar to heartbeats*?

Say I want someone to relax. So I tell him: *Listening to the words ... I am speaking ... you seem to start ... feeling relaxed.*

Does it mean you say each cluster every time the heart beats?",381,NLP,0
"What could be the possible running time for latent Dirichlet allocation? I have a corpus of 45 million+ documents and I have asked LDA to give me 60 topics out of it? Average number of words per doc is 100. I have been training my LDA model for more than 6 hours now on both Kaggle (GUI) and my powerful gaming-oriented PC.

While you at it, please, if you know, let me know any python repository that has successfully implemented short-text topic modeling and has obtained decent results.",489,NLP,0
"What is NLP? I originally followed this sub thinking it was r/LanguageTechnology and kinda stuck around cus I was curious what exactly this is. After doing some googling, I found this on Wikipedia:

&gt;There is no [scientific evidence](https://en.wikipedia.org/wiki/Scientific_evidence) supporting the claims made by NLP advocates and it has been discredited as a [pseudoscience](https://en.wikipedia.org/wiki/Pseudoscience).[\[9\]](https://en.wikipedia.org/wiki/Neuro-linguistic_programming#cite_note-Thyer-9)[\[10\]](https://en.wikipedia.org/wiki/Neuro-linguistic_programming#cite_note-Sharpley_1987-10)[\[11\]](https://en.wikipedia.org/wiki/Neuro-linguistic_programming#cite_note-Witkowski_2010-11)  
&gt;  
&gt;Scientific reviews state that NLP is based on outdated metaphors of how the brain works that are inconsistent with current neurological theory and contain numerous factual errors.[\[12\]](https://en.wikipedia.org/wiki/Neuro-linguistic_programming#cite_note-Von_Bergen_1997-12)[\[13\]](https://en.wikipedia.org/wiki/Neuro-linguistic_programming#cite_note-Druckman_2004-13)

Also, the co-founder seems interesting:

&gt;In 1986, Corine Ann Christensen (December 8, 1954 – November 3, 1986), a former girlfriend of Bandler's friend and cocaine dealer, James Marino,[\[6\]](https://en.wikipedia.org/wiki/Richard_Bandler#cite_note-6) was shot dead in her [Santa Cruz](https://en.wikipedia.org/wiki/Santa_Cruz,_California) townhouse with a .357 magnum owned by Bandler. Authorities charged Bandler with her murder. Bandler testified that he had been at Christensen's house and was unable to stop James Marino from shooting Christensen in her face. After a long [deliberation](https://en.wikipedia.org/wiki/Deliberation), a jury found Bandler not guilty.[\[7\]](https://en.wikipedia.org/wiki/Richard_Bandler#cite_note-7)

But I always see people taking NLP very seriously on this sub.

Just wondering, what are your thoughts on this? I'm trying to better understand this community I happen to lurk on.

&amp;#x200B;

EDIT:

This stuff is nuts.",2052,NLP,0
"Can my NLP project be a good one ? Hi 

So i wan't to use NLP to feel better, and i had an idea, i can't really access good memories (c-PTSD that destroyed everything in my past, i just can't remember good things without being sad)

So the idea is use a nootropic called phenibut, it's really addictive, tolerance rise fast but if used once a week and no more it's ok.
Then with the effect of phenibut, super relaxing and music enhancer, i'll listen to a good korean traditional music that is really smooth. 
This music, i modify itand register a mentra with it (for exemple the mentra would be ""zen"" wich i would not be but just a' exemple)

So i'm on phenibut (happy and relax), listening to a super relaxing music with my voice on this music saying ""zen"" every 15 secondes or so.

Do you think that if i do that one time per week the word ""zen"" will be an effective anxiolytic ?",881,NLP,0
"Creating anchor with body language Guys Im new to NLP so wanna ask some questions.

Say I met random guy and talked with him for a few minutes. If I talk jokes and he laughs, does this mean we have enough rapport (at least, *temporarily*)?

Basically I want to establish a feeling of trust and reinforce it with anchor, so I anchor that feeling when talking jokes to some body gesture e.g. right hand making a C shape like what a previous president did. If I have enough rapport this should work well, but if *somebody* else makes that gesture too will they invoke that guy's feeling of trust?

Another question I have in mind is do you usually use embedded commands with anchor gestures? If I get him into laughter with a joke, that means I have enough rapport to use embedded commands?

Sorry for a bunch of messy questions, just wanna know more about nlp things.

\~a guy that is interested in whatever crazy stuff, interested in psychology now",947,NLP,0
"CLINICAL PTSD, AGORAPHOBIA, CONVERSION DISORDER | NLP &amp; HYPNOSIS TREATME... ",80,NLP,0
"Using NLP to become my best version and kicking out addictions Hello guys, 

This is my very first post here, I have been reading this subreddit for quite some time.  


My situation:

I have overcome lots of things in my life: gambling addiction(mostly sports betting and online poker) a house fire where I lost all my belongings and the loss of both my parents.

  
I have to say I have overcome most of my struggles by reading a lot.   
I have never dove deep into NLP before, yet I've always had an intrest.  
I bought some books around the topic but they were burned in the fire before I could read them.  


I want to use NLP with clear goals: overcoming addictions that are holding me back: my addiction to cannabis and tobacco and to stop watching porn.  


Where should I start. Is this possible with NLP or am I misunderstanding the concept.  


Thank you

Nikola Tesla",879,NLP,0
"How to overcome internal visuals of people’s dissent and imposter syndrome? I’ve gone far using NLP techniques, however these two things are really sabotaging my ability to go past a certain threshold in what I do.
I don’t know why I see the faces of people’s dissent or disapproval when I am doing an extraordinary skill.  I have a feeling it’s likely because I won’t feel legitimate without approval, or I am being viewed as faking it, and it screws up my performance.
Imposter syndrome, not entirely related, however it also comes into play when in making great strides at something.
Please let me know your recommendations, and thanks.",639,NLP,0
"Can one of the natural language people in here help us build a bot to send the NLP-programmers to a better place for their answers? So... I'm not a programmer. I'm an NLP coach and an engineer, but I work with steel and people and factories and such. I could get into programming, however I don't want to. 

I do know enough though to notice that building a bot is not immensely complicated if you know what you're doing. So.... Did you stumble upon this sub assuming it's about natural language processing? And do you know more about coding than most of the NLP coaches in here? Then it'll probably be a LOT easier for you to build a bot that triggers on words a lot more associated with natural language processing! 

Maybe with a message like... ""Hey, did you try to find an answer that has something to do with natural language processing? Well, then this is not the right sub. This sub is for neurolinguistic programming, the human psychology kind. So... even though you're very welcome here, you won't find your answers here. Why not try r/naturallanguage? And feel free to return with any neurolinguistic programming questions you may have!""

Let me summarize all the good that you'd be doing here:

* You'd help your programmer peers find an actual answer to their question
* You'd help us coaches and NLP interessees a lot more sane
* You'd be our hero
* If you haven't yet, you'd find out exactly how to build a bot on Reddit.

Soooo anyone up for this good deed?",1473,NLP,0
"how to ngram decode a sentence?[Discussion] when using a n-gram model decode a sentence? I want to know the process of n-gram decode

Is there any code to reference?",165,NLP,0
"can I use transformer for name-entity-recognition? [Discussion] I want use transformer for name-entity-recognition

the training encoder input ""Lily want to company""

the training  decoder input ""Person O O Location""

can transformer network do this NER task?

NOTE:I want just use all transformer nerual network(include encoder portion and decoder portion),NOT bert",366,NLP,0
"General approach to work with text of phone calls (topics, promises, sentiments, etc.) I have an NLP task. There is a text (telephone conversations). Voice is already converted into text and is divided into agent and customer paragraphs. I need to understand what approach is the best one for the  next tasks:

&amp;#x200B;

1. Who is the customer and who is the agent?
2. Customer Name
3. The topic of conversation
4. Promises made by the operator to the customer (for example, ""I call back tomorrow"")
5. Negative Sentiment (if there is something in the conversation that the subscriber is not happy with)

I am just trying to understand how to handle it. Is it possible to create some kind of general approach for this? If yes, for which packages/publications/books could I pay my attention?",793,NLP,0
"How to overcome fear? Which NLP methods do you use to overcome fear? 

\- dynamic spin release

\- fast phobia 

.., 

What else does come into your mind?",154,NLP,0
"Which NLP training can you recommend? I have completed NLP trainings in Germany being certified as NLP Master Coach DVNLP and IN and I was wondering, if there are any international trainings I could complete in English because I kinda feel like I could learn a lot more. Do you have any suggestions of international training institutes for me?",343,NLP,0
"I am sick of having the conversation what is NLP? NLP is fake. NLP is manipulative. So, I wrote an article on what NLP is. [What is Neuro-lingusitic Programming](https://jamespesch.com/what-is-neuro-linguistic-programming-nlp/)

If you downvote or upvote, please tell me why! So, I can get better and improve. 

Thank you!!",323,NLP,0
"Awaken The Giant Within I just listened to Tony Robbins - Awaken the giant within. For the second time in my life. Once years ago. But didn't have the life experience to make any change for me. Now it feels like something I really needed to hear!

I'm starting to see the how the power of decisions and beliefs are impacting my life

Any other books/resources you think are worth a listen?",389,NLP,0
"Getting ""caught"" using NLP?! Sorry
Mobile
German

Hey people.
I know techniques like 'mirroring' and touching people to build rapport but I hesitate to use them.

1. I don't like it when coworkers touch me because I know that it subconsciously makes me like them and I feel manipulated.
2. I don't want to manipulate people. I want them to like me because of my personality.
3. I don't want to get ""caught"" using NLP. Especially in a group I feel like people would notice my behaviour. 

What do you think? Do you use it to make friends or only in business?
Often when I am focusing on a person I like I start mirroring them without any intention. Walking in a group it also happens that I walk in lockstep with the person I talk to/ think about.
Has anyone experiences like this?

Thanks:) &lt;3


Err hmm. Smileys? Are smileys a NLP technique? AM I MANIPULATING YOU
I don't know.
I should sleep, it is 5.57am.",911,NLP,0
"What are some useful NLP strategies to apply while having a conversation to learn more about the other person? NLP strategies  &amp; tools that are  easily applicable in everyday life that provide useful insights to help us better understand the people we engage with. 

Stuff that is easily understood by anyone, even if they are unfamiliar with NLP teachings, that can be immediately demonstrated through real-life examples.",426,NLP,0
"How long does an anchor ""stay active"" when the daily programming is stoped For exemple if i anchor a zen feeling 1 time a day for two months, with an orcrux (a ring for exemple ?), How long did the anchor stay usefull if i stop programming it everyday ?",253,NLP,0
"Anchoring a Dream Last week I had a dream with an important message for me. I dreamed I was talking with a famous motivational speaker and it was kind of boring. Afterwards - and I totally didn't expect this - his people gave me an invoice for $9500! It made me really angry. 

The dream made me realise that I am on the clock. My time is valuable. I want to keep the urgency of this dream alive. 

I've created an anchor. When I make a fist with my left hand I think about the dream and the intensely motivational state that it brought about. Now when I close my hand I feel some intensity and have an internal dialogue about costs and regret, but it lacks clarity. 

How do I get clarity? How do I pinpoint the message? I want to be able to trigger the anchor and get the realisation immediately, without having to debate it or think it over. Maybe I could stack all the emotion of the experience onto the words ""regret"" and ""do it now."" 

What do you think? Let me know your thoughts. Thanks.",995,NLP,0
I’ve been conducting Intro to NLP Sales Training for a local company &amp; I thought I’d share this past week’s workshop. ,122,NLP,0
What are your favorite NLP podcasts? ,37,NLP,0
What's a good (preferably audio) book that you would recommend for a beginner? I'm interested in learning more and would love an audio program that can perhaps give me insight into the basics?,192,NLP,0
Anyone know techniques to stop my mind from the constant negative chatter? My mind races at a thousand thoughts a second I swear. I just need some help,151,NLP,0
"Opinion on Hypnotherapist/NLP practitioner Hello,

I’m interested in undergoing some NLP therapy. I live in Las Vegas and came across Serena Denmark. She is certified in the National Guild of Hypnosis and National Federation of NLP. 

Are these certification legit? Anyone mind taking a quick look at her or anyone else in Las Vegas to make sure I make a smart decision here? Very excited to try it out.",403,NLP,0
Is there any scientifically proven aspect of NLP? I’m new to this sub Reddit and very interested in NLP. But I haven’t seen much about the scientific basis.,156,NLP,0
A shedload of NLP and related stuff for free download. ,55,NLP,0
NLP Basics ,11,NLP,0
"Meditator with questions Hello,

I'm a long time meditator who is interested in adding NLP to my program. It has been very hard to separate the wheat from the chaff in this domain. Could anyone give me some suggestions on what system/method would go best with an advanced meditation practice. I should add that I have no interest in getting certified or coaching NLP myself and I'd prefer to hear objective experiences from people who aren't invested financially in a particular system. So a few questions:

1. What are the most popular NLP systems?
2. What books are essential?
3. Are there objective resources for finding a good practictioner?
4. what are systems to avoid?
5. are certain systems geared towards certain goals like finances, spiritual development?
6. Lastly, what are peoples success stories with using NLP, which system did you use.

Thank you in advance for your responses?",893,NLP,0
Richard Bandler Interview (September 2019) ,43,NLP,0
NLP Works: How I used NLP to change my Life! ,45,NLP,0
NLP Works! How I conquered my lifelong fear of spiders in just one hour ,72,NLP,0
"Anchoring: What it is, Why it works, How to do it ",50,NLP,0
Any examples of NLP well-formed outcome? I'm new to NLP and want to set a goal. I need some example for reference so I can set some goals for myself to try enhance my life quality,179,NLP,0
"How to find my interests in NLP? I am freshman in NLP research, how to find a intersting topic in NLP? or Could you tell me some interesting topic?",147,NLP,0
"Can I use the swish pattern (or how to use NLP) to create new habits? If you don't know the swish pattern, I recommend this video: https://youtu.be/um9OiPaKuqI

Hi, I've known and read about NLP for sometime but never really practiced it. So I'm starting to do it now in my life. I'm having trouble with this anchoring/trigger thing.

I want to create new habits in my life, but I don't know which NLP technique to use to do it (again, I'm new to this stuff, so certainly I don't know many of the NLP techniques available). 

Tried many Google on trying to find a NLP technique to create new habits, but only found results on breaking bad habits and making new, good ones, using the swish pattern. The thing is: some habits I want to do, doesn't necessarily replace any bad habits I'm currently doing.

For example: I want to make a breathing exercise that I only have time available to do during my commute to work. So, I want to create this new habit. Generally what I do in the commute to work is reading books or watch video tutorials, which are good habits that I don't want to stop doing. I need to create a trigger or an anchor to make this new habit and remind me to do it everyday and I need to feel good about doing it. I've tried alarms in my cellphone but they didn't quite work, because I'm not always at the same time in my commute to work everyday. I need to do it in a certain place. 

There's also this technique of comparing and modifying what you want to do, to something you would like to do. But I don't know if this is ""permanent"". The swish pattern has this thing that creates an trigger that makes things automatically and you've permanently made that habit stick (which worked really fine for me).

So can you use the swish pattern to make new habits (use the ""bad image"" as the image of you not doing the habit you want to do versus the image of you doing it) or there's a NLP technique to creating new habits?",1936,NLP,0
"Is changing your interpretation of things by manipulating them on your mind permanent? I know that you can use this to basically change anything: fears, habits, desires, etc.

Taking fear as an example: you're afraid of something. You take that something and manipulate that in your mind to not be afraid of it (make it smaller, weaker, blurry, black and white and maybe faint away in an eternal black empty space).

Will you fear go away forever or does this technique make this only temporary as an state of mind? Can your unconscious bring it back? If so, how can you prevent it? Is there an NLP technique that makes this changes permanent?",643,NLP,0
"New NLP Streaming Service from The Society of NLP/Richard Bandler!  [https://www.nlpeternal.com/](https://www.nlpeternal.com/) 

Watch and listen to the entire collection of seminars, hypnotic inductions and hypnotic music from Dr. Richard Bandler and John La Valle. This is a legit site created by the Pure NLP team. Those CD's/DVD normally go for a few hundred dollars a piece but now you can access them all for 199 a year!",426,NLP,0
"John Grinder &amp; NLP Meta Model, the logic of Modal Operators ",64,NLP,0
Fear &amp; Stress of Paying Bills Therapy (Part 1) ,51,NLP,0
How to counter NLP when used for manipulative purposes Hello all I was writing to see if anyone had any good tips or articles that cover how to counter NLP when used for the purposes of manipulation? I sometimes see videos on Youtube where I believe NLP is being used to circumvent rational thinking and manipulate people into sales or certain religious beliefs.,362,NLP,0
"This is the sub for NEURO LINGUISTIC PROGRAMMING. For natural language processing, please go to r/LanguageTechnology. All natural language processing posts will be removed. r/LanguageTechnology",193,NLP,0
"embedded commands and contrarian attitudes Bit of a shower thought here.

If you know of someone who seems to automatically take the opposite or find fault in anything you say or solution you find (I'm talking may have long standing deep seated emotional and or anxiety issues about you, sometimes to ridiculous levels) then...

..could it be possible that in using embedded commands their subconscious automatically chooses the opposite to what is being suggested (commanded) ?

For a basic example.

They kind of want to go outside.

I embed command in conversation to  go  outside  now. (thinking this will be simple).

Now they will not go outside. I know they want to but it feels like they are having some external struggle and will not go .

Would reverse psychology work here?

Just something I found of interest. What do you guys think?

&amp;#x200B;

&amp;#x200B;

EDIT - sorry for all the edits.

Did you know new reddit removes additional spaces between words? hmmm.",978,NLP,0
"This is the sub for NEURO LINGUISTIC PROGRAMMING. For natural language processing, please go to r\naturallanguage. Any content unrelated to Neuro Linguistic Programming will be removed. ",186,NLP,0
"Is there any internationally accpeted certificate for NLP? Hi,

I am studying NLP for a while, and I become very interested since I see the result of that in my life.

I was wondering ig do you guys know is there any online cource that I can take a certificate to become a Certified NLP Therapist?

&amp;#x200B;

Thanks",319,NLP,0
How to teach NLP Part 4 ,24,NLP,0
How to teach NLP Part 2 ,24,NLP,0
"Anchor trauma The pain of the happy moments and life I once had, that I no longer have, are a huge source of pain to me.

Reading this NLP book where it asks to recall happy memories/good feelings for the anchoring technique. The crazy thing is, I actually can recall quite a few, I have had some utterly happy moments in childhood. 

But recalling them, while pleasant, causes me to break down uncontrollably into grief. Grief that those happy moments are a memory.. and a reminder that it has truly been possibly almost two decades since I’ve felt truly happy at my core.",573,NLP,0
Anxiety Relief ( 3 x Hours Guided Meditation) ,46,NLP,0
The Fastest Treatment for Anxiety (Guided Mediation) ,53,NLP,0
Therapy Guided Mediation Compilation 1 x Hour ,46,NLP,0
NLP Japan Does anyone know if there are any NLP practitioner trainings in Japan in English?,91,NLP,0
The Best Addiction Rehab in the UK (New Leaf Programme) ,56,NLP,0
The Full NLP Practitioner Training (ATNLP Practitioners Only ) ,63,NLP,0
HOW TO CURE FEAR &amp; PHOBIAS | NLP | REMOVE FEAR | CURE PHOBIA ,65,NLP,0
How much have you spent on NLP so far? ,39,NLP,0
"""NO CLIENTS &amp; NO SALES"" (Try this Technique) ",49,NLP,0
"How do I learn someone's projection map or value/thinking system? In the case of rapport building or attraction building one must use their ""filters"" in order to successfully match. How can one know properly what their filters are? Without being too direct or too obvious about asking it.",288,NLP,0
I FEEL NOT GOOD ENOUGH (Try This) ,34,NLP,0
Well-Formed Outcomes | The First &amp; MOST Important Step for Practicing NLP ,78,NLP,0
"Pacing &amp; Leading | NLP | Sales, Personal, &amp; Professional Skill for Influencing ",87,NLP,0
PTSD &amp; PAST TRAUMA | EMDR | NLP | HAVENING TECHNIQUE | LIFE COACH AND LI... ,80,NLP,0
MIRRORING &amp; MATCHING NLP | MINDFULNESS | NEURO-LINGUISTIC PROGRAMMING SE... ,80,NLP,0
NLP Trainers Training Lesson Plans Part 4 ,42,NLP,0
"It occurred to me that ""A Christmas Carol"" is a story that uses timeline therapy for healing. Just throwing that out there...",125,NLP,0
NLP Trainers Training Part 3 ,29,NLP,0
What is Hypnosis/NLP Anyway?! ,30,NLP,0
Overcome Jealousy ,18,NLP,0
"How do i begin Im fascinated by NLP because i read the secrets to speed seduction by Ross Jeffreys. Problem is i dont know how to apply these methods for example anchoring and using embedded commands. Is there a specific book which can help me apply these techniques, if there is please tell me because i dont know where to begin.",330,NLP,0
A Refresher on Sensory Modalities (minus Auditory Digital) ,59,NLP,0
Stutter Cure using HMS Therapy ,31,NLP,0
"‎Champions Of Mind on Apple Podcasts Champions Of Mind - Episode 224 - 30 Years Of Business Lessons From The Godfather of NLP 

Welcome to the Champions Of Mind podcast - game changing content from the UK’s leading success, mindset and motivational coaches Llewellyn and Rhys Davies and James Burtt.

This week we had the privilege to have the Godfather of NLP here in Europe, David Shephard, on the show to share his three decades of business experience. There are not many people who have more expertise in the professional personal development space so this was an insightful chat about changes in business, systems of success and how the fundamentals of business still apply when starting a brand. David shared how he ended up as a Master Trainer, how he grew his business by cold called his way to gaining clients and how he delivered free training sessions to small teams to grow his skillset and reputation. We talk about value-based sales so it was great to hear that this approach has been the best way to build a sustainable business for 30 years!

[https://podcasts.apple.com/gb/podcast/30-years-of-business-lessons-with-the-godfather-of-nlp/id1241811346?i=1000451987175](https://podcasts.apple.com/gb/podcast/30-years-of-business-lessons-with-the-godfather-of-nlp/id1241811346?i=1000451987175)",1305,NLP,0
Thomas Bjorge's guide to the 3rd Position ,42,NLP,0
"Udemy NLP Course - Am I the Only One Not Enjoying This? I'm taking this course on anchoring and I think I need some other sources for comparison. He doesn't spend a lot of time showing how to do anchoring, maybe a few seconds. Anyone else took this course? what's your opinion? Are there any books on anchoring that you can recommend? I have a lot of NLP books already but none specifically on anchoring.",404,NLP,0
"Everyday NLP I use NLP in one form or another with clients all the time. Typically the reverse spin, submodality shifts, Core Transformation, sometimes Swish, Visual Squash, presuppositions, and the like, not to mention language patterns and the Meta Pattern or Coaching Model. However, I don't necessarily do it all the time in day-to-day conversations or interactions.  


So my question is two-fold.  


1. How do you use NLP in day-to-day interactions?
2. What recommendations to you have for taking NLP and running with it in the real world outside of the office.

I suppose this is by nature a more ""how to do covert NLP"" thread, but I'm not trying to covertly hypnotize people into some ""black hat"" sort of stuff. For me this is more about letting my ability to communicate with NLP and hypnotic operators spread throughout my daily life and not think of it in quite so rigidly defined terms. I would also like to be able to use NLP processes/operators more conversationally with clients, without the sometimes rigid confines of, say, the Visual Squash. I know you can do it conversationally, but that's something I would like to practice before bringing it into client sessions. This thread is perhaps a way to motivate me to do so, while also brainstorming ideas on how to do so without necessarily doing covert change work on the cashier at the Kwik-E-Mart.",1367,NLP,0
"Erotic NLP? Erotic hypnosis gets talked about a lot, and I'm aware of anchoring arousal or orgasmic responses (during the old hypnosis forum days, I recall somebody anchoring orgasms to chocolates with a willing participant). But are there any resources for erotic NLP, without just incorporating NLP techniques or the like into a formal hypnotic trance session? I feel like it's certainly possible, but it's also a realm I have not explored without formal hypnosis. If there's a book or resource worth checking out, please let me know.",536,NLP,0
WHITE SHIRTS ONLY (NLP PERCEPTION TEST ) ,41,NLP,0
"""The Ultimate Introduction to NLP"" by Alessio Roberti, Owen Fitzpatrick, Richard Bandler Book Review ",101,NLP,0
"Anyone have any of the older books by Richard Bandler and/or John Grinder and looking to get rid of them for cheap? I'm searching for cheap old copies of the earlier works by Richard Bandler and John Grinder, specifically ""The Structure of Magic,"" ""Reframing,"" ""Frogs Into Princes,"" ""Use Your Brain for a Change,"" and ""Insiders Guide to Submodalities."" Although I'm open to anything!",383,NLP,0
"Sleight of Mouth I aware this may be a shot in the dark, seeing as not too many people know/talk about it, but is anyone else studying SOM? It would be nice to discuss NLP &amp; SOM with someone without them looking at me like a confused puppy😂",244,NLP,0
5 NLP Sports Coaching Questions (Clifford Stark) ,49,NLP,0
The 12 Rules of Life Mastery ( Master Practitioner Level ) ,59,NLP,0
Presuppositions Of NLP 2.0 Episode 4 - event link in comment ,61,NLP,0
How to resolve a conflict in 5 mins ,36,NLP,0
"NLP with Aphantasia ( I can't create mental pictures) Hello NLP community :)

I am new to this Reddit space and really need help on starting an NLP practice. I am reading up on NLP and I feel like some of the techniques, particularly the skillset of emotional anchoring could have a real positive impact on my life. There is just one fundamental issue that has become a mental block in my progress.

I have what has become known as Aphantasia, in simple terms I cannot create mental images. When I try to create mental pictures I either see nothing or grasp at a fading image that quickly goes black.

Has anyone in this community had positive experiences with NLP despite having similar handicaps ?

Can anyone offer me any tips on how to proceed in spite of this mental limitation?

I welcome all advice, experiences and any NLP tips in general in terms of how to get started. Thank you so much in advance. I am grateful that this resource is available :)",957,NLP,0
NLP as alternative treatment for schizophrenia. Would NLP help cope with the auditory and visual hallucinations experiences by those with schizophrenia?,152,NLP,0
Presuppositions of NLP2 0 Episode 3 ,36,NLP,0
"A small, a bit silent community ... As I see during my reddit time (not long though)  I feel like this community has the potential to be something more and to grow. I will start off by just creating threads for discussions on topics and questions I found and still find challenging and enlightening.

So first one for today would be - What is it that makes attitude different from person to person? 

I encourage you all to think about it and post a reply. I believe we can gain incredible insights by learning from other points of view!

Best of,",547,NLP,0
The Presuppositions Of NLP2.0 - Episode 2 ,42,NLP,0
"The secret codes Anyone read the secret codes by alaa alsadi? My friend has sent it to me as an intro to nlp. The author seems to be very incessive on reading each chapter repeatedly to ""inject it"". Is it worth the time?",220,NLP,0
The Presuppositions Of NLP 2.0 ,31,NLP,0
Decently priced weekend trainings/classes? I don’t like online learning btw. ,77,NLP,0
"Negative Self Talk * What it is, isn't, and how to Shift ",57,NLP,0
"Approach anxiety and general attraction Hi all, just wondering if any of you have used NLP to resolve approach anxiety?",119,NLP,0
"Anchoring myself I want to put an anchor on myself to help me overcome my drinking. 
 The first question is that possible 
And the second question is how to I go about doing that.",179,NLP,0
Should I learn hypnosis or nlp first? ,38,NLP,0
"Fantastic testimony to the power of NLP and Time Line Therapy(R) If you have ever wondered if NLP and Time Line Therapy really work then read this wonderful article written by one of my students.

[https://thriveglobal.com/stories/life-defining-change-why-i-will-always-have-gratitude-for-david-shephard/](https://thriveglobal.com/stories/life-defining-change-why-i-will-always-have-gratitude-for-david-shephard/)",413,NLP,0
"Anchoring help Hello,

Been anchoring for a couple years now to break my states of despair, anxiety etc. Usually it works but last week I was in a real depressive state and It didn't work well. Now I feel like Ive tainted my anchor and not sure what to do? Should I try setting a new one?",288,NLP,0
NLP Trainers Training ,22,NLP,0
"Aren't you bothered by the posts about Natural Language Processing? Why doesn't this sub have a description that says ""NEURO-LINGUISTIC PROGRAMMING"", and/or a pinned post from the mods to alert newcomers this IS NOT a subreddit about [software] programming? 

Just skimming through the hot page, you have about 30% of posts that don't belong to this subreddit. I came on reddit to search answers about machine learning, ended up here, and the first things I see include a meme about BERT and some academic paper. Other post titles can be ambiguous, and totally pass as computer science jargon (embedded commands). In all this, the subreddit rules were the last thing to catch my attention.

So, are you guys worried \*at all\* this sub might become completely flooded or even overruled? From your reactions, it quite looks like the opposite tbh. You guys definitely aren't tired yet of saying ""wrong sub"".

edit: so, Reddit's redesign is responsible for the description being hidden...",985,NLP,0
NLP TRAINERS TRAINING TIPS ,27,NLP,0
The Presuppositions of NLP 2.0 Live ,36,NLP,0
11 rules that change my life ,29,NLP,0
Advice You Didn’t Know You Needed To Hear ,42,NLP,0
"Most comprehensive NLP book? I'm looking for an NLP book that is for intermediate-advanced practioners. I've been reading NLP for quite a while, some articles, texts and etc. Is there a book that has more advanced/intermediate techniques, like more embedded commands, more contexts, more models and etc?",303,NLP,0
"Is self hypnosis effective? Does self hypnosis actually work? I've been trying to change some habits and decrease a anxiety feeling that I have for no reason (I've haven't got any negative thoughts) so I thought I would give this a go, but before I do I would like to ask for your advice. Thank you in advance.",310,NLP,0
Intro books What books should someone who knows practically nothing about nlp read?,83,NLP,0
NLP Metaprograms - Episode 21 ,30,NLP,0
Fear of Cancer Therapy ,23,NLP,0
Transforming self sabotage into success ,40,NLP,0
Belief Change - what to do if you can't elicit a picture I'm trying to do a belief change and I can't bring up a picture for either the old belief or the new belief.  Do you have any tips/tricks to help with this ?  It's the first time I've come across this.,258,NLP,0
"noob so Im trying to find out whether or not nlp is BS or not. I read that some parts are, all of it is BS, It cant be clearly defined therefore anybody can claim something is nlp despite it having anything to do with the topic and all this other contradictory info. im getting fairly annoyed and I just want to know if its something I should pursue. What are your thoughts on this?

p.s. where should I start and how can I avoid the bs?",437,NLP,0
Ericksonian Hypnosis book up for grabs - selling off more of my way-too-large library of NLP &amp; Hypnosis stuff ,114,NLP,0
A Date With Excellence - Live From David's House - webinar link in comments ,76,NLP,0
Being Pushed Out Therapy ,25,NLP,0
"Mirroring and matching question So, I understand how to do mirroring and matching (in theory, I need to practise it practically) but one thing I can’t find online is, would wearing similar clothes to the person or having a similar hair style work too? For example, if we both wore a shirt, not an identical shirt, or both wore a similar kind of shoe, etc. 

Or am I kind of misunderstand Mirroring and Matching? 

Thanks. ✌️",424,NLP,0
"Some NLP Books Light on the Jargon? I've posted in the sub before about how much I've appreciated Core Transformations, an NLP-based book that teaches a fairly simple, adaptive technique for helping to guide others (and oneself) to states of epiphany and transformation. Good stuff! I want more like that. 

I mean no offense by this, but I'm looking to learn some more conversational or intuitive ideas of how language influences our inner states, without getting too into the weeds of NLP jargon. I mean, I've worked with healers, shamans, and practitioners of many kinds who are very transformative with their language and have no particular Linguistic theory training. Somehow they just know (or have learned) how to navigate the psyche with body language and spoken language.

Do any books or methods come to mind along these lines?",837,NLP,0
"How do you trick yourself to become naturally confident? When it comes to girls, I know what to do, I know what to say, I know how to act it all out, I know that I can be witty. But when the opportunity presents itself, when the challenge is there, it would seem like that I'm restricting myself, not really letting my true emotions flow.

How can I trick myself to become confident and be able to talk/lead the way I would normally want to and not be restricted by my self?",474,NLP,0
"“The Rainbow Machine – Tales from a Neurolinguist’s Journal” by Andrew T. Austin for less than 1 pound for a limited time only. “**The Rainbow Machine – Tales from a Neurolinguist’s Journal**” by Andrew T. Austin for less than 1 pound for a limited time only.

The eBook is available only in English.

USA  
[https://www.amazon.com/dp/B07TY5NHVF](https://www.amazon.com/dp/B07TY5NHVF)

UK  
[https://www.amazon.co.uk/dp/B07TY5NHVF](https://www.amazon.co.uk/dp/B07TY5NHVF)

Netherlands  
[https://www.amazon.nl/dp/B07TY5NHVF](https://www.amazon.nl/dp/B07TY5NHVF)

Germany  
[https://www.amazon.de/dp/B07TY5NHVF](https://www.amazon.de/dp/B07TY5NHVF)

India  
[https://www.amazon.in/dp/B07TY5NHVF](https://www.amazon.in/dp/B07TY5NHVF)

Japan  
[https://www.amazon.co.jp/dp/B07TY5NHVF](https://www.amazon.co.jp/dp/B07TY5NHVF)

Australia  
[https://www.amazon.com.au/dp/B07TY5NHVF](https://www.amazon.com.au/dp/B07TY5NHVF)

Canada  
[https://www.amazon.ca/dp/B07TY5NHVF](https://www.amazon.ca/dp/B07TY5NHVF)

Mexico  
[https://www.amazon.com.mx/dp/B07TY5NHVF](https://www.amazon.com.mx/dp/B07TY5NHVF)

Brazil  
[https://www.amazon.com.br/dp/B07TY5NHVF](https://www.amazon.com.br/dp/B07TY5NHVF)

Italian  
[https://www.amazon.it/dp/B07TY5NHVF](https://www.amazon.it/dp/B07TY5NHVF)

Spain  
[https://www.amazon.es/dp/B07TY5NHVF](https://www.amazon.es/dp/B07TY5NHVF)

France  
[https://www.amazon.fr/dp/B07TY5NHVF](https://www.amazon.fr/dp/B07TY5NHVF)",1443,NLP,0
"Looking for real life NLP Success stories I am a blogger, looking to write a piece on real life experiences practicing NLP.    If anyone is interested in sharing their experience, I would love to hear back from you.   I simply am interested in knowing:

1.  what was the most significant change that took place?  and for how long has change lasted?
2. which technique (s) were the most beneficial for you?  which were not? 
3. do you do this by yourself? (i.e. a workbook) or with an nlp professional?
4. would you recommend it to a friend?
5. any thing else you care to say  

thanks!",585,NLP,0
Advanced books suggestions Do you know some books that explore more advanced techniques in NLP?,95,NLP,0
"Please interpret “I don't need to or want to give suggestions like you are drawn to me and think about me all the time.”

Is this a suggestion TO be drawn to and think about this person? Is awareness of the suggestion enough for me to be able to reject it?",256,NLP,0
There should be a HUGE banner saying Neuro Linguistic Programming to lessen the confusion with natural language processing. Please upvote so the mods see it. ,158,NLP,0
Why does Wikipedia say NLP is not scientific? What’s some important concepts on Neuro Linguistic Programming one should know ? ,127,NLP,0
Rare Richard Bandler Video Set - if anyone's interested. More info in comment below. ,85,NLP,0
Robert Lifton on destructive cults ,35,NLP,0
NLP for becoming open/stopping limiting beliefs to romantic relationships ,74,NLP,0
"Loving Core Transformations... other recommendations? Greetings! I've really enjoyed Core Transformations by Connirae Andreas and I'm looking to expand what I've learned there. I've practiced some TimeLine and Visual Squash and other NLP techniques with practitioners but Core Transformations really hits the spot for me. It almost always helps to bring me down into deeply ""felt"" or embodied states which seem to have more profound impact than just mental visualizations or intellectually satisfying ah-ha moments. Hard to believe, but the process does often feel darn-near spiritual at times.

Are there some other books out there where I might explore other NLP techniques that explain how to structure words in ways that help to guide people to accessing deeply felt, personally-intuitive states? I'm especially keen on the idea that we are our own best healers, so I'd like to keep learning techniques that help people discover their own resources through questions (rather than techniques where the practitioner is doing more directing than guiding). Thanks in advance!",1075,NLP,0
"Help with the correct and complete MP3 of Tape 7 of the following  

## Richard Bandler’s Introduction to DHE 

  Recorded live in Chicago in 1998. 

&amp;#x200B;

You can find the set floating around on the net BUT the Tape 7 is either incomplete or is not Tape 7 at all but is just Tape 4 relabeled. I used to have the whole correct thing, all 12, years ago but lost it due to a hard drive crash.

&amp;#x200B;

Thanks!",421,NLP,0
"NLP 2.0 I discovered NLP in 1990, loved it and by 1993 decided to make it my business and career. For me it has always been about modelling excellence, R &amp; D, progress and sharing what I have discovered. That's why after 26 years I have upgraded my programs again. Please take a look and let me know what you think. After all feedback is the breakfast of champions! ;-)  
[https://go.performancepartnership.com/upcoming-courses](https://go.performancepartnership.com/upcoming-courses)

https://preview.redd.it/cdoa7c3kcs231.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=ba76c5c3a65a8770ffbd53066707036f6f72fef6",619,NLP,0
[INTERVIEW] How To Solve The Biggest Problem For All NLP Coaches ,65,NLP,0
🔥 [INTERVIEW] How to Achieve Lasting and Positive Change ,57,NLP,0
How To Speak Like A Leader In A Meeting ,40,NLP,0
"I've heard mixed Reviews of the efficacy/realness of NLP... If you had to recommend one single book to read first, to understand more about NLP, preferably backed with evidence/studies, what would it be?",203,NLP,0
"Techniques Is there some NPL techniques I can use on myself, -- even though I don't have access to formal training on NLP, -- to improve my self-esteem? Can the Dickens Pattern be used to improve my self-esteem, or cure procrastination? I am against those occultic things like affirmations, or trying to visualize yourself with high self-esteem, trying to create that reality with your imagination (with the Dickens Pattern you don't try to create a reality, you just imagine a possibility). But I guess such things  aren't NLP.",528,NLP,0
"NLP Material I have had 1 book on this top, how to take charge of your life (user's guide to NLP) by Richard Bandler. Fascinating book. I like how it was written. Could anyone here on this sub please recommend to me more books on the topic that you find fascinating or useful. I appreciate it. thank you.",304,NLP,0
NLP Metaprograms - Episode 20 - Attention Direction ,52,NLP,0
"books on NLP Hi, I’m very new to the concept of NLP and wanted to learn more. Are there any good books to get started with?",123,NLP,0
"What's the best technique to tackle flying phobia rooted in lack of control? Hi all,  


I'm dealing with late onset of flying phobia, which is based on the lack of control. I'm trying to find a proper technique to tackle it.   


The irony is that I've helped some friends get over their claustrophobia-rooted fear of flying by using the swish technique, but I've not been really successful in using it on myself.  


Any suggestions are welcomed.",448,NLP,0
"Did my therapist try anchoring in my session? Hello there,

this might be a weird question but in hope that someone can help me I ask it anyway. I'm in psychoanalysis and my analyst is also a hynotherapist. We don't do hypnotherapy, we do analysis.

My analyst has a bell in his treatment room and I've always wondered what that is for. I believed it was for hypnotherapy. Anyway. So today I had a session and right before he walked to his armchair to start the session he rang that bell.  I couldn't see it, just hear it. So I thought he must've moved the bell by accident. But then just right after our session when he got up from his armchair he walked straight to that bell and rang it again.

I'm a bit confused now why he did that. Was it some kind of anchoring? What for? For me? For himself? I'm a bit confused and as paranoid as I am I'm afraid he's trying to do something covertly and manipulate me.",909,NLP,0
"advice on NLP trainings? my situation is different than most, so i'll lay it out.  i first got into NLP in 2009.  it helped me with modelling &amp; calibration, but due to severe anxiety i couldn't interact with people enough to apply it.  i had been into energy work to deal with the anxiety, and i kept with that up to now.  now i might be making enough progress that an NLP training will actually be worth it.  

but from what i've seen over the years, NLP has become pretty badly diluted.  it's real stuff and it *can* work, but there are a lot of certified people who aren't very good.  if i was going to look for a training, i would approach it from a  skeptical standpoint; they would have to show to me that they're what i'm looking for.

the main reason to do it would be to get better at cold approaches, networking, interacting with people.  I'm a massage therapist, but stuck as an employee because i haven't been able to do any networking or sell myself.  I have the potential to be a really good life coach, but again i have to be able to approach people before I can do that.  And with anything NLP-related, you're in business for yourself.  There are a few directories, but for the most part you have to win over all your clients, they don't just come to you as if you were employed somewhere.

so NLP training may or may not be a good next step; that's why i'm posting here about it. I'm continuing with the energy work, as that's been essential to get me to where i am now, and i may need to stick with that for a while before doing NLP training.  but i've made enough progress that it's at least worth looking into now.  any thoughts?",1653,NLP,0
"eye accessing cues- are there any besides the typical ones? we all know up-right = visual creative, up-left = visual recall, etc, etc, etc, and it's flipped for 10% of people.... but are there others?  the chart looks incomplete, there's nothing for straight up or down.  and often i'll see peoples' eyes go in between, so they're not on an even diagonal.  are there any more in between the typical ones?  and are there any that are completely different?  our brains have more processing centers than just the 6 things on the chart.",532,NLP,0
NLP Metaprograms - Episode 19 - Modal Operator Sequence ,56,NLP,0
"Need name of floor exercise used to change a limiting belief. As part of a coaching education in non-English, we're taught a few floor exercises with a definite NLP streak to them, and I'd like to find some demo videos of two different techniques so that I can study them in my own time. I've been looking for a few hours, but I can't find videos of the exercises I'm looking for; and so I come to you all for help.

&amp;#x200B;

**Exercise 1** is a timeline exercise where the client defines a timeline on the floor and steps unto a scenario that will most likely happen in the future; a scenario where the client will need to draw from certain resources from the past in order to best handle that future situation.

Roughly speaking the exercise goes like this:

First the client is VAK'ed in the future scenario and the coach observes reactions. Then the client identifies a couple of points in their past where they had those exact resources. In chronological, backwards, order the client is VAK'ed and submodalities tinkered with to enhance the experience, and the client is anchored in turn for every past scenario.

&amp;#x200B;

Finally the client is brought back to the point in the timeline where the client needs those past resources, the anchor is used, and the coach and client observe how the attitude and bodily reactions have changed towards the future scenario.

1. What is the name of that exercise
2. Where can I find video instructions so I can re-watch this as many times as I need?

&amp;#x200B;

**Exercise** 2 uses pieces of paper.

One piece of paper has a limiting belief written on it. The client steps unto the paper and recalls a situation where that limiting belief was evident.

Then the client identifies what other beliefs are needed to change the limiting belief, and those beneficial beliefs are written on new pieces of paper that are placed in a circle around the center piece of paper (the one with the limiting belief on it).

Then in turn the client is VAK'ed and anchored on the supporting beliefs in turn.  After every anchor the coach asks something along the lines of: ""what has already changed about your limiting belief?""

&amp;#x200B;

1. What is the name of that exercise
2. Where can I find video instructions so I can re-watch this as many times as I need?

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

Thank you very much for helping me, it is greatly appreciated.

Kind regards",2427,NLP,0
NLP Metaprograms - Episode 18 - Time Storage Filter ,52,NLP,0
Why doesn’t Neuro-Linguistic Programming give this subreddit to Natural Language Processing?? It is far more popular and I am tired of commenting “wrong sub” on posts ,167,NLP,0
"Any NLP tricks to visual anchoring, I know in nlp people anchor to certain words or things. But how does a person use NLP kind of how Starbucks uses the green straw + green writing on the clear cup. And the white paper cup with the brown band. They use both anchor the brand's reach so to speak onto those objects?",314,NLP,0
NLP Secrets of Influence and Persuasion – Part 2 ,49,NLP,0
NLP Metaprograms - Episode 17 - Emotional Stress Response ,58,NLP,0
NLP Secrets of Influence and Persuasion ,40,NLP,0
"Is it normal to oppose against everything that is suggested to someone? How can one categorize that behaviour? 

And how to work around it? 

Most communication is subliminal, so what causes behaviour in a subject to oppose always?",231,NLP,0
NLP Metaprograms - Episode 16 - Relationship Filter ,52,NLP,0
L. Michael Hall books read in order? What would you say is the best order to get the most out of Michael Hall's books?,118,NLP,0
"Let’s take it offline | Brain, Mind and Language ",49,NLP,0
NLP Metaprograms - Episode 15 - Chunk Size Filter ,50,NLP,0
NLP pricing. Help a sista out. I recently met some people who perform NLP trainings. They said they would do it for a discounted price for me. What is the average amount for a seven day training? I don’t want to be screwed. Thankssss,233,NLP,0
What might be wrong with this picture? ,39,NLP,0
"Looking for suggestions I've become aware of an issue recently.  In many situations I'm assuming I've made an error.  If I don't see my socks immediately when I open my gym bag I assume I have left them and start to either berate myself slightly and/or figure out a plan to deal with this. This happens with questioning if I have locked a door. Brought my laptop cord. And on and on. My default is that I have made the error. 99% of the time all things are as they are supposed to be so evidence is not enough to break this. As I'm writing this I realize it may also show up in conversation and expectations.  ""Did I say that, I bet I made the mistake, etc."" where as I feel most people assume they are right until proven wrong. That is not my default.    


What do you suggest to work on this?",795,NLP,0
"A banner change is needed on this sub All it currently has is bamboo with ""NLP"" written on it. It would be helpful to have something which states ""Neuro-Linguistic Programming"" and maybe images to do with the mind and body.

Too many times I'm like ""Ooo a new post in NLP.. Doh!""",279,NLP,0
NLP Metaprograms - Episode 14 - Work Affiliation Filter ,56,NLP,0
"Latest update on the Netflix Of NLP All 13 current episodes of my NLP Metaprograms video series are now available in the APP. The video platform has been upgraded so that these videos will play in Hi Res and full screen on tablets.  
Also the full manual for the Masters Of Influence training has been added.  
If you don't have access to the app yet then you can get it here.  
[www.davidshephard.com](http://www.davidshephard.com/?fbclid=IwAR1WF-wWIVoRxyrUXjLUeZ8WwqRnMTaS8eCbF6hbP1q8X_E0wATY9KKUcPQ)",502,NLP,0
"Sex, Stalking and NLP Training ",31,NLP,0
Catatrophisation ,17,NLP,0
NLP Metaprograms - Episode 13 - Work Affiliation Filter ,56,NLP,0
NLP Metaprograms - Episode 12 - Action Filter ,46,NLP,0
NLP Metaprogram - Episode 11 - Management Direction Filter ,59,NLP,0
NLP and Unemployment ,21,NLP,0
Loads of free NLP/hypnosis downloads ,37,NLP,0
NLP Metaprograms - Episode 10 - Convincer Demo ,47,NLP,0
Mirroring &amp; Matching | NLP Sensory Acuity | Emotional Intelligence ,71,NLP,0
"There is No Failure, ONLY Feedback. ",36,NLP,0
:) ,3,NLP,0
NLP Metaprograms - Episode 8 - Frame Of Reference ,50,NLP,0
NLP Metaprograms - Episode 9 - Convincer Representation ,56,NLP,0
"If charisma can be taught, why are so many NLP experts so awkward? Let me preface this by saying I’m not trying to do a “gotcha” or debunk/discredit NLP here, I’m just trying to get to the bottom of something:

I’ve been to a few of Richard Bandler’s events here in Britain and, obviously, the man is very charismatic, charming and persuasive. But many of the people I’ve encountered who work directly under him are not, and this seems very suspicious to me. 

For example, one of the top NLP people in the country was at a seminar I attended a few years back and was onstage before Richard. The guy had *zero* stage presence or charisma whatsoever. His delivery was flat and boring, and so the audience just talked over and ignored him as if to say “shut up and bring Richard out already.”

The same goes for one of his female colleagues with whom he wrote a book. I watched a talk she gave and she was stiff, nervous and devoid of any magnetism whatsoever. Shouldn’t her NLP expertise have allowed her to have the audience feeding out of the palm of her hand? Shouldn’t she have been cool, calm and confident instead of tentative and awkward?

I’m not trying to attack these people for no reason, but to ask an important question: if NLP can teach you to become charming and charismatic, why did two people who have **personally worked under Richard for years** completely fail to make an impact on the audience when I saw them on these occasions? ",1450,NLP,0
Pacing &amp; Leading | NLP | Persuasion &amp; Influence ,56,NLP,0
Suggestions on removing a mental block I heard a little bit about NLP and decided to start doing some research... unfortunately I just wasted \~4 hours watching NLP lectures on youtube that contained little to no meat.  I was wondering how I might apply this technique to augment my standard meditation practice in order to combat the anxiety/dread I have for cleaning a cluttered space. What are the basic principles for attacking something of this nature?,457,NLP,0
HMS Therapist Training Cornwall ,32,NLP,0
NLP Metaprograms - Episode 7 - Reason Filter ,45,NLP,0
"Best books/courses to learn NLP? I need an honest opinion about what really is the most effective course out there? Books, video courses anything but it should work. I don't need shit surrounded by fake hype. So far I tried that NLP for dummies thingy and it sucks. Please help ;-;",281,NLP,0
"Will anchoring bring back a memory or a mental state? Hey.

I've been reading Way of the Wolf and Jordan Belfort talks a bit about using NLP and anchors to manage mental states of peak performance, but I'm slightly confused about the details of how this works. He says to anchor yourself with some sort of stimuli when you're at a state of peak performance to be able to bring that state back at a later time, but it's slightly unclear how exactly this happens.

He gives this example of an experiment with a dog, where they would ring a bell every time they fed the dog and eventually the dog would start to salivate just at the sound of the bell, but it's unclear whether the bell triggers a memory about food for the dog, whether it directly triggers a mental state of hunger or whether it triggers a mental state of hunger through the memory of the food.

Will using an anchor set at a mental state of peak performance help bring back the memory of the given situtation that brought you to that state of peak performance and then thinking about that memory will help trigger that same mental state or will using your anchor just bring you right back that mental state without necessarily having to be tied to a specific memory and without having to think about that memory?

What exactly is an anchor supposed to trigger and how can it help me achieve peak performance on demand?

I realise that this stuff probably isn't down to an exact science that works the same every time, but I'm just trying to grasp the details of how this stuff works to give me the best chances of success.

I would really appreciate some expertise on this matter. Thanks guys!",1658,NLP,0
How can I find a NLP 1-on-1 coach thats not an online coach? ,61,NLP,0
"Group Anchors? Does anyone know of any video links to examples of Group Anchoring? 

It would be even better to see one of the older NLP gurus doing it (Bandler,  Andreas, Robbins, etc) but I'll take anything. Any pointers to a good product on this is appreciated as well.

&amp;#x200B;

Thank you,

&amp;#x200B;",312,NLP,0
NLP Metaprograms - Episode 6 - Motivation Direction ,52,NLP,0
Positive Intent: Everyone is Always Doing Their Best ⋆ James Pesch NLP Presuppositions ,87,NLP,0
PART 2 DAMON CART OF NLP GYM | NEUROLINGUISTIC PROGRAMMING | PHILOSOPHY | SELF HELP| MIND MASTERY ,98,NLP,0
A Conversation with Damon Cart | NLP Gym ,41,NLP,0
the FASTEST THERAPY IN THE WORLD FOR addiction PART 2 ,54,NLP,0
The Fastest Therapy for Addiction in the World PART 1 ,54,NLP,0
How To Communicate Using Hypnotic Language Without Fear? - Masterclass ,71,NLP,0
WHAT IS EMOTIONAL INTELLIGENCE | SENSORY ACUITY NLP | MIRRORING &amp; MATCHING | EYE MOVEMENTS LIFE ,100,NLP,0
The NLP Metaprograms - Episode 5 - Adaptation Operator ,55,NLP,0
2 Minute Stress Release Therapy ,32,NLP,0
BODY CONFIDENCE HMS THERAPY ,28,NLP,0
Well Formed Outcomes ,21,NLP,0
"Request for advice on habit / compulsion removal Hi, I have a very minor but annoying habit that I would like to stop - and I think NLP might be a fast way to stop it.

  
I tend to fiddle with / pull at loose threads on the hems of my clothes. I have done this for as long as I can remember. If there is a loose thread on a hem, in a cuff, in a pocket I will compulsively roll it between my thumb and index finger. It has the effect of making my clothes look crappy (it pulls things out of shape) and me look odd (especially if the thread is in my pocket :).

Is this something NLP can make me stop? Please could someone advise me on a book to start, or a script to start on?

&amp;#x200B;

Thank you in advance.",713,NLP,0
NLP Metaprograms - Episode 4 - Internal State ,46,NLP,0
Meta Models of Reality ,23,NLP,0
Fear of Success HMS Therapy ,28,NLP,0
NLP Magick - learn why the first book about NLP was called The Structure of Magic ,82,NLP,0
GET PAID MORE Self Worth Therapy ,33,NLP,0
Coaching session using NLP [https://youtu.be/-8B2KDtfgXM](https://youtu.be/-8B2KDtfgXM),87,NLP,0
NLP Metaprograms - Episode 3 - Internal Process ,48,NLP,0
"What is the perfect NLP script for ordering at Chipotle to get you more generous servings? I can afford the extra meat, so this is a tongue-in-cheek post, but I'm also curious what you all would say.",199,NLP,0
"Guidance to help a friend overcome anxiety? About a month ago, a friend was doing a risky recreational activity and got slightly injured. She has since healed, but now she has anxiety/slight PTSD surrounding the activity and other person involved. She WANTS to get over it, both to fix any remaining parts of her damaged psyche, and perhaps so she can try again, but she is having some kind of block and isn't sure why. She was close to being ready to go try again about two weeks ago, and then something changed. She either isn't sure what happened, or won't say, but she still does want to heal from this.

I know that someone wanting to heal is a huge part of these things working, but I don't really know where to start with any ideas for NLP to help.  I'd appreciate any help, so that I can help her. I'm pretty new to this and haven't tried to help others yet, but that's truly what I'd like to be able to do. Thank you all so much!",938,NLP,0
NLP Metaprograms - Episode 2 - External Behaviour ,50,NLP,0
"New to NLP – Reading material/exercises to overcome stress/anxiety when interacting with people Hello everyone,

I am rather new to NLP but have quite a bothersome issue. I won't go in too many details, but in my childhood I was taught to not speak up for myself and that me being assertive was me being aggressive. This (and other things) has led me to very quickly be in a non-calm state when I have a discussion or argument about certain things with people. My chest can tighten and I can feel my heart racing. I am not very well versed in this NLP world. Could anyone please point me to reading material/exercises that I could read and do? My goal is to be a more balanced person who does not react as severely to ""negative"" interactions to other people. I have never tried NLP but I do think it works! Thank you so much for reading this.",842,NLP,0
NLP Metaprograms Episode 1 ,27,NLP,0
Having a practitioner in NLP is like... ,40,NLP,0
"A comprehensive guide to NLP I think NLP should be treated as part art, but is a genuine scientific idea. It has use cases that actually work and this is what's most important.

Galileo Galilei had a series of specific tests that would let experimenters confirm his laws with their own senses, he believed that independent confirmation is the ultimate way of verifying facts.

I've set up a thought experiment which you can use to verify the validity of NLP.

[https://exposingtheothers.com/how-to-control-your-mind-and-thoughts/](https://exposingtheothers.com/how-to-control-your-mind-and-thoughts/)",600,NLP,0
"NLP Therapists in Melbourne (Australia) Hello everyone. I am looking for a therapist that does NLP and maybe also hypnotherapy in Melbourne, Australia.

I have found places that seem geared towards the coaching side. Their websites include words like career, relationship, success, performance, etc.

I am more looking for the therapy end of things. Something like uprooting past conditionings and beliefs, resolve internal conflict, release obsessiveness and self-judgment. I found that on sites that mention these the words Time Line Therapy and hypnotherapy tend to turn up as well.

For the former I have found a lot of places that seem very business-focused. For the latter I have found a few, but they seem a bit dodgy. Maybe I am getting NLP all wrong though and the latter is not a core area of it.

Some help would be highly appreciated.",846,NLP,0
"Learn pillars of hypnoses, 3-i's, patterns in men/women and face reading ",73,NLP,0
"NLP resources that are a little more rational and less ""feel good""? Not sure how to say this, but I guess I just feel very stupid doing stuff like telling myself things that I do not (yet) believe... I am reading a book on NLP right now and it tells me I should affirmate to myself ""I am an attractive person"" or ""There is enough for everyone in the world, I can become affluent"". 

The second one I simply don't believe, the first one seems not even helpful because the fact that I am not very attractive is basically my motivation to do things like losing weight. 

Granted, I haven't gotten past the first few chapters yet, the book might get better... Is this what NLP is about or will there be other, more actionable stuff?",728,NLP,0
Video: Client Session - resolving fear of flying (Trailer) ,59,NLP,0
"""7 Steps for a Guilt-Free 2019"" — Inspired by Steve Andreas' regret resolution pattern ",87,NLP,0
"How to identify, describe, and embed decision-making patterns? Hello! We need to hire several product managers (PM) to conduct product research. We want share our experience with them as fast as possible. Along with general business processes, it is required to ""give"" them decision-making patterns. Here is an example of a pattern that can be useful during PM interview process: when user describes his experience with general phrases (""I usually do like this ..."") -&gt; we need to ask him to tell about the last time when he had this experience.

Are there any approaches in NLP that allow to identify, describe, and further embed these patterns? If so, tell me which direction to look?",689,NLP,0
"Derren Brown:Tricks of the mind. Season one is on amazon prime at the moment.

Also all three seasons  are on ""all4"" but its an awful platform!!",144,NLP,0
Can the moderators help clarify the purpose of this sub? Lately there have been as many posts about Natural Language Processing as there are about Neurolinguistic Programming. Maybe add some rules? A pinned post about what NLP is? There seems to be confusion.,259,NLP,0
"Need help with reparenting a memory and/or believe, any skilled practitioners able to help me over the phone or video chat? I realized yesterday in a deep meditative state that I was told, perhaps many times as a child, that I wasn't creative.  Well, I'm creating a couple businesses and am wondering if this is holding back my growth and potential.  I did an NLP practitioner course/education about a year ago.  I remember that reparenting was an exercise.  I don't have any of my material with me.  I'm curious if a practitioner could help me change this experience or label it in a way that would serve me better.  


Thanks!",628,NLP,0
"Finding a state of mind - need your help Hi everyone,

I need advice on creating an anchor for a specific state of mind. The only issue is that I have no idea what is this state I'm looking for 0\_0.

I will try to explain myself as best as I can:

I want to create an anchor for study and work. There are days that I am sooo focused and I feel that I can learn and do everything. My productivity on these days are on different level than usual. However, I don't understand what exactly this state of mind is.

I read the book *Passion and Reason: Making Sense of Our Emotions by Richard S. Lazarus* to get some information about emotions and hopefully understand the state I want to achieve, however, I didn't find it. According to this book, the emotions divide into three groups containing the following emotions:

Anger, Envy, Jealousy, Anxiety-Fright, Guilt, Shame, Relief, Hope, Sadness, Depression, Happiness and well-being, Pride, Love, Gratitude and Compassion.

None of them describes what I'm feeling - focused, emotionless, detached from anything else, mission oriented.

Can anyone relate to that or offer any help of how to find it. I am familiar with how to create an anchor, but I couldn't create one for this state of mind.

Thank you.

&amp;#x200B;",1266,NLP,0
Some Anchoring and a little bit of Neuroscience ,48,NLP,0
NLP Techniques To Get What You Want: Rewire Your Mind ,54,NLP,0
"Photo Reading - Have you seen anyone demonstrating this ability? John Grinder has a course on it, and they claim that by day 4 students read a book in an hour.

&amp;#x200B;

I have yet to find proof of any human demonstrating this ability. I also asked Steve Andreas and he's never seen it either.

&amp;#x200B;

I've seen many people claim to do it (and teach it), but zero proof.

&amp;#x200B;

&amp;#x200B;",410,NLP,0
"Resolving ""I don't belong"" in one session + followup - Decision Destroyer ",74,NLP,0
"Does anyone else use physical triggers for emotional states? I use this technique where you associate an emotional state to a physical cue. I set up a specific movement, which is pressing my right hand thumb nail into my middle finger nail, and associated it with a confident emotional state that I’ve develop using a NLP exercise.

It takes a very specific process to set up this physical trigger, but it is so effective. Whenever I’m walking into a group of people that makes me anxious I make a conscientious effort to press this trigger. I then immediately feel my shoulders and face muscles relaxing, I feel calm and improve my posture automatically. It makes me realize how tense I was, and really improves my social interactions and feelings of anxiety.

It’s like taking a dose of confidence. It takes a bit of recharging from time to time, using the NLP exercise, but it is such a helpful tool. 

I wonder if someone else does something similar.",954,NLP,0
Anchoring ,10,NLP,0
"Persuasive material Can someone put a link or to indicate a place to download  this stuffs  by  D\_E\_R\_E\_K  R\_A\_K\_E?

 

* **The Barnum Manuscript – Derek Rake**
* **Texting On Steroids – Derek Rake**
* **Boyfriend Destroyer System – Derek Rake**
* **Shogun Sequences Handbook – Derek Rake**
* **Conversational Seduction – Derek Rake**
* **Shogun Method Truth Extractor – Derek Rake**
* **Alpha Male Activator – Derek Rake**
* **Cougar Seduction System – Derek Rake**
* **Shogun Method – Derek Rake**
* **ReSeduction – Derek Rake**
* **Online Dating Playbook – Derek Rake**
* **Manipulation Black Book – Derek Rake**
* **Dark Rapport – Derek Rake**
* **Dark Rake Method – Derek Rake**

&amp;#x200B;",704,NLP,0
"how to tell if an assertion is common sense knowledge? what is the difference between common knowledge(or facts) and common sense knowledge?

&amp;#x200B;

can the machine distinguish these two?

&amp;#x200B;

(Frequency in a certain corpus may help but there are a lot of common sense that usually won't be expressed in text.)",327,NLP,0
"Can I self study an intro course? I got into a secondary NLP course even though I didn't have the prerequisite intro course. How long would it take to study the intro course enough to be able to take and pass the advanced course? The only thing I already know is regular languages and FSAs.

&amp;#x200B;

Description of advanced course:

 1. Creating annotated corpora:  
\* markup, annotation  
\* evaluation measures  
\* corpora and the web   


2. Lexicon and lexical processing:  
\* language modeling  
\* Hidden Markov Models  
\* part of speech tagging (e.g., for a language other than English) to illustrate HMMs  
\* Viterbi algorithm  
\* smoothing   


3. Syntax and syntactic processing:  
\* revision of context-free grammars and chart parsing  
\* syntactic concepts: constituency, subcategorization, bounded and unbounded dependencies, feature representations  
\* lexicalized grammar formalisms (e.g., TAG, CCG, dependency grammar)  
\* treebanks: lexicalized grammars and corpus annotation   


4. Semantics and semantic processing:  
\* compositionality  
\* argument structure  
\* word sense disambigution  
\* anaphora resolution  
\* treebanks: argument structure, WSD (e.g., Propbank, Semcor) 

&amp;#x200B;

&amp;#x200B;

Description of intro course:  
 \* Grammars and the Chomsky Hierarchy  
\* Regular languages, Finite state automata (FSA), probabilistic FSAs  
\* Context-free languages and Push-down automata  
\* Ambiguity and solutions to the problem  
\* Deterministic parsers  
\* Chart parsers  
\* Probabilistic context-free grammars  
\* Modelling semantics  
\* Context-sensitive languages  
\* Turing machines and computability  
\* Models of human language processing  
\* Overview of language technology ",1747,NLP,0
Accidental post-hypnotic suggestion ,36,NLP,0
"Since the sub is kinda slow (dead?) I'll post my own personal NLP strategy that has helped me out. Hope it can help another :) My NLP style is entirely based on changing yourself to improve, not to affect other people. Hopefully it will be interesting anyway :)

---

So you have 3 layers of self that you can control. Your body, mind, and awareness (or consciousness etc).

Your awareness is the one to set the theme of what you want in life, your mind brings up thoughts according to the theme you set, and your body actually does the manual labor to move closer to your goals. 

For body improvement, practice is key. That's about it. Muscle memory and skills. Just basic stuff. 

For mind improvement, you need to see which actions get you which results. Plan for the future, plan efficiently, to achieve goals faster with new strategies. Thinking and planning and deciding HOW to do things. 

Awareness improvement is similar at a higher level, you can see which goals move your life in which direction, and the effects that your goals and strategies have on the whole thing. You can change the course of your life by deciding what types of goals will help you, and what types of goals will not help, and setting your mental state to be open to the good stuff and closed to the bad stuff. 

And thats about it. The way of improving every thing. If you have any thoughts or ideas to improve this please let me know! 

I know this is a wall of text sorry haha.",1463,NLP,0
The social politics of the NLP scene ,37,NLP,0
CREATING A WINNERS MINDSET | INTENTIONAL PREFRAMING | NLP | LIFE COACH |... ,76,NLP,0
"Recursive Frame Analysis on ""One Flew Over the Cuckoo's Nest"" ",62,NLP,0
On the issue of guilt ,22,NLP,0
"The ""tea or coffee?"" bind. ",27,NLP,0
"Masters Of Influence now available online! I am very excited to announce that my entire 3 day Masters Of Influence training is now available on video in my APP. That's over 20 hours of video!  
It covers:  
How to really apply and get results from Cialdini's 6 rules of influence  
Influencing principles from neuroscience  
Ericksonian Hypnosis techniques used by the likes of Obama and Trump  
""Street"" NLP  
Cold reading techniques  
All applied to create real results 1:1, 1:many, face to face, video or writing.  
AND you can give it a go for FREE  
Click here to get started.[www.davidshephard.com](https://www.davidshephard.com)

&amp;#x200B;",649,NLP,0
"Requesting free video resources to learn NLP Any help / recommendations would be great.

Thanks!",96,NLP,0
"Online NLP Resource I get asked a lot of questions about specific NLP techniques. What they are, how you can use them, where you can find out more, what books they are in and which training programs cover them. So I have put together an online resource. An A - Z of NLP techniques and terminology.

You can access it here.

[http://performancepartnership.com/a-z-techniques/](http://performancepartnership.com/a-z-techniques/)

&amp;#x200B;",440,NLP,0
Mindfulness ,12,NLP,0
"102 Neuro-Linguistic Programming eBooks Cleaning up some old hard drives and I don't remember where I got these (Mostly pdfs and word docs) from but this should be available for the next 30 days. Hopefully someone finds them useful.

https://ufile.io/bc9md

File List
* 6 Step Reframing (Howto) by Richard Bandler.pdf'
* 7 Keys to Personal Change.doc'
* 92 Hypnotic Sales Letters.pdf'
* Accelerating NLP Using Meta States.doc'
* Amazing New Mind Power Secret.pdf'
* An Introduction To High-Impact Communication, Covert Hypnosis, And Getting What You Want.pdf'
* Brain101 How to Play the Brain Game for Fun and Profit.doc'
* Brian Tracy - Principles of Success.pdf'
* Crash Course in NLP.pdf'
* Creating Irresistible Influence with NLP.pdf'
* (eBook) - NLP Mastering Relationships.pdf'
* (Ebook - Nlp) Michael Hall - Figuring People Out.pdf'
* (EBook -  NLP) Neuro Linguistic Programming WorkBook - Excellent!.pdf'
* (ebook NLP) Richard Bandler - Using Your Brain For A Change.pdf'
* (ebook NLP) Robert Dilts - Visionary Leadership Skills.pdf'
* (ebook) NLP - The Structure Of Magic Vol I by Richard Bandler and John Grinder (OCR)(1).pdf'
* Essential Communication Skills - Handbook.PDF'
* Eye Contact.pdf'
* Forbidden Patterns - Dark Nlp - Hypnosis - Seduction.pdf'
* Forbidden Patterns (including October Man sequence).pdf'
* Four New Hypnotic Language Patterns.pdf'
* Go Meta to Change Your Personal History.doc'
* Hall, Michael - Figuring Out People (Nlp Metaprograms).pdf'
* _HandWriting - The Secrets to Make Love Happen! Mastering Relationships Using Hand Writing Analysis and NLP - 1993.pdf'
* he Structure of Unconscious Excuses.doc'
* Hogan Kevin - Hypnosis, Nlp, Persuasion And More 415Pages.pdf'
* How To Defeat Shyness.pdf'
* How To Hypnotize Your Lover.pdf'
* How To Make Anyone Fall in Love With You .pdf'
* How To Use Linguistic Binds To Persuade.pdf'
* Hypnosis for Beginners.pdf'
* Hypnosis - Software For The Mind.PDF'
* Influencial Spinning - NLP - The Ultimate Persuasion Strategy.pdf'
* _James, Tad - NLP Master Practitioner - Manual.pdf'
* Kenrick Cleveland - 18 Words.pdf'
* Kenrick Cleveland - 8 Biggest Persuasion Mistakes.pdf'
* Kenrick Cleveland - Maximum Persuasion Workbook - Home Study Course.pdf'
* Lucid Dreaming - The Sleep, Wake, Back To Bed Method.doc'
* Lucid Dreaming - The Sleep, Wake, Back To Bed Method.pdf'
* Magic and Psychic Influence.pdf'
* Magick-Occult - Speed Seduction - Kaiden fox - The Satanic Warlock - NLP and the science of.pdf'
* Magic Of NLP Demystified - A Pragmatic Guide To Communication &amp; Change.pdf'
* Magnetic Attraction (Joseph R Plazo).pdf'
* Mastering The Art Of Persuasion, Influence And Seduction.pdf'
* Mind - Body Connections Strategies.pdf'
* Mind Control - How To Get The Truth Out Of Anyone.pdf'
* Mind Control NLP &amp; Hypnosis.pdf'
* Mind Powers, How to Use and Control Your Unlimited Potential - Christian H Godefroy ! nlp psychology neuro linguistic programming ebook.pdf'
* Neil Strauss - The Game - Mystery Method - Pickup Artists - NLP.pdf'
* NLP 77 techniques.pdf'
* NLP and Hypnosis Skill Building Exercises.doc'
* NLP - Bandler, Richard &amp; Grinder, John - Frogs into Princes - Neuro Linguistic Programming.pdf'
* NLP - David Deangelo - 2004 - Mastery Workbook.pdf'
* NLP ebook - Alan Tutt - Keys To Power Persuasion.pdf'
* NLP ebook Very Practical Tips for building Wealth.pdf'
* NLP - Essential Skills - Money Magnet workbook.pdf'
* NLP Glossary.txt'
* NLP Jeffries, Ross Advanced Speed Seduction Seminar Note.doc'
* NLP Job Interview.doc'
* NLP manual fully hacked.pdf'
* NLP - Mastering Objections.doc'
* NLP Michael Hall - The sourcebook of Magic.pdf'
* NLP Michael Hall - The Spirit of NLP.pdf'
* Nlp Mind - Body Connections Strategies (Share Me) (Self Help Ebook - Pdf).rtf'
* NLP Mind-Lines - L. Michael Hall.pdf'
* NLP - Neuro-Linguistic Psychology - Beyond Words.pdf'
* NLP Opening Lines For Conversation.doc'
* NLP - Patterns of the Hypnotic Techniques of Milton Erickson Vol I - Richard Bandler and John Grinder.pdf'
* NLP - Pilinski, Michael - Without Embarrassment- The Social Coward'''s Totally Fearless Seduction System (b).pdf'
* NLP Secrets of Speed Seduction.doc'
* NLP - Sleight of Mouth - The magic of Conversational Belief Change (by Robert Dilts).pdf'
""NLPTrainer's Training by James Tad.pdf""
* NLP -Whispering in the Wind (John Grinder &amp; Carmen Bostic St Clair).pdf'
* Opening Lines for Conversation.pdf'
* Psychic Seduction 5 (Joseph R Plazo).pdf'
* Psychic Seduction.pdf'
* Psychological Subtleties (How magicians read minds).pdf'
* Reading women body language.pdf'
* Relaxed Cold Calling.pdf'
* Richard Bandler - Trance-Formations - Nlp &amp; The Structure Of Hypnosis.PDF'
* Ross Jeffries NLP Flash cards - hypnotic language patterns (flashcards bandler nlp neuro linguistic).pdf'
* Secrets of Personal Mastery.doc'
* Skill Building Exercises.pdf'
* Social Engineering Handout by Kenrick Cleveland.pdf'
* Speed Business Networking - the Manual.pdf'
* The Art of Approaching.pdf'
* The art of seduction (long version).pdf'
* The Definitive Hypnosis Cheat Sheet.pdf'
* The Facts on NLP by Richard Bandler.doc'
* The Magic You Can Perform With Reframing.doc'
* The Mind Control Manual - Seducing Others With Your Mind.pdf'
* The NLP Goal Setting Model.pdf'
* The Power of Hypnotic Self-Talk.pdf'
* The Power to Influence - Manual.pdf'
* The Secrets of The NLP writing.pdf'
* The Secret To Creating Chemistry - Mastering Relationschips And Seduction Using Nlp Techniques.pdf'
""The Seven C's of Success.pdf""
* The Structure of Unconscious Excuses.pdf'
* Tips and secrets on how to improve yourself, your body and your mind so that you can attract the women you really want into your life.pdf'
* Ultimate NLP Home Study Course.pdf'
* Unstoppable Confidence.doc'
* Very Practical Suggestions for Building Wealth.doc'


",5832,NLP,0
"Self Point: Isn’t it Super Obvious? I read in one NLP book to use the Self Point technique where you basically point to yourself at specific times. Isn’t it obvious when someone is pointing to themselves a bunch of times? 
Thank you.",233,NLP,0
"How Can Derren Brown Do This? Is This Hypnotism, NLP,Suggestion? Is Derren Just Rearranging This Man's Thought Patterns? Brown Seems Like He Can Do What Therapists Try To Do In A Matter Of Minutes. ",198,NLP,0
"Which Milton Erickson book? Hi,  I am looking to buy one of Milton Erickson's books. He was so good at metaphor and hypnosis and I really want to hone my skills.  Can anyone recommend one of his books? ",202,NLP,0
"How valid and proven is NLP? Where to learn? Hey guys I checked a few research papers on NLP. One showed decent results and the others showed no effect. I know there is generally not a lot of research on this and waiting for research to catch up isnt always best. I saw some discounted udemy courses on NLP and I am pretty interested in stuff like this. So let me know what book I can read, free/cheap courses or maybe just a youtube channel.   


Thanks! ",456,NLP,0
"Neuro Linguistic Programming Techniques for Self Acceptance Hello All,

&amp;#x200B;

Could anyone suggest any NLP techniques for self acceptance of perceived physical flaws? ",175,NLP,0
Happy Halloween from the Power Family and yeah we can fly - ,60,NLP,0
"Courses, scams? Sorry about the sensational title, bit reading trough this sub i've read countless times that 98% of the NLP trainers out there are clueless or scam artists, so i wanted to ask if there is actually a trainer or a course with a very good reputation in producion skilled practioners? Have you ever meet one ? ",323,NLP,0
Universal Body Language ,24,NLP,0
Pattern Interrupts: Interrupt Emotional Triggers ,49,NLP,0
Affordable internationally accredited online NLP course Hey I'm trying to find an affordable full length internationally accredited NLP course? It's overwhelming looking on Google I was hoping someone who's had experience in learning from an affordable online course could help me out thanks,291,NLP,0
Superb moments are more exciting when we get to share ! ,56,NLP,0
"Best NLP pattern for weight loss? Is there a specific pattern you would use for weight loss? I've seen someone do the swish for overwriting a compulsion for a certain food, but I haven't seen one for eating in general.

A lot of people have smoking or eating as their only ""me time"" where they feel like they can relax and enjoy something. How would you separate that out so it's just about taking in nutrition when they need it? I've seen the technique where you integrate two ideas with your hands. Is there an opposite technique to separate two ideas?

I'd enjoy hearing your thoughts even if you don't have a specific solution. This stuff is fascinating to me!",664,NLP,0
"Since this day , I have developed a multi million dollar company , saved untold numbers of lives and continue to thrive and cause happiness and life all over America . I have also assisted my own children through difficulties and teen years with excellent results . Thank you Kelly Dunn for this gift ",301,NLP,0
"Persuasive Writing Result! I'm a creative copywriter in radio here in Australia. I have been dabbling in, researching, and practising NLP in my spare time for a year or two. Anyway...

I had to write a rationale to a client as to why the initial idea.

I remembered the basics in my opening paragragh, and, appealed to all modalities possible for my audience, as I had been told that there were six people making this decision, and they all had to come to a consensus.

This is nothing groundbreaking, just some exceptional feedback for myself to see that when you take the time to ingest the information and skill of NLP and place then into action in the appropriate manner, you get results!

Opening paragraph below for your perusal.

Just wanted to post this because I love NLP and all it has to offer.

Thanks!

Paragraph -

Hi Marilyn,
 
I hear your concerns and see what you’re saying. Let me touch on a few of your concerns below and throw forth some reasoning thought behind my initial concept.

The response -
'The majority of consensus is to now go with your initial advert'.

WINNING. :D",1098,NLP,0
"Share Optimism frames I am interested to know about frames you know to create positivity or optimism in your life.

I have been going through anxiety issues recently. It has to do with repetitive thinking about negative frames, feelings, etc. 

One positive frame I've been using is practicing gratitude. So, identify what made me happy that day.

Would appreciate if you could recommend some books on the subject. ",415,NLP,0
"Pacing &amp; Leading is a Powerful Tool that you can use to communicate &amp; to help positively influence others, enjoy! ",122,NLP,0
"This subreddit is for ""Neuro-Linguistic-Programming"" Someone should do a sticky to something like this, because this subreddit is drowning in ""Natural Language Processing"" posts which do not belong here. Is the admin still alive? Or some mods?",243,NLP,0
How NLP can change your life ,29,NLP,0
Focus! Focus! Focus! - Romal Surana ,36,NLP,0
"I’m pretty sure my girlfriend has been covertly manipulating me. When she does things a certain way, I think a certain thing? What are your thoughts? I can I recognize and protect against this type of thing? Sometimes in conversation, she will randomly stick her tongue out, and I think “yes”. There are other incidents of her trying to be manipulative. Any thoughts? What is this and what is going  on exactly and how can I protect against this?",446,NLP,0
Hypnosis/NLP To Manifest Romantic Relationships Using The Law Of Attraction ,76,NLP,0
"What is this I've discovered Last night I was looking for a success hypnosis (I know that sounds silly) and somehow through the mystical magical ways of YouTube discovered NLP and decided to try it after I read a bit about it. 

https://youtu.be/-QdX9NVEHdQ

I used this video which is supposed to boost will power. I listened to it last night and this morning. My mind has only ever been this quiet when I was in a deep meditation and thought I found God. And with a quiet mind it's incredibly easy to get done what you want to get done so I guess in a sense it worked but...

I'd like to an expert or someone more knowledge about this than me to check the authenticity of this if such a thing is possible. Even though it technically did what it said it would I still want to know it's not like deleting parts of my brain and prepping me to be a mindless soldier when I hear a trigger word when WW3 starts. Because I reasonably fear these things. 

Okay thank you.",965,NLP,0
WHAT IS EMOTIONAL INTELLIGENCE | SENSORY ACUITY NLP | MIRRORING &amp; MATCH... ,79,NLP,0
"How closely related is the ability to do nlp naturally and being a sociopath? As sociopaths tend to manipulate people for their own gains, NLP must be applied at a subconscious level no? From before first contact they’re already determining suitable personality traits that they are comfortable working with. Then at first contact they are projecting all the needs that they think the “mark” craves for (attention, praise, respect... etc). These are all NLP techniques no? Then wouldn’t they be able to subconsciously nurture this and be really good at reading and manipulating people? 

Really curious about this field recently. ",630,NLP,0
"Nice to meet My article - you are invited to read and express an opinion :)

[http://myuniqueskill.com/what-is-nlp/](http://myuniqueskill.com/what-is-nlp/)",155,NLP,0
"[Please help] Can you use NLP to change mental associations with music and songs? I would appreciate any help because this can be really annoying and ruin my enjoyment for certain songs.

Basically, I have a type of synaesthesia where I associate physical locations strongly to music. One example is ""Drive My Car"" by the Beatles. When I hear it, it is strongly linked to a street near my childhood home. I don't necessarily think of the song when driving on that road, but when I hear the song, it conjures up mental images of the street.

And as you can imagine, I also associate songs with other things. I listened to Abbey Road heavily during a somewhat sad period of my life, so when I hear the songs I feel a little sad.

There are some songs I really like that are paired with mental associations or emotions I don't like. For instance, I really like a lot of songs by Muse. But last year, I had weekly appointments I needed to drive to. I listened to Muse a lot in my car during those rides.

But those visits were annoying. And they were often during the late afternoon (I don't like the late afternoon sun). So when I listen to the music, essentially I think of images and memories and feelings I don't like.

So, is there any techniques I can use to change these associations?

I really love music but I am afraid to listen to a lot of songs I like for fear of 'tainting' them irreversibly. And this happens in many other aspects of my life besides just music, so it sucks and it makes me frustrated a lot. Any help at all is appreciated.",1549,NLP,0
"NLP Question Hi All! I'm an NLP enthusiast and was wondering if any of you NLP experts can help me with an issue I am having:

I find myself pacing people very closely and ""absorbing"" their emotions when I interact with them. Meaning, if the person will talk/act angrily then I will become angry, sadly then I will be come sad, etc.. Although it is great to empathize with others, sometimes it is to my detriment. Is there a way I can easily disassociate or break pace from people so I won't be affected so easily?",514,NLP,0
"I recenlty had the honor of teaching • Mapping Across, • The Swish Pattern, &amp; • Reframing to break Unwanted Habits to a group of Powerful Counselors &amp; Therapists at Improving Lives Counseling Services, Inc. in Tulsa Oklahoma. This is the first part (Mapping Across) of this Workshop *edited ",299,NLP,0
Any of you have experience with online guidance? My therapist is great with NLP. We've done several sessions and they've worked wonderfully. Have any of you tried online videos where you're guided through the experience? Were they as affective as in-person therapy work?,270,NLP,0
"Audio books I recently got given a Google Play voucher...

And was wondering wether there are some good audio books on the subject that are worth getting? 

Tia",160,NLP,0
The importance of business-centric NER ,39,NLP,0
"How does NLP work? Bandler says he doesnt give a fuck how it works, or what is the mechanism behind it. All he carea about it, is that it works.

But i have been curious?

How did bandler discover NLP techniques?

What is the mechanism by which it works?

Edit : In nlp, we change modalities and that impact us. I am not sure on the terms. But you move a picture of something bad to a place which has all the good picture. Increase or decrease the size, brightness etc. Increase and decrease the volume, tonality etc in your head.

How did bandler discover this stuff? And how does it work?",590,NLP,0
The essential guide to NLP ,27,NLP,0
"How to use NLP with Bereveament and get amazing results The unconscious link between the Mind &amp; Body Connection  

HMS Therapy is a combination of NLP &amp; Therapeutic Hypnosis 

I hope this helps you as Therapists 

[https://www.youtube.com/watch?v=tVPCZqruPs8&amp;t=60s](https://www.youtube.com/watch?v=tVPCZqruPs8&amp;t=60s)",332,NLP,0
The Power of Forgiveness! - Romal Surana ,41,NLP,0
A-Z Of NLP Techniques and where to find them in books and trainings ,68,NLP,0
Conscious and Subconscious Mind - Romal Surana ,47,NLP,0
Power Of Positive Thoughts! ,28,NLP,0
Eye Accessing Cues 🙄😔🧐😶👀 ,25,NLP,0
The Power Of Positive Thoughts! [https://nanhagyanfoundation.com/learning/2018/09/04/power-of-positive-thoughts/](https://nanhagyanfoundation.com/learning/2018/09/04/power-of-positive-thoughts/),194,NLP,0
MIRRORING &amp; MATCHING NLP | MINDFULNESS | NEURO-LINGUISTIC PROGRAMMING SELF HELP | PERSONAL GROWTH ,102,NLP,0
Powerful Nlp session for self esteem ,37,NLP,0
How to reprogram your subconscious mind every minute of the day https://youtu.be/vx6odx6TJxw,92,NLP,0
"Is there any way to alter predictable behavior? For example: I instantly start to feel nervous when a certain cue happens. I know I feel nervous because of that cue.

How would one go about altering, or even removing the reaction (nervousness) towards the cue?",260,NLP,0
Havening an NLP Technique for Treating PTSD &amp; Trauma ,57,NLP,0
"A change in perspective can change your life! 
When we identify with our experiences they start to become the story of who we are.  And when those experiences are not in line with what we truly want we feel like we are struggling, blocked and frustrated with what is showing up in our lives.  When in reality they are just experiences and not who we are so they can be changed.  

In this video I share 4 easy steps to help you to write your own story of your life and experiences.... 


https://youtu.be/Rm8H-9JOss4",516,NLP,0
WHAT'S THE PROBLEM? | NLP 6 STEP REFRAME ,41,NLP,0
"5 NLP Questions that remove pain Part 2 how to use the 5 NLP Questions to remove pain

Where are you in life Not the Pain? 

When are you in life Not Not That Pain?

How is it possible that you are Not Not Not that pain? 

What are you that is not the pain?

Who are you that is not not not the pain?

Good Luck 

[https://www.youtube.com/watch?v=G9EuXQcqtBA](https://www.youtube.com/watch?v=G9EuXQcqtBA)",404,NLP,0
"How to deal with misspelling/jargon out-of-vocabulary words with social media text when using word embeddings for CNN classifier. Hi, I'm relatively new to NLP and struggling with this issue. I'd love to hear your insights on how to best approach this problem:

**Context:**  
I'm working with social media text (sources include twitter, reddit, telegram and other chat rooms) that is related to cryptocurrencies. So if I use a pretrained language model (e.g. spaCy or fasttext) there are many out-of-vocabulary (OOV) words due to misspelling or crypto-specific jargon.

**The problem:**  
When a word is OOV, it has no embedding and gets passed as a vector of 0s to the CNN. Thus the classifier performs terribly on text with OOV word.

**Solutions I've played around with:**  
- Using auto-correct to fix misspellings (doesn't deal well with common misspellings like multiple duplicate letters ""moooooon"", doesn't deal at all with jargon words)
- Using character- or ngram-level embeddings like fasttext (completely misses the mark 90% of the time based on vector similarity and classifier performance)
- Train embeddings from scratch on text sources (prone to overfitting, poor performance on complex sentence structures)

",1226,NLP,0
Hear and Speak Your Natural - NLP keras ,40,NLP,0
"How to create NLP Job Securty as a Practitioner I find using this strategic model every week helps create a good flow of clients

35 words or less what is it you do ? 

[https://www.youtube.com/watch?v=tEKczXnIyEI](https://www.youtube.com/watch?v=tEKczXnIyEI)",259,NLP,0
"THE BEST 5 NLP QUESTIONS The best NLP Questions I use as a Master Trainer of NLP are the 5 Questions.

This short video shows you how to use them when working as a NLP Practitioner

Good Luck &amp; God Bless

[https://www.youtube.com/watch?v=vlWOokmTt-A](https://www.youtube.com/watch?v=vlWOokmTt-A)

Martin Webster Creator of HMS Therapy",338,NLP,0
"3 ways to stop sabotaging yourself So you really want something but you have this feeling of doubt and hopelessness that you just can't seem to shake? 

This video will help you to reprogram your subconscious mind. It will give you 3 techniques you can do right now to stop self sabotaging yourself!

https://youtu.be/kWV-AfWaMp4",329,NLP,0
SLEIGHT OF MOUTH: POWERFUL NLP TECHNIQUE ,41,NLP,0
"Advice Wanted: Most useful exercise/information I am looking to give a short workshop of about 1-2 hours to a group of 10-20 friends. Casual setting.
I am looking for advice on the most useful exercise and/or concepts I could share. All advice is welcome. Thank you!

EDIT: Workshop can be any amount of time I like.",316,NLP,0
How I got unlimited confidence in every area of my life... https://youtu.be/J6HgnNEJddo,87,NLP,0
Positive Thinking = Depression | Cognitive Action &gt; Positive Thoughts | NLP [https://www.facebook.com/jamesppesch/videos/10156481387167889/](https://www.facebook.com/jamesppesch/videos/10156481387167889/),207,NLP,0
NLP Anchoring: What it is | Why it Works | How to do it ,56,NLP,0
Unleash Your Inner Hero! ,25,NLP,0
"Huge Collection of Courses I have a huge collection of dating experts and pick up artists courses. Like Jason Capital, David Tian, Derek Rake, Todd Valentine, Shae Matthews, Deepak Wayne, Julien Blanc, Charlie Houpert, Leo Gura, Alex, Mark Mason, Luke, etc. If you are interested in any of the course offered by them, then message me. ",335,NLP,0
"SIRHUD KALRA FRIENDZONE If you are interested in sirhud kalra's how to get out of the friendzone video, then message me.",120,NLP,0
"NLP techniques to get paid more. I'm starting my own cleaning business and I'm in the process of filling out leaflets to post to people in my surrounding area. (Just a little bit about myself and the services I offer etc)

I'm a novice in terms of NLP, but I've seen how the likes of Derren Brown and Paul McKenna have used the techniques to fantastic effect. 

What sort of sentences/words should I  emphasize?  

Many thanks",426,NLP,0
How We Create &amp; Maintain Beliefs ,37,NLP,0
HOW TO CREATE &amp; REACH GOALS | QUICKFIX MIND SHIFT STRATEGY | NLP DEMO | ... ,80,NLP,0
SELF HELP MAPPING ACROSS | NLP MIND SHIFT STRATEGY | DOUBT &amp; CONFIDENCE ... ,80,NLP,0
SELF HELP | PERSONAL DEVELOPMENT REFRAMING | STOIC PHILOSOPHY | NLP | CO... ,76,NLP,0
The Swish Pattern ,18,NLP,0
SELF HELP WORKSHOP | PERSONAL GROWTH &amp; DEVELOPMENT | MIND SHIFT STRATEGY... ,80,NLP,0
"nlp is a great tool nlp is a great tool,iv helped so many just by the basics,eg embedded commands i use most.but i wish someone could do do a youtube video using nlp techniques to help me with my anxiety,anyways.yes,sometime with friends i like to play around with it.but it helps my mom when shes down i can just use some nlp techniques",337,NLP,0
Any NLP discord channel out there? post it here pls,51,NLP,0
How NLP is transforming the news industry ,42,NLP,0
"How can we change the negative beliefs of a friend to a better ones ? I think this is a good place to ask this 

I have an old friend which life changed entirely when his closest friend remplaces him for a girl . He considered him the only person on the world on which he could trust , he was the most important person of his life 

When his friend replaced him for a girl , the pain was so deep that his life changed completely . He became reactive on relationships , making his boundaries tougher and now each time he thinks someone will defraud him , use him etc. he tells them to fuck off , he can’t have any relationship anymore ( not with a girl I mean - but with anyone ) cause he doesn’t trust anyone , he controls people by making them to no talk to the guy that did that to him breaking some relationships , he can’t have any meaningful relationship anymore etc.

I tried a ton of things : made him realize that what he does is bad ( he said that if it makes it feel good then I don’t give a fuck ) , made him imagine that he could lose to meet another person that would change his life for good , made him realize that this happened only once and that there’s several billion of people in the world ( it’s the first time he trust someone like this ) etc, etc. But he doesn’t want to change , although , I just listened . Said once that what he does is narcissistic ( I was wrong by saying this cause criticizing him does no good ) but nothing more . Also said that the way he changed made me feel o sad  

He also has the belief that you can only have one true friend  . His most important value is to be there when things turn bad and also honesty 

Tried all this but I can’t help him , I really would like that he change his beliefs around relationships in general but I can’t , I also feel narcissistic cause I want him to change , even tho is good

I don’t know about coaching or anything , so I ask you guys : how can I make my friend change his beliefs and help him have a better life ?",2004,NLP,0
Are You Hungry? ⋆ James Pesch ,30,NLP,0
"Request for clarification I searched for this sub in hope of seeing discussions on Natural Language Processing, usually abbreviated NLP, and subscribed without looking at the feed for too long.

That was my bad, I guess, as all posts here are very confusing and completely unrelated to what I thought it would be. 

Natural Language Processing, from my educational background in computational linguistics, deals with theory and methods for analysis or generation of language, usually with machine learning or deep neural networks (what people tend to call Artificial Intelligence).

The posts on this thread, however, are mostly advertisements for some coaching service that is completely irrelevant to language technology, and seems a bit like ""listen to me, cause I'm great, but i actually don't know what I'm talking about""-nonsense. 

Why are there no pinned posts to explain what this sub is about, or any explanation on what is meant by NLP? ",948,NLP,0
FREE NLP TRAINING ,18,NLP,0
"Went to a NLP meetup - got the feeling I was being conned? Hi guys,

Just wanted advice from anyone who practices or is interested in NLP. I went to a meetup group, specifically titled ""How to get results with NLP"" and I am wondering if NLP is real/a type of therapy or if it's a con artist's wet dream.

There was a small talk with a guy telling us that we needed NLP in order to achieve our goals or desires and frankly, I didn't believe him. He kept saying that he would change the way we thought just by the end of the talk but I didn't feel any differently and he kept saying, isn't this worthwhile? NPL has great value to you, it will change your life. He was going on a lot about money too and how the DESIRE for money is the root of all evil etc. and it all seemed really irrelevant. I smelt a rat.

He ended his pitch with breakdown of his course program and it turned out that of course, inevitably we could sign up to his program today, only today and at a reduced cost of just £300 instead of the usual every day price of £3000. This consisted of over 100+ hours of 'homework' before we ever got to speak to him for our 1 and only one-to-one session over the phone. Then we would earn our masters certificate.

I was so disappointed. I felt like this guy was preying on the vulnerable. I felt like there would have been weaker people in the room than me who would have signed up and thought this was going to fix their problems. I also felt like there would have been people in the room who couldn't afford the £300+ and that it was their fault they couldn't 'fix' their lives or mindset because they simply could not afford to. Victim-blaming tactics.

Is NLP an accredited profession? This guy said he had been practicing for over 30 years and had professional certificates etc. but was he lying?

Thoughts?",1821,NLP,0
"anyone nlp....? any nlp practitioners?if so how has it worked in your life?,and how does it work when influencing people?",121,NLP,0
"How to handle premature ejaculation with NLP Guys,

How could I handle premature ejaculation using NLP?

Which strategies should I use?",135,NLP,0
HOW TO MODEL SUCCESS ,21,NLP,0
Have you started applying NLP to your Love life? ,49,NLP,0
"Changing sub modalities on live Guys,

I started studying and practicing NLP a few weeks ago and I have a doubt.

Is it possible to change sub modalities while living a situation? I mean, while talking to someone, is it possible to lower the volume of their voice or viewing in black and white?

Do you guys have any tips abou that?",332,NLP,0
"""other modal"" equivalents for this phrase? The phrase:

""the committee will need to share in lifting some of the weight of this process"".

I consider this a ""kinesthetic mode"" statement. \(mode = ""representational system""\).

I would welcome your suggestions on ""equivalents to this phrase"" in the other modes \(auditory, visual, olfactory, gustatory\).

Background: I'm writing an email to a committee and I need to persuade them to do some of the work they're imposing on me, and the committee leader is resisting.",516,NLP,0
"Do we have a list of things NLP libraries can do well today and pros and cons of using a library for that? Consider all library predefined functions and other functions elsewhere on the web. If it already exists on the internet, like on GitHub, tell me.",253,NLP,0
Who am I? ,10,NLP,0
Allowing Yourself to Be Intentional ,36,NLP,0
"Having a Team Mindset - Visualization Technique # HAVING A TEAM MINDSET

May 30, 2018 | [James Pesch](https://jamespesch.com/author/jamesppeschgmail-com/) | [No Comments](https://jamespesch.com/creating-a-team-mindset/#respond) | [Blog](https://jamespesch.com/category/blog/)

#### SCIENCE OF SUCCESS

# HAVING A TEAM MINDSET

## INTRO

As a Tulsa based Life Coach, Executive Coach, &amp; Business Consultant I have spent many years trying to discover “BETTER” ways of modeling successful ways of behaving and believing. When I was a kid, I had several different sets of parents and grandparents who helped shape my ability to realize that children can’t do anything right,

**anywhere**.

If the child, let’s call him Yames; if Yames behaves one way in one home, or classroom, it is perfectly acceptable and Yames can expect to be treated fairly or as all the other equally loved and respected people in the dynamic, BUT Yames can go to the next home or classroom and behave the EXACT same way and learn that the act is “selfish,” OR and this is the worst and most unnecessary guilt you can heap upon a young mammal who is trying to learn how to survive is to tell them their learned behavior is

“*evil*,” or “*mean*.”

When Yames acts or behaves in ANY manner, whatever the behavior is, when first began, it was most likely a ***learned*** behavior. Let me give you an example, the act of throwing a toy at the wall when someone was mad was introduced to Yames when he was already 7 or 8, far too late in development for him to have adopted a similar coping skill, but as Yames watched his new family hurl items into walls and floors whenever they would get upset, he knew that this was a destructive and seemingly juvenile way to process internal frustrations, but Yames could also perceive that this was not evil or selfish. It was all learned. You could trace it back to childhood behaviors which they could all clearly recall seeing acted out by a caretaker when questioned after the fact.

Last week, as I was jumping into my car to speed away to my next “thing,” I said something to a friend that pissed them off. I said,

&gt;“Hey what’re you guys up to this weekend? We should hang out or something.”

This casual invitation is a habit I picked up in my 20’s when I discovered that most people feel left out or unimportant to most other people. One friend was disappointed and crying to me one evening about how no one liked her or invited her to attend parties, she was mad at me for not inviting her along, and since I always want others to know that I am an open and inviting person who loves company no matter what I am up to and no matter what we are doing, I adopted a habit of inviting others along.

Well, last week, in order to follow my habit of inviting, I messed up and asked someone along who needed more intimate and specific plans because of a chronic illness which makes it difficult for her to commit to evening plans, which I did not provide and did not intend to provide as this was just as honest an invitation as it was casual. I just offer this casual offering to anyone whose company, I enjoy. I want them to know they are welcome to be around me anytime they get the urge and that I am easy to please with activities because the people who are there are the main reason I attend any event or do anything outside studying and reading or exercising.

This was when I learned about systematizing a powerful way of adopting a Team Mentality.

## BEING ON A TEAM

One thing football did positively for me was to help me learn what it means to have a role on a team and the goal of that team being bigger than any one role. The other factor I received from my days of colliding head to head with other hormone raging high schoolers was definitely brain damage,  based merely upon all the most recent studies. So, you might not be able to make these next few paragraphs to make any cohesive sense, but do try. If this starts to sound silly, don’t panic, we will get to a workable and applicable way to do this.

Being a Tulsa Executive Coach isn’t easy, one thing I started to realize was that our high school coaches never taught us how to play the game the wrong way; we spent all of our time rehearsing the Best ways to tackle, the Best ways to run, the Best ways to block, pass, catch, throw, and the list goes on and on, but here is the critical aspect of what I am sharing.: We didn’t focus on the wrong way of doing things EVEN when we executed the wrong way. The whistle would blow and we would hear,

&gt;“Line up. Run it AGAIN.”

We knew someone screwed up, but it didn’t REALLY matter who or what they did wrong, every man on the squad focused even more intently on their role and knew that focusing on executing your own role perfectly was all you could do to try to ensure that it would be executed perfectly. This was a great training system for me to apply to every area of my life. If something works; try to duplicate it.

## APPLICATION

In my own life, I found a way to look at my family like a Team. I recently discovered how amazing my family is.

No really.

I was always looking for ways to improve our

* quality of life,
* success at work,
* school,
* peacefulness, etc.

So, I would focus on what areas needed to improve and what we were doing wrong. I spent a decade being a really crappy Team member and an even worse Coach because I didn’t know I was on a Team. I wasn’t applying the successful model which took us from 1\-9 to 7\-3, losing 2 of those losses and our final game to the State Champs by a closer margin than any other team on their march to victory. We focused on the great attributes of our Team and we focused on executing the correct way. The best way. When we failed, we doubled our mental intent. We didn’t have time to wallow, the average football play lasts 7 seconds. You don’t get time to relive that mistake. Get over it, shake it off, and play like a champion because you are one as long as you don’t quit.

## CONCLUSION

We applied this model to our home. We cheer for one another and look for ways to do things correctly when we fail instead of beating ourselves or one another over the head with guilt and shame to try to rectify learned behaviors. Take a moment and see if there is a way for you to support those Teammates in your life that may not know they are your team mates. Give them an opportunity to feel what it’s like to be cheered for, celebrate their victories as if they were your own, and tell them all the great qualities they have and can use to achieve their goals and watch the floor of the whole village rise.

Be Your Own Hero by adopting a Team Mentality.

[\*\*\*Having a Team Mindset LIVE Video\*\*\*](https://www.facebook.com/jamesppesch/videos/10156357454147889/)

[James Pesch](https://preview.redd.it/mexk9owkn3111.png?width=300&amp;format=png&amp;auto=webp&amp;s=53db0fb53c1cacfb4790c02e018a8e04f544eccf)

**JAMES PESCH WELCOMES YOU BACK!**

KEYNOTE SPEAKER | BUSINESS COACH | CORPORATE TRAINER | SALES TRAINER | HUMAN PERSUASION EXPERT | LINGUIST | NEUROLINGUISTIC PROGRAMMER \(NLP\) EXPERT | HUMAN BEHAVIOR EXPERT | TUTOR | LIFE COACH | DECEPTION DETECTION TRAINER | HR &amp; PERSONNEL COMMUNICATION ANALYST | TULSA BUSINESS CONSULTANT | TULSA BUSINESS MASTERY EXPERT | TULSA’S MOST ENTERTAINING SPEAKER | EDUCATIONAL INSTRUCTOR | TULSA BUSINESS COACH | EXECUTIVE CONSULTANT | BODY LANGUAGE, POSTURE, &amp; MICROEXPRESSION COACH

James is a Human Behavioral Specialist living in Tulsa, who is skilled in Linguistics, NLP, Mentalism, &amp; Psychology creating content so YOU WILL “Be your own HERO.” \-James Pesch

The PATREON PAGE…………………………► [https://www.patreon.com/jamespesch](https://www.youtube.com/redirect?redir_token=sjnoD2jSg8Pt_Bt692NZoIBwcVJ8MTUyNTkwMTYyN0AxNTI1ODE1MjI3&amp;v=d7QDNICyiKg&amp;q=https%3A%2F%2Fwww.patreon.com%2Fjamespesch&amp;event=video_description)

The Website…………………………► [https://www.jamespesch.com](https://www.youtube.com/redirect?redir_token=sjnoD2jSg8Pt_Bt692NZoIBwcVJ8MTUyNTkwMTYyN0AxNTI1ODE1MjI3&amp;v=d7QDNICyiKg&amp;q=https%3A%2F%2Fwww.jamespesch.com&amp;event=video_description)

Twitter………………….► [https://twitter.com/jamesppesch](https://twitter.com/jamesppesch)

Facebook……………..► [https://www.facebook.com/mindninjaJP](https://www.youtube.com/redirect?redir_token=sjnoD2jSg8Pt_Bt692NZoIBwcVJ8MTUyNTkwMTYyN0AxNTI1ODE1MjI3&amp;v=d7QDNICyiKg&amp;q=https%3A%2F%2Fwww.facebook.com%2FmindninjaJP&amp;event=video_description)

LinkedIn………………..► [https://www.linkedin.com/in/jamespesch](https://www.linkedin.com/in/jamespesch)

**YOUR SUPPORT MEANS EVERYTHING!**",8612,NLP,0
RSVP to Receive the First 10 Lessons for FREE upon next week's launch! https://preview.redd.it/brkc01ir52011.png?width=1275&amp;format=png&amp;auto=webp&amp;s=515e696d0c0cdd260c1f8c5b96ab044365cd0fa4,199,NLP,0
"I need to feminize my voice So I've been practicing for the better part of a decade. I've tried all sorts chants, exercises, apps, recording my voice and listening back over it. I've seen so many youtube video about on vocal feminize, any video you could suggest I've seen it. I still can't get my voice to where I want it. I can't maintain a feminine voice. I don't what I'm doing wrong but it's the bane of my transition.",423,NLP,0
"Stop Negative Self Talk  **STOP NEGATIVE SELF\-TALK**  

# ***Scroll Down to Bottom for Video***

May 14, 2018 | [James Pesch](https://jamespesch.com/author/jamesppeschgmail-com/) | [No Comments](https://jamespesch.com/stop-negative-self-talk/#respond) | [Blog](https://jamespesch.com/category/blog/)

#### SCIENCE OF SUCCESS

# STOP NEGATIVE SELF TALK

## INTRO

When I was a child, my parents came to me and told me that they would be getting a divorce. I was heartbroken because everything I knew was about to change. I never felt responsible for the separation, however, I still did not want it to happen. I couldn’t control the environment I was living in nor could I control the changes that were coming.

All of this began to create a story within my mind that initially brought me severe internal conflict and emotional anxiety. Then, a story teller came into my life whose parents were divorced, and they told me that I would NOW be getting 2 birthday parties, 2 Christmases, &amp; 2 summer vacations; now, that person was wrong about how awesome it would be, but for a brief moment, someone told me that a terrible situation had a silver lining and I let myself believe it and IMMEDIATELY, my anxiety went away.

That small reframe was the start of a powerful Linguistic tool that has been the secret sauce to what I can do in an area where so many others battle and now I am sharing it with you to empower you to Be Your Own Hero.

## TELLING YOUR STORY

When I first learned that I too was a story teller, I had no idea what a mirror neuron was. I had no idea that I, along with many like me, was very literally able to shift the way people felt about their circumstances using vivid imagery &amp; by changing perspectives. Let me show you how this works. Try this along with me to see what I mean, most people are great at this and can experience this powerful tool Easily and Confidently.

As you read this now, you will begin to notice that there is a real and powerful internal energy welling up within you, and as that feeling now begins to move &amp; shift you’ll find that this perfectly normal, and you are safe and fine.  I’d like for you to allow yourself now to notice how ready you are to change the voice within to that more powerful and motivational pattern that you have been seeking. Use that energy within you now to generate all the strength and creativity you have to change your Story.

## THE POWER of YOUR STORY

Did you feel that? On a scale from 1 to 10 how intense was the energy inside you? Could you, if you focused your mind on it, increase that feeling?

As you consider the moments when you have criticized yourself and meticulously scrutinized your own flaws, I want you to consider what it would have sounded like to have someone like me very literally standing next to you and saying,

&gt;“Yes, of course you have faults. We all do, but let’s face the fact that you are pretty incredible, kind, smart, &amp; capable of achieving anything you set your mind to.”

Sometimes, if we can’t see our own story differently, it can help to imagine someone else who could see things differently and allow that story to play in our mind for a moment or two.

## ALLOWING ANOTHER to TELL OUR STORY

How would it have looked if every time you began to have doubts about yourself if someone like me were there to look at you and say,

&gt;“You have got this. I know you have whatever it takes, and I am so proud of you for giving it your all every single day?” “You are special and deserve to pursue what you really want.”

Would that have changed how you felt about you. Does it change how you feel about you, now?

There was a movie from my youth called SideKicks where the Protagonist would imagine or day dream Chuck Norris there to help him through difficult circumstances and situations. As a child, I was most fascinated with why he chose Chuck Norris, but the real lesson was burned into my mind: **Perspective.** Story telling is all about perspective. Barry, the hero in the movie began actually training, in his spare time, with a real kung fu master and because of the imagery and daydreams, he was motivated enough to fulfill the dreams in “reality.”

## CONCLUSION

Whatever you focus on in your story will become your reality. In fact, isn’t it true that even now, when you focus on the greatness inside of you, the integrity, work ethic, and memories from all those who have contributed to who you are, don’t you become overwhelmed with a sense of purpose and power? And this new perspective causes a new voice that you are beginning to hear now in your own mind which forces you to consider what your life would be like if firstly, in your circumstances, you began to see the greatness within you rather than the problems in your environment?

Use this technique of story telling to change the way you see your environment. Believe me, if you can dream it, you can achieve it. So stop the negative self\-talk and

[\*\*\*CLICK HERE FOR VIDEO TUTORIAL\*\*\*](https://youtu.be/zAEqmviimWM)

Be Your Own Hero.

###  

[James Pesch](https://preview.redd.it/iwb4juyb7jy01.png?width=300&amp;format=png&amp;auto=webp&amp;s=fc82c258c47c1fd60ddfbc30c74924a3752ea851)

### JAMES PESCH WELCOMES YOU BACK!

KEYNOTE SPEAKER | BUSINESS COACH | CORPORATE TRAINER | SALES TRAINER | HUMAN PERSUASION EXPERT | LINGUIST | NEUROLINGUISTIC PROGRAMMER \(NLP\) EXPERT | HUMAN BEHAVIOR EXPERT | TUTOR | LIFE COACH | DECEPTION TRAINER | HR &amp; PERSONNEL COMMUNICATION ANALYST | TULSA BUSINESS COACH | TULSA CONSULTANT | TULSA’S BEST SPEAKER | MOTIVATIONAL INFLUENCER

James is a Human Behavioral Specialist skilled in Linguistics, NLP, Mentalism, &amp; Psychology creating content so YOU WILL “Be your own HERO.” \-James Pesch

The PATREON PAGE…………………………► [https://www.patreon.com/jamespesch](https://www.youtube.com/redirect?redir_token=sjnoD2jSg8Pt_Bt692NZoIBwcVJ8MTUyNTkwMTYyN0AxNTI1ODE1MjI3&amp;v=d7QDNICyiKg&amp;q=https%3A%2F%2Fwww.patreon.com%2Fjamespesch&amp;event=video_description)

The Website…………………………► [https://www.jamespesch.com](https://www.youtube.com/redirect?redir_token=sjnoD2jSg8Pt_Bt692NZoIBwcVJ8MTUyNTkwMTYyN0AxNTI1ODE1MjI3&amp;v=d7QDNICyiKg&amp;q=https%3A%2F%2Fwww.jamespesch.com&amp;event=video_description)

Twitter………………….► [https://twitter.com/jamesppesch](https://twitter.com/jamesppesch)

Facebook……………..► [https://www.facebook.com/mindninjaJP](https://www.youtube.com/redirect?redir_token=sjnoD2jSg8Pt_Bt692NZoIBwcVJ8MTUyNTkwMTYyN0AxNTI1ODE1MjI3&amp;v=d7QDNICyiKg&amp;q=https%3A%2F%2Fwww.facebook.com%2FmindninjaJP&amp;event=video_description)

LinkedIn………………..► [https://www.linkedin.com/in/jamespesch](https://www.linkedin.com/in/jamespesch)

#### YOUR SUPPORT MEANS EVERYTHING!",6736,NLP,0
Where do our beliefs come from? ,32,NLP,0
"Recorded NLP sessions? Hi all, 

Is anyone aware of recorded nlp sessions available online (paid or free)? 

Thanks!
",117,NLP,0
Fear of heights quick fix? ,27,NLP,0
In this video I talk about 2 nlp techniques I use to stop my negative thinking ,79,NLP,0
"Removing a Phobia of Flying Here's a video I id unpacking a live demonstration of removing a phobia of flying.

[https://youtu.be/eLL3JJOArGs](https://youtu.be/eLL3JJOArGs)",172,NLP,0
ARE YOUR BELIEFS LIMITING YOU? ,31,NLP,0
"NLP PREFRAMING | THE SECRET OF THE HIGHLY SUCCESSFUL [https://jamespesch.com/the\-secret\-of\-the\-highly\-successful/](https://jamespesch.com/the-secret-of-the-highly-successful/) 

PREVIEW:

# THE SECRET OF THE HIGHLY SUCCESSFUL

May 5, 2018 | [James Pesch](https://jamespesch.com/author/jamesppeschgmail-com/) | [No Comments](https://jamespesch.com/the-secret-of-the-highly-successful/#respond) | [Blog](https://jamespesch.com/category/blog/)**SCIENCE OF SUCCESS**

# THE SECRET OF THE HIGHLY SUCCESSFUL

**INTRODUCTION***Disclaimer:  \*If you are* ***NOT*** *in a position to actually attempt to use this technique in order to better your situation or IF you are not interested in optimizing any area of your life, this is a waste of your time; please find another way to spend your time. Thank you!\*  James Pesch is a human behavior specialist and can empower you to achieve whatever goals you can realistically create in your own imagination. If you can dream it, James Pesch can help you Be Your Own HERO and achieve it.* [*Click here to hire James NOW.*](http://www.jamespesch.com/) Most people wish it weren’t so hard to be **happy, content,** or **living** the dream life that American Television promises is lurking behind every scratch off lottery ticket, or behind every App idea that is invented after work over a few drinks. Well, this is not some article I have concocted in order to make you FEEL a little better about a crappy situation nor have I sat down to write this in order for you to have something to do to pass the time while you hide in the bathroom stall from your annoying co\-worker, Jimmy.  Nope, this little gem is being delivered to you by this medium, at this time, for one reason, I have but one mission in writing this silly little article and that is to empower you to\*\*BE YOUR OWN HERO.\*\*Once you read this BLOG and implement the trade secret of the Highly Successful, you will find that you are empowered to PREFRAME the outcomes of your specific obstacles &amp; circumstances. This simple yet powerful mental technique will enable you to overcome adversity more often and experience more successful outcomes. So, let’s intentionally engage with The Secret of the Highly Successful: PREFRAMING. . .[READ MORE](https://jamespesch.com/the-secret-of-the-highly-successful/)",2315,NLP,0
"HOW TO DEAL WITH A BULLY | NLP REFRAME \*\*\*Scroll to Bottom for Video\*\*\*

**HOW DO YOU DEAL WITH A BULLY?**

Yesterday, Minime, my 9 year old, sat down with me to help educate others on a very simple MINDSHIFT STRATEGY™ but HIGHLY Effective for helping people Gain control over their own emotions and FEEL more in control of their circumstances.

This Shift in mindset and mentality known as REFRAMING will set you in the right frame of mind to handle your obstacles from a positive and constructive assertion and will bring more successful models and patterns to your mind in order to have better outcomes.

Use this and all my secrets to empower you to

**BE YOUR OWN HERO!**

[How to deal with a Bully | NLP REFRAME ](https://youtu.be/MWDNeIQgNUM)",755,NLP,0
"NLP Anchoring and drugs *First and foremost, I would like to state that what I am asking is hypothetical. I have no intention to use drugs for this reason or any reason, nor do I promote the idea of using them on the basis of whether or not they may or may not be applicable in NLP Anchoring.*

I recently finished reading a book by the infamous Wolf of Wall St. himself, Jordan Belfort. Belfort discusses the importance of self-management, which I think we can all agree is vital in order to sustain a business. 
One thing that caught my attention and actually brought me to this subreddit, was his avocation of NLP. 

Belfort claimed that by triggering a certain sense during a point of 'absolute certainty', it would be possible to bring back that very feeling on demand. Of course this caught my interest. I mean, who wouldn't want to be able to recall a time that they felt unstoppable, whenever you want?

Belfort recommended a more niche form of NLP Anchoring, known as Olfactory Anchoring (anchoring via the sense of smell), as it the neural-receptors in the nose can be strongly attached to memory (..you ever felt a smell that took you right back to when you were at your grandma's house? - yeah, that kind!). 

What I'm hear to ask is whether a drug-fuelled state can be imitated (at least to an extent) via NLP techniques. For example, setting a relaxation anchor when high on cannabis, or an anchor of confidence and certain while on cocaine. 

Assuming it is possible, would placebo play a role in any physiological changes that may occur? ",1554,NLP,0
Reframe It! ,12,NLP,0
How to stick to your goals ,27,NLP,0
✋How to Stop Binge or Emotional Eating🚀 ,40,NLP,0
"Help with embedded commands please I know the theory of how they work. I tried it on a friend and I thought I didn't a good job... except it didn't work.

Can anyone please help me understand why?",196,NLP,0
"NLP in New York I’m fascinated with NLP and am looking for a quality training course in New York City. When I do a google search, a ton of different options come up. Can anybody recommend something in the city?",210,NLP,0
HOW TO LET GO OF A TOXIC PERSON | SITUATION | LEAVE NEGATIVE PEOPLE | MI... ,76,NLP,0
HOW TO BECOME MORE PERSUASIVE | INFLUENTIAL | CONVINCING | NLP | MIND SH... ,76,NLP,0
Looking for a Discord server on NLP Does anyone know of a NLP focused discord server?,85,NLP,0
WHAT INSPIRES US? | MIRROR NEURONS | PATTERN &amp; MIND SHIFT STRATEGY | NLP... ,80,NLP,0
MIRROR TECHNIQUE | HOW TO BREAK SUBSTANCE ABUSE | HABITS | DIET EXERCISE... ,76,NLP,0
When NLP turns baaad..... ,26,NLP,0
"Focus on what you want Have you heard the expression 'Don't think about pink elephant!'? What do we find ourselves doing? - Thinking about pink elephants, of course. In order to make sense of the instruction, we first have to get a picture in our head of a pink elephant. So we have now done exactly what we have been told not to do, even though it wasn't our intention. That's how negative goals work, too. As we focus on what we don't want, inadvertently we are actually getting it.



Thinking about what you don't want is **problem thinking,** whereas thinking about what you do want is **outcome thinking.** Problem thinking arises when we focus on the thing that's happening now, which is, of course, the thing we don't want. So the only way we will change and experience more of what we do want is to move away from the present problem state and towards the compelling outcome.



There are two directions in life, forward and backwards, towards and away from. NLP was developed and focused back in the 1960s on successful people that were all 'towards'-oriented. They didn't waste their time and energy on what they didn't want but focused on getting what they did want.



'Away from' thinking focuses the mind on the present unsatisfactory situation that they want to change. When we try to get our brain to focus on what it doesn't want, it gets confused. Think about some of these goals; have you ever had goals like these?



&gt;'I want to give up smoking.'
&gt;'I must clean my room.'
&gt;'I've to stop eating junk food.'
&gt;'I hope I don't fail.'



Have you ever told yourself 'I mustn't forget to...' and then found that you did indeed forget whatever it was. Your brain is saying, **'Forget...'** whereas, if instead, you told yourself **'Remember to...',** you'd have a lot more chance of being successful. It happens sometimes with children, doesn't it? People say, 'Be careful don't fall!' and then they do just that. It's because subconsciously they've heard the instruction to fall and they've focused on that word.



When people are focusing on the problem and being in a problem state, there can be fear about what they'll do when the problem's fixed. What will they focus next? Will there be a void in their life? Some people worry that they are no longer worrying about this thing. This is another very good reason for focusing on something enjoyable, something they do want or who they want to be.

### THE WORD 'TRY' IS THE ENEMY OF SUCCESSFUL GOAL-SETTING

It is simply a variation on problem-state thinking. The word 'trying' implies that you doubt whether you'll be successful but nevertheless, you're going to put it out there as a goal. By using the word 'try' no one is convinced you mean it, so you won't get any support. How hard are you trying really? It sounds as if this is a goal that you think you ought to have because someone else has suggested it, or you have read a book or blog post that recommends it, so you're going to give it a 'try'.


Instead, remove the word 'try' and replace it with the belief that's important. It's something you do want to do, so start picturing how success would look. If, when you do this, it isn't that compelling, then change your goal to one that is.

Trying does not form any part of goal-setting and doesn't belong there. Instead, word your goal in the positive as something that you will be or get and take responsibility for achieving it.

### I WANT...

As part of your personal development, start thinking about what you do want in every situation.

Even, for example, when you're shopping and you're looking for a parking space, think in terms of 'I want a parking space' that than 'I bet I won't get a parking space.' Amazingly, it has been proved that people are more likely to get what they desire (yes even parking space) when they focus positive energy on what they want.

It's not just about what we say and how we think, it's the way we live our lives and what we notice. When we notice what's going well - the good decisions and choices we're making and the steps we're taking along the road to our goal - rather than pouncing on the disappointments and false moves, we will find ourselves closer to our goal.


### When, How, Where

Apart from wording your goal in the positive as something you do want, you can also orient yourself toward getting it by adding in words like 'when', for example. 'When I am fit, I will...' or 'When I have passed my exam, I will...' Add to this by building a picture of how you will look and where you will be. Can you get an image in your mind of yourself in the future?

People who are able to picture themselves as a leader; manager and powerful influencer within their organization start working towards this role by dressing the part, networking and impressing those they need to impress. They not only want to see themselves in the role but they also want others to picture them there.

When you talk about that time in the future when you will have achieved your goal, you make it start happening by talking as a different identity. Someone wanting to become healthier eats and behaves differently from someone who has a goal 'trying not to eat the wrong sort of food', and people wanting to move up the career ladder make sure that their work is done to deadline and that they behave in a way that shows them in the best possible light.

When you talk about your goal like this and take on the new identity, it starts to become your reality. Your future is already becoming your present. There is no question in your mind that you will achieve your goal, as using the words 'If I get fit, I will...' would suggest. When we think in terms of 'If I achieve my goal...', it suggests that there is some doubt in your mind that it may not happen. It also firmly places you in today's problem state as opposed to being on the way to a future where your goal has been met.

### **TLDR;** Focusing on what we do want rather than on what we don't want is a matter of thinking first what we really want, imagine what it will be like and taking it on as our identity. This moves us from a state of problem thinking, where the focus is on what we don't want and moves us toward what we want.
",6220,NLP,0
HOW TO READ EYES | INFLUENCING HUMAN BEHAVIOR | MENTALISM | LINGUISTICS... ,75,NLP,0
How to indirectly mirror someones breathing while i am speaking? ,65,NLP,0
"HOW TO CURE FEAR &amp; PHOBIAS | NLP | REMOVE FEAR | CURE PHOBIA  *\*\*SOURCE CREDIT: THIS TECHNIQUE IS MY OWN SPECIFIC METHOD, but it is BASED OFF OF THE TECHNIQUE ON PAGE 195 OF DERREN BROWN’S TRICKS OF THE MIND.\*\*\**

VIDEO TUTORIAL LINK AT BOTTOM

https://preview.redd.it/n66gaf6r9es01.jpg?width=640&amp;format=pjpg&amp;auto=webp&amp;s=edae7e0307b2607c6d0121e6e8ceaa3c0521f35f

**INTRODUCTION**


&gt;**“I can’t breathe,”**  
&gt;  
&gt;**“it makes me shiver uncontrollably,”**  
&gt;  
&gt;**“I can’t sleep,”**  
&gt;  
&gt;**“I break out into sweats.”**

Does any of that sound familiar? Do you have a FEAR or PHOBIA that you would like to be FREE from Now? This MIND SHIFT STRATEGYTM has worked for so many! We have seen instant and total elimination of

* FEAR OF HEIGHTS
* CLAUSTROPHOBIA
* AGORAPHOBIA
* HYPOCHONDRIA
* FEAR OF PUBLIC SPEAKING
* FEAR OF FAILURE
* &amp; PTSD

in my own practice and I am confident that that this simple but powerful technique will work for you NOW. Quickly, before we begin, if you have a fear of visualizing, imagination, cinemas, or movie theatres, please stop reading now and contact me directly for a different method.
Now, before we get started, I need you to take a moment or two and imagine an activity which you are able to do very well. It could be anything, as long as you know for sure that you are excellent at it. Can you Bake a lasagna, are you an athlete, a host, a caretaker, a friend?
THINK now and really picture that activity which you know you are absolutely great at achieving. Now, the feeling you have when you picture yourself doing that action well, that feeling that you feel NOW is the mentality of a winner and the exact mental state that you must really be able to generate before we move on to the first step of this FAST \+ EASY MIND SHIFT STRATEGYTM. So, if you have been intentional and if you feel bold, continue.

**BODY**


Now, as you read these words, and now that you know that you have established a safe and clear mental state that makes you feel empowered and ready to engage with this successful solution to this little issue we can begin. First, imagine yourself entering your favorite movie theater. If you do not have a favorite, it doesn’t really matter, but you must feel safe, secure, and the imagery of the theater must be vivid and real to you. As you smell the buttery popcorn wafting through the air, you may stop by the concession stand and grab a beverage or candy selection before entering the actual auditorium where we will pick our favorite seat. Did you make a selection? What did you get?

https://preview.redd.it/d5hpwrlx9es01.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=511deb877f01f7289ba1ff78a0644c0d59e966cf


Jolly ranchers, skittles, snickers, twizzlers, or traditional popcorn, get whatever you like.


Ok, great! As you find the perfect seat in the theater, you will now notice that there is a room at the back of the theater where the projector for the film is, that room is also for you; when ready, you will control when the movie will begin. The control room is entirely run by you, when you are ready to begin, however, it is important that you only visit the control room IF you need to see yourself watching the memory for any reason, but otherwise you will relax, get comfortable and control this video of your memory from your seat.


As you look around the empty and yet cozy theater, pick your favorite seat and settle down and relax. As you now begin to notice the lights in the theater dim, you will begin to see a movie play out on the screen in front of you. You’ll have to squint a bit because the image is extremely small on the screen in front of you; as you adjust your eyes, you will begin to be amused as you see that the fuzzy, black and white picture is a movie of your memory. The small film is a picture featuring you as the HERO, but it is also of the FEAR or PHOBIA that we are about to eliminate. This is the event that occurred that started this series of emotions. Good. Now focus. The events on the screen will play at normal speed but there will be a few important differences; instead of the feelings we would normally feel, we are now going to layer a comical audio file over the top of the film. There will be no elaborate or scary sounds in this movie, other than humorous ones; be sure to use a laugh track from a sit com or your favorite funny theme song; I use THE OFFICE &amp; It’s Always Sunny in Philadelphia and play this over and over again to drown out the sounds of the original memory.


***Note: Occasionally, as some people have engaged with this image, they have felt overwhelmed by the thought of watching this memory play out; focus and relax, we have a solution for this as well! As you breathe steadily and peacefully, you can take a moment and drift back to the control room where you can watch yourself bravely facing the film of your fear. As you face your fear, now is a good time to encourage yourself and tell yourself how proud you are of yourself for facing this issue.*** 

https://preview.redd.it/ln0vf9m0aes01.jpg?width=710&amp;format=pjpg&amp;auto=webp&amp;s=6603b7719514cfa32596e2516d6e40cd8123c4ba


Now, as the film ends, it is time to take our power back from this memory. As we process how much better we already feel, the movie is over and the reel hums as it comes to an end. As this happens, we are going to stop the movie of our memories on the last frame.Now, look at your face on the screen and step into that body, you will now run that movie in reverse at 2XX the original speed as vividly and colorfully as you possibly can until you arrive back at the start. Fast, QUICKLY, RUN IT ALL THE WAY BACK!
GOOD!


**CONCLUSION**
Now, as you look into the eyes of the new you, the you that has conquered that mental image that you had, how do you feel? Do you now feel freedom, joy, and peace? GREAT!! That means that you have completed this technique and you can live a life of confidence and victory over a minor issue that used to hold you back in some capacity. Congratulations on your Future and please share this content with someone that you want to enable to Be their own HERO just like you were able to do.
IF YOU ENGAGE BETTER WITH A VISUAL EXAMPLE, I HAVE UPLOADED THIS TECHNIQUE TO YOUTUBE AND WILLPROVIDE THE LINK HERE: 

[HOW TO CURE FEAR &amp; PHOBIAS | NLP | REMOVE FEAR | CURE PHOBIA](https://youtu.be/lnPX1HcVWd4) ",6425,NLP,0
"HOW TO GAIN CONTROL IN 3 STEPS | NLP | SELF HELP | JAMES PESCH HOW TO GAIN CONTROL IN 3 STEPS
2018-03-21 | James Pesch | CPT NLP
HOW TO GAIN CONTROL in 3 STEPS
 
INTRO
How many times have you wished you could gain more control? What comes to mind first? As you think now about the time or times you have wanted control, which events rush into your mind fastest and could have turned out better for you? Now, that this imagery is clear, what do you mean by
 
“GAIN CONTROL?”
 
I immediately picture an aircraft pilot plummeting in a random spin toward earth. The pilot, with sweat pouring over his lips into his grit teeth, I know that his mission is to GAIN CONTROL of the craft; we ALL know the pilot is successfully able to GAIN CONTROL as he manages to handle the situation &amp; save the maximum number of lives and prevent as many casualties as possible.
 
STEP 1
As I describe this scenario, you may have your own idea of what it means to GAIN CONTROL of a situation pop into your mind, this is PERFECT! Use your scenario NOW; the main starting point for this technique to work is

Find, create, or invent VIVID and emotionally stimulating IMAGERY and mental avatars like the scenario I just described that give you strong associations with what it means to GAIN CONTROL. For me, this is often reflected in those we see as HEROES
 
 
FIREFIGHTERS &amp; PUBLIC SERVANTS 
TEACHERS &amp; COUNSELORS
DOCTORS &amp; NURSES
RESEARCHERS &amp; SCIENTISTS
WARRIORS &amp; MARTYRS

 
STEP 2
Use the imagery to create a mental scenario. Think and focus now while you begin to use these examples to draw a bridge of correlation; some call this METHOD a parable or analogy wherein you find a way to make your actual situation you are living out (relationship, career, habits, anxiety, etc.) and then you simulate the imaginary scenario to demonstrate to yourself what it would look like for you to be your own HERO, and see yourself GAIN CONTROL over the situation similar to the Pilot/Plane scenario WHILE drawing an emotional correlation to your specific circumstance.
 
2. Connect and correlate the specific outcomes that would demonstrate why you as the pilot of your situation would benefit from being able to GAIN CONTROL, and what the results WILL BE once you do. As the plane hurls toward the earth and everyone on board panics, SEE yourself remain calm and collected, you, as THE pilot, are the ONLY one who is capable of successfully being able to GAIN CONTROL now and you know that you are qualified, strong, and confident that you do have the right answers even if you can not control the outcome fully.
 
As you now begin to see yourself able to right the craft, and with all your strength and courage, you are able to do whatever it takes and grab the control stick and land yourself and everyone else safely, don’t you NOW begin to see how your life will improve and how much easier it will be to GAIN CONTROL of any of your situations once you realize that ONLY you can?

 
 
STEP 3
Finally, How will your life improve once this issue is resolved?!
Whew!
The plane has been landed, by you! CONGRATULATIONS!! You were able to GAIN CONTROL of your situation, and fully create the imagery you need to overcome those fears that use to limit your ability to GAIN CONTROL. Now, that you have done this for your situation (relationship, career, habits, anxiety, etc.,) How has your life improved?
 
3. Focus on the end goal and successful scenario where you were able to be your own HERO and GAIN CONTROL of your situation. Create clear and vivid pictures of the lives which will be changed and the improvements you will feel as you begin to use this FAST &amp; EASY MIND HACK to GAIN CONTROL of this situation. What does it look like now that you have been able to GAIN CONTROL in this area?
 
BE SPECIFIC as you begin to write down exactly how your life will improve.
 
 
CONCLUSION
That’s it!!
As you begin to use this FAST &amp; EASY MIND HACK to GAIN CONTROL of THIS situation, you may find a strong desire to use it for other areas in your life that held you back so here is a summary of the 3 steps!
Find, create, or invent VIVID and emotionally stimulating IMAGERY and mental avatars like the scenario I just described that give you strong associations with what it means to GAIN CONTROL. For me, this is often reflected in those we see as HEROES.
Connect and correlate the specific outcomes that would demonstrate why you as the pilot of your situation would benefit from being able to GAIN CONTROL, How do you know you do need to GAIN CONTROL? BE SPECIFIC.
Focus on the END goal and successfully completed scenario where you were able to be your own HERO and GAIN CONTROL of your situation. Imagine what you will look like, how you will feel about this, and how others will be affected by this positive change.
Great! If you engaged intentionally then you are finished! If not, do it 3 times from start to finish each time connecting more with being the HERO of the scenarios. I highly encourage you to look for more ways to incorporate this POWERFUL technology in other areas of your thought life; also, I do this full time and am available if you have trouble with mental imagery or need scientific data or resources to learn more about how these and all my MIND HACKS work so fast and effectively!

[WATCH THE VIDEO INSTEAD!!](https://youtu.be/z-_SIuWakwM)
 ",5371,NLP,0
"How to change yourself. You have a bad habit. You know it. But you keep doing it. 

Stop worrying about it for just a second, while you're reading this. 

Actively think positive thoughts to yourself constantly. ""I can beat this, I don't have to do that, I will do that other thing."" etc, whatever truly motivates you without giving you bad feelings, or a possibility of a criticism from your bad habit's habitual thoughts.

Do this forever. Your background thoughts (that aren't under your direct control, unfortunately) will catch up and start repeating what you've been thinking for so long. Then your new good thoughts will have become habitual. 

Use your new habitual good thoughts to motivate yourself to act in the way you want. There you go. Changed. 

It takes a lot of time, and effort, but it works 100% if you commit and stay committed. Have fun!",859,NLP,0
"Sources of NLP for absolute beginners? So I've been super interested in NLP since I first come across it some time ago. But I haven't done anything with it because the NLP space is so chaotic and messy and not very friendly to non\-gurus and people who need the benefit of the techniques, but aren't well\-versed about the topic and aren't excellent researchers.

There's various sources I found and various websites, but when I visit the website, I'm often greeted with a site that does not look legit, or a website that is down, or a website that seems not to reflect the power of NLP in the website's construction itself. I mean, you'd think someone with the power of NLP can manage to build a decent, welcoming modern\-looking website.

Overall, the journey to learn NLP is not in any way friendly or encouraging. Sometimes I get the impression that this is a power from the gods. But I never find it easy to acquire the power. It's like I have to sift through a whole bunch of garbage, filter it all out myself, and *maybe* come out the other side with something useful to use in my life.

*Another way of looking at the issue I'm introducing here: It's impossible for me to recommend anyone to start learning NLP. Where would I point them to? Which book would I recommend for them? Which techniques would I give them as an introduction to what NLP can do for them? Where would they then go to learn more about this? I can't expect everybody to research rigorously in order to learn something new.*

*An example of such an issue outside the realm of NLP that has been properly solved: If someone has never used a computer and you want to introduce them to this new power that is computers and the internet, you can simply tell them to buy the newest MacBook, and that's it. Period. Problem solved. Where would they go to learn more about this awesome new technology thing? They can take a look at other products Apple provides.*

*Now this is an extremely well\-solved example, I don't imagine this NLP thing to turn out the same way so soon of course. But just to give you some perspective.*

I bought a ""Best\-Seller"" NLP book \(""NLP 2.0""\) and was disappointed by it. I know I can make more out of it, but it isn't making it very easy on me. Which is part of the problem I'm hoping using NLP techniques would help me deal with. Just to bring up one last point: Rarely can I find NLP techniques that I can *actually apply* and that a*re actually relev*ant to me. I'm very disappointed to find out how hard it is to find generic NLP techniques that deal with extremely common issues that almost everyone in the modern world has. Such as anxiety and whatnot.",2663,NLP,0
"Overcome premature ejaculation by anchoring the time of ejaculation Guys, I am 35 now and suffered from premature ejaculation almost my whole life. I was able to overcome it in the last year through the use of antidepressants and therapy and now I can last about 8-10 minutes in bed which is absolutely amazing for me. However I am still taking the medicines so I can’t consider myself cured yet.

So what if I can anchor the time of ejaculation with some unique discreet physical action (like pressing the tongue against the back of my teeth, for example) so I would only be able to ejaculate when executing this action? Would it be possible?

If possible it would for sure be my last step to consider myself really cured from this problem.

What are your thoughts about that? Any tip or suggestions for me?

Please do not make jokes as it is serious, caused me a lot of problems and affected a lot my self-steem my whole life.

Thank you.",940,NLP,0
"What Milton model books, websites do you recommend voor an NLP practitioner? I would like to learn more about the Milton model to ask good/better questions. What are some good books and websites that you would like to recommend to me?",234,NLP,0
"HOW TO GAIN TRUST using Cold-Reading &amp; Barnum Statements in 4 EASY STEPS https://preview.redd.it/5x6fflir2tr01.jpg?width=828&amp;format=pjpg&amp;auto=webp&amp;s=104deb34859c0c9986fca8f38e742ceb6a58a2f7

**DISCLAIMER:** ***\*\*These techniques and methods are extremely POWERFUL and INFLUENTIAL, always assess and take priority in ensuring that the emotional and mental state of the person you are influencing is healthy. However, if you choose to use these methods solely for personal gain people will begin to distance themselves from you; don't be an idiot and use this power to help others and empower your own life simultaneously to optimize these HACKS!***

## INTRODUCTION

Preachers use it, Psychics live by it, &amp; Mediums can't influence a single soul without it. What non\-magic, magical formula am I sharing?!

https://preview.redd.it/e0yu5mlq2tr01.jpg?width=640&amp;format=pjpg&amp;auto=webp&amp;s=ba1166b09fa1e43dffc712e6db7e59b34424611a

&gt;*How to GAIN Trust using ""Cold\-Reading"" and ""Barnum Statements""*

Everyone uses ***Barnum Statements*** from time to time, but only the greatest communicators and salesmen learn the Psychology behind Human Behavior and the Language that determines the way we FEEL toward others.

Barnum Statement:

&gt;*“Deep down you know your own self worth, however you find that often you are far too critical of yourself and this limits your confidence in many areas.”*

 See, how that sentence seems like it is specifically and uniquely designed for ***you?!***

## It wasn’t. 😉

 Imagine walking into all your future meetings, dates, or emergency scenarios equipped with a secret that would make you the most

##  ·       Trusted

## ·       Influential

## ·       Confident

## ·       &amp; Persuasive

person in the room.

 As you begin to see yourself walking into situations with incredible power and control, this unique gift will endow you with inexplicable favor and success, which area of improvement in your life is the most significant?

 Which situation matters most and which one will change the most in your life once you learn and implement these 4 methods into ALL future social interactions?

 NOW, you can excitedly and with great focus, as you realize and notice the exact scenario you will apply this to for massive growth and optimization, continue.

##  MIND SHIFT STRATEGY (MSS)™ 4 STEPS to use Cold-Reading to Gain Trust

##  1.    Make Statement Personal 

STEP 1: USE “You” &amp; Exact Words provided by your audience

This seems to be common sense, however, most people have no idea how to do this naturally and powerfully, so pay attention closely.

Let me ask you a question. How often do you pay attention to the manner in which others introduce themselves? Has anyone ever made you feel incredible about yourself by remembering your

### ·       Sign: Libra, Scorpio, etc. . .

### ·       Name: If they say “Christopher,” Don’t say, “Chris.”

### ·       or detail: Wife’s name, Pet, vehicle, clothing, etc. . .

Did this person seem like it was difficult for them or as if they were searching for a way to connect or manipulate you, or did their compliments and admiration of you and your talent flow uninhibited from their soul? Focus. Others are incredible. You can see their wonderful and impressive qualities. Say it, casually, and intentionally. Make sure you are heard, and then begin phrasing opinions using “YOU” statements ie

 “You often find. . .” or “You, like most, probably. . .”

##  2.     Use Sincere Language

 STEP 2: UTILIZE Sincere Language

Use words like ***“Feelings,” “Deep down,” “on the inside,” “Honestly,” “Personal”***

Examine your audience closely, \(this works best when you begin working with a single, captive and voluntary audience.

As you focus now, on sitting across from another human, imagine trying to comprehend not what the other person is thinking but what they are feeling. When we teach mirroring techniques, we teach our practitioners to try to feel what others feel by mirroring their body language markers.

&gt;***Posture*** *– If they are leaned back and appear relaxed, so do you; likewise, if seated forward and more engaged in anticipation, mirror this posture*  
&gt;  
&gt;***Eye Contact*** *– Some people love it, some hate it, mirror whatever they prefer*  
&gt;  
&gt;***Limbs*** *– If they are crossing their arms, have hands in pockets, legs crossed, or fidgeting, mirror a similar mannerism as casually and naturally as possible.*  
&gt;  
&gt;***Words*** *– I can not stress this enough; use their words, EXACTLY. If they say they need to relieve “Anxiety,” then you use that exact and sincere word. If they state that they are looking for an affordable solution, then you find and deliver the most “AFFORDABLE,” solution you have, etc.*

## 3.     STEP 3: USE Vague &amp; Ambiguous Language

     STEP 3: Use words like “Rarely, Sometimes, Occassionally, Often, etc.”

Most people are totally ok with words like Most, Often, Rarely, Usually, because although they appear to be leaning toward one end of a behavioral spectrum, there is still plenty of room in these words for cognitive relativism. Most of us can find ourselves agreeing with someone who is opinionated but leaves much space in the conversation to be themselves entirely incorrect about their opinion, that is why these words have a tendency to be useful in building emotional rapport.

There are certain words which do NOT allow for this cognitive space and distances others from our perspective or words:

&gt;***NEVER USE “NEVER;” ALWAYS STAY AWAY FROM USING “ALWAYS”***

\(unless the audience volunteers “Never or Always,” just don’t.

##  4.     Step 4 Create an Out: Offer a double bind option

STEP 4: Offer The Linguistic Double Bind

Many expert and human behavior specialists develop subtle and savvy strategies to get the result they want in conversation by offering an out in the way of a double bind.

##  For Parents: ""You’ve been very well behaved today, would you like to go to bed at 8:00 or 8:30pm?”

## For Sales: “As someone who is obviously, honest and transparent, like myself, would you rather disclose the budget you are working with, or would you just like to move forward to the pricing options we are currently offering?”

## For Cold-Reading &amp; Barnum Statements: “I can tell you are someone who is smart and that causes you have a tendency to hide details from others, however, once you establish trust, you’ll find that you are quite open to sharing your deepest thoughts.”

##  CONCLUSION

As you now begin to use these 4 FAST \+ EASY Steps to increase your own influence, pay it forward and tell others about your ability to optimize your success and future relationships &amp; share this article with as many people as you can.

 Visualize the impact you will have on your social circle and the ways you will be able to help those around you by making them feel incredible about themselves and take special note of the ways this will change the level of influence you begin to experience in your day to day life.

 Thank you so much for reading, and if the above article was not quite good enough, please checkout my 5:00 min YouTube Video where I share the 4 steps quickly!

[GAIN TRUST IN 4 STEPS](https://youtu.be/7npBe4DLRiI)

Enjoy and don't forget to Be Your Own HERO!!

James Pesch welcomes you back! James is a Human Behavioral Specialist skilled in Linguistics, NLP, Mentalism, &amp; Psychology creating content so YOU WILL ""Be your own HERO."" \-James Pesch KEYNOTE | BUSINESS COACH | CORPORATE TRAINER | SALES TRAINER | HUMAN PERSUASION EXPERT | LINGUIST The PATREON PAGE..............................► https://www.patreon.com/jamespesch The Website..............................► https://www.jamespesch.com Twitter......................► https://twitter.com/jppdiddy Facebook.................► https://www.facebook.com/mindninjaJP LinkedIn....................► http://www.linkedin.com/jamespesch Your support means everything!",7991,NLP,0
What techniques do people like Tony Robbins use in their work? ,63,NLP,0
Which are the best patterns to improve learning? Are there any specifically designed for language learning? Thanks in advance.,126,NLP,0
NLP useful skill to learn to teach kids under 5? im thinking of taking a nlp class in nyc ,90,NLP,0
"not sure if the NLP specialist I'm seeing it very good or very bad... I don't really understand NLP but is seems something quite powerful and decided to give it a shot. So I went to this NLP specialist in my area with a very specific need: I want to be more optimistic and have better self-esteem. However, somehow he translated this into desire for inner peace (I had to remember a happy peaceful moment from the past and that feeling would come up in stressful moments). I think I communicated well, I voiced my concern that I was not looking for ""peace"", so I'm not sure if this mismatch of expectations is a feature (he somehow ""reads"" that what I actually need is peace) or a bug (he can't and don't want to act on issues of self-esteem and optimism).

The thing is our family is closing our business and I must do something else from now on. Whatever I start I must do it believing it will succeed, and not in my current pessimistic mood. I wanted NLP to be one tool to help me prosper, but I'm concerned about the specialist. What do you think?",1051,NLP,0
Fun with spacial anchoring. ,28,NLP,0
(Request)David snyder - Real world hypnosis full product ,57,NLP,0
"Using presupposition in affirmations? looking for some examples of this online and not finding much, just am thinking this could be a clever way to use affirmations to bypass the conscious mind a bit and entrench better in the subconscious?  thoughts?",251,NLP,0
"Making a Statement That Bypasses and Makes You Assume Something is True? i know i am not explaining this well enough but I am interested in working with affirmations and phrasing them in such a way (by perhaps using some NLP magic) to make my subconscious mind REALLY BELIEVE what I am saying ... I know there are things like this in NLP, can anyone assist?",357,NLP,0
"Most common POS reorderings between english/spanish Does anyone know which are the most common Part of Speech Reorderings between english and spanish?

There are some that are rather obvious, like [ADJ, NN] ('red car') -&gt; [NN, ADJ] ('coche rojo'), but I've been trying to look for data, but I can't seem to find any clear numbers. 

Any help appreciated.",357,NLP,0
"[HIRING] NLP engineer (France) Join us at vivoka, a team of young people working on a futuristic and well founded project. mp for more information.",147,NLP,0
Who are the major NLP teachers? Are there any recommended training programs? Thanks!,84,NLP,0
"Olfactory anchors? So I've been reading Jordan Belfort's (the guy Wolf of Wall St. is based on) fairly recent book and he includes Olfactory anchoring as a method of state management. 

I can't seem to find much info about it. Can someone please explain what it essentially is and how it works",293,NLP,0
NLP Modelling (more ranting) ,29,NLP,0
"Curious Why would someone replicate exact time of a phone conversation? For example, I end a call after 18 minutes. Next time I speak to that same individual, they end call call exactly after 18 minutes. I’ve noticed a pattern with this individual. Why would they seek to replicate time on a call? Thanks ",305,NLP,0
NLP issue How do I stop using self NLP on myself? ,50,NLP,0
Identifying the Lynchpin in PTSD ,33,NLP,0
What pattern to use to quit watching TV? What's your opinion about which pattern could work best in effectively quitting  watching TV-shows forever?,148,NLP,0
"Any way to become fearless completely? I really want to lose my sense of personality and become like james bond character, immune to rejection , dont fear and become more powerful

Any ways to can recommend me?",210,NLP,0
What have you guys accomplished with NLP? ,42,NLP,0
"How Do I ""program/refrase"" myself when I see a |pit fractal of emotion | in one of my sentences? ",97,NLP,0
Sneaky NLP practices for a Game? ,33,NLP,0
"""One Flew Over The Cuckoo's Nest"" Recursive Frame Analysis ",59,NLP,0
Breaking the Rules - NLP Advanced Mastery Training 2009 with Andrew T. Austin ,78,NLP,0
"NLPers are STILL claiming to be members of the British Board of NLP, over 10 years later... ",92,NLP,0
Sleight of Mouth Patterns in Schizophrenia ,43,NLP,0
Accidental Post Hypnotic Suggestions in Clinical Treatment Settings ,68,NLP,0
"Language, idioms and direction of movement ",43,NLP,0
Business Applications for Natural Language Processing (NLP) ,60,NLP,0
How To Change Your Mental State | Modalities Explained ,55,NLP,0
"Best Beginner NLP resources? What would people recommend? Which books, videos, seminars?",88,NLP,0
"NLP is already a thing? Of course it is. I discovered NLP for myself but obviously a lesser form. I thought it was a combination of CBT and placebo with a dash of awareness of awareness and all that. I’ve been working on models and theories for years trying to describe my experience with this. 

I’m so happy to discover a full community and all the resources to improve my improvement journey. Thanks and AMA about my jimmy rigged NLP such if you like. 

:)",459,NLP,0
"'The Map Is Not The Territory' Explained, NLP BASICS ",53,NLP,0
"Wow, I’m blown away with what I’ve seen done with NLP, what are your favorite resources? I want to learn! So just meeting an NLP therapist and talking with him caused me to change my beliefs and thought patterns.  I want to continue to learn as much as I can.  Some day I’d like to help people with NLP therapy.

Can you please provide any tools or links that were specifically useful?  I’ve been exploring YouTube and using google search.  I’m hoping to find some nuggets of wisdom here that would otherwise take me longer to find.

Thanks so much!",549,NLP,0
How to stop unwanted mental-movies ,35,NLP,0
Autosuggestion And Reprogramming The Subconscious (Franz Bardon Hermetics) ,75,NLP,0
"There are 8 ""rules"" in NLP. Here's 4 of them ",45,NLP,0
"I need help with my health ASAP I believe in NLP by have never tried.  I’ve been dealing with stomach problems for 4-5 months.  I just got food poisoning in Laos 48 hours ago and have returned to Thailand where I can get clean and healthy food. 

What is the best way for me to begin healing myself?  I’m open to any and all suggestions ",337,NLP,0
"How much should NLP therapy cost? I’ve met a therapist while traveling, we’ve talked and I can see that he is passionate about what he is doing and can probably help me.

The problem is, I don’t have a ton of money right now.  And he money that I do have I strongly want to be using for other things.  Maybe my priorities are misaligned, but I don’t think I can justify putting 20%-30% of my net worth into this process.

If it works and I could pay retroactively, then that’d be great.

I am specifically trying to get control of my health.  As I’ve been having stomach problems for the past 4 months.

To master my own health and reprogram my mind and my perception of health, how long should this take and what is a fair price?",730,NLP,0
"The Four Agreements: Best way to be spoken Hi all-- noobie here. 

I have limited experience with NLP, having been to a workshop at one time and working with a couple practitioners in 2009. 


I have read and reread the book The Four Agreements and love them. However, I am wondering if the way they are written/said is counterproductive.


I believe Numbers one and four are okay; 
(1) Be Impeccable with your Word 
(4) Always do your best.  


Its numbers two and three I am concerned about. 
(2) Don't make Assumptions 
(3) Don't take anything Personally.


I have read, and from what I remember, our mind doesn't hear ""don't'"" so these would translate in our mind as ""take things personally"", &amp; ""make assumptions"". 
 

Is this accurate? If so, what is a better way to state these two agreements for myself?


THANKS SO MUCH :)


(edited some grammar and structure &amp; cross posted in Affirmations)
",908,NLP,0
"How long does it take to become an advanced NLP practicioner? NLP is an art. On average, how long does it take for someone after reading several books and/or attending a 20 day seminar, to become an advanced NLP practicioner?",225,NLP,0
"Best Bandler speech/training/presentation? What moved you. What is the best Bandler speech/preso you have seen? What statements kept you riveted? 

It doesn't have to be Bandler. Any other NLP trainer is fine too, as long as it had an impact on you and got you started looking into nlp.

Thanks,",295,NLP,0
"Question about voice tone/pausing/volume/speed Can anyone suggest a book or training plan that can help you get your voice tone to be where it needs to be, and more, like adding a pause while you talk, or talk low volume during a part you want people to pay more attention to listen....
Any resources?",301,NLP,0
"Questions regarding the Circle of Excellence technique Hey guys. I was wondering if someone could help me clarify a few things in regards to the Circle of Excellence.

1- when imagining an event or mindset to put into the circle,  do you imagine only one event when you had that mindset? Or multiple times?

2- if you’ve never been in a situation before so you can’t recall a time (let’s say, recalling a time when you were the most confident in your life) , do you imagine someone else with the desired mindset/outcome you’re trying to achieve?

3- can you use the same trigger for multiple desired outcomes? For example thumb and forefinger touch for confidence, motivation, and self belief?

4- do you keep applying the trigger when you feel it’s wearing off? 

5- do you have to shut the trigger off when your done with the desired effect? For example if the trigger is designed to make you super energetic but now you’ve done your tasks and you need to wind down?

I know there are many different ways to do the Circle of Excellence and it’s a matter of finding the way that works best for you, but does anyone have a video or article they’ve read on it that made the subject finally make sense to them?

Thanks guys ",1222,NLP,0
How to Design POWERFUL Presentations using NLP ,47,NLP,0
General discussion. ,20,NLP,0
"Are there any dangers of NLP? Can NLP give someone disorders they didn't have previously?  Ex: ADHD, Autism Spectrum Disorder, Bipolar, Etc.",140,NLP,0
"Hi I'm a guy who's curious about NLP. I'm kind of nervous about it. Can it do anything dangerous like, against someone's will?",126,NLP,0
"Looking for a sub about verbal ticks I'm interested in tone, pacing, tension, and how to interpret these things and develop a keen ear. Oddly, I don't seem to come across much about this beyond my own experience and theorizing - where should I look?

I'd also love the same level of analysis regarding how people type out their forum posts - sentence structure, paragraph spacing, language selection, etc.",405,NLP,0
Curing a phobia without words. ,31,NLP,0
"Questions on NLP and if it could help me with my thesis Hello to everyone I'm a foreign language student, I'm currently focused on writing my thesis and I'm looking for some interesting ideas.

I've heard of NLP before but I don't really know what it is about. I think it has to do with adopting the right mindset in order to perform well in life... or at least I guess.

My question is could NLP be applied in order to be successful in learning a second language?

I read that NLP has been criticized before, so could I still use it as a theme for my thesis?

If so I'd like to make a study about it, so the next step will require to take the right sources, could you please recommend me a book in particular?

Thanks in advance.",730,NLP,0
What can I usefully watch out for when dealing with an NLP-proficient person? ,78,NLP,0
"Is NLP/NeuroLinguisticProgramming still a thing? I came across a website this days , with a few of the  NLP posts, so this came in mind , is it still a thing .
Maybe will come in handy 
Link: http://neurolp.com/index.php/2017/10/08/the-power-of-your-subconscious-mind/",268,NLP,0
"NLP CLIP: Unlearn ""Learned Helplessness"" ",41,NLP,0
Cripple Sex - just an ordinary day in the residential rehab unit... (Blog post) ,80,NLP,0
"Hypnosis: End Your Financial Problems Using Self-Hypnosis This course is for everyone who are facing financial crunch in their life and are not able to come out of it. It is for those failed entrepreneurs and business owners who are facing mental blocks regarding money and want to end their money problems and attract instant money in their life. By the end of the course you will be able to remove all the mental blocks you have in your mind regarding money problems and you will be able to find new ways of making money. You will be able to remove frustration, tension, negative thoughts and feel more positive about yourself by the end of the course. You will be able to attract money effectively using techniques like affirmations and visualizations.
Discounted Coupon Code: AUGDIS (Actual Price $90, get it for $10)
Discounted Coupon Link:
https://www.udemy.com/hypnosis-end-your-financial-problems-now-using-self-hypnosis/?couponCode=AUGDIS
Note: Offer valid for a limited period.
",988,NLP,0
New Behavior Generator: Guided NLP Walkthrough with Perceptual Positions ,73,NLP,0
"NLP-Let Go Of Your Past Trauma Using NLP Techniques This course is for those who have had a troubled past due to a trauma or tragedy which is affecting their present and they are unable to focus on their work. The trauma could be anything , it could be failure in business, a tragic loss , betrayal in love etc . So anyone facing any kind of trauma in their past and unable to overcome the emotion attached to it should take this course. By the end of the course , students will be able to successfully and effortlessly be able to overcome any of their past tragedy or trauma and move on from that negative emotion and memory completely. By using the powerful and simple NLP techniques, students will be able to overcome the depression caused due to the trauma and become more confident. They will be able to erase the trauma and the emotion attached to it from their mind
https://www.udemy.com/nlp-let-go-of-your-past-trauma-using-nlp-techniques/?couponCode=AUGDIS.",966,NLP,0
Any one done a seminar like this ot a Tony Robbins one? Which should I choose if I do one next year? ,101,NLP,0
The Power of Acceptance with Neuro-Semantic NLP ,48,NLP,0
"MoM Analysis of a Metaphor - ""I Have No Neck"" ",46,NLP,0
Free NLP Crash Course - 15 minutes of watch time ,49,NLP,0
"I only really dabble in NLP, but wanted to share this youtube channel, NLP gym. I feel like it really covers all the bases and he doesnt seem to get that many views. ",166,NLP,0
She's Mirroring him so that's a yes. Right? ,44,NLP,0
"The Problem with Nigel I used to tell this story at trainings, without saying whether it was true or not. It was always interesting to me to watch people's reactions. My favorite was from the NLPers who would immediately start thinking I'm imitating a Bandler story. Few ever worked it out in the context of what came just before it and the training group work that followed it.

[Read the story here](http://andrewtaustin.blogspot.co.uk/2017/08/the-problem-with-nigel.html)

",476,NLP,0
How to make an NLP promotional video ,37,NLP,0
Neuro Linguistic Programming (NLP): Master the language of your brain| SecretSifu ,82,NLP,0
How an NLP book (Steve Andreas: Transforming Your Self) helped me tackle the causes of my anxiety &amp; depression ,115,NLP,0
How to Overcome Perfectionism and Take Action ,46,NLP,0
"Derren Brown. Just saw Derrens latest show 'Underground'.

It was outstanding, he has a really relaxing likeable demeanor. Very confident and natural body language in the live theatre setting. 

Certainly worth a ticket if he is playing near you. We were politely told not to divulge the shows contents so I won't. 

Any other interesting people to check out that you recommend?",378,NLP,0
"Creating Characters to combat motivation issues **Summary**  
I've developed a method for my own use that utilizes acting, to help with motivation and procrastination. Characters are created and developed that have certain aspects that you need in your life. Such as a character may be extremely work-driven and loves tackling monotonous work tasks. Or perhaps they are extremely studious and love absorbing themselves into a college textbook. You then, essentially, act or pretend to be the character. When done right, and with practice, you are willing your mind to treat a situation differently.  

It may sound simple and silly, but there's quite a bit that goes on behind the scenes with getting this to work, and uses some basics of NLP behind how it works.
_____

**Background**  
The inspiration behind all of this stemmed from a podcast I listened to at complete random, of Sir Patrick Stewart in a casual interview. During the interview he explained that for his live plays, in which he'd often perform the same show multiple times a day, he used a technique to keep it fresh each time. This is far far far from verbatim since this was a while ago that I listened to it, but he basically explained that he used a trigger to 'become' his characters.  
For one show, entering onto the stage through a doorway, when he touched the door handle he would become his character fully--and that character was fresh. So even if he immediately performed a second show, the character he assumed was a fresh version of that character who had not yet experienced everything that happened in the previous show. This made everything that happened in the show new and exciting for him.  
He explained that it allowed him to not get drained from the shows, and kept it from feeling monotonous and like he was just on repeat over and over again.  

As I was listening, I immediately was fascinated. I began to think of how this could be used in many other formats. How it could help *me*.   

I suffer from severe motivation issues and depression from the results of my lack of motivation. (Yeah, feedback loop) The logic was pretty sound for what Sir Patrick Stewart was doing, especially after I discovered NLP (Which was a few months after I began developing this), and how the brain could be manipulated into experiencing things.  

This can also be used to improve yourself as a whole, or certain aspects about yourself. Such as confidence or sense of adventure. While you're using this as a trick to start off with--sort of 'fake it till you make it', it goes deeper in that over time you're instilling good habits regarding the aspect of your life. So even though you're just pretending now, they eventually become a part of you (Should you want, anyways)  

I began creating a method that used Sir Patrick's own play acting method, which has developed significantly over time.  

Basically, you create a character or multiple characters that you will act as/pretend to be that character for whatever duration you need. I know it sounds silly and too simple to be true, but despite how simple it sounds, it can be remarkably effective. You'll be not only developing your ability to do this over time with practice, but also developing your characters over time so that they are more believable, relatable, and easier to get in character for.  

If you're interested in this, or giving it a try, I thought I'd share what I've created. I don't know if something like this already exists or not, it's just something I came up with over time. Hopefully someone will find it useful!  
_____

**Starting off/How-to**   
^WARNING: ^*Please ^do ^not ^attempt ^this ^if ^you ^have ^schizophrenia. ^Please ^consult ^whatever ^professional ^you ^talk ^to ^about ^potential ^ramifications ^this ^may ^have ^with ^your ^schizophrenia.*  
^^WARNING ^^2: ^^*These ^^characters ^^may ^^potentially ^^become ^^Tulpas. ^^If ^^this ^^happens, ^^please ^^begin ^^treating ^^them ^^as ^^tulpa ^^instead ^^of ^^fake ^^imaginary ^^characters ^^used ^^solely ^^for ^^your ^^embetterment. ^^If ^^you ^^don't ^^want ^^these ^^to ^^potentially ^^become ^^tulpas, ^^please ^^set ^^out ^^with ^^the ^^intention ^^that ^^these ^^characters ^^will ^^not ^^become ^^tulpas.*   
  
What do you want to improve? Make a list of what your goals are. I'm going to detail all of my own examples for reference, in all of this.  

My goals that I wrote out:  
Motivation for work, chores, and to keep myself in good health.  
Higher sexual libido.  
Higher affection.  
Confidence.  
Studious.  
Curious about everything--engaged in learning.  
Engaged in discovering my faith/beliefs.  
Able to ignore tiredness/get second wind  
Stop snacking throughout the day.  
Healthier meal choices.  

That's quite a few goals... Depending on how many goals you have, you may want to create multiple characters. Honestly, it's just harder to have a single character take on 6+ significant traits. It's more effort on your part to keep all of those traits in check when you're using the characters. So please consider making multiple characters.  

I have three. Corie, Elle, and Elspeth.  

You'll be developing these characters over time, so start off simple. Just the names and what traits they embody, and perhaps a few general personality traits to make them more 'human'. It doesn't matter what gender they are, just make whatever characters you want. All three of mine are female, not really for any good reason. You're basically creating characters like you would for a novel/short story.

Corie: Motivation for work, chores, and to keep myself/herself in good health.  
	-No nonsense, extremely caring, motherly in a somewhat strict way. Cares more about 'my' wellbeing far more than the other two.  
Elle: Studious, Curious - Engaged in learning, Engaged in faith/beliefs discovery.  
	-High energy, whimsical, outgoing.  
Elspeth: Higher Sexual Libido, Higher affection  
	-Pretty laid back, sexually adventurous, loves attention romantically/sexually (Doesn't really care about normal social attention, and in that way is rather introverted.)  
Traits shared by all three: Confidence, Second Wind, No Snacking, Healthier Meal Choices.  

Why some of the traits are shared by all three:    
Confidence - they each have confidence in their own way. Corie is business confidence--no nonsense, doesn't really care what anyone thinks. Elle is a free spirit and, again, doesn't really care what anyone things and doesn't care if people judge her. Elspeth believes she's attractive and hot shit, so there's no being ashamed of her body or appearance, but she couldn't be a public speaker like Corie...unless drunk.  
Second Wind - They're each their own 'entity'. When they're being used, they're using their own minds, not my tired/bored one. So switching alone gives a second wind.
No Snacking and Healthier Meal Choices - They all eat purely for sustenance. Elspeth loves herself some wine or beer, but that's about it.  
_____

**Using your Characters**  
Some of the terms I'll be using for when you're 'acting' or 'pretending' to be your characters will be Assuming, and Imposing. They're better terms imo for this technique than either acting or pretending.   

It's good practice to try using one of your characters at least once a day, so you're practicing. It takes some getting used to, so the more practice the better.  

Getting started, I suggest you use a trigger. It can be a word, phrase, or my personal favorite: a physical gesture. Decide on a trigger for each of your characters. My trigger for Corie is to, on my left hand, put my middle and ring finger together with my thumb at their tips. Then slide my thumb down the two fingers, and back up to the tips.  
(It's possible to do this without any trigger at all, but I suggest doing it with a trigger. It'll create a much more stable and reliable foundation for you)  

""But where am 'I', in all this?"" When you're characters are imposing you, 'you' may be either just somewhere in the back of your brain, observing, relaxing, or just kind of in 'off' mode. Or if you have an imaginary place/landscape (sometimes called a wonderland or mind palace), you may be off in that location doing whatever. It's really up to you. If you have more than one character, you may be off chilling with them somewhere.

To develop your trigger, you'll need to put it to use numerous times. Use your trigger, and then immediately after, pretend that you are your character. Believe it. You are your character. Feel their emotions, feel their drives to do what they were intended for. If they were meant to be highly motivated for work, feel their need to get started working.  

_____

**Development**  
Don't just do the trigger, feel the character, and then let that be that. USE your character each time you activate the trigger and assume your character. You're establishing your ability to be that character too, and to maintain it. Doesn't need to be long, especially if you don't really need them right then and there. But just do it to get a good feel for them.  

THINK like them. They are their own person. Their own thoughts, feelings, opinions that may vary from your own. When your characters are imposing you, go all out. Think as they might think. This will also further develop them, making them more solid in your mind and easier to use. They're like your own little brain babies. (Okay maybe not, that sounds weird)     

Yes, you'll feel silly starting out. Doesn't matter! Do it anyways! No one will know anyhow.     

To give an example, when I first became able to use Corie for a couple hours at a time, I spent the time as Corie, having a conversation in her head with Elle, Elspeth, and 'me' all at the same time. Let your characters talk to 'you', when they're in control. This will help develop them.  

Even when I don't need to use any of them, I'll sometimes have conversations with one or all three while I'm browsing reddit or playing games. It's kind of like having an imaginary friend in your headspace. (This is also how these may eventually become tulpa over time.) It's great entertainment, at the very least. Other people also *can't* hear inside your head to know how crazy you sound having conversations with made-up characters.  

Eventually, if you really start using your characters and doing everything, such as talking to them, fully behaving as them when they're 'imposing' you, they will develop over time. Their personalities may become much more diverse, like an actual person. More complex. This is great! It means that they're growing for you, becoming more lifelike and relatable. You'll become more accustomed to them, their thoughts, emotions, feelings, and it'll be so much easier to assume them.  

Now all three of my characters are way more developed than the original outline of them I listed above. They're intricate and complex characters that are easy for me to assume when I need them, because I'm now so familiar with them. At this point, they each have their own appearances too so that when I'm 'talking' to them, I can imagine what they look like. Corie is the most developed, since I need to focus most of my time on work at this point in time in my life, but Elle and Elspeth will be getting more attention once I'm in a better work place (Which is improving because of Corie!)  


I'm happy to answer any and all questions. :) And please let me know if you try this out!",11450,NLP,0
What is NLP? How would you describe in 10 effective words? ,59,NLP,0
"Guys i am going to do a NLP training. 
I am in Hanoi and i wanted to learn NLP training. I found only one teacher that seems to be cheap and legit. Only i don't know how could i find if i should go for a course with him? Or how would you choose? what should i look for in terms of good NLP skill? any advice is appreciable. ",324,NLP,0
"Creating and Utilizing characters to act as, to conquer motivation issues. Hi all! So I'm extremely new to NLP, but stumbled upon it *after* I designed a method to fight procrastination and motivation issues with myself. I've only known about NLP for a couple months now, and haven't done much reading into beyond super light stuff.  
It only just struck me a couple days ago that what I've been doing might actually be linked with NLP. To be honest I don't know enough about it to say for certain if it is or isn't. I'd love to know if this can in fact be considered NLP, but if not, please let me know! 

So basically, I've created three characters that each serve a specific purpose in my life. They each represent qualities either of myself, or that I wish to have myself, in perfect quantities. Their ultimate purpose is to continually develop them and utilize them to, in an acting/power-of-imagination sense, impose my body. So when I need the one that is extremely hardworking to push through work motivations I'm having, I'll force her personality to impose my own. I am no longer myself, I am instead her. My self is off floating around in the brain somewhere, enjoying the relaxation of doing nothing.  
These characters are continually being developed in detailed ways so that they are believable, relatable 'people' which causes them to become easier to 'use' as time progresses.
It's worked extremely well so far, and results are improving over time and with character development. Mileage may vary depending on how much effort you put into the start of each 'imposition'.
 
Inspiration for this method: Interview with Sir Patrick Stewart's in which his stage acting methods was talked about.

It's a method I've been developing for a while now, but have never heard of anything quite like it before. I'm really curious if this sort of thing already exists. Since this is something I just came up with, it would be nice if there was already something already developed so I have additional help designing it. There's SO much more detail about all of this, but I'm being intentionally brief since, again, I'm not entirely sure this counts as NLP in the first place. I'm still fresh little newbie with only a vague understanding of it. If anyone wants more details about any part of it, I'm happy to answer.

Anyways, thanks for reading!",2349,NLP,0
Pretty simple explanation... but powerful if executed. ,55,NLP,0
"How could I overcome someone's hatred and/or ignorance? Somebody feels she has valid reasons to hate me. We have had a difficult history together... Although this is not entirely true I apologized from her multiple times, but the she continues to hate me.

I want to build trust, increase her respect towards me and make her see me as a high value person again, who is worthy to her attention.

Is there any technique or method which can help me in this really hard situation?

",478,NLP,0
"What is your essential Neuro Linguistic Programming reading list? I am beginning research on this fascinating area for my undergraduate thesis, hence it would be interesting to see what people consider as the key works within and without NLP.",242,NLP,0
What is your opinion on escaping the body form of an individual? Are there any techniques that can be used in order to overcome self body shaming? ,147,NLP,0
"Ranting Again, this time about fake qualifications... ",54,NLP,0
"New here! Hey i'm new here, i stumbled upon this sub after doing some lucid dreaming research. I have a dsd whos into nlp and taught me a nlp trick when i was younger, anyways see ya around.",190,NLP,0
NLP Resource States and How to Find Them ,41,NLP,0
"Looking for a NLP Hypnotist Hi, Im looking for an NLP hypnotist that is able to control someone over a video call/chat to see if we have similar interests. If you have more than 5 years of experience message me your age and why you got interested in it",252,NLP,0
More ranting. This time about NLP Modelling ,44,NLP,0
NLP technique important for sales and negotiations ,51,NLP,0
Where do I start? ,18,NLP,0
"NLP and sex? I saw this guide online on how to make a girl have multiple orgasms and how to make her squirt. I'm sure you have seen it, a lot of oil is used and if you search for it you will find it.

Now, the guy in the video says he has trained his girlfriend to have an orgasm just by him whispering some words in her ear. And I believe this guy, because he seems so skilled.

This must count as some kind of NLP right?
How can I learn this technique? And is anyone in here experienced with this type of use of NLP?

I'm new and I find it hard to practically apply NLP. Thanks!",580,NLP,0
NLP Ethical Influence Tip: Save the Best for Last ,50,NLP,0
Meta Model question: What is the difference between simple deletions and unspecified nouns/verbs? They seem pretty similar to me. What distinctions am I missing? ,162,NLP,0
My 3 Question Headache Cure PLUS how I cured a migraine through texting ,72,NLP,0
"How to use NLP for deeply embodied stuff, even without Internal Representations ",80,NLP,0
On Guilt. ,10,NLP,0
10 Questions with David Snyder ,31,NLP,0
"Facebook AI Open-Source ‘The FLORES-101 Data Set’, For Better Translation Systems Around The World (Paper included) [FLORES-101](https://l.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2Ffacebookresearch%2Fflores%3Ffbclid%3DIwAR18XHNF2irnZUwv6ZwTrfX9LTX7NFQW-GHaVPTMffhKWPQcgBjgMD1A8mo&amp;h=AT03MSblPXA3bufCYi2Xy_GR-fxCQFim0iX2w2ZkT5fiiJLjE-Zltua9obJXi4oRaOaqVHEEZ3QNRN8SXrC8lt5lLw3P4Hnlvw6L3ldX5I_z35thn6992Ve6b8H5wYE_JiYd_A), a first-of-its-kind, many-to-many evaluation data set that covers 101 languages from around the world, is now open-sourced. FLORES-101 is a tool that allows researchers to test and refine multilingual translation models such as M2M-100 quickly. To speed work on many-to-many translation systems worldwide, Facebook AI makes [the complete FLORES-101 data set](https://github.com/facebookresearch/flores?fbclid=IwAR1pjSZbSRQhxv9QccaCCApZLS0zltZy0uji24rWt9QxTR0HgWkNCGu_F2M), and associated technical report, and various models freely available for anybody to use.

Full Article: [https://www.marktechpost.com/2021/06/12/facebook-ai-open-source-the-flores-101-data-set-for-better-translation-systems-around-the-world/](https://www.marktechpost.com/2021/06/12/facebook-ai-open-source-the-flores-101-data-set-for-better-translation-systems-around-the-world/) 

Paper: https://arxiv.org/abs/2106.03193",1324,LanguageTechnology,1
Can anyone please guide me to a video based Automatic speech recognition (ASR) course ? ,88,LanguageTechnology,1
"How to pass BERT output to dense layer I am confused about which out of BERT transformer should be fed to dense layer. BERT transformer output is \`batch,sequence length, 768 \`. So I am confused about how to pass it to the dense layer",235,LanguageTechnology,1
Detecting Hallucinated Content in Conditional Neural Sequence Generation (NLP Paper Walkthrough) ,97,LanguageTechnology,1
Text Classification using spaCy v3.0 transformers in Python | Natural Language Processing Tutorial ,99,LanguageTechnology,1
"Towards Emotional Support Dialog Systems Hello everyone,

Really excited to share with you that our paper ""Towards Emotional Support Dialog Systems"" got accepted to the main conference of the Association for Computational Linguistics this year (ACL2021). We strongly believe that dialogue systems have the potential to become powerful emotional supporters that can help many individuals with their daily struggles. Towards this goal, we created a high-quality dataset of emotional conversations between trained crowdsourcing workers and will be making this dataset publicly available.

Feel free to read our [paper](https://arxiv.org/abs/2106.01144) for more details.

Thank you and wish you a great day!",704,LanguageTechnology,1
"Using gpt neo checkpoints Hi,
I downloaded gpt neo from theeye.eye on my pc.
It downloaded a various checkpoints.
How do i use them? ... Because in order too load and use model I'd need encoder. Json, pytorch. Bin, etc..",220,LanguageTechnology,1
"A Search Engine with a normalized scoring function Hey!  
I am involved in a project where I am trying to create a programming language that uses machine learning to compile text as computer code, some info here ([https://github.com/quantleaf/quantleaf-language-documentation](https://github.com/quantleaf/quantleaf-language-documentation)). This is not yet open source as a whole, but I am currently in the process of doing so. The first subproject to be released is a search engine library that enables you to score documents with a value between 0 and 1 (I call it “zero-to-one” score). In short, this is done by evaluating the product of how close we are to a perfect match regarding document length and query length, but also in terms of the amount of tokens, in the document and in the query.

I have created a small recipe search demo for this to showcase it benefits (and potential drawbacks)

[https://quantleaf.github.io/probly-search-demo/](https://quantleaf.github.io/probly-search-demo/)

When searching “Garlic Chicken”, the first two results are:

For the “zero-to-one” scoring
“Garlic Chicken” score 1.
“Garlic-Sherry Chicken” score: 0.7307692307692308

For BM25 (standard parameters)
“Garlic Oven Fried Chicken” score 8.564332563809089
“Garlic Chicken“ score 8.455662889754347

For the BM25 algorithm the perfect match is not the top score. You could circumvent this behaviour by adjusting the parameters of the BM25 algorithm. Or is it in conjunction with a term matching algorithm. But for my programming language project, this was not good enough. I needed a score to be 1, to know when we are 100% matching, 0.5 if we are matching with a 50% relevance, hence I created this.

The search engine is written in Rust, but you could use it in any Node project if you write a little bit of WASM bindgen code (see the demo source code).

Library source code: [https://github.com/quantleaf/probly-search](https://github.com/quantleaf/probly-search)Demo source code: [https://github.com/quantleaf/probly-search-demo](https://github.com/quantleaf/probly-search-demo)

I am curious with what your take is on this scoring function, would you find it useful in comparison to the solution that you currently are using? (Especially for title/label matching)",2263,LanguageTechnology,1
"Best approach to find (similar) matches for a string in a corpus of documents? Context: I'm a hobbyist that got into NLP superficially, so I don't expect a definitive answer. I'd be grateful if you could just point me in the right direction, because I was overwhelmed with all the different approaches after googling it.

Here's what I want to do:

I have laws and legislation split into its smallest pieces. Each piece is a specific string. So one or two lines, maybe a paragraph, with a specific rule saying what you can and cannot do, who is responsible for what, how many days you have to do something, what is the punishment for breaking it etc. I also have a corpus of written tests (about 70k total), with questions that are often about said rules. For each small string, I want to see how many times it was mentioned in tests, so I can rank them in regards to which subjects are more likely to be asked about in tests.

How would you go about doing something like that?",977,LanguageTechnology,1
"Extract text from image which is printed columnwise Hi, I want to extract text from image where text is printed columnwise (like research papers) does anybody know any libraries that will help me with it? Or advice how to approach with such use case? 

Thanks in advance!",271,LanguageTechnology,1
"Can't run gpt3 xl Hi All,
I downloaded the model from
https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/

after which i changed model_path in config.json to:
""model_path"" : ""C:\Users\GPT_NEO_2\GPT3_XL""

Whenever i run the following code:
model = GPTNeoForCausalLM.from_pretrained(""C:\Users\GPT_NEO_2\GPT3_XL"")

i get an error:
f""Error no file named {[WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME + '.index', FLAX_WEIGHTS_NAME]} found in ""
OSError: Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index', 'flax_model.msgpack'] found in directory C:\Users\GPT_NEO_2\GPT3_XL or from_tf and from_flax set to False.

and while running :
generator = pipeline('text-generation', model=""C:\Users\GPT_NEO_2\GPT3_XL"")

i get following error:
f""Unrecognized model in {pretrained_model_name_or_path}. ""

I have the latest TF and torch (both cpu).

Thanks",868,LanguageTechnology,1
"Detecting all words/entities with a certain attribute Hi folks. I wanted to find all words that indicated a certain attribute. For example, consider finding all words that are 'hot'. That could include 'fire', 'flame', 'lava', 'heat' or a 'stove'.

The approach I came up with is to use word embeddings and the resulting word graph. If I start with 'hot', I could add all words within a certain distance, and then perform this process for all subsequent nodes to find paths of a certain max total distance. All vertices in this subgraph would be my answer. But I would still have to account for colloquialism.

Are there any existing frameworks or datasets that already do it like this or with any other methods?

Thanks in advance!",732,LanguageTechnology,1
At OpenDialog we make NLU more useful by making it work less. Combining context and pro-active conversation management to reduce reliance on NLU understanding state. ,166,LanguageTechnology,1
Custom Named Entity (Disease) Recognition in clinical text with spaCy v3 Transformers in Python ,96,LanguageTechnology,1
"How to use gensim topic modeling to predict sentences in a document? Im able to predict document topics (such as document A is 60% Topic 1, and 40% topic 2). But cant find a way to classify what sentences are Topic 1 and which ones are Topic 2. One way of doing this could be to use sentence boundaries to extract sentences and then use gensim to predict each sentence but im looking for a formal way to do it.",410,LanguageTechnology,1
"How to understand the ""valence"" of DMV in unsupervised dependency parsing? I have read the paper "" Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency "", which proposed a classical model DMV( Dependency Model with Valence ) for unsupervised dependency parsing.

As far as I know, the conception ""valence"" is used to describe the number of  “action element” which is dependenct by a verb.

But I can't find more details about how the DMV used ""valence"" in aforementioned paper excepted the model's name.

Can anyone solve my doubts? Thank you very much!",585,LanguageTechnology,1
"Problem while doing daily topic modeling I've been trying for weeks to do daily topic modeling in this [methodology](https://github.com/Stveshawn/contextual_topic_identification). What I did was divide my dataset into groups by day and loop through the architecture by mini dataset. The first ""problem"" is that I'm getting a warning that I can't loop in an autoencoder and the second is that my metric values don't change much despite the dataset I use (I'm using average coherence at the end, but if I run the code with another dataset with other texts, the average coherence remains similar). Anyone could tell me what I'm doing wrong and how do I model the daily topics correctly, as I'm doing this without any examples (because I couldn't find any).

Code:

    for i, group in enumerate(data_groups.groups):
        #LDA
        
        #BERT
        
        #Concatenation 
        
        #Autoencoder
        AE = Autoencoder()
        AE.fit(ldabert)
        vec = AE.encoder.predict(ldabert)
        
        #Kmeans
    
        #Metrics

Warning:

    WARNING:tensorflow:5 out of the last 5 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x00000161989E3AF0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.

Metric values I'm having: +- 0.55 in coherence, doesn't change much beyond that",1920,LanguageTechnology,1
"How often is NER (named entity recognition) a process in your model building process? I recently started, kaggle notebooks don't suggest it's that important.


 But I recently got access to Prodigy, and it took me an embarassngly long time to install it in Pycharm. But the concept and ease of use is  convenient but i am not sure how often business/ orgs require this. 

If you're working in a real world project/Enterprise, what is your exp. with NER. 

Also if you happen to know a repo or list of public projects lemmino.",525,LanguageTechnology,1
"How to apply Mutual Information for feature selection in Text Classification where features are words? I am writing a text classifier and want to use MI for feature selection but not sure how to compute when the features are words. If I compute scores for features, say using TFIDF, and then apply MI on it, then I get the same top features as with just TFIDF and there is no improvement in results.

Is there any resource I can refer for the same?",448,LanguageTechnology,1
"Tutorial on how to replace missing values in a data frame by the column mean (i.e. mean imputation) using R Hey, I've created a tutorial on how to replace missing values in a data frame by the column mean (i.e. mean imputation) using the R programming language: [https://statisticsglobe.com/replace-missing-values-by-column-mean-in-r](https://statisticsglobe.com/replace-missing-values-by-column-mean-in-r)",406,LanguageTechnology,1
"Clustering word vectors for topic discovery without the actual documents (overlapping &amp; hierarchical) Hi everyone,

For my project I have a set of word vectors which I have to classify in an unsupervised manner to identify topics, similar concepts, etc. The requirement is to allow each word to belong to multiple topics (overlapping), and allow the topics to include other subtopics (hierarchical).

The problem is that because I don't have documents it's not straightforward to apply topic modeling ideas here.

I know one can look at the problem from pure clustering perspective and use kmeans/GMMs/HDBSCAN/deep learning based clustering, but the problem is that most of such methods assume non-overlapping or non-hierarchical clusters, and there's little research on hierarchical overlapping clustering.

I've been also thinking to leverage community detection methods on graphs, as it's possible to treat each word as a node, however, such methods could be computationally expensive, and I just want to make sure there's no a more natural choice before pursuing this.

Would appreciate any ideas, thank you!",1116,LanguageTechnology,1
"Question answering with Wikipedia API I'm working on a question answering system based on the Wikipedia API. A problem is that ""wikipedia.search"" yields a couple of articles, the first one not always being the best one. Any ideas how to chose the most appropriate article from the results?",289,LanguageTechnology,1
"Speech summarization datasets Hi everyone,

I'm currently learning about text summarization and I'd really like to fine-tune a transformer model for summarizing speeches (i.e. texts of speeches by political leaders or parliament debates, NOT voice recordings).

Does anyone know about an English dataset containing both the speeches and the respective summaries? It would also be great if someone had an idea as to where one could source such a dataset (e.g. websites from parliaments). I already checked the websites of the US senate, UK government etc., but no luck so far.

Thank you!",587,LanguageTechnology,1
The creators of GPT-Neo just released a 6B parameter open-source version of GPT-3 called GPT-J-6B ,98,LanguageTechnology,1
"Google AI Introduces ByT5: Pre-Trained Byte-to-Byte Models for NLP Tasks Google researcher’s new study suggests modifying the conventional transformer architecture to process byte sequences in natural language processing (NLP). The new competitive byte-level models can effectively balance computational cost trade-offs of contemporary large language models.

Tokenization splits the sentences into a sequence of tokens. Most NLP tasks follow a tokenization procedure to preprocess the data. However, tokenization can struggle with typos, irregularities in spelling and capitalization, morphological changes, and out-of-vocabulary tokenization problems.

Summary: [https://www.marktechpost.com/2021/06/08/google-ai-introduces-byt5-pre-trained-byte-to-byte-models-for-nlp-tasks/](https://www.marktechpost.com/2021/06/08/google-ai-introduces-byt5-pre-trained-byte-to-byte-models-for-nlp-tasks/) 

GitHub: https://github.com/google-research/byt5

Paper: https://arxiv.org/abs/2105.13626",983,LanguageTechnology,1
"Language Model for Summarization Hi all,

What is the best word embedding to use for automatic text summarization? I want to make extractive summaries of documents.",164,LanguageTechnology,1
"Unsupervised Relation Extraction using BERT attention scores Hey, 

Given that I have a Named Entity extractor trained over a BERT pre-trained model, is it possible to utilize the already computed attention scores for extracting Relations between these entities?  

Obviously, categorizing the active relations is still a challenge, but is it possible to detect if a relation is active only by using the attention scores? Specifically if the BERT model is only trained for NER.",477,LanguageTechnology,1
"Relationship extraction I would like to extract relationships in plain text between two named entities.  

I first wanted to try with Machine Learning but it's complicated to find an annotated corpus for training ... Then I wanted to create patterns (for example: PERSON live LOCATION) but it is not not very precise because I'm trying to find relationships between each pair of named entities (and honestly it takes a long time to write a good dictionary).

Do you have any suggestions for doing this more efficiently please? Maybe a corpus that exists, an algorithm next to which I pass ? 

Thaaaanks :)",605,LanguageTechnology,1
"How to build a model for german paraphrase generation? I need to generate german paraphrases and I was already looking at some huggingface models which work really well for english sentences. For example tuner007/pegasus\_paraphrase or Vamsi/T5\_Paraphrase\_Paws.

    tokenizer = PegasusTokenizer.from_pretrained(model_name)
    model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device) 

I can't find any german models for paraphrasing on Huggingface. How do I build a model on my own? Is this the best way to generate german paraphrases with transformers or should I use other methods? Thanks!",623,LanguageTechnology,1
[d] Inspecting Neural Networks with Canonical Correlation Analysis (CKA/SVCCA Video) ,85,LanguageTechnology,1
"I am experimenting with using NLP to measure market sentiment with custom classifiers for neutrality, FUD, hype etc. stuff typical for crypto world. I trained my classifiers in natural.js with data I pulled from social media. This is my attempt number one ",256,LanguageTechnology,1
"Is there a text list of words and their variations? For example:

exist exists existed existing 

run ran running 

And so forth?",129,LanguageTechnology,1
"Job opportunity: Assistant Professor in Speech Technology at the University of Groningen (the Netherlands) Very interesting job to teach at a new [MSc. in Voice technology](https://www.rug.nl/masters/voice-technology/)!

Key points:

* English language program
* 80-100% full-time position (depending on how many classes you want to teach)
* Balance between teaching and research is 60/40 (really!)

In addition to supervising theses within your area of expertise, you will support the teaching and/or curriculum development of courses in speech synthesis, speech recognition, Python, and machine learning for voice tech (all courses already have detailed week-by-week descriptions but lack student-ready syllabi, giving you some creative freedom -- more information about the courses, including learning outcomes, is available upon request):

● Speech Synthesis I and II  
● Speech Recognition I and II  
● Python for Voice Technology (and Intro to Python at the undergraduate level)  
● Machine Learning for Voice Technology

[More details](https://www.rug.nl/about-ug/work-with-us/job-opportunities/?details=00347-02S0008E4P)  (qualifications, application procedure, etc.)

Deadline: 13 June 11:59pm (CEST - European time)",1225,LanguageTechnology,1
"GAN-BioBERT: A Methodology For Assessing Reporting Trends In Clinical Trials (Paper Summary) I saw this paper and thought it was an interesting application of sentiment analysis. 

[https://arxiv.org/abs/2106.00665](https://arxiv.org/abs/2106.00665)

The paper proposes using GAN-BERT with BioBERT for 3 class sentiment classification in clinical trial abstracts as a way to assess reporting trends in literature. 

They also found that the accuracy of the algorithm was far better than using an expert rater (which is the standard right now in clinical literature for these types of studies).

I'm curious what the r/LanguageTechnology world thinks; This seems like a really cool application but I'm a total novice when it comes to NLP (coming from a signal processing background)",781,LanguageTechnology,1
"Need direction related a project, like what topics(study material) should I look into. I want to do a contextual analysis on two topics and learn the in what  context some common keywords has been used in both of the topics. Thank  you.",236,LanguageTechnology,1
Benefits of Using PHP for Web Development ,42,LanguageTechnology,1
"Advice - Choosing a Master's in NLP (France) Hello,
I am planning on studying NLP in the fall and have been accepted to two programs that really interest me.
They are the University of Strasbourg's Master in Language Technology and the PluriTAL program in Paris that is organized by Paris 3, Paris Nanterre, and Inalco.
I was wondering if anyone here has studied in one of these programs or knows anyone that has and would have any pros and cons between them.

Thank you for your help!",485,LanguageTechnology,1
"Clustering latent representation vectors with a size less than the number of clusters  

I'm doing topic modeling for the first time in my life and I have a problem. My intention is to model daily topics, but my number of daily samples varies a lot, from 5 samples in one day to 100 in another, for example. The desired number of topics is 7, so I have problems from the first day of the dataset.

The methodology I'm following is [this](https://blog.insightdatascience.com/contextual-topic-identification-4291d256a032).

Then the vector resulting from the LDA+BERT concatenation is passed in an Autoencoder and then used in a clustering model. This is where I have the problem at hand. My number of clusters is 7, but the representation vectors are 5.

With this I have the error:

ValueError: n\_samples=5 should be &gt;= n\_clusters=7.

Does anyone know how I could fix this?",878,LanguageTechnology,1
FREE WEBINAR - Automating Data Annotation with MicroModels - Automating processes within the workflow to improve efficiency &amp; guarantee high quality ,153,LanguageTechnology,1
"How to identify sentences from a stream of characters? Hi all. I have a little problem I am facing and I'm not quite sure where I should start looking. I have an application where I want to identify sentences inline as the user types. The goal is to operate on the sentences and provide feedback to the user without them needing to indicate the sentence as completed.

Does anyone have any ideas on how to effectively identify complete sentences as they are being typed? The quick solutions I've thought of seem a little too simple and error prone. 

Please let me know.",570,LanguageTechnology,1
"John Snow Labs Spark-NLP 3.1.0: Over 2600+ new models and pipelines in 200+ languages, new DistilBERT, RoBERTa, and XLM-RoBERTa transformers, support for external Transformers, and lots more! ",192,LanguageTechnology,1
"What's the algorithm pipeline for Google's ""People Also Ask""? I tried to find some well-summarized answers, but there really isn't any.

I can only guess some combination of:

* Query Expansion
* Query Embedding and Finding Semantically Similar Queries
* Query-Answer scores(from the field data) for raking PAA suggestions
* Text Summarization(for the snippet of answers)
* Natural Language Generation(for the ""general"" form of questions)

Anyone can give a bit more detailed version of what is actually going on behind?

Thanks!",529,LanguageTechnology,1
"google-research/mozolm - A language model serving library, with middleware functionality including mixing of probabilities from disparate base language model types and tokenizations along with RPC client/server interactions. ",225,LanguageTechnology,1
"Building a corpus of AAC users speech/writing: ""Know of any AAC users who want a faster system? This research study aims to build a large open database for researchers and developers to make far better systems. End users can now help this effort directly"" ",256,LanguageTechnology,1
Preventing ‘Hallucination’ In GPT-3 And Other Complex Language Models ,70,LanguageTechnology,1
"ByT5: Towards a token-free future with pre-trained byte-to-byte models (Research Paper Summary) Most of the pre-trained language models operate on sequences of tokens corresponding to word or subword units. 

This paper proposes Token-free models that instead operate directly on raw bytes. 

https://link.medium.com/qKwrXYXqTgb

P.S. Most of this blog was written from my mobile device. Please excuse brevity and typos.


Actual Paper: https://arxiv.org/pdf/2105.13626v1.pdf",475,LanguageTechnology,1
"Hugging Face: Unable to use emojis for masked language modelling? I am new to Hugging Face and masked language modelling (MLM), and I was wondering how to include emojis when doing such a task.

I have a dataset with tweets, with each tweet containing an emoji at the end - here is a sample of my data:

| ID |        Tweet |
| -------- | -------------- |
| 1    | Looking good today 😎         |
| 2   | Weather is so hot, lol ☀️          |
| 3  | I hate you!!! 🤬        |

At the moment, I have fully trained my masked language model using my dataset, but when I predict something, it does **NOT** output or predict the emojis. It just predicts words.

This is my desired input from using my dataset for MLM:

```
""You look great [MASK]""
```

This is my desired output from using my dataset for MLM:

```
[{'score': 0.26041436195373535,
  'sequence': 'You look great 😎""',
  'token': 72,
  'token_str': '.""'},
 {'score': 0.1813151091337204,
  'sequence': 'you look great 💯""',
  'token': 2901,
  'token_str': '!""'},
 {'score': 0.14516998827457428,
  'sequence': 'you look great 👌',
  'token': 328,
  'token_str': '!'},]
```

However, this is what I am actually getting from my output:

```
[{'score': 0.26041436195373535,
  'sequence': 'You look great?""',
  'token': 72,
  'token_str': '.""'},
 {'score': 0.1813151091337204,
  'sequence': 'You look great.""',
  'token': 2901,
  'token_str': '!""'},
 {'score': 0.14516998827457428,
  'sequence': 'You look great!',
  'token': 328,
  'token_str': '!'},]
```

I know it is possible to do this, but how do I do it? I am close, but not very.

Likewise, I have my model fully trained on my dataset, but it just does not seem to output emojis, even though I have included them in the training. 

Does something need to be included to accept emoji? If so, what?

Thanks - I would really appreciate the help!",1846,LanguageTechnology,1
"Master's degree in Computational Linguistics (Stuttgart) Hello everyone!

I'm an Italian uni student, and I'm going to graduate in foreign languages and literature in October (English and German).

I'm interested in studying CL after my Bachelor (Stuttgart seems to be a good choice, and spending some time in a German-speaking country would enable me to improve my German), but I don't know whether I stand any chance of getting accepted into this programme.

I took a couple of linguistics exams, but since in Italy Bachelors of linguistics don't exist at all, all I could do was enrol in the languages programme. Stuttgart university accepts Bachelors related to linguistics, but I'm still afraid that my degree won't be enough to satisfy their admission requirements.

I know the basics of programming (Python), but I actually don't know what I could do so as to stand a better chance of getting accepted. 

Is there anything else I could do?",946,LanguageTechnology,1
"More than 10K of you downloaded the free NLP transformers course... Wow! Three days ago I made a video explaining how my NLP transformers course would be entirely free as part of a limited-time promo. I shared that video here and in a couple of other subreddits too, r/learnmachinglearning and r/Python being two.

Three days and 10823 downloads later, here we are! I thought we'd be lucky to hit 1K!

Incredible response, and very happy to be able to have been able to give so many of you an opportunity to access the course where some of you may not have been able to otherwise. I'm looking forward to working with all the students and helping you guys out, just please don't all ask me questions at once! 😬

Thanks all, truly humbled by the response - it's really *really* cool, it has blown my mind.

For any of you that are still interested, I will leave a final discount link [here](https://www.udemy.com/course/nlp-with-transformers/?couponCode=MEDIUM), thanks all!",972,LanguageTechnology,1
Debiasing large pretrained language models using distributional control ,72,LanguageTechnology,1
Does anyone know of coreference resolution tools where you can specify the entity? ,83,LanguageTechnology,1
"ConvoKit corpus research Hello! I am a PhD researcher. I am looking urgently for someone having used ConvoKit and knowing how to extract coded variables through it. Please reply here or email me at:

[ax23@kent.ac.uk](mailto:ax23@kent.ac.uk)",241,LanguageTechnology,1
"Hugging Face: How to test masked language model after training it? I have followed this tutorial for masked language modelling from Hugging Face using BERT, but I am unsure how to actually deploy the model.

Tutorial: [https://github.com/huggingface/notebooks/blob/master/examples/language\_modeling.ipynb](https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb)

I have trained the model using my own dataset, which has worked fine, but I don't know how to actually use the model, as the notebook does not include an example on how to do this, sadly.

&amp;#x200B;

On the Hugging Face website, this is the code used in the example; hence, I want to do this exact thing but with my model:

    &gt;&gt;&gt; from transformers import pipeline
    &gt;&gt;&gt; unmasker = pipeline('fill-mask', model='bert-base-uncased')
    &gt;&gt;&gt; unmasker(""Hello I'm a [MASK] model."")
    
    [{'sequence': ""[CLS] hello i'm a fashion model. [SEP]"",
      'score': 0.1073106899857521,
      'token': 4827,
      'token_str': 'fashion'},
     {'sequence': ""[CLS] hello i'm a role model. [SEP]"",
      'score': 0.08774490654468536,
      'token': 2535,
      'token_str': 'role'},
     {'sequence': ""[CLS] hello i'm a new model. [SEP]"",
      'score': 0.05338378623127937,
      'token': 2047,
      'token_str': 'new'},
     {'sequence': ""[CLS] hello i'm a super model. [SEP]"",
      'score': 0.04667217284440994,
      'token': 3565,
      'token_str': 'super'},
     {'sequence': ""[CLS] hello i'm a fine model. [SEP]"",
      'score': 0.027095865458250046,
      'token': 2986,
      'token_str': 'fine'}

Any help on how to do this would be great.",1668,LanguageTechnology,1
"How to use BERT multilingual embedding I have a task where i want to use multilingual embeddings for 2 different languages(one of them being english). 
I first checked fasttext but its aligned vectors does have one pf my language. So i check a basic vector aligning algo and it was using common words between two languages to align them. But one of my language does not have english characters so cant use that algo.

Then i read about BERT embeddings and found multilingual model on their git repo. But i dont know how to get word embeddings using using that model.
So, does anybody know how to use the embeddings from bert multilingual model.",644,LanguageTechnology,1
"How do I find similar words to certain words (NLP, other techniques)? Hi, I'd like to write a program that does the following: you put in a word (or words) and as an output you get a similar word (words) to it. So if I give the words 'investment banker' as an input, it should give back the words 'investment analyst' or 'private equity manager'.

The language wouldn't be English, but French, German and Dutch. Which frameworks, libraries and techniques should I look at to implement this and how do I make it work as accurate as possible? Does anyone have any experience with this that you'd like or good guides on it? Is it hard to implement this program? Thanks in advance.",677,LanguageTechnology,1
"Incorrect tags when parsing test case I’m interested in parsing manual test cases into classified/formatted data so I can extract the intent of the tester and then generate automated test cases (e.g. Selenium). I’ve been playing around with Python’s spaCy library and noticed that when I process the following string, I get incorrect tags:

“User enters Password and press tab key”

“press” is incorrectly tagged as a noun when it should really be a verb. I realize that the word should have been “pressED” and may be the reason why tags are coming out wrong, but is there anything that can be done to work around typo issues like these?",637,LanguageTechnology,1
How to implement ALBERT for sarcasm detection Hi everyone I search for good resource to implement ALBERT for sarcasm detection on Reddit dataset...can u help me?,161,LanguageTechnology,1
Building Grammar Correction API in Python with Gramformer and FastApI ,70,LanguageTechnology,1
"Does a truly comprehensive rule-based grammar for the English language exist? Or is there any (recent) study that discusses their limitations? Hey,

I'm working with rule-based language models and I'm wondering if there is anything that could be called a ""comprehensive set of rules""  for the English grammar. My gut feeling tells me that it is close to impossible to catch all possible grammatical variations of the English language, but I'd love to be proven wrong!

I'm know of Lambeks Pregroup grammar and Montague grammar but I'm not sure if any of those formalism captures the English language without major flaws.

I appreciate any references!",650,LanguageTechnology,1
"Trouble with gImageReader (Tesseract) - Exporting to PDF (Linux Mint) I've been using gImageReader for a few years and I've always used it to export the OCR'ed text as an invisible text layer over the existing pdf scan, resulting in a searchable pdf file, which is useful for research, working with the text etc.

I was doing this on Windows 10, and recently installed a dual boot with Linux Mint and reinstalled Windows due to slow-down. Now I reinstalled gImageReader and I can no longer find the option to export to PDF with text layer. In fact I'm having trouble even finding much mention of said function online. I don't know if I originally installed some fork or beta or what, but the way the (probably) official version of the program looks is not what I'd been using so far.

I was always able to set certain post-production parameters, such as size of the invisible text etc.

Any ideas where I could find that version again? I'd prefer a Linux Mint compatible version, but Windows would also be fine, if that's the only one.  
Thanks!",1045,LanguageTechnology,1
Best weekly digest to keep up with NLP research? I would like to subscribe to some weekly news about NLP in order to be up-to-date with latest research in NLP. Could somebody please recommend me one?,199,LanguageTechnology,1
Advice for fake news classifier research paper I am playing to write a paper in the NLP field on fake news classifier. Any suggestions about what I could try out and write on? ,176,LanguageTechnology,1
Can you guys help me understand how word embeddings is not a symbolic representation of language? I understand that models like GPT 3 are auto regressive models which uses word statistics but what I am struggling to understand is why the discernable semantic information that you can get from a word2vec model is not symbolic representation in at least in some way?,365,LanguageTechnology,1
Training T5 model in just 3 lines of code with ONNX Inference ,62,LanguageTechnology,1
"Answer matching using sentence similarity Hi, I am relatively new to NLP. I am trying to write a code that allows me to grade elementary student's  Science questions. For keyword-based answers, I am able to easily match student's answer to the model answer using lemmatization and such.

**Question: Why did you classify Animal A as a mammal?**

**Answer: fur/ produces milk/ give birth/ warm-blooded**

matching the keywords in the answer is relatively straightforward.

&amp;#x200B;

However, the issue comes when we need to match answers that are more complex.

**Question: what is the relationship between the amount of water given and the height of the seedling?**

**Answer: As the amount of water given increases, the height of the seedling increases.**

&amp;#x200B;

A student may phrase the above answer differently and get a correct answer. (e.g. more water given, taller the plant grows). Take note that students are not graded on language skills ( i.e. grammar and spelling to a reasonable extent.)

I have tried using a pre-trained corpus to calculate the cosine similarity and use that to determine whether an answer matches the model answer or not. However, the resulting cosine values are rather indistinguishable. As a result, I am not able to pick a suitable threshold to ""differentiate"" right from wrong.

I have considered breaking down the answers into clauses and perform a similar analysis on them individually before adding up the scores for each clause. I could possibly use the dependency matcher of spacy to fix the correct subject-verb-object relation in an attempt to extract semantic meaning using algorithms. I guess the next step forward would be to apply machine learning. Thank you for taking the time to read this. I would greatly appreciate any inputs from anyone here. Thanks again!",1820,LanguageTechnology,1
"Text classification for item matching, best setup? Hi there, I am building a text classification model to match the name and description of a customer's item (e.g. name: ""suction press nip"", category: ""paper machine parts"") to a list of 10k basic items (name: ""steel, unalloyed"", category: ""metals""). I have some initial matched data to test and I will get more and more, hopefully.

I've build a sentiment analysis program in the past, this is a good example of what I used: [https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/](https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/) (Spacy, Scikitlearn).

This current problem is more complex though, it's 1 to 10k+ match and not binary (or max 5, 6 values), the string for the item is short and absolutely at the discretion of the source (client item log).

Which reads/tutorials/examples would you suggest to take a look at? (in Python please)",955,LanguageTechnology,1
"[Discussion] What should be the data driven chatbot architecture using NLP2SQL?  Given a table, I am able to convert natural language questions into appropriate SQL query with transformers.

The architecture of a chatbot should be:

1. Natural language question to SQL query translation using transformers (this part is completed)
2. Fed SQL query to SQL engine and collect response (this part is completed)
3. Convert SQL engine response to Natural Language Response

How can I accomplish the last part? What kind of architecture or model should I use?",553,LanguageTechnology,1
To Negate a word Any method to negate a Verb or Noun word in the sentence using NLP,83,LanguageTechnology,1
"What is happening when you say ""play me butter"" to your Alexa/Google/Siri? Hi,

It might be a novice question, but could be a deeper one as well.

Anyone out there please guide me with what is actually happening when you say,

**Play me Butter**

to your Alexa/Google/Siri?

What I know is that, the ML/NLP engine under the hood will

1. Classify the intent -&gt; Music Play
2. Recognize what ""Butter"" means -&gt; It is a song by BTS
3. Tell ""here's Butter by BTS"" to the user
4. And actually play it.

But this is just the high-level sketch. For example, the word ""Butter"" was meaning the food ingredient before BTS launched the single.

**How will the engine behind update their Knowledge Base according to new releases? In a timely manner?** 

I can't imagine it's being done manually. And,

What will be the pipeline of classifiers for finally recognizing Butter as a BTS song? I doubt it'll be done with a single classifier.

Thanks a ton in advance!",955,LanguageTechnology,1
"Is there a cloud service where I can run my LDA code at a good speed? I have a deadline tonight and I wont make it because the code takes several days to finish making the corpus and then passing them it through LDA.

&amp;#x200B;

Edit: I use gensim in python",260,LanguageTechnology,1
"Build and query a book similarity index &amp;#x200B;

https://reddit.com/link/nrp6bw/video/dzi6hndkh4371/player

This application builds a local txtai index using book data from [openlibrary.org](https://openlibrary.org). It supports natural language queries to find the best matching books.

Applications that support natural language queries open up exciting possibilities. Take conversational AI as an example, you wouldn't expect users to speak in an abrupt way that is typical with traditional token-based search systems.

GitHub: [https://github.com/neuml/txtai](https://github.com/neuml/txtai)  
Application: [https://github.com/neuml/txtai/blob/master/examples/books.py](https://github.com/neuml/txtai/blob/master/examples/books.py)",740,LanguageTechnology,1
"Text classification: When to use sequence models over bag-of-words model? In the recent update on his upcoming book ""Deep Learning with Python. 2nd edition"" F. Chollet refers to research done in 2017: He and his team did a systematic analysis of text classification using different data sets. He claims that they discovered a simple rule of thumb: If the (number of samples / mean sample length)  &gt; 1500 one should use a sequence model, if  it is &lt; 1500, then one should use a bag-of-bigrams. 

It seems they didn't publish this finding in a research paper but only in a [Google guide to text classification](https://developers.google.com/machine-learning/guides/text-classification) (without any names except 'Google'). I am gathering this from the fact that Chollet only refers to the guide. The guide gives a little bit more information: they ran 450k experiments "" across problems of different types (especially sentiment analysis and topic classification problems), using 12 datasets, alternating for each dataset between different data preprocessing techniques and different model architectures"" ([source](https://developers.google.com/machine-learning/guides/text-classification/step-2-5)). 

As this was done 2017, it would be very interesting to see whether this rule is still valid with the context-sensitive language models like Bert. Does anyone know about research checking this claim in recent years?",1420,LanguageTechnology,1
"Advice for how to approach classifying apartment posts on facebook? I want to develop software that helps streamline connecting people with roomates/apartments. Right now I'm using a few facebook groups where people post about either looking for someone to rent a room in their apartment or they post about needing a room. I'd like to write something that automatically parses the data out of these that people commonly need. Like: Is this apartment pet friendly? How much is the room? What neighborhood is it in? How many bedrooms? etc.

My background is in computer vision for robotics with CNNs, so this is a totally different domain I'm not familiar with. From the research I've done so far, it sounds like I should look into entity recognition and relationship extraction. But I'm not sure what models are good for that, how much labeled data I need to get started.

I'd be willing to put a few thousand dollars into data annotation I think if that could get me something I could use for my own apartment search.

What models should I look into? What data labeling services/tools? How should I approach this?",1113,LanguageTechnology,1
What are state-of-the-art methods for abstractive text summarization ? ,71,LanguageTechnology,1
"Assessing the “Value” of a question. Hi, All. I am new to NLP but have a problem I'd like to solve. I host a podcast of sorts at work and I am always on the lookout for great questions. Since I also enjoy python and data science I decided to see if I could uncover anything using those tools.

Did a bit of research and found a corpus of transcribed npr interviews. That seemed like a good place to start so I wrote the code to tokenize all the sentences and find those ending in '?'. So, that's all the questions.

In 3M+ utterances, there are a bunch of questions. Many are...not good questions.

'Beth, are you out there?'

'What do you say, Bob?'

'Is that right?'

Fewer are actually good or more meaningful questions.

'Clinton and Obama are pretty similar on policy issues, so why would a Democrat switch loyalties like that?'

""The issue in the Hollywood writer's strike is, do writers get paid for work that winds up online?""

In general, these questions tend to be longer and have larger words in them but, I am curious if there are any established methods for determining the meaning or value of a question? I am not really certain even what the right word is to use (meaning, value, etc).

Anyways, I always open to learn something new. If anyone can point me in the right direction, i'd appreciate it. Thanks!

&amp;#x200B;

Code thus far for anyone interested:

`import nltk`

`from nltk.corpus import stopwords`

`from nltk import word_tokenize`

`from nltk import sent_tokenize`

`import numpy as np`

`import pandas as pd`

[`nltk.download`](https://nltk.download)`('nps_chat')`

&amp;#x200B;

`npr = pd.read_csv('C:\Users\...\Desktop\Python Scripts\Data\utterances.csv')`

`npr = npr[npr['utterance'].notna()]`

&amp;#x200B;

`tokens = npr['utterance'].apply(lambda x: sent_tokenize(x))`

`#put tokens into the DF`

`npr['utterance_tokenized'] = tokens`

`#build lists of question sentences`

`is_question = npr['utterance_tokenized'].apply(lambda x:`

`[q for q in x if '?' in q])`

`#put questions into the DF`

`npr['questions'] = is_question`

`#identify non-empty lists of questions`

`has_question = npr['questions'].apply(lambda x: True if (len(x) &gt; 0) else False)`

`#put boolean results into data frame`

`npr['has_questions'] = has_question`",2272,LanguageTechnology,1
"What have you read recently? I work in the field of NLP and realized I wasn't reading enough papers on what's going around in the field. What is the latest paper you read and that leaves you mindblown (or at least excited)?

It can be any fields of NLP, I'm just curious.",271,LanguageTechnology,1
"Few Shot Learning in BERT Hi, I'm interested to know whether the few-shot learning can be done on BERT for sentence similarity.  


Update: Found the pipeline in hugging face for zero-shot learning.  
[https://huggingface.co/facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)  
But still, I'm not sure about the few-shot though.  
",355,LanguageTechnology,1
"Sentiment Analysis with flair on poetry - model evaluation Hi! I am a digital humanities student and a newbie in NLP and for my dissertation I tried sentiment analysis with flair library on around 517 poems. My problem is: I used a pre-trained model and it labeled my poems as Negative/Positive, but I do not know how to evaluate how accurate this was. My data was unlabeled before applying the pre-trained model.  

If someone could please help me, that would be great. Thank you!",481,LanguageTechnology,1
"If you are looking to automatically extract information from PDFs or scanned images, check out this article on how to leverage OCR to create training data. [https://towardsdatascience.com/how-to-annotate-pdfs-and-scanned-images-for-nlp-applications-f7b7b1db5c4a](https://towardsdatascience.com/how-to-annotate-pdfs-and-scanned-images-for-nlp-applications-f7b7b1db5c4a)

Disclaimer: The OCR feature mentioned in the article is only available for paid subscriptions, if you are interested by a demo send us an email at [admin@ubiai.tools](mailto:admin@ubiai.tools).",563,LanguageTechnology,1
"Assigning a category to a word Hey there,
I am waiting currently waiting my master's thesis and I had an idea for an analysis. 
I have a list of domain names and I was wondering whether I could automatically categorize them. I already tried the word embedding methods GloVe methods as presented by Stanford in 2013, googleNews negative 300 and Facebooks fasttext. My best results were with fasttext but they are still somewhat unreliable and when I look at the vectorization of words I find the their neighbours but I don't really get ""super-categories"" of these words. 
I would love to have something like:
f(""cars-for-sale.com"")=  ""car sales website"".
Am I overlooking some super obvious method for this? I am new to the field of NLP so please forgive me my ignorance.",770,LanguageTechnology,1
"What advantage is there to treating sentiment analysis as a classification problem vs a regression problem?  Many of the papers and tutorials I'm reading on sentiment analysis seem to treat the problem as classification problem where the goal is to classify some text as ""positive"", ""negative"", or ""neutral"". However, sentiment is discrete concept, so why would you treat is as such? Wouldn't you need to model sentiment as a float value between say -1 and 1, and then preform a regression? Is there are problem with the latter solution that the former method fixes?",566,LanguageTechnology,1
"Minimum text length in diary entries for detecting emotions/sentiment and classifying levels of mental health issues Hi, 

I am designing a study where I will collect daily diary entries and try to predict emotions/sentiment as well as levels of mental health issues (the ground truth scores will be acquired through daily questionnaires). Since I am new to NLP, what would be a good minimum text length of these diary entries for using machine learning/NLP methods and why? I would be grateful for any relevant sources as well.

Thanks!",537,LanguageTechnology,1
"Facebook AI Demonstrates How The Power Of Transfer Learning Can Boost Code Autocompletion Accuracy By Over 50% Autocompletion has become a handy and widely used tool in contemporary messaging and other writing tasks. It is also an essential feature of an integrated development environment (IDE) for computer programming. Recently, research has shown that autocompletion can be powered by deep learning, thus allowing software language models to achieve significant accuracy improvements by training on real-world datasets collected from programmers’ IDE activity. However, a common issue with less popular programming languages is that the available IDE datasets may be insufficient for training.

In a [paper](https://arxiv.org/pdf/2105.05991.pdf), a research team from Facebook demonstrates how transfer learning can enable pre-training on non-IDE, non-autocompletion, and different-language example code sequences before fine-tuning on the autocompletion prediction task. The proposed method improves model accuracy by more than 50 percent on small fine-tuning datasets and over 10 percent on 50k labeled examples.

Summary: [https://www.marktechpost.com/2021/05/31/facebook-demonstrates-how-the-power-of-transfer-learning-can-boost-code-autocompletion-accuracy-by-over-50/](https://www.marktechpost.com/2021/05/31/facebook-demonstrates-how-the-power-of-transfer-learning-can-boost-code-autocompletion-accuracy-by-over-50/?_ga=2.133350797.2041055336.1622397040-488125022.1618729090)

Paper: https://arxiv.org/pdf/2105.05991.pdf",1531,LanguageTechnology,1
[D] Probing Classifiers: A Gentle Intro (Explainable AI for Deep Learning) [Video] ,83,LanguageTechnology,1
"Your opinions: Where are conversational interfaces actually useful? Hi. I'm working in a company that has recently started accepting projects on conversational and vocal interfaces but I am personaly at a point in my path where I often doubt that these kind of applications with the current level of performance are actually useful outside very specific cases (like for disabled people).

So, in the spirit of the ""are chatbots just a fad?"" I would like to know your opinions on the following subjects:

# Are the current tecnologies for conversational and vocal interfaces really useful? Are we just offering on the market something that is ""cool"" but useless? What are examples of conversational and vocal interfaces the general public is using in their lifes right now? Where might we use these technologies in the very near future that we still don't?",855,LanguageTechnology,1
Into NLP - Part-of-speech tagging ,34,LanguageTechnology,1
[2105.13626] ByT5: Towards a token-free future with pre-trained byte-to-byte models ,84,LanguageTechnology,1
Entity-level Factual Consistency of Abstractive Text Summarization (Research Paper Walkthrough) ,96,LanguageTechnology,1
Noob question - What are the better tools/programs that general code using instructions in natural language/plain English? Generate code*,137,LanguageTechnology,1
"BERT - Annotated Paper + Paper Summary Everyone who is interested in NLP or even DL and ML for that matter, has definitely heard about the BERT family of models. BERT, RoBERTa, DistilBERT and many many more. This paper ""BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"" first introduced this and it has now completely changed the way AI practitioners are solving and looking at NLP problems these days.

As a part of my Paper Notes series, I have gone through the paper and created an informative summary of the paper. This time it goes a bit longer than the previous paper summaries, but it had to be done. The paper contained many tiny interesting nuggets that I had to include. Check out the links below and happy reading!

Paper Summary -  [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/)

Annotated Paper -  [https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/BERT.pdf](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/BERT.pdf)",1118,LanguageTechnology,1
Google’s Multitask Unified Model (MUM) Transforms How Google AI Understands Complex Queries ,92,LanguageTechnology,1
"Facebook AI releases Dynaboard: A New Evaluation platform for NLP Models Last year, Facebook AI released [Dynabench](https://dynabench.org/?fbclid=IwAR1ScAjtZoAb_PwA0-rPlCQYxWS-9-iGJpcYKQ5eyFUvLnQVVbSZtV8j3as), a platform that radically rethinks benchmarking in AI, starting with natural language processing (NLP) models. Going forward, they have now announced a new evaluation-as-a-service platform for comprehensive, standardized evaluations of NLP models called [Dynaboard](https://dynabench.org/dynaboard.pdf?fbclid=IwAR3rTJa8jRQtaJp8FFp5wf-eyWZ2QmXHTapxHjmpqw-j1t0Sw9n1xD31ASs). Dynaboard can perform apples-to-apples comparisons dynamically without common issues from bugs in evaluation code, inconsistencies in filtering test data, backward compatibility, accessibility, and several other reproducibility issues.

Dynaboard enables AI researchers to customize a new Dynascore metric based on multiple axes of evaluation, including compute, accuracy, robustness, memory, and fairness.

Full Summary: [https://www.marktechpost.com/2021/05/30/facebook-ai-releases-dynaboard-a-new-evaluation-platform-for-nlp-models/](https://www.marktechpost.com/2021/05/30/facebook-ai-releases-dynaboard-a-new-evaluation-platform-for-nlp-models/?_ga=2.190349480.2041055336.1622397040-488125022.1618729090)

Github: https://github.com/facebookresearch/dynalab

Paper: https://dynabench.org/dynaboard.pdf",1390,LanguageTechnology,1
"Structuring free text, then performing analysis vs. Performing analysis on unstructured free text Hi reddit, NLP newbie here

I am trying to understand if there is any value in creating a table out of free text, versus predictive analysis on the free text itself. 

For context, I am working with 2 million clinical notes from the MIMIC-III dataset, and I would like to tabluate all this unstructured data. Would this yield much value, considering I could design a bespoke predicitve model directly on to the free text? Would there be much difference in results from the free text compared to its structured counterpart?",620,LanguageTechnology,1
Python bindings for LibreTranslate ,35,LanguageTechnology,1
"Comparing between Cosine Similarity and Word Mover's Distance So basically what i'm attempting is to benchmark Cosine and WMD for a very generic use-case of textual similarity (This is probably quite the noob question).

My dataset is a parallel corpus made of pairs of paraphrased sentences. My issue is mainly in how I can compare the two metrics since they are of different order of magnitude:

* Cosine is \[-1, 1\] (or \[1, 0\]).
* WMD is not bound by an upper limit and goes from 0 to \~3.83 for my small dataset, and I can't think of a way to normalise it without introducing a bias.

My use case does not involve some form of classification, in which case I could just compare normally by how good the classification is. As for setting a threshold, (for example if WMD(S1, S2) &lt; 0.5 then consider the pair to be similar) and I really don't know how I can set a threshold based on this benchmarking alone because all pairs are actually paraphrases, so the expected output is always a distance of 0, while my results are quite sparse.

What would you guys suggest (a metric to compare with or a way to determine a threshold for the two of them, or anything else) to pick which of the 2 is performing better?",1216,LanguageTechnology,1
"Spacy Matcher, POS Tagging and Grammatical Errors Hi everyone, 

I'm a language learning/teaching researcher who has been trying to develop little tools to help other researchers recently. As you might guess, low-proficiency learner texts are quite a pain in the neck in this context due to frequent grammatical errors. 

So I'm actually looking for suggestions regarding a particular POS tagging problem as seen below:

""I think very good idea."" 

There, the learner obviously means ""I think -it is a- very good idea"" but since the function words are not there, POS taggers cannot accurately identify the parts of speech in the sentence. 

How could one deal with such sentences from a matching/POS tagging perspective? 

I have been considering ""next word generation"" but so far I haven't tested it properly. So any suggestion is welcome. 

Thanks in advance big time.",870,LanguageTechnology,1
"Loading mbart-large-50-one-to-many-mmt is very slow Whenever i try to run :
model = MBartForConditionalGeneration.from_pretrained("" [local path]/mbart-large-50-one-to-many-mmt"")
My computer ether freezes or it takes 15-20 minutes to load the model.

I am using it for translation
Code: https://huggingface.co/facebook/mbart-large-50-one-to-many-mmt

Any solution fo this?

-Thanks",380,LanguageTechnology,1
The competition that involves linguistics and logic ,52,LanguageTechnology,1
How to dramatically improve the reasoning ability of GPT-3 ,59,LanguageTechnology,1
"Any free open-access NLP annotation tools? I am looking for a web based tool for an NLP annotation task (similar to docanno for example). 

The catch is that I have to create a project and make it easily accessed with a link without the need to create an account per each annotator.

Any tool that comes close to this criteria?",327,LanguageTechnology,1
"Trying T5 11B Aside from self-hosting, where can I try non-fine-tuned T5 11B? 

If that’s not possible, are there any publicly available examples of its output?",160,LanguageTechnology,1
"Benchmarking An NLP Tool Just as the title says, I'm looking for a way to benchmark the sentiment analysis component of two or more NLP tools like retextjs and Stanford Core NLP. Are there any tools or techniques out for doing this?",232,LanguageTechnology,1
NLP Certifications Which are some coveted NLP Certifications available in the market having good industry recognition?,118,LanguageTechnology,1
"Semantic similarity between programming languages and math terminology I've wondered how good neural nets can get at predicting semantic similarity at a more abstract level between different topics. For example, finding context between math terminology like 'vector' and 'matrix' and programming terminology like 'list/array' and '2d array'. Also, finding context between common data types, functions, concepts between different languages and frameworks/libraries. Ideally, I'd want a binary output that takes two strings as input to compare.

Question is what would be the approach to a problem like this, would it need carefully labelled data ('translating' the terminology between the two topics) or is a self-supervised method at all possible? I've only recently got into data so I could be way off here on what is possible and what is not.",844,LanguageTechnology,1
"FastSpeech2 DTW computing, and other TTS evaluation methods Hi all, at the moment im working on a TTS system and i want to evaluate it. Unfortunately i can not do the usual Mean opinion score, so i'm looking at more numeric evaluations to compare my generated speech to gt.

While looking for some i found the fastspeech2 paper where they use Dynamic Time Warping (DTW) to evaluate the pitch in the generated speech. Here they explain it as: ""average DTW distances (DTW) of pitch in ground-truth and synthesized audio"".

Now, when i try to calculate the DTW with [this](https://dynamictimewarping.github.io/python/) python package i seem to get strange results. In the fast speech 2 paper their DTW distances are between 24-26. Mine are all between 9000-10000 when i try to do this.

the way i try to do this is as followed(in a short pseudo code):

    for all gt and prediction:
        distance = DTW(gt,pred)
        total += distance
    result = total/number_of_samples

I've tried several distances from the [scipy page](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html) but none of them seem to get close to the range of 24-26 (when i use the seuclidean distance i get a total of 96, but this still seems too high)

the only way i can come close to the 24-26 range is by also averaging the dtw (with normal euclidean distance) by the time series length as followed:

    for all gt and prediction:
        distance = DTW(gt,pred)/length_of_prediction
        total += distance
    result = total/number_of_samples

can someone tell me if this is correct or how i should calculate the average DTW distance instead? Maybe any links (i tried to read the refrence from the paper to Meinard Muller's chapter on this, but i couldnt figure out which distance to use from there).

Any other metrics to evaluate speech is also always welcome :)

Thank you and kind regards!

(i'm tagging u/rayeren since he posted the FS1/FS2 papers on r/MachineLearning and i think he's one of the authors. if so, can you comment on this please? :) )",2071,LanguageTechnology,1
[P] Do Context-Aware Translation Models Pay the Right Attention? ,65,LanguageTechnology,1
"What is considered as a small learning rate? I using the BERT transformer to solve the classification problem.

I am confused with the size of the learning rate of the BERT

The author suggests of using one of the following parameters

    learning rates: 3e-4, 1e-4, 5e-5, 3e-5
    
     

I know that a small learning rate makes our model learn very slow, however it also helps prevent overfitting, in contrast to big learning which learns faster but it can lead to overfitting.

&amp;#x200B;

When  I use a learning rate of 1e-5 I get the following result

    {""train"": {""eval_examples_count"": 8548, ""metrics"": {""f1_weighted"": 0.721, ""f1_macro"": 0.7201, ""accuracy"": 0.7255, ""roc_auc"": 0.8883}, ""time_spent"": ""0:02:06""}}
    {""valid"": {""eval_examples_count"": 2849, ""metrics"": {""f1_weighted"": 0.6766, ""f1_macro"": 0.6784, ""accuracy"": 0.6816, ""roc_auc"": 0.8545}, ""time_spent"": ""0:00:42""}}
    {""test"": {""eval_examples_count"": 2850, ""metrics"": {""f1_weighted"": 0.7003, ""f1_macro"": 0.7008, ""accuracy"": 0.7046, ""roc_auc"": 0.8685}, ""time_spent"": ""0:00:42""}}

However when I use 5e-5:

    {""train"": {""eval_examples_count"": 8548, ""metrics"": {""f1_weighted"": 0.1617, ""f1_macro"": 0.1647, ""accuracy"": 0.3269, ""roc_auc"": 0.5159}, ""time_spent"": ""0:02:07""}}
    {""valid"": {""eval_examples_count"": 2849, ""metrics"": {""f1_weighted"": 0.1743, ""f1_macro"": 0.1704, ""accuracy"": 0.3412, ""roc_auc"": 0.5321}, ""time_spent"": ""0:00:42""}}
    {""test"": {""eval_examples_count"": 2850, ""metrics"": {""f1_weighted"": 0.1758, ""f1_macro"": 0.1705, ""accuracy"": 0.3435, ""roc_auc"": 0.5208}, ""time_spent"": ""0:00:42""}}

IT DOES NOT LEARN ANYTHING

&amp;#x200B;

So I thought 1e-5 is small and 5e-5 is the big learning rate, am I right? 

What is the ascending order of these learning rates?

Which one considered as big and which is considered as small?",1808,LanguageTechnology,1
"XLNet - Annotated Paper + Paper Summary Although BERT became really popular after its release, it did have some limitations. And there were certain limitations associated with autoregressive methods like ELMo and GPT as well. XLNet was introduced to get the best of both worlds while at the same time not include their weaknesses.

In continuation of my Paper Notes series, I have written an informative summary of the paper. Personally, reading the XLNet paper was a very fun experience. I was amazed at every step, how they were including stuff to make the whole model work so well. The paper contained many interesting concepts that I had to give time to understand. So don't worry if you don't get it on the first go. Check out the links below and happy reading!

Paper Summary - [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/)

Annotated Paper - [https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/XLNet.pdf](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/XLNet.pdf)",1115,LanguageTechnology,1
"Paraphrase Generation given Input Sentences I need a good way to generate paraphrases from given input sentences. The input sentences already have the same meaning/are paraphrases. But I want more paraphrases (for example with different syntactic structures, with synonyms etc.). Ideally, I want to combine lexical and syntactic paraphrasing. What could be the state of the art solution for this problem? I read in some paper about phrase based translation methods but I don't think this suits my task. I am open for paper suggestions!",535,LanguageTechnology,1
English Paraphrasing in Python with Parrot ,43,LanguageTechnology,1
Summarizing NLP Research Papers ,32,LanguageTechnology,1
"GPT-2 - Annotated Paper + Paper Summary The GPT-2 model was a major breakthrough in the path of creating a general multitask NLP system that was totally unsupervised. It demonstrated that given a large training corpus and a large model size, the language model was capable of learning the knowledge required for solving these tasks. It was not perfect, however, and performed poorly on some tasks as well.

I went through the paper and have written an informative summary of the paper. The paper was quite easy to follow and the experimentation section had interesting observations. Check out the links below and happy reading!

Paper Summary -  [Language Models are Unsupervised Multitask Learners](https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/)

Annotated Paper -  [https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT2.pdf](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT2.pdf)",963,LanguageTechnology,1
For GPT3 users - What's the longest output you've had it write? How long was it and what was it about?,102,LanguageTechnology,1
"Transformer-XL: Attentive Language Models Beyond a Fixed Length Context My first youtube video discussing a research paper -

**Transformer-XL: Attentive Language Models Beyond a Fixed Length Context** [https://youtu.be/2cmVnQNWQt8](https://youtu.be/2cmVnQNWQt8) 

Check it out!",278,LanguageTechnology,1
"Zero-Shot Knowledge Distillation From GPT-3 Hi there,

I recently wrote a blog post about zero-shot knowledge distillation from big language models with a method called DINO (Datasets from Instructions 🦕) that does not require any (labeled or unlabeled) data or access to model internals. The key idea is to prompt the LM to generate an entire dataset from scratch on which much smaller models can then be trained.

 I'd be very happy to hear your thoughts (both on the blog post and on the method) 😊

📝 Link: http://timoschick.com/research/2021/05/19/dino.html",561,LanguageTechnology,1
"Extracting topics from 250k facebook posts Hi there!

I'm working on a project in which I need to extract the topics from a collection of 250K facebook posts. 

Can someone recommend (in high level) what's the best approach to do something like that? Ways that will generate the best results.

Thanks!",301,LanguageTechnology,1
Efficient System for Grammar Error Correction on Mobile Devices (Research Paper Summary) ,89,LanguageTechnology,1
Can you classify texts without any labeled examples? Putting zero-shot classification to the test. ,99,LanguageTechnology,1
"EleutherAI Develops GPT-3’s Free Alternative: GPT-Neo In today’s era, all top benchmarks in natural language processing are dominated by Transformer-based models. In a machine learning model, the most critical elements of the training process are the model code, training data, and available computing resources.

With the Transformer family of models, researchers have now finally come up with a way to increase the performance of a model infinitely by increasing the amount of training data and compute power.

OpenAI did this with GPT-2 and with GPT-3. They used a private corpus of 500 billion tokens for training the model and spent $50 million in computing costs.

Full Article: [https://www.marktechpost.com/2021/05/24/eleutherai-develops-gpt-3s-free-alternative-gpt-neo/](https://www.marktechpost.com/2021/05/24/eleutherai-develops-gpt-3s-free-alternative-gpt-neo/?_ga=2.62220524.1924646600.1621739878-488125022.1618729090)

Github: [https://github.com/EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo)",1019,LanguageTechnology,1
"Is there a way to score the structure of a sentence against a corpus of other sentences? Using Python.

Having trouble putting this question into words.

Is there a way to score the structure of a sentence against a corpus of other sentences?

So if a sentence if badly written in terms of grammar/word order, the score would be low.

I guess one would need to know the most common sentence structure within the corpus and work back from there?

Any ideas much appreciated and apologies for the vagueness of the question.",521,LanguageTechnology,1
"Tokenising SpaCy constituency parse output Hi all, so I'm doing some constituency parsing with SpaCy and benepar, and i'd ideally like to be able to tag the words in a sentence with their corresponding constituents. 

Currently however, Spacy allows you to print the parse string, but I'd like to be able to attach the actual syntactic structure to each word, as a POS tagger would do. So for the following code; 

    import spacy
    import benepar
    
    nlp = spacy.load('en_core_web_md')
    
    if spacy.__version__.startswith('2'):
      nlp.add_pipe('benepar', config={'model': 'benepar_en3'})
    else:
      nlp.add_pipe(""benepar"", config={""model"": ""benepar_en3""})
    
    doc = nlp(""Last Tuesday, I thought to myself that I saw a cat."")
    sent = list(doc.sents)[0]
    
    constituents = sent._.parse_string
    print(constituents)
    
    &gt;&gt;&gt; (S (NP (JJ Last) (NNP Tuesday)) (, ,) (NP (PRP I)) (VP (VBD thought) (PP (IN to) (NP (PRP myself))) (SBAR (IN that) (S (NP (PRP I)) (VP (VBD saw) (NP (DT a) (NN cat)))))) (. .))
    

I'd like for each word to be tagged with it's corresponding grammatical structure, but currently this is just one string. Any ideas on how to achieve this?",1211,LanguageTechnology,1
"Get POS tags for multiple spacy documents EDIT - solved 

I have a list of strings, which I have turned into SpaCy documents in order to be able to POS tag each string as such; 

    example_sents = [nlp(sent) for sent in example_sents[0:2]]
    print(example_sents)
    
    &gt;&gt;&gt; [well as long as you accept that it is  testing uhyou know  alleviate, she cant accept that we want to be the care givers]

However, when I try and POS tag multiple strings, I get the error 'AttributeError: 'spacy.tokens.doc.Doc' object has no attribute 'pos\_''

Does anyone know why this is happening when I try to iterate over each spacy document as such; 

    for token in example_sents[1:2]:
      print(token.text, token.pos_)

Thanks!",731,LanguageTechnology,1
Analyzing the vast coronavirus literature with CoronaCentral ,61,LanguageTechnology,1
Controllable Text Summarization in Python based on CtrlSum ,59,LanguageTechnology,1
"One sentence highlight for every NAACL-2021 Paper Here is the list of all NAACL-2021 (Annual Conference of the North American Chapter of the Association for Computational Linguistics) papers, and a one sentence highlight for each of them. The proceeding was released today.

[https://www.paperdigest.org/2021/05/naacl-2021-highlights/](https://www.paperdigest.org/2021/05/naacl-2021-highlights/)",395,LanguageTechnology,1
"Advanced Math for NLP I am in my last year of undergraduate studies and want to use my electives on math. Other than basic math like Probability theory, Statistics, Linear Algebra, or Multivariate Calculus, are there any other important math classes that would be beneficial for a career or research in NLP in 2021-2022?

I assume ODEs can help with Neural Nets and higher statistics are always beneficial but there must be more.",429,LanguageTechnology,1
"Search on vector embeddings What would be the way to go for a search engine to search on similar words. When I check txtai or similar implementations they use pooling (averaging) on words. What I am looking for is that if i search on a query of 3 words for example that it searches for the 100 closest words (cosine similarity) for each word and creates some rank. But this rank should not be compromised by matching words of only one semantic meaning. 
Example if I search for:  “great iphone tutorials” this ranking should not be compromised by documents with a lot of matching similarities for “great” with no mentioning for iphone and tutorials.",649,LanguageTechnology,1
"NLP : Access feature vector from Pipeline object I am new to machine learning.  I am trying to build a text classifier.

I have the following code:

    title_tfidf = TfidfVectorizer(use_idf=True, max_df=0.12, stop_words='english', preprocessor=custom_preprocessor) 
    text_tfidf = TfidfVectorizer(use_idf=True, max_df=0.12, stop_words='english', preprocessor=custom_preprocessor)  
    
    preprocess = ColumnTransformer([('title_tfidf', title_tfidf, 'Title'), ('text_tfidf', text_tfidf, 'Text')])  
    
    model = make_pipeline(preprocess, LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000))  
    
    model.fit(x_train, y_train)

I access the feature-names list and coefficients in the following way:

    regressor = model.named_steps['logisticregression'] 
    print (""Coefficients: "", regressor.coef_) 
    feature_names = preprocess.get_feature_names()

how can one retrieve the feature vector?

Would be grateful for any inputs!",989,LanguageTechnology,1
"Interesting seminars and talks? I have to attend a quota of seminars as part of my degree, but the offerings at my university are a bit limited. Is there an equivalent to [http://researchseminars.org/](http://researchseminars.org/) for language technology / computation linguistics / that sort of space?

Or, if you don't know of a large universal one, does your research group have a website where you publish your upcoming seminars?",434,LanguageTechnology,1
A Graph-based Text Similarity Method with Named Entity Information in NLP ,74,LanguageTechnology,1
"Spanish keyword extraction [D] Hey guys am creating a keyword extraction model to get keywords from text, for Spanish language. Any and all resources or ideas are welcome. Thanks 😊",180,LanguageTechnology,1
"Laptop for deep learning Hello,

What is the best laptops for deep learning under 1000$?

Thank you",99,LanguageTechnology,1
"Classification of bank statement transactions Hello everybody, I am a student working on his bachelor thesis.. I have a some ML background but I am quite new to NLP.

Shortly, the goal of my thesis is to extract text from CSV bank transactions files and EBICS exports and categorize them into about 20 categories.. So far I have tried with a basic Naive-Bayes classificator getting +- 65% accuracy but nothing more.. do you have any tips on some specific models or papers that could help me achieve a higher precision?

To add some more details, the bank statements that I have so far are not that big in terms of transactions, I have about 400 rows of data right now.. this is probably impacting the accuracy but it's kinda a long process to gather those kind of files.

Additionally, the data from the CVS and EBIC files are quite short sentences.. around 60/80 characters per line.

Thanks in advance for any suggestion! have a nice day :)",942,LanguageTechnology,1
"How to write a code/mini program that collects articles? To clarify, is it difficult for a non-programmer to write a code or mini program that would collect articles based on several simple criteria or maybe there are some other solutions for this purpose?

Example: I want to collect and gather all articles published on BBC website between January 2021 and April 2021 that contain the word ""doggo"" for the purposes of corpus linguistics. 

I am aware of BootCat but I never managed to get it work properly, though",515,LanguageTechnology,1
An Efficient System for Grammatical Error Correction on Mobile Devices (Research Paper Walkthrough) ,100,LanguageTechnology,1
"gImageReader/Tesseract: Having trouble with old texts and spaces between words I'm working with some older texts, were the letters are sometimes a little smudgy.

The letters and words get recognized near-perfectly, and when I look at the hOCR or html file, the text looks perfect.

But when I **export to PDF** with an invisible text layer the spaces between words frequently go missing, for paragraphs at a time. This is annoying when trying to highlight parts of the text and then copy-paste those excerpts.

Any advice?

Other than these old texts, gImageReader is absolutely amazing and does exactly what I want.",617,LanguageTechnology,1
"Automating a Web Scraper I'm trying to automate the process of web scraping. I have a dataset of real-estate data with fields like Title, Price, Street, Postcode, etc. I want to make a model that takes the content of a webpage and returns words/phrases that are similar to/match those in the dataset. This way I won't have to write a custom web scraper for every website. I think this is similar to keyword extraction except that I want the model to learn a dataset of keywords and then extract those keywords from a webpage.

Can anyone guide me on how to approach this problem?",579,LanguageTechnology,1
"Using document formatting for NLU Can anyone recommend any NLU services, libraries, or methods that can perform NLU tasks using both language tokens and simple document formatting such as indentation level. For example, in lecture notes, indentations could help refine coreference resolution by understanding that there’s a good chance indented text is highly related to less indented text on an earlier document line or paragraph.",431,LanguageTechnology,1
"Is Word Sense Disambiguation outdated? I recently published a blog post about how the original WSD task formulation is not suitable for modern domain-specific and enterprise disambiguation settings and how Target Sense Verification can improve this situation. Here is the link: [https://annabreit.medium.com/is-word-sense-disambiguation-outdated-ef05a139576](https://annabreit.medium.com/is-word-sense-disambiguation-outdated-ef05a139576)  


Would be glad for any feedback and am happy to discuss!",498,LanguageTechnology,1
Text Extraction New to NLP. Can anyone suggest me how I can use NLP to extract important details from large documents? Thanks in advance.,137,LanguageTechnology,1
Automatic Extraction of Hypernym Relations from Text ,53,LanguageTechnology,1
"1 line to visualizations for dependency trees, entity relationships, resolution, assertion, NER and new models for Afrikaans, Welsh, Maltese, Tamil, and Vietnamese - John Snow Labs NLU 3.0.1 for Python # NLU 3.0.1 Release Notes
We are very excited to announce NLU 3.0.1 has been released!
This is one of the most visually appealing releases, with the integration of the [Spark-NLP-Display](https://nlp.johnsnowlabs.com/docs/en/display) library and visualizations for `dependency trees`, `entity resolution`, `entity assertion`, `relationship between entities` and `named
entity recognition`. In addition to this, the schema of how columns are named by NLU has been reworked and all 140+ tutorial notebooks have been updated to reflect the latest changes in NLU 3.0.0+
Finally, new multilingual models for `Afrikaans`, `Welsh`, `Maltese`, `Tamil`, and`Vietnamese` are now available.




# New Features and Enhancements
- 1 line to visualization for `NER`, `Dependency`, `Resolution`, `Assertion` and `Relation` via [Spark-NLP-Display](https://nlp.johnsnowlabs.com/docs/en/display) integration
- Improved column naming schema
- [Over 140 + NLU tutorial Notebooks updated](https://github.com/JohnSnowLabs/nlu/tree/master/examples) and improved to reflect latest changes in NLU 3.0.0 +
- New multilingual models for `Afrikaans`, `Welsh`, `Maltese`, `Tamil`, and`Vietnamese`
- Enhanced offline loading


## NLU visualization
The latest NLU release integrated the beautiful Spark-NLP-Display package visualizations. You do not need to worry about installing it, when you try to visualize something, NLU will check if
Spark-NLP-Display is installed, if it is missing it will be dynamically installed into your python executable environment, so you don't need to worry about anything!

See the [visualization tutorial notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/visualization/NLU_visualizations_tutorial.ipynb)  and [visualization docs](https://nlu.johnsnowlabs.com/docs/en/viz_examples) for more info.

![Cheat Sheet visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/cheat_sheet.png)

## NER visualization
Applicable to any of the [100+ NER models! See here for an overview](https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition)
```python
nlu.load('ner').viz(""Donald Trump from America and Angela Merkel from Germany don't share many oppinions."")
```
![NER visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/NER.png)

## Dependency tree visualization
Visualizes the structure of the labeled dependency tree and part of speech tags
```python
nlu.load('dep.typed').viz(""Billy went to the mall"")
```

![Dependency Tree visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/DEP.png)

```python
#Bigger Example
nlu.load('dep.typed').viz(""Donald Trump from America and Angela Merkel from Germany don't share many oppinions but they both love John Snow Labs software"")
```
![Dependency Tree visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/DEP_big.png)

## Assertion status visualization
Visualizes asserted statuses and entities.        
Applicable to any of the [10 + Assertion models! See here for an overview](https://nlp.johnsnowlabs.com/models?task=Assertion+Status)
```python
nlu.load('med_ner.clinical assert').viz(""The MRI scan showed no signs of cancer in the left lung"")
```


![Assert visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/assertion.png)

```python
#bigger example
data ='This is the case of a very pleasant 46-year-old Caucasian female, seen in clinic on 12/11/07 during which time MRI of the left shoulder showed no evidence of rotator cuff tear. She did have a previous MRI of the cervical spine that did show an osteophyte on the left C6-C7 level. Based on this, negative MRI of the shoulder, the patient was recommended to have anterior cervical discectomy with anterior interbody fusion at C6-C7 level. Operation, expected outcome, risks, and benefits were discussed with her. Risks include, but not exclusive of bleeding and infection, bleeding could be soft tissue bleeding, which may compromise airway and may result in return to the operating room emergently for evacuation of said hematoma. There is also the possibility of bleeding into the epidural space, which can compress the spinal cord and result in weakness and numbness of all four extremities as well as impairment of bowel and bladder function. However, the patient may develop deeper-seated infection, which may require return to the operating room. Should the infection be in the area of the spinal instrumentation, this will cause a dilemma since there might be a need to remove the spinal instrumentation and/or allograft. There is also the possibility of potential injury to the esophageus, the trachea, and the carotid artery. There is also the risks of stroke on the right cerebral circulation should an undiagnosed plaque be propelled from the right carotid. She understood all of these risks and agreed to have the procedure performed.'
nlu.load('med_ner.clinical assert').viz(data)
```
![Assert visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/assertion_big.png)


## Relationship between entities visualization
Visualizes the extracted entities between relationship.    
Applicable to any of the [20 + Relation Extractor models See here for an overview](https://nlp.johnsnowlabs.com/models?task=Relation+Extraction)
```python
nlu.load('med_ner.jsl.wip.clinical relation.temporal_events').viz('The patient developed cancer after a mercury poisoning in 1999 ')
```
![Entity Relation visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/relation.png)

```python
# bigger example
data = 'This is the case of a very pleasant 46-year-old Caucasian female, seen in clinic on 12/11/07 during which time MRI of the left shoulder showed no evidence of rotator cuff tear. She did have a previous MRI of the cervical spine that did show an osteophyte on the left C6-C7 level. Based on this, negative MRI of the shoulder, the patient was recommended to have anterior cervical discectomy with anterior interbody fusion at C6-C7 level. Operation, expected outcome, risks, and benefits were discussed with her. Risks include, but not exclusive of bleeding and infection, bleeding could be soft tissue bleeding, which may compromise airway and may result in return to the operating room emergently for evacuation of said hematoma. There is also the possibility of bleeding into the epidural space, which can compress the spinal cord and result in weakness and numbness of all four extremities as well as impairment of bowel and bladder function. However, the patient may develop deeper-seated infection, which may require return to the operating room. Should the infection be in the area of the spinal instrumentation, this will cause a dilemma since there might be a need to remove the spinal instrumentation and/or allograft. There is also the possibility of potential injury to the esophageus, the trachea, and the carotid artery. There is also the risks of stroke on the right cerebral circulation should an undiagnosed plaque be propelled from the right carotid. She understood all of these risks and agreed to have the procedure performed'
pipe = nlu.load('med_ner.jsl.wip.clinical relation.clinical').viz(data)
```
![Entity Relation visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/relation_big.png)


## Entity Resolution visualization for chunks
Visualizes resolutions of entities
Applicable to any of the [100+ Resolver models See here for an overview](https://nlp.johnsnowlabs.com/models?task=Entity+Resolution)
```python
nlu.load('med_ner.jsl.wip.clinical resolve_chunk.rxnorm.in').viz(""He took Prevacid 30 mg  daily"")
```
![Chunk Resolution visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_chunk.png)

```python
# bigger example
data = ""This is an 82 - year-old male with a history of prior tobacco use , hypertension , chronic renal insufficiency , COPD , gastritis , and TIA who initially presented to Braintree with a non-ST elevation MI and Guaiac positive stools , transferred to St . Margaret\'s Center for Women &amp; Infants for cardiac catheterization with PTCA to mid LAD lesion complicated by hypotension and bradycardia requiring Atropine , IV fluids and transient dopamine possibly secondary to vagal reaction , subsequently transferred to CCU for close monitoring , hemodynamically stable at the time of admission to the CCU .""
nlu.load('med_ner.jsl.wip.clinical resolve_chunk.rxnorm.in').viz(data)
```

![Chunk Resolution visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_chunk_big.png)


## Entity Resolution visualization for sentences
Visualizes resolutions of entities in sentences
Applicable to any of the [100+ Resolver models See here for an overview](https://nlp.johnsnowlabs.com/models?task=Entity+Resolution)
```python
nlu.load('med_ner.jsl.wip.clinical resolve.icd10cm').viz('She was diagnosed with a respiratory congestion')
```
![Sentence Resolution visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_sentence.png)

```python
# bigger example
data = 'The patient is a 5-month-old infant who presented initially on Monday with a cold, cough, and runny nose for 2 days. Mom states she had no fever. Her appetite was good but she was spitting up a lot. She had no difficulty breathing and her cough was described as dry and hacky. At that time, physical exam showed a right TM, which was red. Left TM was okay. She was fairly congested but looked happy and playful. She was started on Amoxil and Aldex and we told to recheck in 2 weeks to recheck her ear. Mom returned to clinic again today because she got much worse overnight. She was having difficulty breathing. She was much more congested and her appetite had decreased significantly today. She also spiked a temperature yesterday of 102.6 and always having trouble sleeping secondary to congestion'
nlu.load('med_ner.jsl.wip.clinical resolve.icd10cm').viz(data)
```
![Sentence Resolution visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_sentence_big.png)

## Configure visualizations
### Define custom colors for labels
Some entity and relation labels will be highlighted with a pre-defined color, which you [can find here](https://github.com/JohnSnowLabs/spark-nlp-display/tree/main/sparknlp_display/label_colors).    
For labels that have no color defined, a random color will be generated.     
You can define colors for labels manually, by specifying via the `viz_colors` parameter
and defining `hex color codes` in a dictionary that maps `labels` to `colors` .
```python
data = 'Dr. John Snow suggested that Fritz takes 5mg penicilin for his cough'
# Define custom colors for labels
viz_colors={'STRENGTH':'#800080', 'DRUG_BRANDNAME':'#77b5fe', 'GENDER':'#77ffe'}
nlu.load('med_ner.jsl.wip.clinical').viz(data,viz_colors =viz_colors)
```
![define colors labels](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/define_colors.png)


### Filter entities that get highlighted
By default every entity class will be visualized.    
The `labels_to_viz` can be used to define a set of labels to highlight.       
Applicable for ner, resolution and assert.
```python
data = 'Dr. John Snow suggested that Fritz takes 5mg penicilin for his cough'
# Filter wich NER label to viz
labels_to_viz=['SYMPTOM']
nlu.load('med_ner.jsl.wip.clinical').viz(data,labels_to_viz=labels_to_viz)
```
![filter labels](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/filter_labels.png)


## New models
New multilingual models for `Afrikaans`, `Welsh`, `Maltese`, `Tamil`, and`Vietnamese`

| nlu.load() Refrence                                          | Spark NLP Refrence                                           |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [vi.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_vi.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_vi.html) |
| [mt.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_mt.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_mt.html) |
| [ta.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_ta.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_ta.html) |
| [af.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_af.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_af.html) |
| [af.pos](https://nlp.johnsnowlabs.com/2021/04/06/pos_afribooms_af.html) | [pos_afribooms](https://nlp.johnsnowlabs.com/2021/04/06/pos_afribooms_af.html) |
| [cy.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_cy.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_cy.html) |

## Reworked and updated NLU tutorial notebooks

All of the [140+ NLU tutorial Notebooks](https://github.com/JohnSnowLabs/nlu/tree/master/examples) have been updated and reworked to reflect the latest changes in NLU 3.0.0+


## Improved Column Name generation
- NLU categorized each internal component now with boolean labels for `name_deductable` and `always_name_deductable` .
- Before generating column names, NLU checks wether each component is of unique in the pipeline or not. If a component is not unique in the
  pipe and there are multiple components of same type, i.e. multiple `NER` models, NLU will deduct a base name for the final output columns from the
  NLU reference each NER model is pointing to.
- If on the other hand, there is only one `NER` model in the pipeline, only the default `ner` column prefixed will be generated.
- For some components, like `embeddings` and `classifiers` are now defined as `always_name_deductable`, for those NLU will always try to infer a meaningful base name for the output columns.
- Newly trained component output columns will now be prefixed with `trained_&lt;type&gt;` , for types `pos` , `ner`, `cLassifier`, `sentiment` and `multi_classifier`

## Enhanced offline mode
- You can still load a model from a path as usual with `nlu.load(path=model_path)` and output columns will be suffixed with `from_disk`
- You can now optionally also specify `request` parameter during  load a model from HDD, it will be used to deduct more meaningful column name suffixes, instead of `from_disk`, i.e. by calling `nlu.load(request ='en.embed_sentence.biobert.pubmed_pmc_base_cased', path=model_path)`


### Bugfixes
- Fixed a bug that caused  resolution algorithms output level to be inferred incorrectly
- Fixed a bug that caused stranger cols got dropped
- Fixed a bug that caused endings to miss when  .predict(position=True) was specified
- Fixed a bug that caused pd.Series to be converted incorrectly internally
- Fixed a bug that caused output level transformations to crash
- Fixed a bug that caused verbose mode not to turn of properly after turning it on.
- fixed a bug that caused some models to crash when loaded for HDD

# Additional NLU resources
* [140+ updates tutorials](https://github.com/JohnSnowLabs/nlu/tree/master/examples)
* [Updated visualization docs](https://nlu.johnsnowlabs.com/docs/en/viz_examples)
* [Models Hub](https://nlp.johnsnowlabs.com/models) with new models
* [Spark NLP publications](https://medium.com/spark-nlp)
* [NLU in Action](https://nlp.johnsnowlabs.com/demo)
* [NLU documentation](https://nlu.johnsnowlabs.com/docs/en/install)
* [Discussions](https://github.com/JohnSnowLabs/spark-nlp/discussions) Engage with other community members, share ideas, and show off how you use Spark NLP and NLU!

# 1 line Install NLU on Google Colab
```!wget https://setup.johnsnowlabs.com/nlu/colab.sh  -O - | bash```
# 1 line Install NLU on Kaggle
```!wget https://setup.johnsnowlabs.com/nlu/kaggle.sh  -O - | bash```
# Install via PIP
```! pip install nlu pyspark==3.0.1```",16577,LanguageTechnology,1
"Dataset to study structure of comparison questions  I have to study the structure of questions comparing electronic products, e.g., Computers, Laptops, Tablets, etc.

Example questions:

1. What are the disadvantages of a Chromebook as opposed to a regular laptop computer?
2. what are the pros and cons of buying a dell laptop?
3. what can a $1000 apple laptop do which $500 Lenovo laptop not? is an expensive one worth it?
4. are hp laptops good for students?

I would be grateful If anyone can provide a lead to such a dataset. Currently, I am looking at questions on Quora, Reddit, and other forums manually.",612,LanguageTechnology,1
"Looking to extract insights from your unstructured text? Check out this article on how to combine NLP and knowledge graphs to uncover new information. If you have a specific use case that can benefit from knowledge graphs, please share below, we would love to learn about the different applications of NLP. If you have any question, please reply below or send an email at [admin@ubiai.tools](mailto:admin@ubiai.tools)

&amp;#x200B;

[https://walidamamou.medium.com/building-a-knowledge-graph-for-job-search-using-bert-transformer-8677c8b3a2e7](https://walidamamou.medium.com/building-a-knowledge-graph-for-job-search-using-bert-transformer-8677c8b3a2e7)",653,LanguageTechnology,1
"How does Google Translate's add-on translates a web page so perfectly? I'm talking about the little translate button at the top left (can't put images).

&amp;#x200B;

As I understood it takes the html and translates the text in it. What I can't comprehend is it fits the correct words perfectly, for example in the &lt;strong&gt; tag.

For example &lt;p&gt;there is &lt;strong&gt;some text here&lt;/strong&gt; google will translate it&lt;/p&gt;

After the translation google finds the exact match to put inside the &lt;strong&gt; tag. They can't translate it separately it would distort the meaning. Even sentence is complex and inside the tag does not make a sentence on its own they handle it somehow.

I tried to make it as clear as possible. I've been searching but I couldn't come up with a solution. Do you have any ideas? Please share.

&amp;#x200B;

Edit (Clarification)

&amp;#x200B;

We have a sentence: ""It’s recommended that you do that in a [virtual environment using virtualenv](https://pythonbasics.org/virtualenv/).""

&amp;#x200B;

as you can see ""virtual environment using virtualenv"" is let's say tagged. After translating this sentence google is able to tag the same piece in the target language. For example

&amp;#x200B;

Es wird empfohlen, dies in einer [virtuellen Umgebung mit virtualenv zu tun](https://pythonbasics.org/virtualenv/) .

&amp;#x200B;

How they can match the meaning and ""tag"" the correct positions? Obviously translation of the tagged part won't be same if it's done independently from the original sentence. So how they match it?",1571,LanguageTechnology,1
"M.Sc. Computer linguistics in Germany Hi, hi!

Basically, my case is the opposite as the one archived here:

https://www.reddit.com/r/LanguageTechnology/comments/8w06zn/msc_computational_linguistics_in_germany/

I come from a theoretical/computational physics background and would like to properly educate myself in computational linguistics. Linguistics has been much of a hobby for me, while math and programming were part of my curriculum. 

Now I am finishing my master's on physics of complex systems and had one introduction to speech and text analysis, as well as a deep learning course.

This last information is also relevant, since I wouldn't like paying the tution fees for a second master (Zweitstudiengebühren) which would be applied in BaWü (Heidelberg, Tübingen, Stuttgart)

Any recommendations here?",815,LanguageTechnology,1
A New Google Research Introduces FNet By Replacing Self-Attention Sublayers With Simple Linear Transformations Achieving 92% Accuracy and Runs Up To Seven Times Faster On GPUs And Twice As Fast On TPUs (Paper link in comments) ,227,LanguageTechnology,1
"Is this approach to summary production novel? Hello! Been playing around with summary generators, and I'm pretty sure I came up with a novel approach, which should produce a summary that has the most resemblance to an author's typical writing style. Basically I'm just trying to find out if there's a word for this algo yet or not. Project is called Bite, and can be found [here](https://github.com/cyberrumor/bite). 

&amp;#x200B;

Say we have the following corpus:  


""Mary had a little lamb.""  


Bite will tokenize the sentence like so:  


* mary had
* mary had a 
* mary had a little
* mary had a little lamb
* had a
* had a little
* had a little lamb
* a little
* a little lamb
* a lamb

Next, it creates a frequency map to count all occurrences of each token, assigning scores to sentences by adding up the sum of token scores. 

&amp;#x200B;

Is there a word for this type of categorization?",901,LanguageTechnology,1
Data Augmentation Techniques in NLP ,36,LanguageTechnology,1
What are some resources to learn about incorporating images into word vectorization model? ,91,LanguageTechnology,1
Neural nets for word sense disambiguation? ,43,LanguageTechnology,1
"Automatically label syntactic structures in text I'm curious as to the best way to tag the syntactic structures of some text. So not just POS tags, but things like complement and relative clauses? Are there taggers specifically for this?",237,LanguageTechnology,1
MUM: a new AI milestone for understanding information ,54,LanguageTechnology,1
Māori are trying to save their language from Big Tech ,54,LanguageTechnology,1
"Debugging a NN model Hi all,

I have trained a simple multi-input NN. I have 4 inputs ( one text field &amp; other 3 categorical variables : cat1, cat2 , cat3).

It is a classification model.

For the text field, I use Glove embeddings in the Embedding layer, followed by LSTM layer.

For the other 3 categorical fields, I just encode them in a dense layer. Finally, I concatenate these 2 layers followed by softmax for classification. Below is the main block of code :

&amp;#x200B;

    embeddings_dictionary = dict()
    glove_file = open('glove.6B.100d.txt', encoding=""utf8"")
    for line in glove_file:
        records = line.split()
        word = records[0]
        vector_dimensions = asarray(records[1:], dtype='float32')
        embeddings_dictionary[word] = vector_dimensions
    glove_file.close()
    embedding_matrix = zeros((vocab_size, 100))
    for word, index in tokenizer.word_index.items():
        embedding_vector = embeddings_dictionary.get(word)
        if embedding_vector is not None:
            embedding_matrix[index] = embedding_vector
    
    # Text input handling
    text_input_1 = Input(shape=(maxlen,))
    embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(text_input_1)
    LSTM_Layer_1 = LSTM(128)(embedding_layer)
    
    
    # categorical variables handling
    layout_input_2 = Input(shape=(3,))
    dense_layer_1 = Dense(10, activation='relu')(layout_input_2)
    dense_layer_2 = Dense(10, activation='relu')(dense_layer_1)
    
    # Concatenating the above 2
    concat_layer = Concatenate()([LSTM_Layer_1, dense_layer_2])
    dense_layer_3 = Dense(10, activation='relu')(concat_layer)
    output = Dense(3, activation='softmax')(dense_layer_3)
    model = Model(inputs=[text_input_1, layout_input_2], outputs=output)
    
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
    print(model.summary())

The problem is , when I infer/predict using the above model weights, the respose is like all the weightage is being given to the categorical variables only , irrespective of the text input.

e.g. predict (text1, cat1, cat2, cat3)  &amp;  predict (text2, cat1, cat2, cat3)  is exactly same.

Even if I provide blank text as input, the output remains same if I don't change the categorical variable values.

Can anybody help troubleshooting this ?",2368,LanguageTechnology,1
"Deploying a transformer-based text classification NLP model with FastAPI Hello all,

Recently I wrote an article about deploying spaCy with FastAPI for NER. As many people told me it was helpful, I did a new article about deploying transformer-based models with FastAPI for text classification (using Facebook's Bart Large MNLI model).

FastAPI  is a great is great framework for API development in Python in my opinion. It helped me save a lot of troubles when developing the [NLPCloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=d13c16ae-b7d8-11eb-8529-0242ac130003) API.

Here's the article:

[https://nlpcloud.io/nlp-machine-learning-classification-api-production-fastapi-transformers-nlpcloud.html](https://nlpcloud.io/nlp-machine-learning-classification-api-production-fastapi-transformers-nlpcloud.html?utm_source=reddit&amp;utm_campaign=d13c16ae-b7d8-11eb-8529-0242ac130003)

I'd love to have your feedback on this! Have you ever deployed transformer-based models to production? If so, which tools did you use?

Thanks!",1043,LanguageTechnology,1
Explanability for Transformers with Transformers-Interpret — A Model Explainability Tool ,89,LanguageTechnology,1
"Long Form Automatic Transcription from Audio (Transcribe Movie Dialogue to Text) - How Do I Do It? So I'm looking to create a subtitle file from a film that is only in Portuguese. 

I can do it by hand but that would take sooo long. 

I figure that I can probably extract the audio and use Google's transcription API to do a majority of the work for me, but I'm not sure how to go about doing that with a long length file.

Any tips?",433,LanguageTechnology,1
Text Summarisation Techniques in NLP ,37,LanguageTechnology,1
The importance of language technologies for the future of the public health system (in Spanish) ,96,LanguageTechnology,1
"Spacy 3.0 multi class Textcat If anyone has any experience with this I would love to pick your brain / ask you a little assistance. 

Been struggling to correctly initialize my model.",183,LanguageTechnology,1
"Spacy NER Model 2 Entities versus 6 Hello all! Currently have a Spacy NER model that looks for 6 custom entities, while only two (Person and Organization) are truly important.

Would it be beneficial to cut out the other four entities in the model? My boss added them because he thought they would help with accuracy essentially through lessening the chance of false positives for person and organization. 

I personally believe the model would perform fine with only Person and organization, but am new to data science so just wanted to verify my hunch.",554,LanguageTechnology,1
"Is it conference paper worthy? My final year project is on Multi-class text classification and simply explores existing techniques (TF-IDF and Word2Vec) on a new and different dataset. Pipeline is standard: Data Pre-processing and Cleaning, Vectorization and Dimensionality Reduction, Model splitting and Training, Hyperparameter tuning, model re-training and finally model evaluation.

Is it worthy of sending it as a paper to conferences even though it is nothing novel technique-wise but is on a different dataset?

Any guidance would be appreciated!",553,LanguageTechnology,1
"Automatic question generation from input paragraph I am doing my individual research on automatic question generation from the paragraphs which will be input in to the system. I have divided this idea into three steps,

1. sentence selection
2. complex sentence simplification
3. sentence classification based on POS and NE tagged information

I would be grateful to hear your opinions based on the methodology which i am going to use. Please be kind enough to mention if i need to upgrade the methodology or any other changes that i should consider.",550,LanguageTechnology,1
IBM's Project Codenet will teach AI to code in dozens of programming languages. Extremely vast dataset ,103,LanguageTechnology,1
"Are there tools that tell a word's difficulty? Are there any tools that determine a word's difficulty? GPT3 has this thing where it can simplify things so that a 2nd grader would understand, how would it distinguish a word's level of complexity?",245,LanguageTechnology,1
"New to NLP. Looking for library recommendations. Hello. I recently decided to start exploring the use of NLP for solving language and machine translation problems.

I'm familiar with Python but don't really know my way around what NLP libraries are out there as of 2021. **Does anyone have a list of must-know Python-based NLP libraries (I assume Python is the best tool for NLP) and NLP libraries specific to machine translation?**

Thank you in advance.",455,LanguageTechnology,1
"How to create a model like BERT or GPT? I am a sophomore and have studied ML and DL for last 6 months. Recently I started working on NLP. I was wondering how these models (BERT, GPT) are created. Following are some questions

1. Is it possible to create my own model? I was thinking of making a hybrid of encoders and decoders of transformers. 

2. Is it even feasible to create a model at my stage? 

3. How much time would it take to create it?",446,LanguageTechnology,1
"Techniques to handle over-fitting with text classifier Hi, I'm currently using a dataset, that has 6 different target labels. The number of instance of target 1-6 is 20k, 16k, 3k, 2k, 1.5k, 1.2k.

I'm making use of fastText to classify. The model gets over-fitted and for most of times gives the targets that have 20k (target 1) or 16k (target 2) instances.

The accuracy given by fastText is 90% but that is because of over fitting.
In another try, I provided (almost) equal amount of all instances ( &lt;= 2k ) for each target label and the accuracy had become 78%.

Is there any better way of handling this problem without neglecting thousands of instances?

Any suggestions would be helpful!",695,LanguageTechnology,1
"Is a primitive method using NLTK and counters viable? This question might be inappropriate here, but IDK where else to ask it. I have trouble getting a hugging face model to work, but I do have sufficient data and not enough time. I want to 'predict' a sentence from a previous one, or a sentence fragment. What is the simplest thing that I can do with NLTK and counters that is still useful?",392,LanguageTechnology,1
"10 popular Keyword Extraction Techniques in NLP This blog lists out (All?) popular Unsupervised Keyword Extraction Algorithms in NLP. 

Here, I summarize almost 10 papers w.r.t all these techniques. Enjoy the read! 🎉 


https://link.medium.com/4ah0jdgXhgb",255,LanguageTechnology,1
"CQP vs LexiDB Has anyone here gotten LexiDB to work? I recently found LexiDB as a potenialy corpus querying alternative to CWB's CQP.

These tools allow you to query large corpora (my current largest corpus has 800 million tokens in it) for patterns. For example, you could search for ""the &lt;adjective&gt; *"" and the tool will quickly return a list of matches.

I have CQP working already but LexiDB sounds a little better for my needs. Unfortunately so far I can't get it to work.",483,LanguageTechnology,1
"Ides for making a podcast summarizer? In short: I listen to lots of interesting podcasts, but with my limited human memory, I tend to forget some of the juicier details that I would like to remember.. My solution to this would be to summarize podcast episodes that I find memorable so that a quick glimpse at the highlights would bring the topics back to memory. However, instead of spending time doing manual summations, I would like to practice some ML/NLP applications by making my own podcast summarizer.

My thought is to divide the problem into two main steps:

1. Audio to transcript
2. Transcript summation/extraction of main topics

What ideas and thoughts do you all having regarding which models/libraries to use for the two steps? Or any other inputs on how to approach it?

Super excited to hear your thoughts!",823,LanguageTechnology,1
"NLP job opportunities for Master graduates(no PhD) Hello,

I am a Computer science Master graduate from a top EU university.  During my Masters I focused mainly on NLP and I did 2 internships as an NLP engineer. I am currently looking(not actively looking) for opportunities in NLP in Europe(mainly London, Switzerland, and maybe Germany). 

But I am having trouble finding opportunities that match my profile and that I like. The main issue that I have is that a lot of big companies(not start ups) say that they do NLP but in reality most of the time you end up working on software engineering tasks. The other problem is that all the posts that I like require a PhD.  I also applied in FAANG companies but they only accepted me to interview for a Software engineering roles. 

1. Do any of you work at a FAANG company as a Software engineer and work on NLP projects ? What exactly is your job, do you come up with architecture of the model or do you just build the infrastructure around the model ... ? 
2. For those who work in NLP but don't have a PhD can you give me an example of NLP projects that you do at work and if possible tell me for which company you work. 

Any recommendations of companies that do NLP and hire people without a PhD are welcomed.",1262,LanguageTechnology,1
"Is it just me or is mobileBERT is much slower than DistilBERT on Huggingface When I train mobileBERT on GTX 1070, I get 3.8 it/sec. However, when I train DistilBERT on the same GPU, I get 15 it/sec. Am I missing something? The paper on MobileBERT states that MobileBERT should be faster.",287,LanguageTechnology,1
"Is there a network available for download that has a very long attention span? Something that could summarize a book, for instance. Title, basically sums up my question.",169,LanguageTechnology,1
"Regex to detect age in a sting literal on Python Hi everyone! I'm wrapping my head around regular expressions and I managed to write a simple line of code that detects age. I tried my best, unfortunately that doesn't work every time because there are several ways people can express their age.

1) ""I'm 25""
2) ""I'm 25yo""
3) ""I'm 25 years old""
4) ""I'm 25M""

My regex is very simple re.search(r'\d+\w+', text) and it works fine with 2 and 4 since there's no space between them. Do you have any idea to improve this regex so that it works with all of them?

Thanks for your time!",576,LanguageTechnology,1
EmbedRank: Simple Unsupervised Keyphrase Extraction using Sentence Embeddings ,78,LanguageTechnology,1
"Large summarization dataset in Portuguese I'm looking for a large (more than 100K records) corpus dataset for a summarization task in Portuguese. Something like the CNN/DailyMail dataset ([https://huggingface.co/datasets/cnn\_dailymail](https://huggingface.co/datasets/cnn_dailymail)) but in Portuguese.

Does anyone know of such a dataset?",340,LanguageTechnology,1
Evolution of NLP search methods and the latest method - Neural Search. What is it and how to get started with it ,113,LanguageTechnology,1
"After-the-fact conversational topic analysis? I've developed a conversational architecture for my robots that works fairly well in an unsupervised setting.  The robots recognize the folks they talk to (if they've been introduced) and save the interactions and the full text of the dialog between them by date.  

I am looking for something that can summarize the primary subjects of the conversations after the fact, so that I can sort of ""categorize"" the subjects that interest various folks that the robots talk with.  

I've found things like this:  https://convokit.cornell.edu/documentation/tutorial.html

These seem to offer some useful direction - just wondering if anyone else has any suggestions to summarize a given interaction?  Thanks.",747,LanguageTechnology,1
"I have a list of names of publicly listed companies (around 1000) and want to do textual analysis of information on their websites. Where do I start? I mean is there already a database that I can use. If not, how do I get their website address without looking each one up on google and how to automate the process of collecting information. I use python if you need to know. Any suggestion helps. Thankyou",405,LanguageTechnology,1
SpaCy 3.0 Text Classifier Anyone have experience using Spacy 3.0 Text classifier for multi class classification? I have 6 classes but after running the model get all 0s for the accuracy score. Code is at work but I can provide it tomorrow if anyone has had a similar use case! I have a feeling I’m making a mistake regarding the multiple classes,345,LanguageTechnology,1
"Best approach for preprocessing in 2021 Hi everyone!

What would you recommend for an efficient preprocessing pipeline these days? I was checking spaCy but they don't offer bi-gramization out of the box right?

So to include bi-grams I was thinking on gensim.

But there things get hairy. Gensim needs tokens per sentence per doc to learn the phrase model, then I need to tokenize/segment first.

I don't like this since I wanted to use spaCy for the tokenization and lemmatization as a final and single step.

Also, I guess spaCy's lemmatization is much better than NLTK's

All seems like a mess and perhaps can shade some light on this.

Finally, will bi-grams affect lemmatization? Which is the correct order?

Thanks for your insights!!



Edit 1: And how about frequency filtering and min/max length filtering?

Edit 2: Is it better to TF-IDF or just stop words removal?

Edit 3: This preprocessing if for topic modelling",926,LanguageTechnology,1
"Anonymous Walk Embeddings (Graph ML Research Paper Walkthrough) This research talks about using Random Walk inspired Anonymous Walks as graph units to derive feature-based and data-driven Graph Embeddings in an unsupervised fashion. 🔥 

https://youtu.be/VVml3nDiM3E",265,LanguageTechnology,1
"Looking for a Claim Extraction/Sentence Segmentation Framework Hi All, I’m looking for a framework that allows for the extraction of knowledge claims/factual statements from a given text. For example…

Input: “Joe Biden won the election, making him the 46th President of the United States”

Output: “Joe Biden won the election”, “Joe Biden is the 46th President of the United States”

I know this has some overlap with sentence entailment, but that’s not exactly what I’m looking for.

Does anyone know of anything like this? Thanks! :)",536,LanguageTechnology,1
Beyond Accuracy: Behavioral Testing of NLP Models with CheckList (BEST PAPER ACL) ,82,LanguageTechnology,1
Parrot: Paraphrase based utterance augmentation framework | Python #NLP ,72,LanguageTechnology,1
Sentiment Analysis Recommendations on Review data ,50,LanguageTechnology,1
"What are some good Poster Submission venues for NLP I am an undergraduate in my third year, I've been working in NLP research for over a year now. My university however, is not very research inclined and there is very little good guidance on good poster submission/publication venues. While I've researched and found places to submit complete research papers, there's very little information on poster submission venues. Any information would be helpful!",454,LanguageTechnology,1
"Webpage summarization: Given the URL of a company webpage, what does the company do? I'm faced with a task: Given the URL of webpage, that links to the homepage of a company, can you describe to me, in 1 to 10 sentences, what the company does?

Are there any pre-made solutions for this? I already have a massive amounts of supervised training data for the problem.",365,LanguageTechnology,1
"Graph-Based Framework for Structured Prediction Tasks in Sanskrit by Dr. Pawan Goyal. This is a search-based structured prediction framework, which expects a graph as input, where relevant linguistic info is encoded in the nodes, &amp; the edges are then used to indicate the association b/w these nodes. ",305,LanguageTechnology,1
"Linguistics student looking for advice to advance in NLP/Computational Linguistics Hello! I am currently a second-year (third) linguistics major student, and I am looking for an academic advice. I am looking forward intro declaring a minor in computer science or math. The problem is that I do not know what to commit to (and I can only choose one). I am aiming to apply to grad school in data science / natural language processing / computational linguistics. I have already developed some Data Science projects and I have developed pretty good coding skills by myself (Python, C++, Swift), and I am pretty much equally good at math. However, I am not entirely sure which minor would help me more career-wise. Can you guys help me out?",736,LanguageTechnology,1
Cognitive Scientist Terrence Deacon Says Current AI Lacks Symbolic Representations &amp; True Language Comprehension Abilities ,127,LanguageTechnology,1
Flashcard Generator - using a multi-transformer pipeline to self-correct ,73,LanguageTechnology,1
"Importance of end symbol in N gram I'm reading a book 'Speech and Language Processing'. Here it is mentioned that without an end-symbol, the sentence probabilities for all sentences of a given length would sum to one.
I searched google to understand this statement but couldn't get satisfactory answers. Could anyone help me understand this.",341,LanguageTechnology,1
Leveraging BERT for Extractive Text Summarization on Lectures ,62,LanguageTechnology,1
"Descriptions of Named Humans Does anyone know where to find a dataset of adjectives used to describe humans in a text? I know named entity recognition exists, but I don't know how I would extend this to only extract human entities and then select adjectives which describe them.",278,LanguageTechnology,1
Topic modelling for single document? The common techniques to do topic modelling (eg. SVD) depend on multiple documents. What's a good method for extracting topics for a single document?,186,LanguageTechnology,1
FactSumm: Factual Consistency Scorer for Abstractive Summarization ,67,LanguageTechnology,1
"Machine learning workflows to summarize, translate, transcribe and more &amp;#x200B;

https://reddit.com/link/nacy0z/video/enf2j74sbly61/player

What if we want to extract and summarize text from documents, also handle translation, combine the  outputs and load it into an Embeddings index. Enter workflows! The demo above takes a list of GitHub project pages, extracts text from HTML, summarizes the text and builds a similarity search index. This same concept could be applied towards a list of company pages, wikipedia  pages and more. This is just one example of what txtai workflows can do.

txtai workflows are a simple yet powerful construct that takes a callable and returns elements. Workflows are streaming by nature and work on data in batches, allowing large volumes of data to be processed efficiently. The amount of functionality provided by machine learning models continues to grow rapidly. txtai provides an easy way to interface with these models. The following is a non-comprehensive list.

\- Questions - Extractive question-answering using a text context  
\- Labels - Apply labels to text using a zero-shot classification model  
\- Summary - Abstractive text summarization  
\- Text Extraction - Extract text from documents  
\- Transcription - Transcribe audio to text  
\- Translation - Machine translation

Workflows allows joining these models together to create powerful data transformations. Workflows can also be constructed in JavaScript, Go, Rust and Java via the API.

See the following links for more information.

[GitHub](https://github.com/neuml/txtai) | [Workflow builder](https://github.com/neuml/txtai/blob/master/examples/workflows.py) | [Documentation](https://neuml.github.io/txtai) | [Article](https://towardsdatascience.com/run-machine-learning-workflows-to-transform-data-and-build-ai-powered-text-indices-with-txtai-43d769b566a7)",1876,LanguageTechnology,1
Are AAAI and IAAI tier I conferences? Are [IAAI](https://aaai.org/Conferences/IAAI/iaai.php) INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE and AAAI Conference on Artificial Intelligence considered as tier I conferences in AI? Both of these seem to be quite popular but I am not sure if these could be called tier I. How can I find out if a particular conference is a tier I conference?,401,LanguageTechnology,1
BERT-QE: Contextualized Query Expansion for Document Re-ranking ,64,LanguageTechnology,1
"¿Master in Digital text Analysis (UAntwerp-Belgium) vs Master in Computational linguistics (UStuttgart-Germany)? Hello! I'd like to know your opinions about these two programs. I am applying to both, but I'm not sure which one is better. I have a background in Applied linguistics and I would like to move towards Computational linguistics.
I have many argumentd for each program, in fact, so far I have a tie. But what I haven't been able to consider are arguments about the quality and prestige of the programs, so I turn to you. Please, I want to read your opinions!

Thank you in advance!",592,LanguageTechnology,1
NLP Keywords to Sentences Text Generation with {keytotext} ,59,LanguageTechnology,1
"Generating Sentences from Keywords using Transformers “keytotext” introduces the idea of building a model that would translate keywords into sentences using amazingly powerful T5 model. 

For example- 
Input: India, Capital, New Delhi
Output: The capital of India is New Delhi.

Interesting? Then read this walkthrough, 
Blog: https://lnkd.in/dS9_AH7
GitHub: https://github.com/gagan3012/keytotext",397,LanguageTechnology,1
"How to pass in two count vectorizers as separate features into MLP? I have two text columns. 

The objective is to vectorize each column separately. And then pass them into a MLP, so that the model can also understand which column a word is coming from.

But I am confused as to how to actually implement this.

Any help or resources would be appreciated.",355,LanguageTechnology,1
"Anyone know of any papers that look at latent NER? Entities which are not exactly mentioned in the text. So far I could only find this one

https://www.aclweb.org/anthology/K18-1020/",182,LanguageTechnology,1
"Any known applications/developments between decentralised ledger (crypto) systems and MT/CAT? I have been searching around online weekly for any news or interesting developments in the cryptosphere/LSP world.

Keywords like decentralization, verification, remote, payment, trustless, exchange, control ... to me sound like there is much room for overlap, but there seems to be very little out there besides conjectures of NMT going to the ""cloud"" then the cryptosphere, which doesn't mean anything... 

Crtain coins/platforms like to market themselves as being a translation-crypto coin/platform but it is merely replacing QC and payment with crypto-incentives (that are not very motivating for a career translator).

I would to hear any thoughts on this from fellow LSPs and interested parties who are into augmenting their capabilities with what technology has to offer.

I have cross-posted this across some relevant subreddits.
Thanks in advance!",950,LanguageTechnology,1
"NER for Resume parsing Hello,

I need to do resume parsing with French resumes. I will use Prodigy ([prodi.gy](https://prodi.gy)) to annotate my dataset and make my model.

My question is : is it better to annotate with the full resume or annotate line by line  ,

for me, the position of the text in the full document is an information but maybe spacy don't care.

Thanks !",374,LanguageTechnology,1
"Best way of integrating a NLP model in Python with a .NET Core API? I've recently started working as a .NET web developer building services, but as a side project I'm working on a text summarization API.

I would like to serve [this model](https://huggingface.co/facebook/bart-large-cnn/tree/main) through the API. How should I go about it? I haven't found much after searching, is there any tutorials out there that I'm missing? Do I need to create the API in Python instead of C#?

I would really appreciate any help. Thank you very much and sorry about my English.",567,LanguageTechnology,1
Pooled Contextualised Embeddings for NER | Research Papers Summary 017 ,71,LanguageTechnology,1
Is there a python library or API that is able to check the grammar of a sentence? ,82,LanguageTechnology,1
Text Generation using GPT-Neo ,30,LanguageTechnology,1
"How to increase the performance? I am trying to solve a multi-class emotion classification problem using BERT (NLP). It is my first ever BERT model and my model ended up overfitting.

    {""train"": {""eval_examples_count"": 10481, ""metrics"": {""f1_weighted"": 0.8869, ""f1_macro"": 0.8401, ""accuracy"": 0.8884, ""roc_auc"": 0.9686}, ""time_spent"": ""1:27:10""}} 
    {""valid"": {""eval_examples_count"": 3493, ""metrics"": {""f1_weighted"": 0.6112, ""f1_macro"": 0.5257, ""accuracy"": 0.6184, ""roc_auc"": 0.8177}, ""time_spent"": ""0:28:37""}} 
    {""test"": {""eval_examples_count"": 3494, ""metrics"": {""f1_weighted"": 0.6191, ""f1_macro"": 0.5282, ""accuracy"": 0.6259, ""roc_auc"": 0.8271}, ""time_spent"": ""0:28:26”}} 

Classes were pretty unbalanced, that's probably why it is overfitting i thought, and then I used the undersampling technique to come over this issue.

    Train:
    sad          756
    angry        756
    surprised    756
    smile        756
    kind         756
    
    Test:
    sad          235
    surprised    235
    angry        235
    kind         235
    smile        235
    
    Val:
    sad          235
    surprised    235
    angry        235
    kind         235
    smile        235

I know the dataset became too small, but it is just for training my model faster and quickly experiment with different approaches. In fact, using my entire dataset and this testing dataset both have been giving pretty similar results.

After balancing my dataset result looks like this:

    {""train"": {""eval_examples_count"": 3780, ""metrics"": {""f1_weighted"": 0.8719, ""f1_macro"": 0.8719, ""accuracy"": 0.8725, ""roc_auc"": 0.9744}, ""time_spent"": ""0:38:33""}} 
    {""valid"": {""eval_examples_count"": 1215, ""metrics"": {""f1_weighted"": 0.5165, ""f1_macro"": 0.5165, ""accuracy"": 0.5185, ""roc_auc"": 0.7993}, ""time_spent"": ""0:12:16""}} 
    {""test"": {""eval_examples_count"": 1215, ""metrics"": {""f1_weighted"": 0.5264, ""f1_macro"": 0.5264, ""accuracy"": 0.5276, ""roc_auc"": 0.8051}, ""time_spent"": ""0:12:28""}} 

So, dropping instances and balancing the dataset DIT NOT really solve the overfitting problem.

Then I decided to use the Drop Out technique and made it equal to 0.5

            ""keep_prob"": 0.5,
            ""learning_rate"": 1e-05,
            ""learning_rate_drop_patience"": 2,
            ""learning_rate_drop_div"": 2.0,
            ""load_before_drop"": True, 
            ""attention_probs_keep_prob"": 0.5,
            ""hidden_keep_prob"": 0.5,

that really solved the overfitting issue, however, badly affected on the performance of the training/model:

    {""train"": {""eval_examples_count"": 32, ""metrics"": {""f1_weighted"": 0.4099, ""f1_macro"": 0.3828, ""accuracy"": 0.4688, ""roc_auc"": 0.6969}, ""time_spent"": ""3:57:36"", ""epochs_done"": 3, ""batches_seen"": 357, ""train_examples_seen"": 11340, ""learning_rate"": 1e-05, ""loss"": 1.4732106072562081} 
    {""valid"": {""eval_examples_count"": 1215, ""metrics"": {""f1_weighted"": 0.3798, ""f1_macro"": 0.3798, ""accuracy"": 0.4272, ""roc_auc"": 0.7127}, ""time_spent"": ""4:06:33"", ""epochs_done"": 3, ""batches_seen"": 357, ""train_examples_seen"": 11340, ""impatience"": 0, ""patience_limit"": 2}} 

Now, I think I solved the overfitting problem, however, How can I improve the overall performance/accuracy? Maybe increasing the learning rate? I can really try all possible approaches but it is really taking too much time of mine, each training is roughly about 4-5 hours (On the small dataset) on the big one it goes to 15-17 hours. It is really hit or miss right now. I am really new in this field but i need to solve this problem. Any idea how to increase the performance would be appreciated. Thank you",3595,LanguageTechnology,1
"How to deal with the processed sentences whose length are equal to 0 and not greater than the minimal length of sentences of model After doing data cleaning and preprocesses for sentences like converting words to their lemma or lowercase, some sentences are equal to 0 and not greater than the minimal length of sentences of the model.

So I think the best way to deal with these sentences is to delete those sentences whose length is 0. 

But I am wondering if there is a more suitable way in this situation?

Thanks in advance.",529,LanguageTechnology,1
"[Q] Why is interpretability important in natural language processing? This is easier to answer for models that make high stakes decisions (e.g., surgical risk assessment; self-driving car slamming brakes; etc.,), but I would like to understand why we care about interpretability in NLP. ",287,LanguageTechnology,1
How come we haven't seen the Albert architecture trained by the Electra pretraining method? It seems like a low hanging fruit that the architecture that usually have the top results be trained by the pre-training regimen that usually have the top results.,255,LanguageTechnology,1
"MLE/NLP Engineer Positions at Big Tech Hi,

I'm currently a rising senior majoring in CS at a top 10 university in the US. I'm debating between graduating just with bachelor's or pursuing a master's in NLP/Human Language Technology (would only take me extra two semesters). I'm mainly interested in NLP, Text Mining and recommendation systems (haven't taken speech processing yet). My school is huge (top 3-5) in the NLP realm, and this is important not because of the rankings but because of the support and opportunities I'll have access to during my masters. If I graduate just with a bachelor's, then I'm considering an SWE role at big techs. If I end up doing a master's, then an applied ML position (MLE, NLP engineer, etc) at big techs.

I have taken some courses related to NLP/DL; I enjoyed them, but at the moment I'm not sure if I liked it enough to do a master's in it and potentially commit my career path to it. Job prospects and competitiveness of getting such positions at big techs would factor a lot in my decision.

I'm wondering how competitive it is to get an MLE/NLP engineer positions at big tech firms like FAANg, Linkedin, etc. What would the expectations/requirements be for MLE/NLP positions (ML- and NLP-related knowledge, research/internship experience, personal projects, publication, Leetcode, etc)? Also, what would an engineer at such positions work on on a regular day? In your opinion, what are the pros and cons of each role (MLE/NLP engineer vs. general SWE)? What would be the kinds of advantages an MLE/NLP engineer would have over general SWEs?

Thank you!",1596,LanguageTechnology,1
"[Tutorial] Implementing different question-answering models with Hugging Face This tutorial covers how to implement 5 different question-answering models with Hugging Face, along with the theory behind each model and the different datasets used to pre-train them. We'll also look at the varying baselines for each of the models in terms of F1 and EM scores.  

Topics covered include:

* The Transformer Architecture
* Popular Datasets and Evaluation Metrics
* BERT (Bidirectional Encoder Representations from Transformers)
* ALBERT: A Lite BERT
* ELECTRA
* BART
* Issues with Long Document Question-Answering Using Standard Models
* LONGFORMER: the Long-Document Transformer

Tutorial link: [https://blog.paperspace.com/question-answering-models-a-comparison/](https://blog.paperspace.com/question-answering-models-a-comparison/)

Run the full code on a free GPU: [https://ml-showcase.paperspace.com/projects/question-answering-models](https://ml-showcase.paperspace.com/projects/question-answering-models)

Questions and comments encouraged!",1043,LanguageTechnology,1
Computer-Aided Design as Language ,34,LanguageTechnology,1
"NLPCloud.io for transformer-based models in production (PyTorch and TensorFlow) Some of you might already know the [NLPCloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=23caa648-af1d-11eb-8529-0242ac130003) project I recently launched. The idea was to help developers and data scientists deploy spaCy models to production in a minute. It came from the fact that, as a developer, I spent much more time than I wanted on this DevOps part in my NLP projects. I was also seeing quite a lot of ML projects failing because teams didn't have the skills to deploy their new models to production... 

I had a lot of user requests asking me to support Hugging Face transformer-based models too, in addition to spaCy models. So I'm happy to announce that, after weeks of hard work, **it is now possible to deploy your own transformer-based models** to [https://nlpcloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=23caa648-af1d-11eb-8529-0242ac130003) , whether they are running on PyTorch or TensorFlow!

It can be useful in 2 situations:

* You developed your own models from scratch but you are having a hard time using them in production (because it takes an API, because resource consumption is very high, because you need high-availability, because server costs are too high, because you don't have advanced DevOps skills, etc.)
* You already use one of the Hugging Face pre-trained models on [NLPCloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=23caa648-af1d-11eb-8529-0242ac130003). It's not working so bad but you want to fine-tune them in order to adapt them to your own needs.

You can choose to either have your models run on CPU or GPU, depending on your requirements, and you can upload as many models as you want. Each new model has its own API endpoint, so you can use some of them in production while others are for testing only. It also makes it easy to urgently rollback to a previous model if needed.

Internally, everything is based on FastAPI and tons of Docker containers (if you are curious about our infra please don't hesitate to ask, I will be glad to comment).

For more details here is the API documentation: [https://docs.nlpcloud.io/#upload-your-transformers-based-model](https://docs.nlpcloud.io/#upload-your-transformers-based-model)

I really hope you will like it and find it useful!

I would love to have your opinion on this new feature. Please don't hesitate to answer this post!",2455,LanguageTechnology,1
"Does a sequential fine-tuning process for BERT make sense? Hello everyone,

Quick question: I am working on a low resource language that even large multilingual models such as [mBERT](https://huggingface.co/bert-base-multilingual-cased) fail to represent properly. So, can I fine-tune these models on MLM just like they were originally trained and then fine-tune it again on a specific task? In other words:

1. Fine-tune mBERT on the masked language modeling task (using a domain-specific corpus)
2. Fine-tune the resulting model on a different task (say semantic analysis)
3. Test the model

Does this make sense? Is this equivalent to training a BERT model from scratch using the same multilingual corpus in mBERT, with my corpus added to it, or is it different? If so, how's it different?

Thank you for your time. I really appreciate any knowledge on the matter.",867,LanguageTechnology,1
"Comparing individual predictions of two models Is it okay to use Matthews Correlation Coefficient (phi coefficient) to compare the predictions of two different models?

&amp;#x200B;

Is this code correct:?

&amp;#x200B;

from sklearn.metrics import matthews\_corrcoef  

&amp;#x200B;

model4 = MultinomialNB() 

model4.fit(X\_train, y\_train) 

y\_pred4 = model4.predict(X\_test)  

&amp;#x200B;

model5 = BernoulliNB() 

model5.fit(X\_train, y\_train) 

y\_pred5 = model5.predict(X\_test)  

&amp;#x200B;

matthews\_corrcoef(y\_pred4, y\_pred5)",545,LanguageTechnology,1
"Are there any open-source aspect-oriented sentiment analysis APIs out there? Hey guys, I need some help. I need to find an open-source API for doing aspect-oriented sentiment analysis within this week. My brother is doing his first software project right now and I'd like to help him with his research. I was wondering if there are any sentiment analysis APIs or tools that can scan for the sentiment value of a specific topic word. Or maybe a sentiment analysis tool or trick that can be combined with a TF-IDF algorithm in order to do the same job.

Thanks in advance.",570,LanguageTechnology,1
"Recursive neural networks Normally, a recursive neural network is trained by back-propagating through every level of a tree (from the top to the bottom).

Did anybody encounter an example where a recursive neural network is trained by back-propagating on each level separately, calculating loss and updating weights on each level of the tree?

I am trying to do that on a ""pyramid"" binary tree, where the whole pyramid has the same consistent structure. I am looking for other similar examples so I can see how to improve my implementation.",540,LanguageTechnology,1
"SpaCy NER Scorer/Example Question Created a custom NER model, which takes a blob of text as input. 

I’m trying to get the f1-score, recall, etc by using the Scorer.score method. This method requires a list of Example objects. 

Does anyone have or know of any code examples of how to accomplish this task? I looked for hours on the internet today without any luck, could definitely just be a silly mistake on my part as I’m still relatively new to the field !",460,LanguageTechnology,1
"Help on Relation Extraction Hello everyone, I need some help, or can I say, a second opinion on what I'm doing.

In the last few months I've been using spacy and nltk. Currently I'm working on a project in which I have to link multiple entities across multiple sentences. The main goal is to develop a knowledge graph. Now I'm using  mainly the dependency parser and other vanilla nlp techniques to filter out the relations between entities.

Now I'm seeking a second opinion,  after searching a few papers I'm finding a lot of work using machine learning, but I'm a total noob in that area. I'm feeling that I'm using the wrong approach.

Can someone help me out in the sense of, what I'm doing is actually not that bad or should I follow other approach, but again, I'm a total noob in the ML world.",800,LanguageTechnology,1
German Question Answering and Dense Passage Retrieval Datasets [https://deepset.ai/germanquad](https://deepset.ai/germanquad),125,LanguageTechnology,1
"KNN performs better on Word2Vec pretrained embedding than on TF-IDF vector representation Hi All.

I am doing a project on multi-class text classification and could do with some advice.

I have a dataset of reviews which are classified into 7 product categories.

Firstly, I create a term document matrix using TF-IDF (tfidfvectorizer from sklearn). This generates a matrix of n x m where n in the number of reviews in my dataset and m is the number of features.

Then after splitting term document matrix into 80:20 train:test, I pass it through the K-Nearest Neighbours (KNN) algorithm and achieve an accuracy of 53%.

&amp;#x200B;

In another experiment, I used the Google News Word2Vec pretrained embedding (300 dimensional) and averaged all the word vectors for each review. So, each review consists of x words and each of the words has a 300 dimensional vector. Each of the vectors are averaged to produce one 300 dimensional vector per review.

&amp;#x200B;

Then I pass this matrix through KNN. I get an accuracy of 72%.

&amp;#x200B;

As for other classifiers that I tested on the same dataset, all of them performed better on the TF-IDF method of vectorization. However, KNN performed better on word2vec.

Can anyone help me understand why there is a jump in accuracy for KNN in using the word2vec method as compared to when using the tfidf method?",1358,LanguageTechnology,1
"Is there a reason text autoencoders mostly employ RNNs for encoder and decoder, instead of Transformers? Hi. 

A trend I noticed in many papers that employ Autoencoders in text is that they use RNNs for encoding/decoding.

This might have something to do with data insufficiencies in fields that call for text autoencoders. (Since transformers are known to require a lot of training data). I am not sure.

I’d appreciate any insight into this!

Edit:

Some examples:
- [A Probabilistic Formulation of Unsupervised Text Style Transfer](https://arxiv.org/pdf/2002.03912.pdf)
- [Plug and Play Autoencoders for Conditional Text Generation](https://www.aclweb.org/anthology/2020.emnlp-main.491.pdf)
- [DialogWAE](https://arxiv.org/pdf/1805.12352.pdf)",745,LanguageTechnology,1
"I need help to find a NLP topic for my thesis Hi everyone,

I am a second year Master's student and want to write my thesis about an application of NLP model, however, I am struggling to find an original idea. It would be great if you can give me some ideas/advices, because I am really nervous at this point.",309,LanguageTechnology,1
"Something like style transfer for language processing? This may be a silly question that is an existing topic already heavily explored by others more involved in NLP than me, but are there tools for taking a paragraph of text and transforming it into a machine-generated paragraph written in another person's style, but having similar content?  Essentially, I am looking for the NLP equivalent of what has already been done to death with GANs on image data (e.g. StyleGAN2).  Thanks in advance for any help!",507,LanguageTechnology,1
"Emotion and Empathy in NLP Hey everyone!

I'm a master's student currently doing my research in the field of NLP and I've recently become heavily invested in the idea of implementing concepts such as emotion and empathy in dialogue systems. I was wondering if there was anybody else who was also interested in this topic and wanted to share our resources together? I've made a simple [list of papers](https://github.com/Sahandfer/Empathetic-COAI-Papers) I've read so far (not complete yet). Feel free to let me know if you're interested or if you have any opinions or suggestions. Thanks and wish you have a great day.",618,LanguageTechnology,1
"200+ State of the Art Medical Models for NER, Entity Resolution, Relation Extraction, Assertion, Spark 3 and Python 3.8 support - John Snow Labs NLU 3.0.0 
## 200+ State of the Art Medical Models for NER, Entity Resolution, Relation Extraction, Assertion, Spark 3 and Python 3.8 support in  NLU 3.0 Release and much more
We are incredibly excited to announce the release of `NLU 3.0.0` which makes most of John Snow Labs medical healthcare model available in just 1 line of code in NLU.
These models are the most accurate in their domains and highly scalable in Spark clusters.  
In addition, `Spark 3.0.X`  and `Spark 3.1.X ` is now supported, together with Python3.8

This is enabled by the amazing [Spark NLP3.0.1](https://nlp.johnsnowlabs.com/docs/en/release_notes#300) and [Spark NLP for Healthcare 3.0.1](https://nlp.johnsnowlabs.com/docs/en/licensed_release_notes#301) releases.

# New Features
- Over 200 new models for the `healthcare` domain
- 6 new classes of models, Assertion, Sentence/Chunk Resolvers, Relation Extractors, Medical NER models, De-Identificator Models
- Spark 3.0.X and 3.1.X support
- Python 3.8 Support
- New Output level `relation`
- 1 Line to install NLU  just run `!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash`
- [Various new EMR and Databricks versions supported](https://github.com/JohnSnowLabs/spark-nlp/releases/tag/3.0.0)
- GPU Mode, more then 600% speedup by enabling GPU mode.
- Authorized mode for licensed features

## New Documentation
- [NLU for Healthcare Examples](https://nlu.johnsnowlabs.com/docs/en/examples_hc#usage-examples-of-nluload)
- [Instrunctions to authorize your environment to use Licensed features](https://nlu.johnsnowlabs.com/docs/en/examples_hc#authorize-access-to-licensed-features-and-install-healthcare-dependencies)


## New Notebooks
- [Medical Named Entity Extraction (NER) notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/medical_named_entity_recognition/overview_medical_entity_recognizers.ipynb)
- [Relation extraction notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/relation_extraction/overview_relation.ipynb)
- [Entity Resolution overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/entity_resolution/entity_resolvers_overview.ipynb)
- [Assertion overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/assertion/assertion_overview.ipynb)
- [De-Identification overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/de_identification/DeIdentification_model_overview.ipynb)
- [Graph NLU tutorial](https://github.com/JohnSnowLabs/nlu/blob/3.0rc1/examples/webinars_conferences_etc/graph_ai_summit/Healthcare_Graph_NLU_COVID_Tigergraph.ipynb)


## AssertionDLModels

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [assert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html) | [assertion_dl](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html)                   |
| English  | [assert.biobert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html) | [assertion_dl_biobert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html)                   |
| English  | [assert.healthcare](https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html) | [assertion_dl_healthcare](https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html)                   |
| English  | [assert.large](https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html) | [assertion_dl_large](https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html)                   |

##  New Word Embeddings

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [embed.glove.clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html) | [embeddings_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html)                   |
| English  | [embed.glove.biovec](https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html) | [embeddings_biovec](https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html)                   |
| English  | [embed.glove.healthcare](https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html) | [embeddings_healthcare](https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html)                   |
| English  | [embed.glove.healthcare_100d](https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html) | [embeddings_healthcare_100d](https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html)                   |
| English  | en.embed.glove.icdoem | embeddings_icdoem          |
| English  | en.embed.glove.icdoem_2ng | embeddings_icdoem_2ng          |

## Sentence Entity resolvers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | embed_sentence.biobert.mli | sbiobert_base_cased_mli          |
| English  | resolve | sbiobertresolve_cpt          |
| English  | resolve.cpt | sbiobertresolve_cpt          |
| English  | resolve.cpt.augmented | sbiobertresolve_cpt_augmented          |
| English  | resolve.cpt.procedures_augmented | sbiobertresolve_cpt_procedures_augmented          |
| English  | resolve.hcc.augmented | sbiobertresolve_hcc_augmented          |
| English  | [resolve.icd10cm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html) | [sbiobertresolve_icd10cm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html)                   |
| English  | [resolve.icd10cm.augmented](https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html) | [sbiobertresolve_icd10cm_augmented](https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html)                   |
| English  | [resolve.icd10cm.augmented_billable](https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html) | [sbiobertresolve_icd10cm_augmented_billable_hcc](https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html)                   |
| English  | [resolve.icd10pcs](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html) | [sbiobertresolve_icd10pcs](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html)                   |
| English  | [resolve.icdo](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html) | [sbiobertresolve_icdo](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html)                   |
| English  | [resolve.rxcui](https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html) | [sbiobertresolve_rxcui](https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html)                   |
| English  | [resolve.rxnorm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html) | [sbiobertresolve_rxnorm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html)                   |
| English  | [resolve.snomed](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html) | [sbiobertresolve_snomed_auxConcepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html)                   |
| English  | [resolve.snomed.aux_concepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html) | [sbiobertresolve_snomed_auxConcepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html)                   |
| English  | [resolve.snomed.aux_concepts_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html) | [sbiobertresolve_snomed_auxConcepts_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html)                   |
| English  | [resolve.snomed.findings](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html) | [sbiobertresolve_snomed_findings](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html)                   |
| English  | [resolve.snomed.findings_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html) | [sbiobertresolve_snomed_findings_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html)                   |

## RelationExtractionModel

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | relation.posology | posology_re          |
| English  | [relation](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html) | [redl_bodypart_direction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html)                   |
| English  | [relation.bodypart.direction](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html) | [redl_bodypart_direction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html)                   |
| English  | [relation.bodypart.problem](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html) | [redl_bodypart_problem_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html)                   |
| English  | [relation.bodypart.procedure](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html) | [redl_bodypart_procedure_test_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html)                   |
| English  | [relation.chemprot](https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html) | [redl_chemprot_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html)                   |
| English  | [relation.clinical](https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html) | [redl_clinical_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html)                   |
| English  | [relation.date](https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls) | [redl_date_clinical_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls)                   |
| English  | [relation.drug_drug_interaction](https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html) | [redl_drug_drug_interaction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html)                   |
| English  | [relation.humen_phenotype_gene](https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html) | [redl_human_phenotype_gene_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html)                   |
| English  | [relation.temporal_events](https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html) | [redl_temporal_events_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html)                   |



## NERDLModels

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
|English  | [med_ner.ade.clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html) | [ner_ade_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html)                   |
| English  | [med_ner.ade.clinical_bert](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html) | [ner_ade_clinicalbert](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html)                   |
| English  | [med_ner.ade.ade_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html) | [ner_ade_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html)                   |
| English  | [med_ner.anatomy](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html) | [ner_anatomy](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html)                   |
| English  | [med_ner.anatomy.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html) | [ner_anatomy_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html)                   |
| English  | [med_ner.anatomy.coarse](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html) | [ner_anatomy_coarse](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html)                   |
| English  | [med_ner.anatomy.coarse_biobert](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html) | [ner_anatomy_coarse_biobert](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html)                   |
| English  | [med_ner.aspect_sentiment](https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html) | [ner_aspect_based_sentiment](https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html)                   |
| English  | [med_ner.bacterial_species](https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html) | [ner_bacterial_species](https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html)                   |
| English  | [med_ner.bionlp](https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html) | [ner_bionlp](https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html)                   |
| English  | [med_ner.bionlp.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html) | [ner_bionlp_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html)                   |
| English  | [med_ner.cancer](https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html) | [ner_cancer_genetics](https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html)                   |
| Englishs | [med_ner.cellular](https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html) | [ner_cellular](https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html)                   |
| English  | [med_ner.cellular.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html) | [ner_cellular_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html)                   |
| English  | [med_ner.chemicals](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html) | [ner_chemicals](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html)                   |
| English  | [med_ner.chemprot](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html) | [ner_chemprot_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html)           |
| English  | [med_ner.chemprot.clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html) | [ner_chemprot_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html)           |
| English  | [med_ner.clinical](https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html) | [ner_clinical](https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html)           |
| English  | [med_ner.clinical.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html) | [ner_clinical_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html)           |
| English  | med_ner.clinical.noncontrib | ner_clinical_noncontrib          |
| English  | [med_ner.diseases](https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html) | [ner_diseases](https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html)           |
| English  | [med_ner.diseases.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html) | [ner_diseases_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html)           |
| English  | [med_ner.diseases.large](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html) | [ner_diseases_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html)           |
| English  | [med_ner.drugs](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html) | [ner_drugs](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html)           |
| English  | [med_ner.drugsgreedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html) | [ner_drugs_greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html)           |
| English  | [med_ner.drugs.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html) | [ner_drugs_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html)           |
| English  | [med_ner.events_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html) | [ner_events_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html)           |
| English  | [med_ner.events_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html) | [ner_events_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html)           |
| English  | [med_ner.events_healthcre](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html) | [ner_events_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html)           |
| English  | [med_ner.financial_contract](https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html) | [ner_financial_contract](https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html)           |
| English  | [med_ner.healthcare](https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html) | [ner_healthcare](https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html)           |
| English  | [med_ner.human_phenotype.gene_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html) | [ner_human_phenotype_gene_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html)           |
| English  | [med_ner.human_phenotype.gene_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html) | [ner_human_phenotype_gene_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html)           |
| English  | [med_ner.human_phenotype.go_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html) | [ner_human_phenotype_go_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html)           |
| English  | [med_ner.human_phenotype.go_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html) | [ner_human_phenotype_go_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html)           |
| English  | [med_ner.jsl](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html) | [ner_jsl](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html)           |
| English  | [med_ner.jsl.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html) | [ner_jsl_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html)           |
| English  | [med_ner.jsl.enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html) | [ner_jsl_enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html)           |
| English  | [med_ner.jsl.enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html) | [ner_jsl_enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html)           |
| English  | [med_ner.measurements](https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html) | [ner_measurements_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html)           |
| English  | [med_ner.medmentions](https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html) | [ner_medmentions_coarse](https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html)           |
| English  | [med_ner.posology](https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html) | [ner_posology](https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html)           |
| English  | [med_ner.posology.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html) | [ner_posology_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html)           |
| English  | [med_ner.posology.greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html) | [ner_posology_greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html)           |
| English  | [med_ner.posology.healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html) | [ner_posology_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html)           |
| English  | [med_ner.posology.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html) | [ner_posology_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html)           |
| English  | [med_ner.posology.large_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html) | [ner_posology_large_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html)           |
| English  | [med_ner.posology.small](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html) | [ner_posology_small](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html)           |
| English  | [med_ner.radiology](https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html) | [ner_radiology](https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html)           |
| English  | [med_ner.radiology.wip_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html) | [ner_radiology_wip_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html)           |
| English  | [med_ner.risk_factors](https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html) | [ner_risk_factors](https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html)           |
| English  | [med_ner.risk_factors.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html) | [ner_risk_factors_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html)           |
| English  | med_ner.i2b2 | nerdl_i2b2          |
| English  | [med_ner.tumour](https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html) | [nerdl_tumour_demo](https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html)           |
| English  | med_ner.jsl.wip.clinical | jsl_ner_wip_clinical          |
| English  | [med_ner.jsl.wip.clinical.greedy](https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html) | [jsl_ner_wip_greedy_clinical](https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html)           |
| English  | [med_ner.jsl.wip.clinical.modifier](https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html) | [jsl_ner_wip_modifier_clinical](https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html)           |
| English  | [med_ner.jsl.wip.clinical.rd](https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html) | [jsl_rd_ner_wip_greedy_clinical](https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html)           |


## De-Identification Models

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [med_ner.deid.augmented](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html) | [ner_deid_augmented](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html)           |
| English  | [med_ner.deid.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html) | [ner_deid_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html)           |
| English  | [med_ner.deid.enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html) | [ner_deid_enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html)           |
| English  | [med_ner.deid.enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html) | [ner_deid_enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html)           |
| English  | [med_ner.deid.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html) | [ner_deid_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html)           |
| English  | [med_ner.deid.sd](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html) | [ner_deid_sd](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html)           |
| English  | [med_ner.deid.sd_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html) | [ner_deid_sd_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html)           |
| English  | med_ner.deid | nerdl_deid          |
| English  | med_ner.deid.synthetic | ner_deid_synthetic          |
| English  | [med_ner.deid.dl](https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html) | [ner_deidentify_dl](https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html)           |
| English  | [en.de_identify](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html) | [deidentify_rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html)           |
| English  | de_identify.rules | deid_rules          |
| English  | [de_identify.clinical](https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html) | [deidentify_enriched_clinical](https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html)           |
| English  | [de_identify.large](https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html) | [deidentify_large](https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html)           |
| English  | [de_identify.rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html) | [deidentify_rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html)           |
| English  | de_identify.rb_no_regex | deidentify_rb_no_regex          |



# Chunk resolvers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [resolve_chunk.athena_conditions](https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html) | [chunkresolve_athena_conditions_healthcare](https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html)           |
| English  | [resolve_chunk.cpt_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html) | [chunkresolve_cpt_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html) | [chunkresolve_icd10cm_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.diseases_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html) | [chunkresolve_icd10cm_diseases_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html)           |
| English  | resolve_chunk.icd10cm.hcc_clinical | chunkresolve_icd10cm_hcc_clinical          |
| English  | resolve_chunk.icd10cm.hcc_healthcare | chunkresolve_icd10cm_hcc_healthcare          |
| English  | [resolve_chunk.icd10cm.injuries](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html) | [chunkresolve_icd10cm_injuries_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.musculoskeletal](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html) | [chunkresolve_icd10cm_musculoskeletal_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.neoplasms](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html) | [chunkresolve_icd10cm_neoplasms_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.poison](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html) | [chunkresolve_icd10cm_poison_ext_clinical](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.puerile](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html) | [chunkresolve_icd10cm_puerile_clinical](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html)           |
| English  | resolve_chunk.icd10pcs.clinical | chunkresolve_icd10pcs_clinical          |
| English  | [resolve_chunk.icdo.clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html) | [chunkresolve_icdo_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html)           |
| English  | [resolve_chunk.loinc](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html) | [chunkresolve_loinc_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html)           |
| English  | [resolve_chunk.rxnorm.cd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html) | [chunkresolve_rxnorm_cd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html)           |
| English  | resolve_chunk.rxnorm.in | chunkresolve_rxnorm_in_clinical          |
| English  | resolve_chunk.rxnorm.in_healthcare | chunkresolve_rxnorm_in_healthcare          |
| English  | [resolve_chunk.rxnorm.sbd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html) | [chunkresolve_rxnorm_sbd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html)           |
| English  | [resolve_chunk.rxnorm.scd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html) | [chunkresolve_rxnorm_scd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html)           |
| English  | resolve_chunk.rxnorm.scdc | chunkresolve_rxnorm_scdc_clinical          |
| English  | resolve_chunk.rxnorm.scdc_healthcare | chunkresolve_rxnorm_scdc_healthcare          |
| English  | [resolve_chunk.rxnorm.xsmall.clinical](https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html) | [chunkresolve_rxnorm_xsmall_clinical](https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html)           |
| English  | [resolve_chunk.snomed.findings](https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html) | [chunkresolve_snomed_findings_clinical](https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html)           |


# New Classifiers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | classify.icd10.clinical | classifier_icd10cm_hcc_clinical          |
| English  | classify.icd10.healthcare | classifier_icd10cm_hcc_healthcare          |
| English  | [classify.ade.biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html) | [classifierdl_ade_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html)           |
| English  | [classify.ade.clinical](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html) | [classifierdl_ade_clinicalbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html)           |
| English  | [classify.ade.conversational](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html) | [classifierdl_ade_conversational_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html)           |
| English  | [classify.gender.biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html) | [classifierdl_gender_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html)           |
| English  | [classify.gender.sbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html) | [classifierdl_gender_sbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html)           |
| English  | classify.pico | classifierdl_pico_biobert          |


# German Medical models

| nlu.load() reference                                         | Spark NLP Model reference          |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [embed]    | w2v_cc_300d|
| [embed.w2v]    | w2v_cc_300d|
| [resolve_chunk]    | chunkresolve_ICD10GM|
| [resolve_chunk.icd10gm]    | chunkresolve_ICD10GM|
| resolve_chunk.icd10gm.2021    | chunkresolve_ICD10GM_2021|
| med_ner.legal   | ner_legal|
| med_ner    | ner_healthcare|
| med_ner.healthcare    | ner_healthcare|
| med_ner.healthcare_slim    | ner_healthcare_slim|
| med_ner.traffic    | ner_traffic|

# Spanish Medical models
| nlu.load() reference                                         | Spark NLP Model reference          |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [embed.scielo.150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html) | [embeddings_scielo_150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html)| 
| [embed.scielo.300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html)   | [embeddings_scielo_300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html)| 
| [embed.scielo.50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html)  | [embeddings_scielo_50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html)| 
| [embed.scielowiki.150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html)   | [embeddings_scielowiki_150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html)| 
| [embed.scielowiki.300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html)   | [embeddings_scielowiki_300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html)| 
| [embed.scielowiki.50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html)   | [embeddings_scielowiki_50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html)| 
| [embed.sciwiki.150d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html)   | [embeddings_sciwiki_150d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html)| 
| [embed.sciwiki.300d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html)   | [embeddings_sciwiki_300d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html)| 
| [embed.sciwiki.50d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html)   | [embeddings_sciwiki_50d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html)| 
| [med_ner](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)   |  [ner_diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)| 
| [med_ner.neoplasm](https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html)  | [ner_neoplasms](https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html)| 
| [med_ner.diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)  | [ner_diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)| 

# GPU Mode
You can now enable NLU GPU mode by setting `gpu=true` while loading a model. I.e. `nlu.load('train.sentiment' gpu=True)` . If must resart you kernel, if you already loaded a nlu pipeline withouth GPU mode.

# Output Level Relation
This new output level is used for relation extractors and will give you 1 row per relation extracted.


# Bug fixes
- Fixed a bug that caused loading NLU models in offline mode not to work in some occasions


# 1 line Install NLU
```!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash```

# Install via PIP 
```! pip install nlu pyspark==3.0.1```


## Additional NLU ressources

- [NLU Website](https://nlu.johnsnowlabs.com/)
- [All NLU Tutorial Notebooks](https://nlu.johnsnowlabs.com/docs/en/notebooks)
- [NLU Videos and Blogposts on NLU](https://nlp.johnsnowlabs.com/learn#pythons-nlu-library)
- [NLU on Github](https://github.com/JohnSnowLabs/nlu)
- [Suggestions or Questions? Contact us in Slack!](https://join.slack.com/t/spark-nlp/shared_invite/zt-lutct9gm-kuUazcyFKhuGY3_0AMkxqA)",38411,LanguageTechnology,1
"Call for Participation in a Shared Task about occupations detection in clinical texts Hi, everyone!

I posted this earlier to r/MachineLearning, but I just found out about this subreddit and thought people here might be more interested. Sorry if it's not allowed!

I'm a researcher from the Text Mining Unit at the Barcelona Supercomputing  Center, and I wanted to share with you some information about MEDDOPROF,  a Shared Task that we are currently organizing focused on the detection  and normalization of professions and employment status in clinical  texts in Spanish.

Even if these types of entities might seem really niche, every day we learn more and more about their importance. Just think about how someone's occupation  can have a radical impact in their physical and mental health, habits,  lifestyle choices, ... There is even an entire medical specialty,  occupational medicine, that is centered around this topic. In the context of the current pandemic, many people with specific occupations  have been specially affected (for instance, health professionals and  other essential workers). The detection of these terms will help  researchers to better characterize health risks of specific occupations.

Outside  medicine, we foresee that the systems resulting from MEDDOPROF may be  used in fields such as social care, human resources, legal NLP and even gender studies. Personally, I think one of the main contributions of the  task is the inclusion of employment status in a broad sense. We have  annotated unemployed and retired people, family caregivers, people who  are homeless, people who depend on government subsidies, etc, which strengthens the social side of this project.

Additionally,  each mention in the corpus (which includes 2000 documents from over 20  different medical specialties) has been normalized to either the  European Skills, Competences, Qualifications and Occupations  classification (ESCO) or SNOMED-CT. These are multilingual vocabularies,  which we hope might inspire similar tasks in other languages (to the  best of our knowledge, there haven't been any similar tasks yet).

We  released the training set some weeks ago, and on June 1st we will  release the test set. If you are interested in the task, want to see  some annotated examples, the data or the annotation guidelines, ...  please check out the task's website:[ https://temu.bsc.es/meddoprof/](https://temu.bsc.es/meddoprof/)

Thank you if you have read up to here, I hope to see at least some of you at the task!",2526,LanguageTechnology,1
"I am working on a topic modelling paper and I need your help Hello everyone,

I'm currently working on a topic modelling paper and I in order to evaluate it, I need your help. Topic models are unsupervised clustering methods used to group related words together and extract topics. To evaluate such topic I will in part use the word intrusion task ([source](https://papers.nips.cc/paper/2009/hash/f92586a25bb3145facd64ab20fd554ff-Abstract.html)). The task consist in polluting a topic (group of words) with an intruder. The assumption is that better the topic the easier it is to find the intruder. If you want to help, follow the link to a small 2min survey. No personal information is needed.

Link to the survey : [https://forms.gle/svWu4XmR2PAMeiCD9](https://forms.gle/svWu4XmR2PAMeiCD9)

Thank you to all who will find the time to help :)

&amp;#x200B;

PS: don't hesitate if you have any questions",903,LanguageTechnology,1
"[Request] Curated Advanced NLP Resources Is there a curated list of Advanced Natural Language Processing (NLP) Resources (Model Zoo, GitHub Repositories, Datasets, etc.)  
I think Advanced NLP is progress in the last 3 years since BERT and GPT which are SOTA versatile models and also great for Transfer Learning (Zero-Shot and Few-Shot Learning) like Hugging Face models. Essentially, they've changed the way we approach NLP problems today.

I could not find it on the internet (including on GitHub, Kaggle, Medium, or Reddit.) And, I know about [NLP Progress](http://nlpprogress.com/) and [The Super Duper NLP Repo](https://notebooks.quantumstat.com/).

I need it for a project, and any help is greatly appreciated.",717,LanguageTechnology,1
Tagalog: a text labeling platform for teams ,44,LanguageTechnology,1
EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks ,93,LanguageTechnology,1
"Q: Multi-Document Keyword Extraction Hello there,

im currently doing my research for my bachelor thesis.  
What is a nice technique to extract keywords from one or multiple documents. Currently i've tried TF-IDF, RAKE and YAKE. The hard part somehow is to find the Least Common Multiple of the documents.

Could someone help me out?

&amp;#x200B;

appreciate it",362,LanguageTechnology,1
"Help with aligned word embeddings Hello NLP redditors.

I'm currently working on a project where I need to find the similarity between sentences. At the moment I'm using cosine similarity with [fastText word embeddings](https://fasttext.cc/docs/en/pretrained-vectors.html) and TF-IDF weights. On single languages results are pretty good so far, my boss is happy.

The next step is to solve the same problem with multiple languages. My idea is to use the same technology but with [aligned fastText word embeddings](https://fasttext.cc/docs/en/aligned-vectors.html). In theory, using vocabularies with aligned word vectors should mean that words with similar meanings (in different languages) have a similar vector. Thus, comparing ""the car runs fast"" and ""l'auto corre veloce"" (Italian) should bring a high cosine similarity.

In practice, results are disappointing. Even a comparison between simple particles such as ""yes"" and ""sì"", or even ""no"" and ""no"", bring a cosine similarity well below any acceptable threshold.

Is there anyone here that had some experience with aligned word vectors and can tell me if it's a lost war or there is something I can do to improve the results?

We currently train our own vocabularies on Wikipedia and other sources, and we align the vocabularies using [MUSE](https://github.com/facebookresearch/MUSE) with default settings (0-5000 dictionary for training, 5000-6500 dictionary for evaluation and 5 refinements).

Any help will be appreciated. Thanks in advance.

**Edit: Thanks all for the answers! Will take a look at each suggestion.**",1576,LanguageTechnology,1
"Blog for papers in NLP/CompLing Hi everyone! I have started here a blog for discussing various papers in the field of NLP/CompLing: https://selfassuredpaperreads.medium.com/
Do give it a read and let me know what you think!",223,LanguageTechnology,1
Text classification: words to numbers In most standard applications of text classification - using algorithms like word2vec/glove/bert ... is text data automatically converted into vectors of numbers? Can you then use regression models right away once the text has been converted into numbers?,293,LanguageTechnology,1
"Chatbot with R Hi everyone!  


I need to create a Bot that uses NLP with R!  


has anyone ever done this?  


Thank you so much!",130,LanguageTechnology,1
"How is it possible to extract a single word (may be in textgrid) from the textgrid file of a sentence through a code in praat? 1. How is it possible to extract a single word (may be in textgrid) from the textgrid file of a sentence through a code in praat?

&amp;#x200B;

Suppose, from a long file I have put boundary and created one textgrid containing 4 tokens sentences containing the same key word life. The sentence is ""He loves life and laughter."" The tokens are:

&amp;#x200B;

S1\_life1-life, S1\_life2-life, S1\_life3-life, S1\_life4-life. I need to write a praat code that will separate the ""life"" from the sentence text grid.

&amp;#x200B;

2. Will the same code be applicable for the same function in a different sentence where the positioning of the keyword is different. For instance, ""Life has a different meaning in the mountains."" In here, they keyword is at the beginning of the sentence.",906,LanguageTechnology,1
"Inevitable Manual Work involved in NLP I have feeling that not many people are willing to admit - but ultimately, is a significant part of many data mining projects (e.g. checking data quality, parsing through data, etc.) still done manually? 

For example here is an example I just made up relating to Supervised NLP (Natural Language Processing) Classification : Suppose I have 1000 medical reports of patients, containing unstructured text made by a doctor during a hospital visit. For a given patient, each report contains all the text notes that the doctor made for that patient, for visits between 2010 and 2020. These reports make mention of the patients bio data (e.g. age, gender, medical history, etc.) and the details of the symptoms that the patient is experiencing over a long period of time (e.g. let's say that these reports are 2000 words on average). The problem is, different doctors have different styles of writing - each of these 1000 reports is different from another. If a human were to read the report, the human could figure out what happened to the patient - did the patient have a serious condition (let's call this ""class 1"") or a non-serious condition (let's call this ""class 0""). This is what we are interested in predicting for future patients based on the limited medical notes made by doctors for these future patients. 

The problem is - there is no clear and fast way (not that I know of) to take the 1000 medical reports that are available, and label each report as ""class 1"" or ""class 0"". For example, for ""class 0"" : one of the doctors could clearly write at the end of a report ""all medical tests were conducted and the results and were all negative"", and another doctor could end the report by saying ""the patient should seriously consider changing their lifestyle and eat healthier food. benign."" . 

In this example, how would someone assign labels to all these 1000 cases, without manually reading them and deciding if the information in the report corresponds to a ""serious condition"" or a ""non-serious condition""? I was thinking of using something like ""sentiment analysis"" to capture the ""mood"" of these reports, and use sentiment analysis a method to informally gauge if the tone of the report is ""dark"" (serious condition) or ""light"" (non serious condition). But I am not sure if this is the best way to approach this problem. Is there a way to do this without reading all the reports and manually deciding labels?

In the end - this is what I am interested in doing : suppose a new patient comes in and on the first visit, the doctor makes some quick notes (e.g. patient is male, 30 years old, 180 cm, 100 kg, non-smoker, frequently complains of chest pains, no high blood pressure, works a construction worker and takes daily medicine for acid reflex). Just based on these quick notes and the 1000 reports available (NOTE: I am trying to illustrate a point here, that the medical notes for the new patient and the 1000 reports DO NOT have the same format), can a researcher predict (supervised classification, e.g. decision tree) if this patient will have a ""serious"" or a ""non-serious"" condition?

PS: suppose the doctors have a very detailed medical encyclopedia on their computers - can this medical encyclopedia be used alongside the 1000 medical reports to improve the prediction results?",3343,LanguageTechnology,1
"Overfitting I am trying to solve multi-class classification problem using BERT.

My training accuracy is way higher than the validation and test sets so I assume that is clearly overfitting.

&amp;#x200B;

This is the configuration for my BERT Classifier

    ""class_name"": ""bert_classifier"",
            ""n_classes"": 5,
            ""return_probas"": True,
            ""one_hot_labels"": True,
            ""bert_config_file"": ""/content/ru_conversational_cased_L-12_H-768_A-12/bert_config.json"",
            ""pretrained_bert"": ""/content/ru_conversational_cased_L-12_H-768_A-12/bert_model.ckpt"",
            ""save_path"": ""sst_bert_model/model"",
            ""load_path"": ""sst_bert_model/model"",
            ""keep_prob"": 0.5,
            ""learning_rate"": 1e-05,
            ""weight_decay_rate"": 0.001, #this line added after the overfitting
            ""learning_rate_drop_patience"": 5,
            ""learning_rate_drop_div"": 2.0,
            ""in"": [
              ""bert_features""
                ],
            ""in_y"": [
              ""y_onehot""
            ],
            ""out"": [
              ""y_pred_probas""
            ]

Here is my plan to overcome the issue of overfitting.

1. I added a little `more data` to the current one, (just a bit, at least something that I could find)
2. I added `""weight_decay_rate"": 0.001`, to basically regularize the weight of the model
3. I increased the `batch size to 32`. It was 16 before ( I don't know if that helps)
4. I set a `""learning_rate_drop_patience"": 2,` so it stops training after 2 epochs if there no is an improvement.

&amp;#x200B;

The last thing I wanna do is to increase my dropout

    DropOut = 0.5

&amp;#x200B;

**QUESTIONS:** 

1. However, I can't figure out which parameter is actually ""dropout"" as there is no parameter called directly dropout.  Mainly because the language I am targeting is Russian and I need to use Deep Pavlov [http://docs.deeppavlov.ai/en/master/apiref/models/bert.html](http://docs.deeppavlov.ai/en/master/apiref/models/bert.html) not the hugging face directly so Deep Pavlov Bert Classifier has only these 3 things that are kinda close to what I am looking for (`dropout function`)

&amp;#8203;

    keep_prob             -     dropout keep_prob for non-Bert layers
    attention_probs_keep_prob – keep_prob for Bert self-attention layers
    hidden_keep_prob –           keep_prob for Bert hidden layers

which one is the `Dropout`? which function I need to make 0.5 out of these three?

   2.    I have 3 `epochs` in total, is it correct that I am making `patience rate` (it stops training after 2 epochs if there is no improvement) ==  2, instead of 1? 

   3.    I am thinking to minimize the `learning rate` as well. Now it is currently `1e-05.`  Should I make it maybe 1e-3? 

   4. And overall, what else can you suggest to me to overcome the issue of overfitting? is the plan good enough to tackle this problem?",2902,LanguageTechnology,1
"How to quickly test my model? In my last post [https://www.reddit.com/r/learnmachinelearning/comments/n4ejdp/text\_classification\_problem\_overfitting/](https://www.reddit.com/r/learnmachinelearning/comments/n4ejdp/text_classification_problem_overfitting/)  I already said that I am trying to solve the multiclass text classification problem using BERT.

&amp;#x200B;

After the first training which took 13 hours, I noticed that my model is overfitting. I did some changes to my model, and now I wanna test it out. I can't afford training the whole model everytime because each time it takes huge amount of time, so how can i quickly test it to see how my changes affected my model? Maybe I can test it on a smaller dataset? I heard this is a way of doing so, however, i am not sure if I should use my test set to do so? if yes, should I break my test into 3 pieces (train,test,val ?) again? My test set is not that big (around 3000 rows of data) and I am wondering if such a small data can do the job? 

&amp;#x200B;

I am new in this field, and this is the first me doing this, so it would be highly appreciated if anyone could guide me through it. What are the best practices that allow me to quickly test my model?",1220,LanguageTechnology,1
"Jargon Detection and Normalization Hello,

I am a undergraduate student writing my final thesis right now. I am exploring links between different social media platforms in regards to sentiment. How does Sentiment „flow“ from thematically and temporally near posts. 

One of the platforms that I explore is 4chan and I am having a lot of problems with the special vocabulary there. Most resources I can find solve twitter specific problems where a character limit is involved. The lack of such a limit presents me a plethora of variations of the same token. Elongated words and written out laughter with typos really messes up my work. 

Additionally I can‘t correct typos easily because there are many special words that aren‘t written in any dictionary. Some of these I could identify through urban dictionary, but many others are falsely corrected by a spellchecker. 

With the FastText-Algorithm I could identify a couple hundred tokens and normalize them, but I suspect there is a bunch more. Right now I am reading Dr. Farrell‘s work on jargon detection: http://oro.open.ac.uk/70529/

I wonder if you guys could give me some pointers. Thank you very much!",1160,LanguageTechnology,1
"Any software that can annotate (grapheme/phonogram) in a word with the matching phoneme? I  am trying to find a software that could tell   
\-if the letter ""y"" in a word is a vowel or a consonant.  
\-Or if ""ti"" should be read as ""sh""

I found multiple tool that return a list of phoneme but none that tell me which letter in the original word match each phoneme (an alignment).  
I assume this is doable because this is essentially what speech-to-text tool are doing.  


But I would like a tool that give me a list of matching pair (grapheme/phoneme) so I display the annotation on the the correct range of letter in the original word.",637,LanguageTechnology,1
"Stop Word and Tokenization (with R) I am learning about NLP and trying to understand how to tokenize text and remove stop words.

I tried the following line of code (from ""quanteada) and got the following error:

first_method &lt;- tokens(tdm) %&gt;% tokens_remove(stopwords(""en""), pad = TRUE)

Error: tokens() only works on character, corpus, list, tokens objects.

Has anyone ever gotten this error before?

I posted the full details to my question over here: https://stackoverflow.com/questions/67376045/r-error-only-works-with-character-objects

Thanks",556,LanguageTechnology,1
The giant leaps in language technology -- and who's left behind ,64,LanguageTechnology,1
LINE: Large-scale Information Network Embedding (Machine Learning with Graphs) ,79,LanguageTechnology,1
CrossWeigh: Training NER with Imperfect Annotations | Research Papers Summary 016 ,82,LanguageTechnology,1
"What model to create a sentence generator? Hello,
I'm new to this field.
What model would you recommend for creating single sentences? I'm looking for something a bit more advanced than a markov chain. I would like to have it choose a random word using a word probability distribution, and then give probability distributions for generating the next and previous words, repeating this until the start and end of a sentence.",423,LanguageTechnology,1
"Clinical Natural Language Processing – Challenges, Tasks and Datasets #NLProc ",78,LanguageTechnology,1
"Advice on a clustering algorithm for a corpus of order forms that will sort documents into 'document' types without knowing how many different kinds there are. I have a collection of order forms from different manufacturers. Each manufacturer will generally use the same standardized order form but there is some variance where for whatever reason, a manufacturer might have 5-6 different order forms that they use.

Since each document contains a ton of legal language, I need to create a specific pipeline for each order form type, but I don't know how many different types there are in my corpus to begin with.

Can someone recommend a clustering algorithm to that I can use to figure out all the different document ""types"" that are in the corpus?

Thanks!",759,LanguageTechnology,1
"topic modeling over many subreddits Hello everyone,

As part of my dissertation, I want to examine the general question of ""What do people talk about on stock market-related subreddits?"" To this end, I have identified around 200 subreddits that I would like to examine, and I plan to examine both the posts and comments to these subreddits. I am trying to decide the best and most efficient way of extracting the content of thesis subreddit discussions.

Some features of the data that I think are relevant to my decision:

1. Some subreddits are naturally focused around a certain topic or set of topics (e.g., r/technicalanalysis or r/thcinvesting) while others are more general (e.g., r/StockMarket).
2. The size of the subreddits (i.e., # of posts and comments) varies dramatically (e.g., r/wallstreetbets with millions of members vs r/stockanalysis with a couple of hundred members).
3. The size of my dataset is going to be extremely large (unsure of exact size yet, but just r/wallstreetbets is a large amount of data).

&amp;#x200B;

A couple of options that I am considering include LDA, author-topic LDA (with ""author"" being the subreddit) ([https://radimrehurek.com/gensim/models/atmodel.html](https://radimrehurek.com/gensim/models/atmodel.html)), and BERTopic ([https://github.com/MaartenGr/BERTopic](https://github.com/MaartenGr/BERTopic)).

I am also wondering whether I should manually group subreddits by category (e.g., technical analysis subreddits, fundamental analysis subreddits, penny stock subreddits, general subreddits, etc.) first and then run topic analysis separately for each group?

Any and all thoughts are greatly appreciated, and I am definitely open to hearing about alternative approaches or concerns that I haven't discussed. I am trying to weigh the pros and cons of each approach to make sure that my methodology is not obviously sub-optimal to some alternative.

Thanks in advance!",1921,LanguageTechnology,1
"Stanford's Chirpy Cardinal Hi Everybody, 

Wondering if anyone has experience with Stanford's NLP project Chirpy Cardinal. Looking to find a freelance consultant that has experience with Chirpy architecture. Any suggestions on where I might find some good leads?

Thank you.",274,LanguageTechnology,1
"Concerns about studying NLP I asked you guys about the opinion of choosing which domain to 

choose between NLP and CV few weeks ago.

&amp;#x200B;

I finally decided to study NLP area more deeply.

&amp;#x200B;

But I have concerns. It is really difficult to get accepted in 

NLP Labs and artificial intelligence graduate school.

&amp;#x200B;

So I made two plans if I failed to have master degree in graduate school.

&amp;#x200B;

Plan A

Studying NLP in data science graduate school

This plan have risk; there are no lab in this graduate school 

so I have to study NLP by myself.

&amp;#x200B;

Plan B

Studying CL(Computational Linguistics) in language graduate school

Also this plan have risk; the same reason with Plan A

&amp;#x200B;

There are not that many NLP labs in Korea, so I wonder if I go to

plan A and B, I can work in NLP field.

&amp;#x200B;

The best way is to read articles published by data and language graduate school, 

and NLP lab in AI graduate school and compare it, but the sad thing is

I don't have enough time left to contact labs and graduate schools above.

&amp;#x200B;

I guess that there are some people who graduated those graduate school 

that I mentioned, or at least worked with the people who went to Plan A and Plan B.

&amp;#x200B;

I am looking forward to have any advice from you guys.

I appreciate to the people who gave me advice last time when I asked about

the prospective domain to choose.

&amp;#x200B;

Thank you for reading my post and wish you guys all have good days today !",1540,LanguageTechnology,1
"Master’s Thesis in Computational Phonology Hey NLPeople,

&amp;#x200B;

at the end of the year, I’m planning on writing my Master’s Thesis. My Master’s program is a combination of Computational Linguistics and Cognitive Science. I already know that I want to focus on the broad area of computational phonology.

Because we live in a productivity-driven dystopia apparently I’m already thinking about what would look good on my CV which I will embroider with the topic I will have spent a lot of time working on.

&amp;#x200B;

Do you guys have any general ideas for open research topics in phonologically driven speech recognition for example? Or would you nudge me towards literature in that field that comes to mind?

&amp;#x200B;

I’m really only looking for general inspiration right now.

&amp;#x200B;

Thanks a lot!",821,LanguageTechnology,1
"Extracting term definitions from research papers Say I have a research paper (or papers) and I want to see how they  define a common term. For example, the papers might be on the topic of ""engagement"" and have definitions for the term they use in the paper. I was wondering if it would be possible to extract those definitions given the tools of NLP.

From some light research this seems to be a ""Terminology extraction"" problem. There are some papers on the topic, but my problem seems easier than that. I already know what to look for (e.g., the term ""engagement""), I just want to capture a definition of that from a text.

An idea I had was to search the text for the term I'm looking for (e.g., engagement) and then capture the text around it for further processing; For example, a sentence or two near the ""engagement"" term. But it's the additional processing I'm not sure about.

Any ideas? Thanks.",904,LanguageTechnology,1
"Sentiment analysis of long text How do you deal with large text when using sentiment analysis? Even if SA is often used to analyze small pieces of text like tweets and reviews, it's often complex to encode a sentence because negative and positive phrases modify each other and end up to modify the overall sentiment of the review. This is even more true when dealing with texts made out of multiple sentences. A reasonable way to handle that would be to analyze the sentiment of each sentence and then get the average score for that text. Are there better ways to improve the accuracy of our analysis with large texts?
I found an article of Shocher and colleagues where they classify sentiment phrase-by-phrase  rather than on the sentence level. 

This is the paper
https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf",819,LanguageTechnology,1
"Parallelization for Author-topic models (atmodel) in Gensim While LDA in Gensim supports both multiprocessing and distributed computation, the author-topic model implementation does not at the moment. Much of the infrastructure that allows both multiprocessing and distributed computation should however already be in place (class models.ldamulticore), as the model inherits it from LDA. Therefore, I was asking myself if these functionalities could be enabled also for atmodel without major issues",498,LanguageTechnology,1
"Best evaluation metrics for the BERT model My dataset is a little unbalanced:

    smile        4852 
    kind         2027 
    angry        1926 
    surprised     979 
    sad           698 

pretty same for the validation and test sets. My goal was to predict the emotion of user tweets, which I already did, however now I am wondering what are the best evaluation metrics for this type of problem?

My initial metrics score is as following

    {""train"": {""eval_examples_count"": 10481, ""metrics"": {""f1_weighted"": 0.8869, ""f1_macro"": 0.8401, ""accuracy"": 0.8884, ""roc_auc"": 0.9686}, ""time_spent"": ""1:27:10""}} {""valid"": {""eval_examples_count"": 3493, ""metrics"": {""f1_weighted"": 0.6112, ""f1_macro"": 0.5257, ""accuracy"": 0.6184, ""roc_auc"": 0.8177}, ""time_spent"": ""0:28:37""}} {""test"": {""eval_examples_count"": 3494, ""metrics"": {""f1_weighted"": 0.6191, ""f1_macro"": 0.5282, ""accuracy"": 0.6259, ""roc_auc"": 0.8271}, ""time_spent"": ""0:28:26""}} 

I am pretty new to ML and NLP in general so I am kinda confused. I have several questions here:

1. Why train f1\_macro is so high but valid and test sets are not that high? How to interpret it?
2. How to interpret the above result in general?
3. I've seen someone using Matthews Correlation Coefficient for the kinda similar task, but I heard that is only for the binary class problem, How can i use it for the multiclass problem?
4. How valuable roc\_auc information in a multi-class classification problem?

In general, even though metrics above are kinda giving me not really result, but i ve been testing the model manually and it just work pretty good.",1593,LanguageTechnology,1
What are some easy research topics for beginners related to nlp? I am supposed to be doing a research paper(my first time) and thought nlp related topics would be good. Can you suggest some easy to understand research papers that I can refer to understand how this research paper thing actually works?,301,LanguageTechnology,1
"Training a model on an entire dataset by dividing the dataset into chunks &amp; loading the model back again untill all chunks of the dataset are trained I am working on the **CUAD**(Contract Understanding Atticus Dataset) which is a Q&amp;A based dataset. But training 80% of the dataset in one go is impossible due to resource constraints. I am using the boilerplate code provided by HuggingFace Transformer docs for Q&amp;A task [here](https://huggingface.co/transformers/examples.html). My hands are tied with Google Colab Pro. So, it's not possible for me to use multiple GPU's in training the dataset. Inspite of using the hyperparameters below, I'm unable to avoid errors due to memory constraints like ""CUDA out of Memory"" etc.

```
args = TrainingArguments(
    'cuad-roberta',
    evaluation_strategy = ""epoch"",
    learning_rate=3e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=2,
    weight_decay=0.01,
    save_steps=5000,
    logging_steps=5000,
    save_total_limit=100,
    gradient_accumulation_steps = 12,
    eval_accumulation_steps = 4,
)
```
Under these circumstances, I have divided my training set(80%) into 4 parts with each part holding 25% data. So, using any Q&amp;A supported pretrained model from Transformers, I've trained the first 25% of the training data and then saved the model in a directory of my drive. Then, I have loaded that tokenizer and model from the saved directory and trained the next 25% of my training data on the same model as shown below. 

```
tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/models/cuad-25%-roberta-base')
model = AutoModelForQuestionAnswering.from_pretrained('/content/drive/MyDrive/models/cuad-25%-roberta-base')
```

I repeated the step two more times to complete training the model on the entire training data.

Now, my question is that, **Is this approach correct in terms of training a model when I have resource constraints? If it is correct, will this approach hurt the performance of my model?** I'm relatively new to ML and NLP so please kindly consider any silly mistakes.

Also, any sources for understanding, visualising or implementing the Q&amp;A task through HuggingFace Transformers would be really helpful.",2258,LanguageTechnology,1
"A model to evaluate audio clips similarly Dear all, I'm relatively new to NLP and ML. Currently I'm working on projects in which I have to compare and score two audio clips. Original clip (single sentence) will be from movie character (animated or real) and in second clip human will try to mimic it. I have to come up with my model to determine similarly and score human clip out of 5.
Factors which I have considered to be used are:
1. Getting sentence from speech and comparing with original
2. Evaluating similarly from audio spectrogram (spectral centroid and zero crossing rate)
3. Identification of emotion from speech and using emotions embedding (assuming it's available on internet) to measure similarly (will probably use cosine similarly)
I couldn't come up with more factors. Can you please help me come up with new comparison factors or suggest how can I approach this problem in better way?
Thanking you in anticipation...",937,LanguageTechnology,1
"Looking for Datasets on Meronyms and Hypernyms - Full sentences Hi everyone, 

I am in the hunt for open-domain datasets on Meronyms and Hypernyms. There are a few available online, but so far I've found only sets of entity pairs and **my research requires full sentences.** 

Any suggestion on where to search for datasets like this will be very much appreciated. Also if you happen to have one, please Dm me!

Thanks",418,LanguageTechnology,1
"Call for Teachable NLP Challenge Hi NLP lovers,

I found this exciting Teachable NLP Challenge! Is there anyone who wants to participate with me?

Teachable NLP Challenge is free and open to everyone interested in training their own AI without coding! All you need to be prepared for is good ideas and datasets.

* When: 05/05/2021 - 05/18/2021 11:59 EDT
* How: You just need to submit your AI model link and explanations on your AI (Good example: [https://forum.ainetwork.ai/c/ai-showcase/11](https://forum.ainetwork.ai/c/ai-showcase/11))
* Prizes: Apple Store gift cards, Winners' interviews will be broadcasted through AI Network Youtube Channel(1.48K subscribers)

To participate, submit your info via [https://forms.gle/XfUuNSS2heAn7JtH7](https://forms.gle/XfUuNSS2heAn7JtH7). You will receive an invitation email!

Check how Teachable NLP works: [https://forum.ainetwork.ai/t/teachable-nlp-how-to-use-teachable-nlp/65](https://forum.ainetwork.ai/t/teachable-nlp-how-to-use-teachable-nlp/65)Or watch a 1-minute tutorial video: [https://youtu.be/hzujZOT1qz8](https://youtu.be/hzujZOT1qz8)",1092,LanguageTechnology,1
"Classification dimension mismatch I’m building a classifier to predict whether or not a mechanical part is or is not automotive based on its description using a training dataset of approx. 13k labeled data points. After preprocessing, vectorizing, and TF-IDF the array is (12,918, 16,230). 

After training, I tried predicting on a new dataset of 173 descriptions. After preprocessing, vectorizing, and TF-IDF the new data array is (173, 492) and I’m getting a “dimension mismatch” error when passing it through my models’ predict function. 

Can anyone help with how to get the new unlabeled test data shaped to fit the training data?",635,LanguageTechnology,1
3 Things I Learned About SPACs Using Knowledge Graphs ,54,LanguageTechnology,1
"[D] metrics for measuring similarity of word vectors between different models Hi!

I have 2 separately trained word2vec models. I want to be able to compare the same word in both models and see how they relate to each other and their surroundings through a metric. Now of course, I can't use the cosine similarity as the word vectors are completely different. I also want to avoid using the `wv.most_similar()` method as I would like to effectively capture the similarity of the same word in both models through some kind of metric if it were possible (as if i were using the cosine similarity between 2 words in the same model), but i am not sure of any that may exist! 

Does anybody know of any such metrics or ways of comparing two word vectors like this?

thank you!",771,LanguageTechnology,1
"Chai - Build your own chat AI in 10 minutes Wanted to introduce [chai](https://chai.ml) to y'all - it's a free, open-source platform for developers to build and deploy their own chat AIs. Currently over 100 AIs are hosted on the platform, which has over 12,000 conversations 😱

Following the tutorials at [https://chai.ml/docs](https://chai.ml/docs) you can deploy your own in under 10 minutes. I think this is such a cool way to learn NLP and it's really fun seeing your score go up on the [developer platform](https://chai.ml/dev).",533,LanguageTechnology,1
"Does anyone have any suggestions for determining sentiment associated with key words in online reviews? Pls help Hi guys, 

I’m working on a student assignment that essentially involves determining whether the sentiment is good/bad for certain keywords.

For example if a review is: 

‘I love the facilities and staff here, however the bathrooms were dirty.’

I need to be able to determine that the facilities and staff were positive and the cleanliness was negative. 

Does anyone have any suggestions for how to go about this? 

Relatively new to NLP!!!",556,LanguageTechnology,1
"The NLP Index: 3,000+ code repos for hackers and researchers. [self-promotion] Want to introduce “The NLP Index”, a new asset in NLP code discovery. It's free and open to the public.

It houses over 3,000 code repositories that one can search including a side bar with some of the most important topics in NLP today. The engine is search as you type and typo tolerant (it’s crazy fast). The index includes the arxiv research paper PDF, ConnectedPapers link, and its GitHub repo.

[https://index.quantumstat.com/](https://index.quantumstat.com/)",544,LanguageTechnology,1
Facebook's Internal Report About Its Role In The Capitol Insurrection -- Interested in Thoughts about the analysts' approach to text analytics as an adjunct to their social network and time analytics ,200,LanguageTechnology,1
Reading material for Topic Modelling As the title suggests I'm looking for some reading material (or videos) on Topic Modelling (esp. Hierarchical Topic Modelling). My aim is to understand the concept.,201,LanguageTechnology,1
"I added translation models to the NLPCloud.io API, based on Helsinki NLP's Opus MT Hello!

Many users were asking me to add transformer-based translation models to the [NLPCloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=f80a8332-aaaf-11eb-bcbc-0242ac130002)'s API. So I added the 12 following NLP models based on Hugging Face transformers and Helsinki NLP's Opus MT:

* English to French
* French to English
* English to Spanish
* Spanish to English
* English to German
* German to English
* English to Dutch
* Dutch to English
* English to Chinese
* Chinese to English
* English to Russian
* Russian to English

I personally find the accuracy of these translation models very good, and the latency is pretty good too. Here's the link to the docs: [https://docs.nlpcloud.io/#translation](https://docs.nlpcloud.io/#translation)

**I would LOVE to have your opinion on this guys!**  Do you think that quality is good enough for  production use? Are there other languages you would like to see?

If you want to have a try, the API is free for up to 3 requests per minute, but if it's not enough please don't hesitate to ping me so I can grant you more requests.

Thanks!",1185,LanguageTechnology,1
"I often help companies do branding, and I'm wanting to find words that are simple to hear, are there words that are unique and easy to hear? I imagine in voice recognition training data there are simple nouns which you can hear clearly, and with no similar words?",263,LanguageTechnology,1
"If you can build AI chatbots, what kinds of chatbots do you want to build? Hi everyone, I'm really interested in building chatbots. I'm wondering what kinds of chatbot people need. 

If you can build your own AI chatbots, what kinds of chatbots do you want to build? (e.g., BTS chatbot, the chatbot that brings your loved one back from the dead, and business chatbot, etc.) Why you need those chatbots?",402,LanguageTechnology,1
Gan-Bert for Topic modelling Need suggestions to fine tune Gan-Bert model on mechanical design dataset. Any leads would be appreciated. I can share the detailed problem if you can suggest some insights.,202,LanguageTechnology,1
"Speech Recognition Training Data Tools? I have an upcoming project that has a speech recognition component, but I'm pretty unfamiliar with the basics of this branch of NLP.

I suspect that I will need to create my own training dataset because the speech will involve a lot of niche terminology. From [what little I've seen on the tooling side](https://prodi.gy/docs/audio-video#transcribe), it looks like people segment audio into rough sentence equivalents, and then just type in a transcript to an input field. 

Is this best practice for creating speech recognition models? I would think you would need to provide word-level alignments to the audio, and that the models would be word level sequence models. But the training data tools seem mostly to facilitate capture of whole sentence transcripts.

Any help is appreciated",827,LanguageTechnology,1
Into NLP - Text Normalization ,30,LanguageTechnology,1
"Biomedical datasets suggestions for fine tuning Bert variant models on three tasks As part of an internship, I have to build a biomedical knowledge graph from textual data, to do this I have to go through the tasks of named entity extraction and relation extraction as well as coreference resolution using BERT variant models. My problem is the availability of fine tune data for the three tasks.

Is there any open access datasets that I can use to fine tune my models in the three previous tasks in the biomedical domain?",523,LanguageTechnology,1
"What specific math subjects do I need to fully understand every NLP/Computational Linguistics journal article (espeically the most important ones)? Please state specific math subjects in the format used below (these are simply examples).

* Calculus 1

* Calculus 2

* Calculus 3

* Calculus 4

* Discrete Math

* Introduction to Probability

* Introduction to Statistics

* Statistical Inference

* Linear Algebra

Thank You",425,LanguageTechnology,1
"Incorporating Text Data for a Sales Forecasting Model There's a dataset which has conversational data with customer &amp; sales persons &amp; I have to incorporate that data into a sales forecasting model. 

The conversational data is mainly in sentiment - related topics (For reference)

What NLP technique can be applied here to maybe get a feature out that can act as a dependent variable when I am predicting my final sales output?

PS. I have other variables that can help to predict sales, but I need to use the text data as well. How am  I supposed to move forward with this?",582,LanguageTechnology,1
Any good tutorials for creating a semantic search engine? I'm pretty new to NLP so I hope you can help me out. I have a file with \~700 paragraphs on a similar topic and I want to make a semantic search engine so that the user can input a query and it will return the paragraphs that match the closest. Thanks!,310,LanguageTechnology,1
BERTweet (SOTA) for Named Entity Recognition in Social Media | Research Papers Summary 015 ,91,LanguageTechnology,1
"Handling Multi-lingual text I was wondering whether there are models that can handle text input with two or more languages.

For instance, a multilingualBert can probably understand both English and Spanish:

`(en) This is a test for language model`

`(es)Esta es una prueba para el modelo de lenguaje`

But it fails in understanding the following :

`(en-es) This is a prueba for language modelo`

`(en-jp) This is a テスト for language モデル`

A text that has both English and Spanish or Japanese(where even the script is different than Latin).",541,LanguageTechnology,1
Aspect-based Document Similarity for Research Papers (Research Paper Walkthrough) ,82,LanguageTechnology,1
"Government complaint dataset I'm building a open source government complaint redressal system. For the purpose of complaint classification (for routing the complaints to a department), I need a existing government complaint dataset. If anyone has come across any such datasets, please help by commenting.

It basically needs to have a complaint column and a column with department name to which the complaints belong",416,LanguageTechnology,1
"Feedback for NLP project  Hi all,

My  name is Kushel Ramanayake, and I am a current 4th undergraduate at  Informatics Institute of Technology affiliated with University of  Westminster UK. As part of my Final Year Thesis, I am required to gather some feedback on my project, from experts within the domain. The project I am working on is similar to an automatic question generation system, and main technology is NLP.

Ideally we'll get on a 1 on 1 zoom call (\~15 minutes), where I'd go over my project and you'd give me feedback on;

* The Scope
* The Architecture of the Solution
* The Implementation of the Solution

The data collected will only be used for my thesis and will be discarded with afterwards. No personal information other than maybe name and occupation will be collected. 

Thank you in advance,

Kushel Ramanayake",834,LanguageTechnology,1
"I fine-tuned a language model on left and right leaning political commentary on Reddit [https://preview.redd.it/t60n4t6z08v61.gif?format=mp4&amp;s=b3e2748ba96ec723266ee5c8e14bd49081cb6c57](https://preview.redd.it/t60n4t6z08v61.gif?format=mp4&amp;s=b3e2748ba96ec723266ee5c8e14bd49081cb6c57)

Information:  
For my dissertation project I fine-tuned a pre-trained language model on a self-mined dataset of ""left"" and ""right"" leaning subreddits to classify comments and subreddit's.

I mined the data over a few months using praw, I used a list of around 20-25 different subreddits taking between 10-20,000 comments from each from within the past year, so the model is quite American election biased but the model was fine tuned a few weeks ago so the comments you are seeing the gif it has not seen before.

I used DistilBert to fine-tune the model on pre-processed text, I spent a few months fine-tuning different models on different versions of the data set until I minimised overfitting and got a decent validation to training trade-off.

I also made a fun venn diagram tool to help find similar subreddits, I used this tool with a much larger sample size to help find similar leaning subreddits to help remove my personal bias although I am certain the left-wing subreddits tend to the far left more than the right which is why you may see a fair bit of negative biden commentary leading more left than right.

Disclaimer:  
The venn diagram tool and the subreddit classifier tool utilise praw which has a decent rate limit so may take 10-20 seconds before it returns a result, I have moved to psaw although loading times have not improved much.

View this project: [https://reddit-political-analysis.com/](https://reddit-political-analysis.com/)",1747,LanguageTechnology,1
"Corpus overlap and inflection I'm researching language mixing and English borrowings and I need to compare two txt corpora (one in English, one in Polish) to find words that overlap between them, excluding stop words, and preferably the frequency of overlap as well. I'm using Python. One difficulty is that Polish is an inflectional language, so e.g. ""to ghost"" is ""ghostOWAĆ"" in Polish. How can I account for that in my code so the program to find the instances of these inflected words as well?

Thanks!",506,LanguageTechnology,1
Anyone know of any papers about training with a traditional pretraining task (MLM) simultaneously with a finetuning task; as opposed to first doing pretraining then finetuning ? ,178,LanguageTechnology,1
"How to approach finding Similar wording in paragraph I have data which is basically a group of paragraphs that I know could be related.

Inside a group of paragraphs (let’s say 100 paragraphs). There could be a sentence of two that are almost/exactly the same text across 60% of the paragraphs. Another completely different similar text across 20% of the paragraphs while the other 20% might be unrelated. 

I have billions of these groups of 100 paragraphs and have no idea where to start! Thanks for any pointers",514,LanguageTechnology,1
"Best way to find the (best) solution to any task? What would be a good method to find, for example, the best method to compare two sentences for having the same or a similar meaning? I mean something like a overview or a search engine, based on a problem/solution pattern. Finding the best ways to solve a problem, without asking around a lot or looking into abstracts for papers.",380,LanguageTechnology,1
"Latest trends in topic modelling? Any expert in the domain here?

I had learnt about LDA/ LSA, and it seems that BERT artchitecture is not a fit if there's no pretagged labels on the topics. What are some of the latst trends? Are there any good teams/ companies working on this sector?",285,LanguageTechnology,1
Transformers Pegasus - how do I fine tune another language? How do I fine tune another language? Who will tell you?,115,LanguageTechnology,1
"How to get Training data for NER? Hello,

I'm currently working on a NER that extracts products and capabilities of company based on texts from their website. At the moment I don't have any training data that I could use to train my NLP Model. I need German training data.

I found this farmework: [https://github.com/NorskRegnesentral/skweak](https://github.com/NorskRegnesentral/skweak) and it looks great to automatically label data, but I would still need some kind of structured data in form of gazetters or another ML model to automatically annotate words.

One thing I tried is using Masked Language Model to predict missing words. I found this approach here: [https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a](https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a)  
If one of the predicted words is in my target dictionary then I tag this word with the corresponding entity. This approach works in some cases, but it is not very accurate and I'm not sure how to deal with products that consist of multiple words.

Do any of you have an idea where I could find training data in German with labeled products or how i could generate the training data myself?

Thanks",1213,LanguageTechnology,1
"YOLOv5 equivalent for NLP classification? I'm coming from the world of computer vision and was wondering whether there's the YOLOv5 model equivalent for NLP? I'm looking to train a classification model to categorize short strings (ex: classifying area-related words - Acreage, Land Area, acres, estimated size, lot size, deeded acres, and land area - all as ""area""). I'm thinking of using transfer learning on a custom dataset.

Are there industry-standard models that you'd recommend looking into? Or, put differently, models that offer a good tradeoff between performance and ease-of-use?

Edit: Looking for deep learning approaches to NLP. Perhaps with a [fast.ai](https://fast.ai) flavour",692,LanguageTechnology,1
Text Classification using Convolutional Neural Network with TensorFlow 2.1 in Python | #NLProc ,95,LanguageTechnology,1
"Introducing mlconjug3. A Python library to conjugate verbs in French, English, Spanish, Italian, Portuguese and Romanian (with more in beta version) using Machine Learning techniques. Hi everybody!

&amp;#x200B;

A couple of years ago, I made a [post to announce the release of mlconjug](https://www.reddit.com/r/Python/comments/bb8400/mlconjug_a_python_library_to_conjugate_verbs_in/),  a Python package/library to conjugate verbs (even made-up verbs, or verbs coming from slang and not covered in traditional conjugation tables) in French, English, Spanish, Italian, Portuguese and Romanian using Machine Learning techniques.

Since then, mlconjug has had a lot of success with thousands of students of foreign languages using it as a standalone application to improve their conjugation skills, but it has also been incorporated as a library dependency in more than a dozen different python software projects ranging from traditional NLP tasks using Machine Learning, to twitter bots, Voice Assistants, and even games.

It has also been used in several Academic publications, for example: [""Generative Grading: Near Human-level Accuracy for Automated Feedback on Richly Structured Problems""](https://arxiv.org/abs/1905.09916) where it is used for automatically grading students essays, and  United States citizenship exam questions.

I released a new and improved version of the software, called [mlconjug3](https://github.com/SekouDiaoNlp/mlconjug3) as it is only compatible with Python 3.x, and had many enhancements and bug fixes. The accuracy of the conjugations models has improved a lot and I am in the process of implementing regional European languages (in beta version for now), like Catalan,  Valencian , Basque language, etc... as well as slavic languages (Czech and Polish for now).

Those new languages should be available in the beginning of summer and I am looking for native speakers of the languages that are in beta status to test the software and check that the conjugated forms are correct.

You can install mlconjug3 from [PyPi](https://pypi.python.org/pypi/mlconjug3) or [Anaconda](https://anaconda.org/conda-forge/mlconjug3).

Some of the features of mlconjug3 are the following:

* Easy to use API.
* Includes pre-trained language models with 99% + accuracy in predicting conjugation class of unknown verbs.
* Easily train new models or add new languages.
* Easily integrate MLConjug in your own projects.
* Can be used as a command line tool.

I invite everyone to try it out and if you are a native speaker of Catalan,  Valencian , Basque, Czech or Polish and are willing to beta-test the software, please pm me, you are would be greatly appreciated, and it will make mlconjug3 more versatile and therefore more useful.

Thanks Everyone,

Peace,

SekouDiaoNlp",2787,LanguageTechnology,1
"Which domain in master degree to choose Hello guys, I am living in korea and majored arabic in HUFS(Hankook university of foreign studies).

I got interest on singularity concept since 2013, and I got into this field

from learning machine learning and big data in the academy funded by government.

&amp;#x200B;

Now I am preparing to go artificial intelligence in graduate school, which is master degree

but there are not that many information in korea that which part; computer vision, natural language processing, have more good prospect.

&amp;#x200B;

I was actually thinking about studying nlp, because I majored language in university; english and arabic, and korean speaker, so I applied to the program that only teach nlp for 6 months.

&amp;#x200B;

But I saw one opinion in the forum that nlp have no future after gpt3, and if gpt4 comes out,

there will be no future for nlp.

&amp;#x200B;

So I felt that I have to consider which part will survive for long.

&amp;#x200B;

One of the member in A.I group recommended me that Medical image analysis is hot trend now,

so it is better to study computer vision for the future.

&amp;#x200B;

From now, I have to decide which part I should study more deeply in order to appeal graduate school that I had interest on specific field.

&amp;#x200B;

Briefly, the question is two.

&amp;#x200B;

1.Which part of A.I will survive long? CV or NLP? And which part is more prospective regarding

job opportunity and studying ph.D?

&amp;#x200B;

2.I applied to two kind of government program, first one is teaching general artificial intelligence

including brief lesson of CV and NLP. Second one is the program that only teaches NLP. I am 

wondering which program is better for me entering A.I field.

My second question will be depend on the first question, because if the future of NLP is dark,

I will apply to general A.I program and then study medical image analysis and contact the graduate school in CV lab.

&amp;#x200B;

thank you for reading all of my stories and questions.",2038,LanguageTechnology,1
"Best way to choose topic keyword for text? Hi everyone, been a lurker here for a while. I'd like some advice on an NLP task I'm working on.
 I have a list of paragraphs, and a list of possible key words/phrases to choose from. I would like to assign the most probable keyword from the list for each paragraph, based on semantic meaning. 
Currently I am just computing an embedding for the keyword and the paragraph and comparing the two, but the results aren't great. What would be the best way to do this? Is there some preprocessing that would improve results? Thanks in advance",580,LanguageTechnology,1
"[Call For Participants] MESINESP2 (BioASQ / CLEF2021 shared task) on semantic indexing of heterogenous health content: literature, clinical trials and patents **\*\*\* CFP2  MESINESP2 track: Medical Semantic Indexing (BioASQ – CLEF 2021) \*\*\***

[https://temu.bsc.es/mesinesp2/](https://temu.bsc.es/mesinesp2/) 

**MESINESP2 Awards by BSC-Plan TL \[2,700€\]**

**Test sets and additional data are now available**

There is a pressing need for advanced multilingual semantic search strategies for health related content like literature, patents and clinical trials (cross-genre). The use of semantic search techniques in combination with structured vocabularies is critical for sophisticated searches or content analysis as needed by healthcare professionals, researchers, the pharmaceutical industry, patient groups and private citizens.

Following the impact of past BioASQ tracks for benchmarking studies (e.g. BioBERT) and organization of other initiatives like BioCreative or IberLEF, we propose three semantic labelling subtracks using the widely used DeCS vocabulary (similar to MeSH terms):

**MESINESP-L – Scientific Literature**: for automatic labelling of medical literature abstracts in Spanish (including recent COVID-19 literature).

**MESINESP-T – Clinical trials**: for automatic labelling of clinical trials summaries.

**MESINESP-P – Patents:** for automatic labelling of health-related patents in Spanish to improve patent intelligence.

**Key information**

**Web**:[ https://temu.bsc.es/mesinesp2](https://temu.bsc.es/mesinesp2) 

**Registration**:[ http://clef2021-labs-registration.dei.unipd.it/](http://clef2021-labs-registration.dei.unipd.it/) (BioASQ **Task 3 - MESINESP**)

**Data**: [https://doi.org/10.5281/zenodo.4707104](https://doi.org/10.5281/zenodo.4707104)

MESINESP2 is organized in close collaboration with widely used multilingual medical literature databases (BIREME/WHO, ISCIII/Spain), which expressed a direct need for advanced technologies to accelerate manual indexing efforts for the contents in Spanish (spoken globally by over 572 million people). They do face a challenge to keep up with the increasing number of published medical papers when using purely manual indexing.

A large manually indexed collection of training documents will be provided. These documents have already been automatically annotated (&gt; 1.5 million entity mentions) with  medical entities such as diseases, medical procedures, drugs and symptoms to facilitate the use of complementary strategies like *multi-label classification*, *multilingual transformers*, *graph matching*, *text similarity,* *advanced term matching* or *named entity recognition components*. 

Participating systems will be directly useful for ongoing medical literature indexing efforts, and thus improve competitive intelligence/prior art searches, enable complex search queries needed for evidence-based medicine, clinical decision making, or elaboration of clinical practice guidelines and serve as base for future tasks on semantic indexing of medical records or content in other languages. 

**Important dates**

* April 19: Updated Train, Validation and Test sets release
* April 19: Additional datasets release (Medical entities present in documents)
* April , 30: BioASQ9 Lab u/CLEF 2021 Registration Deadline
* May, 7: Start of the evaluation period
* May, 17: End of the evaluation period
* May,28 :Submission of Participant Papers at CLEF2021
* July, 2: Camera ready paper submission.
* Sep 21-24: CLEF 2021 Conference

**Publications and BioASQ/CLEF2021 workshop**

Teams participating in MESINESP2 will be invited to contribute a systems description paper for the BioASQ (CLEF 2021) Working Notes proceedings, and a short presentation of their approach at the BioASQ 2021 workshop.

**Main Track organizers**

* **Martin Krallinger**, Barcelona Supercomputing Center (BSC), Spain.
* **Luis Gascó**, Barcelona Supercomputing Center (BSC), Spain.
* **Anastasios Nentidis**, National Center for Scientific Research Demokritos, Greece.
* **Elena Primo-Peña,** Biblioteca Nacional de Ciencias de Salud. Instituto de Salud Carlos III, Spain.
* **Cristina Bojo Canales,** Biblioteca Nacional de Ciencias de la Salud. Instituto de Salud Carlos III, Spain.
* **George Paliouras**, National Center for Scientific Research Demokritos, Greece.
* **Anastasia Krithara**, National Center for Scientific Research Demokritos, Greece.
* **Renato Murasaki,** BIREME – Organización Panamericana de la Salud (WHO), Brasil.

**Scientific Committee**

* **Tristan Naumann,** Microsoft Research (USA)
* **Prof. Xavier Tannier,** Sorbonne Université and LIMICS (France)
* **Lucy Lu Wang,** Allen Institute for AI (AI2) (USA)
* **Prof. David Camacho,** Applied Intelligence and Data Analysis Research Group, Universidad Politécnica de Madrid (Spain)
* **Prof. Oscar Corcho,** Ontology Engineering Group, Universidad Politécnica de Madrid (Spain)
* **Parminder Batia,** Amazon Health AI (USA)
* **Prof. Irena Spasic,** School of Computer Science &amp; Informatics, co-Director of the Data Innovation Research Institute, Cardiff University (UK)
* **Jose Luis Redondo García,** Amazon Alexa, Amazon (UK)
* **Carlos Badenes-Olmedo,** Ontology Engineering Group, Universidad Politécnica de Madrid (Spain)
* **Prof. Allan Hanbury,**  E-Commerce Research Unit in the Faculty of Informatics, TU Wien (Austria)
* **Prof. Alfonso Valencia,** Barcelona Supercomputing Center (Spain)
* **Prof. Stefan J. Darmoni,** Department of Biomedical Informatics, Rouen University Hospital (France) and LIMICS (France)
* **Rezarta Islamaj,** National Center for Biotechnology Information (USA)
* **Prof. Rafael Berlanga Llavori,** Universidad Jaume I (Spain)
* **Prof. Henning Müller,** University of Applied Sciences Western Switzerland – Valais (Switzerland)
* **Prof. Gareth J.F. Jones,** School of Computing at Dublin City University (Ireland)
* **Georg Rehm,** Deutsches Forschungszentrum für Künstliche Intelligenz (Germany)
* **Petr Knoth,** Research Studios Austria Forschungsgesellschaft mbH (Austria)
* **Natalia Manola,** CEO at OpenAIRE AMKE (Greece)
* **Prof. Jesús Tramullas,** Departamento de Ciencias de la Documentación e Historia de la Ciencia, Universidad de Zaragoza (Spain)",6257,LanguageTechnology,1
"AI Tutor: A Computer Science Domain Knowledge Graph-Based QA System I didn't build it. But I found it interesting. What could be other alternative approach to solve this problem?

[Research Paper](https://publications.waset.org/10011676/ai-tutor-a-computer-science-domain-knowledge-graph-based-qa-system-on-jade-platform)",321,LanguageTechnology,1
"Any idea what technology google map uses to tag reviews into categories I was reading reviews for a food outlet and saw tags like ""wrap"" ""hummus"" ""chicken"" ""lady"" ""platter"" ""sauce""",180,LanguageTechnology,1
"Recommendation about regression + language I am not sure if this question fit here, but I am working with a regression task on a songs dataset,  one of the variables is the name, I am trying to use it to get a better prediction.

What I did:
I use the python library langdetec to get the language of the name, transform this categorical variable to numerical, and it work good, but I want to improve it.

- Langdetec is not too good, I saw Spanish songs catalogued as italian, but others better libraries takes a lot of time to evaluate all the dataset (this dataset has 130000 rows).
- My transformation from categorical to numerical is basic (number=1, other=2, af=3, ..., en=13, etc).
- I think using a word embedded would be too much for this kind of problem, there is other 17 variables in the data set and the results are good, I just want to take advantage of the name too.

Does anybody recommend me something?
It would be great if anybody recommend me a paper to base on because this task is for an assignment.

Also, I am thinking in a future research about best way to have language categorical variable or language prediction based on frequency.

Thanks in advance for any comments.",1194,LanguageTechnology,1
"Free NLP assignments If anyone is teaching NLP this summer, feel free to use any of these free assignments from OpenClass: [https://openclass.ai/catalog/nlp](https://openclass.ai/catalog/nlp)

These assignments leverage learning principles proven to optimize knowledge retention, and identify &amp; bridge knowledge gaps to personalize the experience for learners. The idea is to provide a low-stakes environment for learners to master fundamental concepts at their own pace.

Our mission with this project is to improve &amp; democratize education. We're looking to grow our community of NLP educators, so if you're interested in contributing, feel free to join on. (Note: you can also keep your assignments private.)",718,LanguageTechnology,1
"Measuring accuracy of my clusters I am trying to do multi classification for sentences.

What are the best ways to make sure my clusters are homogenous?

(I am having a coverage of aprox 80%))

Edit; More context as requested;

I am working on clustering methods for textual data (sentences). Implemented an unsupervised clustering method . When I go through the output, it makes sense. I went through literatures to see what metrics would to tell us ""how good the clusters are"" and got confused. This will help me compare my methods to other methods out there and maybe tweak my method to perform better. I would like to know from the fellow researchers if there are methods which worked best for you which :  
 1. gives a score on homogeneity of clusters  
 2. gives a score on what's the optimal inter-cluster distance.  
 3. gives significance of a cluster  
 4. gives a number on ""optimal number of clusters""",913,LanguageTechnology,1
How to make a multilingual translator like Google Translate ,60,LanguageTechnology,1
"Do we need to balance a dataset when using BERT My dataset looks like following:

    smile        7957
    kind         3399
    angry        3288
    surprised    1702
    sad          1124

My goal is to classify an emotion using BERT.

and I decided to use SMOTE to balance it back

    train['numeric'] = train['emotions'].replace(to_replace=['smile', 'kind','angry','surprised','sad'], value=[0,1,2,3,4])
    
    smote = SMOTE()
    tv = TfidfVectorizer(stop_words=None, max_features=100000)
    tfidf = tv.fit_transform(train['content'].values.astype('U'))
    X_SMOTE, y_SMOTE = smote.fit_resample(tfidf, train['numeric'])
    

Should I even worry about balancing this dataset when using the BERT? or it is handled by BERT already?",741,LanguageTechnology,1
How do you guys find/ keep up to date with the latest NLP papers? ,66,LanguageTechnology,1
"The Conversational AI &amp; NLP Summit is hosting speakers from DeepMind, Facebook, Nestle, McDonald's and more next week. See the full list of talks, networking opps and speakers below! ",187,LanguageTechnology,1
"Using Deep Learning Based On Semantic Features for Emotion Classification in Tweets Hello

I am a student and I am working on emotion analysis with deep learning. My supervisor asked me to extract semantic features from the text ( convert raw data into useful semantic features ) before using deep learning. But I am confused when I read research about text classification with deep learning. DL does not need feature extraction only using different types of word embedding to convert data, I need to clarify this. Is it possible to extract features before applying Deep Learning.? Is there any research that can help me with that? Any suggestions tools or techniques to extract semantic features from the text.",711,LanguageTechnology,1
"Is causal language modeling (CLM) vs masked language modeling (MLM) a common distinction in NLP research? The [huggingface documentation](https://huggingface.co/transformers/v2.5.0/examples.html) states:

&gt;GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss.

Is this a common distinction you'd find in the NLP literature? Is it a sensible distinction in your opinion? While I totally agree with the first category, I don't understand why you would call BERT &amp; co. ""masked language models"", since causal language models do the actual masking in next token prediction.",677,LanguageTechnology,1
"Natural Language Generation Systems - are there any communities or open source projects around here? Hey everyone! Made a post and started delving into this space a few months ago when i decided to take this on for my Final Year Project. Find my post here: [https://www.reddit.com/r/LanguageTechnology/comments/jvo6ec/what\_is\_the\_technology\_behind\_automated\_insights/?utm\_source=share&amp;utm\_medium=web2x&amp;context=3](https://www.reddit.com/r/LanguageTechnology/comments/jvo6ec/what_is_the_technology_behind_automated_insights/?utm_source=share&amp;utm_medium=web2x&amp;context=3)

&amp;#x200B;

I've been building and simultaneously reading papers and enhancing my designs. Wanted to ask if anyone else is or has been working on this type of projects. Would love to know if there is a community in this space or any open source projects happening because from what i see, people working on this would be mainly also working in the companies that are building these systems :/ Would love to hear what challenges have been faced while building these systems. Coding is hard enough, creating a system that generates text that makes grammatical sense and fluent is SUPER difficult.",1189,LanguageTechnology,1
AI-Generated Blog Content with GPT-Neo (GPT-3 Alternative) - Python Web App ,76,LanguageTechnology,1
"How to add the Gradient Accumulation to my model? I am following this tutorial  [dp\_tutorials/Tutorial\_3\_RU\_Fine\_tuning\_BERT\_classifier.ipynb at master · deepmipt/dp\_tutorials · GitHub](https://github.com/deepmipt/dp_tutorials/blob/master/russian_tutorials/Tutorial_3_RU_Fine_tuning_BERT_classifier.ipynb)

However, I am constantly running out of memory so I decided to use Gradient Accumulation to reduce the memory size as it was suggested in my previous post. How can I add it to my model? I can’t find any tutorial that explains it well.

&amp;#x200B;

I already reduced the batch size however it did not helped that much

Thank you.",645,LanguageTechnology,1
"Use NLP to detect and classify emotion in text Hi everyone! I'm currently trying to develop a text classification program that detects emotions and social instances from written text of individuals diagnosed with depression. I thought it'd be a good idea to use lists of words for specific categories in order to build a vocabulary, like for social words I used ""family, friends, girlfriend"" and so on. This classification task is still simple though because it doesn't take into account context and negation (""not happy"" = unhappy). For example, the program can detect a word like ""bad"" but that word could be unrelated, like ""bad driver"". That doesn't give me any hint about the emotional state of that person but ""bad"" still gets detected as part of ""bad driver"". The downside of making a vocabulary using a list is that it can't contain complex words such as ""pissed off"" because they're two words and the text is tokenized at the beginning of the process, so they are passed in as two different words, ""piss"" and ""off"".  I'm trying to implement bigrams to add a little bit of context. I'm open to any suggestion and ready to hear from you.",1144,LanguageTechnology,1
Bitextor: a tool for generating translation memories from multilingual websites ,80,LanguageTechnology,1
"For binary classification, is it better to train with more focused examples? I'm in the process of labeling data that I'll be using for an NLP binary classification project.  Some of my text is longer and some of it is shorter.  

If I have some text (call it two paragraphs), that exhibits traits of both classifications (label A and label not-A), is it better to break the text apart and apply the specific labels to each?  

Or is it better to keep the text together and apply just one of the labels (label A, since it does include some text that is ""A"")?  

If I keep the paragraphs together, I don't want the model to just learn that long text = A.  But if I break it apart, will the model struggle with longer text when it was only trained on single paragraphs?",767,LanguageTechnology,1
"Researchers From NVIDIA, Stanford University and Microsoft Research Propose Efficient Trillion-Parameter Language Model Training on GPU Clusters (Paper and Github link included) Large-scale transformer-based language models have produced significant gains in natural language processing (NLP) in recent years. However, training such models is difficult because no single GPU has enough memory to accommodate parameter totals that have grown exponentially in recent years. Even if these parameters could be trained on a single GPU, limited computing power would result in longer training times without model parallelism. 

In a [paper](https://arxiv.org/pdf/2104.04473.pdf) by NVIDIA, Stanford University, and Microsoft Research, a research team has proposed a new parallelization schedule that improves throughput by more than 10 percent with a comparable memory footprint. The paper demonstrated that such strategies could be composed to achieve high aggregate throughput when training large models with nearly a trillion parameters. 

Summary: [https://www.marktechpost.com/2021/04/19/researchers-from-nvidia-stanford-university-and-microsoft-research-propose-efficient-trillion-parameter-language-model-training-on-gpu-clusters/](https://www.marktechpost.com/2021/04/19/researchers-from-nvidia-stanford-university-and-microsoft-research-propose-efficient-trillion-parameter-language-model-training-on-gpu-clusters/) 

Paper: https://arxiv.org/pdf/2104.04473.pdf

Github: https://github.com/nvidia/megatron-lm",1511,LanguageTechnology,1
Automatic Title Generation for Text with Transformer Language Model (Research Paper Walkthrough) ,97,LanguageTechnology,1
"nlp project with biobert Hi everyone, 

I am currently doing a project where I am supposed to be using named entity recognition with models such as biobert to process clinical data and possibly link them to ICD (International Classification of Diseases) codes however I am currently extremely lost. 

I am planning on using spacy to do the pre-processing on the data however, I am unsure as to what format biobert needs the dataset in.

Would anyone be able to point me in the right direction?",493,LanguageTechnology,1
"Named Entity Recognition and possible extensions such as Non-noun phrases Hey, people! Hope you are all well!

I've been reading some papers on NER and started to notice a development towards a more general and less strict definition of the task. Firstly, at MUC, only named mentions (proper nouns) would be considered. Then, at ACE, both nominal (common nouns) and pronominal phrases were added and also recursive mentions were considered. Further, other datasets kept exploring broader settings, like LitBank accepting personifications and recursive mentions in literary texts, HAREM enforcing ambiguity, etc.

As a part of this extension of the NER definition, do you believe that non-noun phrases can also be considered as mentions? Do you see other constructions being considered as mentions in the future?  

Also, if there are other interesting extensions you might know or believe may occur, please share!",913,LanguageTechnology,1
"NLP and Mathematical Generation? Hello everyone,

I am currently developing a system (for one of my thesis components) which analyses the mathematical relationships between numbers extracted from a given text and generates alternatives, unfortunately there are requirements such that the numbers can't be negative of a majority of 0.

I am currently making improvements on my current system however I think it would be wise to look at more feasible alternatives as this is for my thesis however I have not found any previous work in this niche. Does anyone have any ideas on this front please?",593,LanguageTechnology,1
"NLP Help | Scraping Question Hello, 

I have a few hundred txt documents, each containing a few sentences about someone's history with substance abuse. 

Based on 2 types of substances, I am trying to go through each file and collect the frequencies of 4 entities: **status** (e.g., past, current, none), **method** (e.g., inhale, chew), **amount** (e.g., 2 packs, 3-4 glasses), and **frequency** (e.g., a day). 

Example txt file: ""He does not use tobacco. He sometimes drinks wine. He does not use drugs ever."" 

**My dilemma:** again, I need to scrape the frequencies of these entities (essentially attributes of some type of substance abuse), ***but the wording/sentence structure varies a lot*** \--&gt; the **status** entity, for example, could see 'past', 'currently', 'still', 'sometimes', 'on weekends', 'back in 1984', etc... just a whole lot of things. I think I need to employ some NLP technique to classify/annotate this stuff but I am not sure how. **Any ideas?**

Thank you so much for your consideration.",1020,LanguageTechnology,1
"NLP and Skill Transfer I would like to start a career in NLP. Before investing my time in NLP, I would like to know if the skills needed in NLP can be transferred to another fields or not. For example, if in the future I want to work in DSP( digital signal processing), or Image processing, can NLP skills applied in these fields? 


Thank you very much! 

Note: I am asking because tomorrow I have an interview for  NLP job in a startup company. My background is CS, networking and a limited knowledge in Genetic algorithms  .

Edit: language mistakes",552,LanguageTechnology,1
"WHY DO I KEEP RUNNING OUT OF MEMORY (RAM) Guys I am trying to develop a BERT model that basically predicts an emotion. (Emotion classifier)

I never got a chance to eventually finish my training because I every time run out of Memory Ram

I tried Google Colab (ran out of memory) then tried Kaggle Kernel (ran out of space as well), both 12 and 16 GB RAM.

I do not know what is wrong with it? how people even train this type of model?

&amp;#x200B;

    ......DATA LOADING AND PREPROCESSING...
    
    !pip install deeppavlov
    from deeppavlov.dataset_readers.basic_classification_reader import BasicClassificationDatasetReader
    data = BasicClassificationDatasetReader().read(
        data_path='./',
        train='../input/pikabucsva/train_pikabu_a.csv',
        valid=""../input/pikabucsva/validation_pikabu_a.csv"", 
        test=""../input/pikabucsva/test_pikabu_a.csv"",
        x = 'content',
        y = 'emotions'
    )
    
    #ITERATOR
    from deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator
    # initializing an iterator
    iterator = BasicClassificationDatasetIterator(data, seed=42, shuffle=True)
    
    #BERT PREPROCESSOR
    !python -m deeppavlov install squad_bert
    from deeppavlov.models.preprocessors.bert_preprocessor import BertPreprocessor
    bert_preprocessor = BertPreprocessor(vocab_file=""../input/bertmodel/vocab.txt"",
                                         do_lower_case=False,
                                         max_seq_length=64)
    
    #SIMPLE VOCABULARY
    from deeppavlov.core.data.simple_vocab import SimpleVocabulary
    vocab = SimpleVocabulary(save_path=""./binary_classes.dict"")
    
    
    #ONEHOTTER
    from deeppavlov.models.preprocessors.one_hotter import OneHotter
    one_hotter = OneHotter(depth=vocab.len, 
                           single_vector=True  # means we want to have one vector per sample
                          )
    #PROB TO LABELS
    from deeppavlov.models.classifiers.proba2labels import Proba2Labels
    prob2labels = Proba2Labels(max_proba=True)
    vocab(prob2labels([[0.6, 0.4], 
                       [0.2, 0.8],
                       [0.1, 0.9]]))
    
    
    #BERT CLASSIFIER
    from deeppavlov.models.bert.bert_classifier import BertClassifierModel
    from deeppavlov.metrics.accuracy import sets_accuracy
    
    bert_classifier = BertClassifierModel(
        n_classes=vocab.len,
        return_probas=True,
        one_hot_labels=True,
        bert_config_file=""../input/bertmodel/bert_config.json"",
        pretrained_bert=""../input/bertmodel/bert_model.ckpt"",
        save_path=""sst_bert_model/model"",
        load_path=""sst_bert_model/model"",
        keep_prob=0.5,
        learning_rate=1e-05,
        learning_rate_drop_patience=5,
        learning_rate_drop_div=2.0
        )
    
    #TRAINING
    # Method `get_instances` returns all the samples of particular data field
    x_valid, y_valid = iterator.get_instances(data_type=""valid"")
    # You need to save model only when validation score is higher than previous one.
    # This variable will contain the highest accuracy score
    best_score = 0.
    patience = 2
    impatience = 0
    
    # let's train for 3 epochs
    for ep in range(3):
      
        nbatches = 0
        for x, y in iterator.gen_batches(batch_size=256, 
                                         data_type=""train"", shuffle=True):
            x_feat = bert_preprocessor(x)
            y_onehot = one_hotter(vocab(y))
            bert_classifier.train_on_batch(x_feat, y_onehot)
            print(""Batch done\n"")
            nbatches += 1
            
            if nbatches % 1 == 0:
                # валидируемся каждые 100 батчей
                y_valid_pred = bert_classifier(bert_preprocessor(x_valid))
                score = sets_accuracy(y_valid, vocab(prob2labels(y_valid_pred)))
                print(""Batches done: {}. Valid Accuracy: {}"".format(nbatches, score))
                
        y_valid_pred = bert_classifier(bert_preprocessor(x_valid))
        score = sets_accuracy(y_valid, vocab(prob2labels(y_valid_pred)))
        print(""Epochs done: {}. Valid Accuracy: {}"".format(ep + 1, score))
        if score &gt; best_score:
            bert_classifier.save()
            print(""New best score. Saving model."")
            best_score = score    
            impatience = 0
        else:
            impatience += 1
            if impatience == patience:
                print(""Out of patience. Stop training."")
                break",4545,LanguageTechnology,1
"production-ready NLP/NLU for restaurants? Hi

about 3 years ago Google showed a demo in which a human-sounding ""AI assistant"" made a phone call to a shop (a hairdresser, if I remember well). Is anybody using this technology in production, for example to accept restaurant/takeaway orders? Is DialogFlow production-ready? What's the SOTA in this field?

Thanks!",360,LanguageTechnology,1
A Rigorous Study on Pretrained Model for NER | Research Papers Summary 014 ,75,LanguageTechnology,1
"Tutorial on how to specify a large formula for a regression model Hey, I've created a tutorial on how to specify a large formula for a regression model using the R programming language. The tutorial shows different tips and tricks to make your code more efficient: [https://statisticsglobe.com/write-model-formula-with-many-variables-in-r](https://statisticsglobe.com/write-model-formula-with-many-variables-in-r)",413,LanguageTechnology,1
"Generating Datasets with Pretrained Language Models In our most recent paper, we introduce ""Datasets from Instructions"" (DINO 🦕) and show how LMs can create entire datasets from scratch if provided with instructions. These datasets can be used to train much smaller models.

This is an early draft, so I'd be very happy to hear your thoughts 😊

📄 Paper: https://arxiv.org/abs/2104.07540

🖥️ Code: https://github.com/timoschick/dino",431,LanguageTechnology,1
BART: Denoising Sequence-to-Sequence Pre-training for NLG (Research Paper Walkthrough) ,87,LanguageTechnology,1
"Does anyone know of any biomedical entity search engines out there? I'm doing some research on biomedical language models and would like to use search engines as a baseline. What I have in mind is something like the [Biomedical Entity Search Tool (BEST)](http://best.korea.ac.kr/). Polysearch is also a candidate, but for some reason it's not working anymore.

I'd appreciate it if someone who knows of other search engines like these ones would be able to provide some tips on where I could find more. Thanks!",510,LanguageTechnology,1
"Deciding between Saarland and Gothenburg Hello everyone!

Over the last few months I've been knee deep in the process for applying to grad school in compling/language technology, and now that decisions have come back I'm left with the choice between programs at Saarland University and the University of Gothenburg.

My research so far has favored Saarland: it's often mentioned in online forums and lists of the best compling universities, it ranks highly in the CSRankings for NLP in Europe (number 6 is you only consider English language master programs), and appears to be well regarded on this subreddit. Gothenburg, on the other hand, is rarely if ever mentioned, and ranks in the 50s in CSRankings for NLP in Europe. Indeed, I only applied for Gothenburg at the suggestion of a friend who goes there, when I was panicking after getting rejected from Edinburgh (which came as a surprise, given that it was my alma mater).

While this may appear to be an open and shut case, given the above, as far as I can tell Gothenburg does appear to have one major advantage over Saarland: job placement. According to my friend, Gothenburg works closely with Swedish tech firms to place its students, and indeed it actively encourages and facilitates collaborating with local firms on your dissertation. Saarland, on the other hand, does not appear to provide this level of support: according to the Saarland program's study coordinator, Saarland does not offer individual support for placement, and it appears that the university's relationship with local companies is not a strong as Gothenburg's appears to be.

My question is, does anyone here have experience with Saarland in terms of career placement? And is Saarland truly a better overall choice than Gothenburg?

Thank you in advance!",1787,LanguageTechnology,1
"Word embeddings and neural machine translation Can someone help me understand what is the purpose of word embeddings in neural machine translation models? 

I understand (mostly) he word embeddings in the context of sentiment analysis and other tasks, but what purpose does it have in the context of neural machine translation? Doesn't the neural model learn everything itself without needing any embedding space?",413,LanguageTechnology,1
"DUC 2004 Dataset Many of you were asking for the DUC 2004 dataset. Although it is available through their website, I have also uploaded it to GitHub and Kaggle for a better workflow. You can download it from here. 

&amp;#x200B;

 [UsmanNiazi/DUC-2004-Dataset: This Repo Contains the DUC 2004 Dataset (github.com)](https://github.com/UsmanNiazi/DUC-2004-Dataset)   


 [DUC 2004 Dataset | Kaggle](https://www.kaggle.com/usmanniazi/duc-2004-dataset)",448,LanguageTechnology,1
"Why is batch size a relevant hyperparameter for BERT? BERT is made up of Transformer encoders, and therefore uses layer normalization instead of batch normalization. Layer normalization normalizes each datapoint independently from other datapoints in the batch. So why would batch size matter for BERT's performance? Where else does it factor in?",346,LanguageTechnology,1
"Please give me some suggestions. I’m doing a school project which is building a chatbot to help students learn English and give tips for the TOEIC test (I dont come from English native speaking country), so far my ideas are using masked language models to help solving the reading test in TOEIC and I’m kinda stuck there. Can you give me some suggestions of what to implement and maybe some SOTA open source chatbots and how to tune it into specific domain (English and TOEIC) that would be super life saving. Thanks in advance",527,LanguageTechnology,1
"Project NLP / ML from scratch - Beginner Hi All, 

I am writing here because I wish to get in touch with those brilliant minds that are active in the group and learning from them :)   
In a course about Finance, I saw how I could use TextBlob and Scrapy on ICO whitepapers and how to exploit experts´ posts in [icobench.com](https://icobench.com) to determine a sentiment - based on Bayes Classifier. We have used Python. 

My goal is to demonstrate that I can be a point of reference for NLP and ML in a company (relative terms). I would proceed following: 

* Setup scraper/crawler to obtain data from several websites
* Store the data in google firebase /firestore
* Cleanup the data for ML
* Create a training set/test set
* Build a ML model (classification/prediction)
* Measure effectiveness
* Report on what I did 

A) Do you suggest me to be familiar first to Vue.js? I have no experience with HTML, CSS and java and I assume that crawl data &gt; store &gt; clean &gt; give label would take a relevant amount of time.

B) If I build a text corpus from wikipedia and use it for my data, which are the problems that I may encounter and take into account? Fx the text blob offers a classifier from movie database but if you apply it on another context like finance, then you need specific label for certain words. 

C) Which models can be more interesting to compare? 

I thank you in advance for the attention given to this post. 

  
Have a good weekend",1460,LanguageTechnology,1
Di resources What are some good document intelligence resources to begin with?,78,LanguageTechnology,1
"Benchmarking of pretrained models for semantic textual similarity Hello community!

So basically what we want to achieve is a benchmarking of pretrained models in order to find out which ones are the most useful/efficient for calculating semantinc text similarity and whether one multi-lingual model would do, or separate models are better for this case (it's nothing really serious, this is for a scholar project but not the study per-se, just a way for us to argument a choice)

The context is not bound to a particular field. In practice, we will be comparing sentences from documents from any and all fields (a situation similar to plagiarism detection) so fine-tuning on a field-specific corpus is not an option and we will test the models as-is.

We are seeking recommendation:

* **For the corpora**: We will be benchmarking for languages other than english (but english is also included). Where can we find parallel corpora (for each language) that are suitable for this task? if not, what means can we use to build a small corpus of our own without doing manual paraphrase
* **For the metrics**: Is cosine distance good enough for the comparison? (by taking 1.0 as the perfect similarity score and the averages of all cosine distances accross the corpus, it will be pretty much like an accuracy  score).
   * What metrics/distances are better suited for the comparison, and what would you recommend for textual similarity in general?

oh and one last point that is not related to this benchmarking in particular. Supposing we have two documents (for instance 2 research papers) that we want to compare agaisnt eachother:

* What tokenization strategy would you recommend, considering we will compare the tokens from document 1 to those of document 2, then highlight the matching text in both documents (again, the usecase is very similar to plagiarism detection). The goal is minimal pairs loss.

Tokenization by sentence/punctuation doesn't seem to play nicely  as sentences in one document can be very long and can include many sentences of the second.

&amp;#x200B;",2077,LanguageTechnology,1
"Links to nlp applied projects/thesis relating to ""law/legal"" Can anyone recommend any projects, thesis available online in which nlp methods are applied to data from the legal/law domain? I am trying to learn more about this topic, and so far couldnt find anything other than the ""LEGALBert"" paper.

Does anyone recommend checking out any masters university projects where nlp was applied to data from the legal domain? I already know the basics *about word clouds, lda topic modelling and sentiment analysis - I am looking for a bit more intermediate level stuff about information extraction, transfer learning, summarization, fuzzy match, etc. 

Thanks",654,LanguageTechnology,1
"MA in Linguistic and Literary Computing (TU Darmstadt): Thoughts? Hello, fellow Subredditors!

I'm curious to know if any of you are familiar with this program and if you could share your opinion about it.

I have a BA in Linguistics and I've done some work during my degree with corpora and have some experience in programming, specifically in Python. Mostly computer applications for linguistic research and digital humanities, more than anything else. I'm interested in studying a master's degree and Germany (for many reasons) is my place of choice. I've already looked at the programs at Tübingen and Stuttgart and the info I've seen on this subreddit about those two has been very useful. However, I haven't found much about the program at Darmstadt, and I'm particularly interested in this program, given its digital humanities component.

I would really appreciate your input! Thanks a lot!",898,LanguageTechnology,1
"[D] WTF these results are incredible?!! What do you guys think of this? BERT seems to learn equally well on word-shuffled sentences? Link to paper: https://arxiv.org/abs/2104.06644

Is BERT just a giant bag-of-words??

Relevant meme: https://imgur.com/a/RGUtOvt",261,LanguageTechnology,1
"Text Generation from Another Text Hi everyone. I want to ask you if this is possible:

Consider we have a minimal text, or just a couple of keywords, say X. And we have another large text or corpus, Y, that we assume they are about similar topics. Assume that X is a sentence and Y is a book.

Is it possible to generate a text Z that may be to an answer or comment to X that using facts or style from Y?

E.g. consider X is a text or keywords like ""I like to swim in Maldives!"". Y is a news about Maldives. Then Z might be a generated text like ""Maldives has the most clean beaches in the world.""

Thank you all.",613,LanguageTechnology,1
The first ever Gesture Generation Challenge. More info in comments ,67,LanguageTechnology,1
NLP equivalent to speaker diarization? ,39,LanguageTechnology,1
"Classification problem with text and numerical features I am working on a classification problem whose data includes both text and numerical features. My first approach to tacke this problem was to convert the text features into embeddings and append those as new features to original dataset. The problem with approach is that since embeddings are usually of high dimensions, they overwhelm the numerical features.

Second approach I used was append every feature together into a string, convert it into embedding and train the model. This gave me very poor accuracy.

Is there any other way I can handle this problem?",619,LanguageTechnology,1
Aspect analysis Best method for aspect analysis ??,50,LanguageTechnology,1
"How to do undergrad research the right way? I just graduated with a degree in CS and now I'm interested in doing research in NLP. The material available online, ever-changing technology and also the hype makes me overwhelmed. Also the real research happens at the PhD level which is far too above from an undergrad level. How do I approach things? Will reading more papers and learning more about the theory helps? Or should I just dive into the practical parts and start using pretrained models and libraries? 

The way I like to do things is first to have a solid theoretical foundations of a certain topic after which I look around for implementations. I am more of a theory person, but NLP is more practical than theory. 

My goal is to focus on only 1 field for my future studies and I've chosen Nlp. Now I want to get good at it, but it's not clear how?",859,LanguageTechnology,1
"Research paper mapping for BERT: foundational work &amp; latest advancements Sharing our new [interactive research graph for BERT](https://crossminds.ai/graphlist/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-60709500c8663c4cfa875fc4/), which maps important prior contextual representation work and various pre-trained language models derived from BERT.  Hope you find it helpful!

Here are the papers (and video presentations) included in the graph:

* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (core paper)
* Attention Is All You Need
* Semi-supervised Sequence Learning
* Deep contextualized word representations
* Universal Language Model Fine-tuning for Text Classification
* Improving Language Understanding by Generative Pre-Training
* RoBERTa: A Robustly Optimized BERT Pretraining Approach
* Defending Against Neural Fake News
* Language Models as Knowledge Bases?
* ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
* XLNet: Generalized Autoregressive Pretraining for Language Understanding
* Cross-lingual Language Model Pretraining
* 75 Languages, 1 Model: Parsing Universal Dependencies Universally
* Multi-Task Deep Neural Networks for Natural Language Understanding
* MASS: Masked Sequence to Sequence Pre-training for Language Generation
* Unified Language Model Pre-training for Natural Language Understanding and Generation
* SpanBERT: Improving Pre-training by Representing and Predicting Spans
* ERNIE: Enhanced Language Representation with Informative Entities
* ERNIE: Enhanced Representation through Knowledge Integration
* Knowledge Enhanced Contextual Word Representations
* VideoBERT: A Joint Model for Video and Language Representation Learning
* Contrastive Bidirectional Transformer for Temporal Representation Learning
* ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks
* VisualBERT: A Simple and Performant Baseline for Vision and Language
* Fusion of Detected Objects in Text for Visual Question Answering
* Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training
* LXMERT: Learning Cross-Modality Encoder Representations from Transformers
* VL-BERT: Pre-training of Generic Visual-Linguistic Representations
* UNITER: Learning UNiversal Image-TExt Representations
* Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks
* DeBERTa: Decoding-enhanced BERT with Disentangled Attention",2500,LanguageTechnology,1
"Finished an NLP based binary text classified for user reviews, what else can I do with this to add to my project? Looking for other types or ways to use this data and come up with analysis on it. I feel like my project needs a little more but I’m unsure what else I can do with what I have. It takes in user reviews, like Yelp reviews, and tags them as either positive or negative.",381,LanguageTechnology,1
"Is there a way to get document wise perplexity in gensim LDA For example, how perplexed is our model assigning the topic that it assigned to a document. The reason I want to know this is because I want to weed out low frequency documents that were wrongly assigned to high frequency topics",289,LanguageTechnology,1
"Checkout our team-based cloud-first text annotator. *Long time lurker, first time poster!* 

*otso has just launched our Annotator, and have built with the needs of many in this sub!*

&amp;#x200B;

**A cloud-based Text Annotator built for Machine Learning Engineers and Data Scientists.**

We have been working on otso Annotator for over two years. It began as an internal tool, used to manage annotation and data labelling for our own machine learning projects. As the tool and the interface developed, we began providing it to select enterprise customers. We’ve received so much significant positive feedback from our clients, that we decided to launch the Annotator as a standalone product.

otso Annotator provides three key benefits; the user experience, which prioritises ease-of-use and understanding. The project management features, which let you allocate and manage annotation tasks. Finally, as a cloud-first tool, you no longer need to have annotators use a CLI to get started, which makes it a much easier tool for teams to use.

**Why a focus on the user experience of teams?**

Text annotation is best done in a team environment. Ideally, machine learning engineers and data scientists will set up and run projects, while subject matter experts provide annotations. We have built otso Annotator with these different user types in mind - enabling seamless project setup for project admins and an easy and keyboard enabled annotation experience for annotators.

With this public launch, we are granting users and teams that sign up during April an extended trial period of 30 days.

To check it out, head to [otso.ai/annotator](https://otso.ai/annotator).No credit card required.",1693,LanguageTechnology,1
"At which linguistic patterns and features attention heads of BERT look to ? Hello,

I am developing a an extractive text summarizer for french language, I would like to explore the self-attention mechanism to see at which linguistic patterns it looks to by generating a heat map or? Any help?",292,LanguageTechnology,1
Youtube Video Transcript Summarization with Hugging Face ,57,LanguageTechnology,1
hello I need DUC 2004 dataset for my master project .... if anyone share it with me i'll so thankful ,101,LanguageTechnology,1
"Deep learning on multiple computers Hello,

I'm using BERT language model for predicting mental health issues. Later, I'll be using more language models for comparison. The dataset is big and the whole computation requires GPUs which I don't have. I was wondering if there's any quick way to make a training cluster with multiple CPU based computers to make training faster. I'm no knowledge of distributed computing so idk how to proceed with this. Do you guys have any idea?

Thanks.",485,LanguageTechnology,1
"[Q:] MLM for short phrases Hello there,
Im currently working on my Bachelorthesis and have only little knowledge on NLP. 
My idea was to finetune a BERT Model on a task that is a mix of fill-mask and text-summarization. 

Basically i would scrape for short sentences about a specific topic and display those. The user would have to crate template sentences with masked entries. And finally the transformer guessing those masked entries.

Does this sound realistic?

Could someone provide any information about how to fine tune something like that? Is it eveb possible to focus the transformer on a specific text?

Thanks !",622,LanguageTechnology,1
Tweet Scraping using Twint and Sentiment Analysis using Hugging Face Transformers #python #NLProc ,98,LanguageTechnology,1
"Plan to make twitter sentiment analysis Hello,

I am going to make a simple sentiment analysis of Twitter posts. Basically searching for a word(name) and than retrieving n tweets and later returning the distribution positive, negative, neutral.",244,LanguageTechnology,1
"[D] is this a correct application of the cosine similarity? Suppose you have downloaded the pdf of every Shakespeare play on your computer. Suppose now you want to find the name of a Shakespeare play that you read in high school, but you can't remember it's name - however, you do remember the general plot of the play, e.g. ""a danish prince is visted by his father's ghost and holds a skull in his hand while delivering a speech"". (Btw this is the plot of hamlet)

Suppose you type this sentence in - could the cosine similarity be used to find out which play is most similar to this sentence? Is there a common way to solve this kind of problem?",647,LanguageTechnology,1
"Suggestion for web scraping Can somebody urgently suggest me the best method to extract a specified section from a .txt webpage.

Example-  [https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-000413.txt](https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-000413.txt)   

Suppose, I want to extract all text from the section ITEM 7. Management's Discussion and Analysis section. Now, the catch is that there are several such webpages and not all of them start with Item 7 \[they have Management's discussion and analysis common though\]. How to do this?",568,LanguageTechnology,1
"Can you suggest me a Transformer Model for fine-tuning for my thesis? Hello!  For my MSc Artificial Intelligence thesis I would like to work on fine-tuning transformers for NLP. I will have 5 months to complete my thesis. I don't have too much experience in NLP because the course was more focused on Computer Vision. I find transformers very interesting and I want to work on fine-tuning for dialogue systems and text classification. I haven't fully decided on the dataset or what classification I want to do. 

1. Can you suggest a topic for text classification? I am looking for an interesting subject that is also relevant for employers. 
2. There are so many different transformer models that I can work on. Which ones do you think would be the best? 

Thank you in advance!",779,LanguageTechnology,1
NER for Social Media Texts with Semantic Augmentation | Research Papers Summary 013 ,84,LanguageTechnology,1
"Given a corpus of about 10k keywords and another keyword x. How do I extract the top 5 most similar words to x. 
I have found various topic modelling algorithms, but they seem to depend on the interrelationships among words used in a document to determine the similarity. I want to achieve something similar to keyword clustering based on its meaning and retrieve top 5 most similar words of a query keyword. Any suggestions or path to solve this problem will be very helpful.

This community has been amazing for a fresher like me and thank you to each and everyone who takes out their time to help out people like me.",619,LanguageTechnology,1
"How to award a score (1-100) for closeness to the right answer? Have a product in the works which includes a quiz element. Would like to support fill-in-the-blank for entering text answers, but not require perfect spelling.

If correct answer is “Guatemala”, would like to give 100 pts for exact match, and (for example) 93 for Guatamela, and 45 for Gwattanala, and so on. 

Are there good examples of this in the wild?",419,LanguageTechnology,1
"Any good NLP roadmap? I'm a beginner in data science. I would like to have a roadmap to follow and understand the broad extent of this amazing field.

May be it could be difficult to find something specific for NLP. In that case, please share Machine Learning or Data Science roadmaps that you may have.

Beginners of this community would appreciate it!",353,LanguageTechnology,1
"Embedding2triples, which ML algorithm? I have a training set of news articles that I wish to condense into triples, and I actually can evaluate how well these triples describe the original text and use it as a training signal, but I'm new to the field of NLP and uncertain about my options.  Which ML algorithms would you consider to take a BERT embedding layer as input and learn to turn/decode it to triples based on the mentioned training signal?",449,LanguageTechnology,1
Event extraction/Highlight detection from transcript ,53,LanguageTechnology,1
"when doing NLP, do you usually augment your data with a ""pre defined corpus"" specific to the field your data comes from? when doing NLP, do you usually augment your data with a ""pre defined corpus"" specific to the field your data comes from?

E.g. if you are working with medical data, do nlp procedures require you to use a predefined corpus from the medical domain?",367,LanguageTechnology,1
"[Tutorial] Neural Machine Translation With Attention With Keras This tutorial gives a step-by-step guide to implementing an RNN model (encoder-decoder sequence-to-sequence with attention mechanism) for French to English translation using Keras.

Additional topics covered include:

* The Problem With Sequence-to-Sequence Models for Neural Machine Translation
* An Introduction to Attention Mechanisms
* Categories of Attention Mechanisms
* Applications of Attention Mechanisms
* Neural Machine Translation Using an RNN With Attention Mechanism (Keras)

Tutorial link: [https://blog.paperspace.com/seq-to-seq-attention-mechanism-keras/](https://blog.paperspace.com/seq-to-seq-attention-mechanism-keras/)

Run all of the code on a free GPU: [https://ml-showcase.paperspace.com/projects/neural-machine-translation-with-keras](https://ml-showcase.paperspace.com/projects/neural-machine-translation-with-keras)",906,LanguageTechnology,1
"Erasmus or Uppsala? Hi everyone!

I got accepted into the Erasmus master in Language and Communications Technology and also into the University of Uppsala master in Language Technology. I need help deciding between the two.

If you don't know, the Erasmus program implies attending two universities, one year each, out of the seven universities of the consortium. They will tell me what universities i will attend once i say yes. Therefore, i don't know exactly what the course will be.

I feel more inclined to choose Uppsala because:
The logistics of living in Sweden for two years are easier than living in two different countries.
The program is free, as opposed to Erasmus in which i have to pay a fee.
I actually know the course of studies program, as opposed to Erasmus in which i will find out later.
I believe studying in Uppsala will make it easier for me to find a job and a life in Sweden.
I have a boyfriend living four hours away from Uppsala (i try not to let this be a factor but who am I kidding, it is).

On the other hand, i think maybe the Erasmus program has a better reputation and also living in two random countries sounds like fun.

If you have any advice it would be welcome. thanks!",1209,LanguageTechnology,1
"Intro to nlp and text mining I am completely to new to nlp/text mining and am interested in learning more about it (on an applied side). Suppose I have a dataset with 1000 doctor comments (no ""labels"", i.e. I don't know if they are positive or negative) for different comments. The question is now, what can I do? 

Just by doing some google searches, it seems like the two popular things to do with this data is LDA and sentiment analysis. Just by looking how to do this in R, this looks doable. But what are some other popular algorithms that can done on this kind of data? I assume that through LDA and sentiment analysis, you can ""cluster"" these doctor comments together? If you have a new comment, you can see which from clustering which cluster this comment belongs to? 

In this example, how would the newer BERT algorithm come into play? Is there a main way to extract ""insights"" from these comments? Based on the text in these comments, are there algorithms that can determine if its possible to see which medical conditions  are more lethal, which age groups/demographics of patients are healthier, which medications are people taking, etc. ? Or is this a very advanced problem?

Thanks",1196,LanguageTechnology,1
"NLP Problem  

Hi all - I'm working on what I think could be an low-level NLP problem at work, and was wondering if anyone here had any ideas on modules I could/should look into that might provide a solution.

Problem: so, the company I work for deploys networked hardware into the field, and that hardware often requires repairs. Our repair folks take down a lot of unstructured data when they get the hardware back up and running, stored as a note on salesforce. I'm trying to figure out a way to process this information to get some meaning from it (e.g., are repair cases most often networking or electrical issues, or from weather damages...etc.,).

Solution: I've been trying to figure out a way to sort all of this information. One idea was to make a list of all strings in all notes, strip out all articles/conjunctions, and then create some word cloud. My hunch is that someone has probably solved something very similar to this, and I wanted to socialize it in this community to see if anyone had any thoughts!

I would massively appreciate any ideas, or pointers on what modules I should be looking into! Just to be clear, we use Python/Jupyter Notebooks.",1166,LanguageTechnology,1
"Means to detect plagiarism in textual documents Hello guys, 

I came here to ask you for a help. I'm writing my final thesis right now on Detecting plagiarism in text documents. And I have to be honest with you guys, It is over my head. Deadline is in 1 month and I don't know how to make a working piece of code. So I wanted to ask you if you are willing to help me or at least tell me if there is something I can do. 

So the task is to compare documents (preferably bunch of documents to one suspicious document) and find if the suspecious document is plagiarism to any of those documents in reference collection. Currently I have done loading multiple pdf files, making dataframe, preprocess the text data (tokenisation, lemmantisation, stemming, stopwords removal, punctuation removal, lowercased) and vectorization. Then I applied cosine similarity to it and it kinda works.

My question is if there is any possible way to apply Support Vector Machine or Naive Bayes to this task? and if so, how do I set it up? The main goal was to apply some Machine Learning algoritm but I took much bigger piece of pie than I was supposed to.

I am really really desperate and any information will help me. Thank you",1209,LanguageTechnology,1
"Summer schools in NLP 2021 Hi, any leads on upcoming NLP summer schools.",72,LanguageTechnology,1
Stanford NLP Stanza NLP package - Biomedical NLP models demo #NLproc #python #clincialNLP ,90,LanguageTechnology,1
Question Answering (Seq2Seq) Visualization with Transformer Neural Networks ,76,LanguageTechnology,1
"Are word embeddings semantically ""meaningful""? It is a common consensus that words appearing in similar contexts are semantically similar. But this definition breaks drastically when we consider antonymy relationships - &gt; For. eg., the word ""positive"" and ""negative"" generally appear in similar contexts and are assigned spatially close vectors. In the inherent sense, it does not contain meaning. 

Word2vec was trained on contextual information is somehow not commonly referred to as contextualized word embeddings. However. embeddings realized through BERT and Elmo are termed contextualized word-embeddings. Is there a reason for this? Or perhaps I might be wrong here and it was already called contextual word embedding. The embeddings through BERT and ElMO can be called dynamic embeddings at best, but it is not difficult to construct dynamic embeddings through word2vec embeddings. Does BERT embedding inherently have any other semantic information (non-positional, non-context), other than a more complicated interaction layer (and perhaps more data)?",1063,LanguageTechnology,1
"MarianMT usage with Spacy possible? Hi everyone,

is it possible to use MarianMT transformer models with spacy-transformers? Similar to [https://spacy.io/universe/project/spacy-transformers](https://spacy.io/universe/project/spacy-transformers)  
MarianMT: [https://huggingface.co/transformers/model\_doc/marian.html?highlight=mariantokenizer#multilingual-models](https://huggingface.co/transformers/model_doc/marian.html?highlight=mariantokenizer#multilingual-models)

Thanks!",477,LanguageTechnology,1
Multi-Document Summarization: The Wikipedia Current Events Portal Dataset (Dataset and Colab notebook) ,103,LanguageTechnology,1
"How do I predict sentiment of unseen text (After training and testing) Using scikit learn, I managed to train my model but dont know how to use the model to predict new text passages. I have watched tons of tutorials but none of them go beyond training and testing. Below is the code Im using. Any help is appreciated.

        data_source_url = ""/path/to/file.csv""
        airline_tweets = pd.read_csv(data_source_url)
        
        features = airline_tweets.iloc[:, 10].values
        labels = airline_tweets.iloc[:, 1].values
        
        processed_features = []
        
          # I do some text processing here and then append the text to processed_features
        
                
        vectorizer = CountVectorizer(analyzer = 'word', lowercase = False)
        features = vectorizer.fit_transform(processed_features)
        features_nd = features.toarray() # for easy usage
        
        X_train, X_test, y_train, y_test  = train_test_split(features_nd, labels, train_size=0.80, random_state=1234)
        
        log_model = LogisticRegression()
        log_model = log_model.fit(X=X_train, y=y_train)
            
        predictions = log_model.predict(X_test)",1188,LanguageTechnology,1
"Overfitting? Input data with too many misspellings Im working on predicting answers given question using seq2seq. So far it is as simple as can be . Encoder and decoder are just an LSTM each.

However, even if train loss decreases w epochs, validation loss is too high.

And the question answer train data has many misspellings and words from another language sometimes.

How can I help this? I already included Dropout layers with learning rate 0.2 to 0.5 for input, and 0.8 for output but validation loss still too high..

Accuracy also is so low. Like 5%",557,LanguageTechnology,1
"Simple corpus service Hi all,
Long time lurker, first time poster here. 

I’m turning to the hivemind because I’m at my wit’s end.

I’ve been tasked with providing a simple corpus service that returns tokens, sentences, etc. from a huge corpus based on certain criteria. These criteria are used to filter the corpus according to the annotations of the corpus entries. For example I should be able to return all nouns in the corpus back to the client. 

My first thought was to parse the corpus and load it into some sort of database which I could then query. But it didn’t work because of the technical limitations at my job. Also it seems like a lot of overhead to just do simple queries on the data. 

Then I thought I would load the corpus into a pandas dataframe, keeping it in memory for the lifecycle of the corpus service and querying it when client requests come in. But I find the solution a bit hacky. For example, it becomes very brittle when I try to map the schema of the corpus to my service’s own internal schema. 

Does anyone have experience with this problem? Is there a more straightforward approach that I haven’t thought of before? 

Thanks for taking the time to read this!",1195,LanguageTechnology,1
"What's the word on Potsdam's MSc Cognitive Systems? Hey everyone!

I'm applying for my Master's right now and the threads in this sub have been tremendously helpful! I was wondering if anyone is studying / knows someone who's studying at Uni-Potsdam. I've seen next to nothing about the program in this sub. If I'm being honest, that program looks the most appealing of the European programs I've come across in my research. So, to anyone who can provide some insight, I ask:

* Is it as heavy on the Machine Learning / Deep Learning front as it would appear from the course plan?
* Is it technical and applied (as opposed to theoretical and scholarly)?
* What's the general vibe? How do you and/or the students like it?

You can totally stop reading now but in case anyone's interested:  
I've been accepted to UEF and Edinburgh so far, hearing from Uppsala and/or Gothenburg on Friday. I'm Canadian, so the Swedish schools are either scholarship or $$$$$$. German applications aren't due for a little while so I have this nice little intermission to pause and ponder. Any advice is welcome! Thanks!",1100,LanguageTechnology,1
AI (GPT-2) Text Generation in Python with Kaggle Dataset | FineTuning GPT Model ,80,LanguageTechnology,1
"What are the top 15 conferences in Natural Language Processing? I see several websites listing top X conferences in NLP (Natural Language Processing), but I am not sure if there is some kind of ranking for these conferences. It will be amazing if anyone has some clue about it. Thank you very much for any hints or pointers or answers. :)",338,LanguageTechnology,1
[N] Grammarly releases a grammatical error correction (GEC) dataset for the Ukrainian language ,95,LanguageTechnology,1
"Requesting feedback on project related to tracking how news events change over time This sub has been very helpful in the past so I am hoping I can get some feedback. For my project, I am essentially trying to find a way to detect changes over time to a news narrative. At this stage I have applied an algorithm to successfully group together the stories that follow the development of the same event. So now I need to find a way to track, analyze, and maybe quantify how the events (and their coverage changes) . My current approach is using topic modeling to find important keywords in each of the articles. Then, I use those key words to map how the stories change overtime. So in the most basic of terms, I am identifying key words in the first article in each narrative chain and then comparing how those keywords change and are different from key words identified in subsequent articles about the same event.  Does this approach sound reasonable?  Is there anything else I should be trying instead?  Thanks everyone!",1022,LanguageTechnology,1
LDA Topic Modelling Explained with implementation using gensim in Python -#NLPRoc tutorial ,91,LanguageTechnology,1
Best practices in NLP What's your experience in NLP and what do you think are the common strategies and best practices to follow as a beginner?,143,LanguageTechnology,1
"Dissertation Ideas - NLP + blockchain Hello everyone.

I am currently thinking about possible dissertation ideas that would combine NLP and blockchain. I got solid background in both modern NLP and distributed systems using blockchain (knowledge about theory, cryptography, investing, solidity...). 

If you have any idea that you are willing to share, it is more than welcome.",377,LanguageTechnology,1
"[D] Can we add CNN on top of BERT  I have dense neural network for BERT, How do I change that to Conv1D, maxpooling, and flatten, before connected to dense layer. 

\`\`\`

 

    class BertBinaryClassifier(nn.Module): 
    def __init__(self, dropout=0.1): 
    super(BertBinaryClassifier, self).__init__()         
    self.bert = BertModel.from_pretrained('bert-base-uncased')         
    self.dropout = nn.Dropout(dropout)         
    self.linear = nn.Linear(768, 1)        
     self.sigmoid = nn.Sigmoid()                   
    def forward(self, tokens, masks=None):         
    _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)         dropout_output = self.dropout(pooled_output)         
    linear_output = self.linear(dropout_output)        
     prediction = self.sigmoid(linear_output)         
    return prediction
     # Config setting 
    BATCH_SIZE = 4
     EPOCHS = 5

\`\`\`",940,LanguageTechnology,1
"Adding Keywords to a Classification Model Is it possible for one to add keywords on a classification model?  (eg different sentences present in a dataset that may relate to -- company, school , Monuments, temples etc)  
So if I have a sentence that is classified as 'school' --&gt; maybe I can map the keyword - 'School' , 'Boarding school' , ' classroom' etc in the model so it can give better accuracy.

This is just a hypothetical thought I had, nothing pertaining to any existing dataset/ problem",500,LanguageTechnology,1
"Use word2vec for topic modelling Hi, so I'm working on a topic modelling project whereby I'm trying to extract relevant job skills from a corpus of UX Researcher job postings. 

I've gone through all the standard steps of pre-processing and EDA and have already achieved some meaningful results by using TF-IDF with K-means clustering. With trigrams I've been able to return several clusters of skills which have things like; 

* qualitative quantitative research
* quantitative research method
* quantitative qualitative research
* user centre design
* psychology human computer
* brand include vogue
* deliver high quality
* user behaviour attitude
* interview usability test
* communicate research result

So I'm quite happy so far but it can be better no doubt. I've made word vectors of the dataset using word2vec, and the model is able to provide meaningful similarities between words for example; 

    model = Word2Vec(gensim_input_set, min_count=10, size=100, workers=3, window=5, sg=1)
    print(model.wv.most_similar('psychology')[:5])
    
    &gt;&gt;&gt; [('cognitive', 0.9959733486175537), ('computer', 0.9905843734741211), ('experimental', 0.9882711172103882), ('hci', 0.9872062802314758), ('anthropology', 0.9846541285514832)]

But now I'd like to know if it's possible to take the word embeddings from the model and cluster them somehow?",1355,LanguageTechnology,1
Question about BERT Embeddings Am I correct in thinking that BERT uses wordpiece to create a unigram type vector that is then used as the input into the BERT system. But the embeddings we use after the pre-training is done is not just those initial embeddings it is the weights of all the parts of the network. Is this correct?,327,LanguageTechnology,1
"Fuzzy Matching/Logic - Concept, Utility, Implementation, and Complex Scenarios We’ve been seeing a surge in the requests for Fuzzy Matching/Logic techniques. We tried to learn more about these techniques and the implementation process and have written about it. We have also covered some complex scenarios and their solution.  


Here’s a detailed view on the concept, its utility, and the implementation: [https://nanonets.com/blog/fuzzy-matching-fuzzy-logic/](https://nanonets.com/blog/fuzzy-matching-fuzzy-logic/)  


Can anyone share their experience on implementing Fuzzy Matching in their product or solution?",615,LanguageTechnology,1
"How to use spacy and a tailor-made transformer in sentiment analysis? Hi

I m trying to set up a Spacy pipeline with my one transformer but it does not work as the transformer I generated is not compatible.

Dows anyone have an updated example of this pipeline set up, including the compatible method of creating the Transformer?

Thanks!",338,LanguageTechnology,1
"Deploying chatbot on a website? Hey,   
I'm trying to create a chatbot website. The chatbot is developed using Spacy and Tensorflow + Flask. I tried Heroku but ran into the limits pretty quickly. And since I'm a broke student, don't really want to pay for it just yet. 

Any suggestions? Is it impossible to avoid paying for it?",328,LanguageTechnology,1
"Real-time language generation? Is there such a thing as a real time language generator? 

Apologies if the question may seem silly, I'm a researcher from a completely different field. I only know of gpt3 but it has a significant delay in the responses. We would like to use it to give some semblance of life to virtual characters in a research project. 

What would be the next best thing? Should I come back in 5-10 years? Any pointers would be greatly appreciated.",466,LanguageTechnology,1
Kaggle Natural Language Processing with Disaster Tweets – Top 14% Solution with BERT (TensorFlow) ,98,LanguageTechnology,1
"Most efficient method for converting ~300k sentences to word embeddings? I'm working on a project where I need to get the word embeddings of around 300,000 sentences so that I can compare them with other sentences using cosine similarity. Since I'm dealing with so many sentences, what would be the most efficient way to get the word embeddings? So far I've been using the [sentence-transformers](https://github.com/UKPLab/sentence-transformers) package (my model is initialized as SentenceTransformer('bert-base-nli-max-tokens')), but using this would take around 24 hours if not more to get the word embeddings of all sentences. If that's as fast as it can get I'm fine with that, but was wondering if there's a way to do this more efficiently?",746,LanguageTechnology,1
"Conditional Text Generation (About Me) I'm looking for resources/code which will allow me to generate text (Introduction/About Me) based on defined text inputs like Name, Place,  Occupation, etc.   


I was able to find a good relatable paper but there is no available code for [ToTTo: A Controlled Table-To-Text Generation Dataset](https://paperswithcode.com/paper/totto-a-controlled-table-to-text-generation) . If anyone have done something in generation or have some idea then it will be helpful.  
thanks in advance.",520,LanguageTechnology,1
"Any good tutorial for NLP using Tensorflow for classification, word embedding, ... WITHOUT imdb dataset. I can't find any good tutorial to learn all these strategies with my own csv dataset with labeled reviews. As I don't have the same data structure of imdb dataset, I can't follow any tutorial based on official documentation. Every f tutorial is based on the same example!

Anyway, I would appreciate any good resource!",423,LanguageTechnology,1
"New date announced for language tech series, on Terminology Management Tools The third webinar in the language tech series now has a date!

[https://us02web.zoom.us/webinar/register/8416174045220/WN\_DQnctbT7RD-Q7BzYzy2TsQ](https://us02web.zoom.us/webinar/register/8416174045220/WN_DQnctbT7RD-Q7BzYzy2TsQ)",305,LanguageTechnology,1
"More examples or longer examples for small dataset classification problem I have a problem in this vein, and was wondering what everyone thought:

**Problem:** Classify 2-line snippets of poems by their author.

**Data:** I have 9 poets from whom I've collected anywhere from 150-250 poems.  Assume that this dataset can't get any bigger.  

**Method:** Log. Reg./SVM/KNN as basline, ultimately implementing word-level and subword-level CNN (which is the point of the experiment, mining out the different between word-level vs. subword level models in this domain).

Now my question is, with such a small amount of data, should I split the poems into 2-line snippets for training, and thus have \~2500-5000 examples per author, or should I train with larger chunks that will have more semantic information (e.g. 4 lines, entire stanzas, entire poems)?  Additionally, assuming the answer is something along the line of ""it depends,"" what are some tools for exploring this tradeoff and deciding what to train with?

Thanks!",1021,LanguageTechnology,1
"How to Train a Joint Entities and Relation Extraction Classifier using BERT Transformer with spaCy 3  Looking to add relation extraction classifier to your NER?

Checkout our [new article](https://walidamamou.medium.com/how-to-train-a-joint-entities-and-relation-extraction-classifier-using-bert-transformer-with-spacy-49eb08d91b5c) on how to train a joint entities and relation extraction classifier using BERT Transformer with spaCy 3.",437,LanguageTechnology,1
Machine Translation in Argos Translate (2021) ,46,LanguageTechnology,1
"Made website with best resources I could find on Transformers I spent \~50 hours looking for the best resources on various Transformer models and organized them into a website [backprop.org](https://backprop.org/).

The website is intended to help everyone learn faster by directly connecting you to the best resources without having to hunt everything down yourself.

Check it out! If people find this useful I'll add more pages on more NLP topics. Already planning on adding pages about RNNs, LSTMs, GRUs, etc.

Also, if you know any great resources that I missed send it my way!",581,LanguageTechnology,1
"Beginners in NLP, need your feedback. Made this video explaining building neural semantic search using open-source project Jina(think search like google). How good is this explanation? ",185,LanguageTechnology,1
"The Conversational AI and NLP Summit takes place later this month. See the speaker list and agenda below! Speaking companies include Deepmind, Facebook and more. ",162,LanguageTechnology,1
"Training AI on anti-corruption  Hi everyone, 

please excuse me if this is the wrong place to post! 

Myself and a small team are looking to train an existing AI tool to be able to identify corruption in organisations, and I am looking for historical examples to train it on. In particular, at this stage, I am looking for concrete text-based examples where evidence of corrupt practices is clearly visible (e.g. from meeting protocols/minutes, emails or other company communications, …) and should be from corruption scandals where text document-based leaks took place e.g. from Panama papers, Fifa scandal, Enron, 1MDB, Odebrecht, etc. 

At this stage of the project, we do not have the resources to go through the corpora manually to look for these concrete examples, therefore I have been looking for examples from the corpora which have been cited in academic papers or newspaper articles. At a later stage, the AI would be able to go through the corruption corpus itself. So far, my search to find such examples have been unsuccessful. 

I realise that corruption is a very clandestine topic, which is probably why I am having such difficulty. If anyone has ideas of how I can find articles etc where the data has already been interpreted and show concrete examples of corruption, then I would very much appreciate it!

Many thanks!",1338,LanguageTechnology,1
"Trankit v1.0.0 - An open-source Transformer-based Multilingual NLP Toolkit for 56 languages is out. Hi everyone,

We just released the version 1.0.0 for our Transformer-based Multilingual NLP toolkit named Trankit which outperforms the popular SOTA Stanford NLP (Stanza) in many tasks over 56 different languages.

### 💥 💥 💥 The new version v1.0.0 offers:

* **A trainable pipeline for fundamental NLP tasks over 100 languages**.
* **90 new pretrained transformer-based pipelines for 56 languages**. The new pipelines are trained with XLM-Roberta large, which further boosts the performance significantly over 90 treebanks of the Universal Dependencies v2.5 corpus. For **English**, Trankit is significantly better than Stanza on sentence segmentation (**+9.36%**) and dependency parsing (**+5.07%** for UAS and **+5.81%** for LAS). For **Arabic**, our toolkit substantially improves sentence segmentation performance by **16.36%** while **Chinese** observes **14.50%** and **15.0%** improvement of UAS and LAS for dependency parsing. Performance on other languages is also significantly improved. The detailed comparison between Trankit, Stanza, UDPipe, Spacy on other languages can be found  [here](https://trankit.readthedocs.io/en/latest/performance.html#universal-dependencies-v2-5) .
* **Auto Mode for multilingual pipelines**. In the Auto Mode, the language of the input will be automatically detected, enabling the multilingual pipelines to process the input without specifying its language.  Check out how to turn on the Auto Mode [here](https://trankit.readthedocs.io/en/latest/news.html#auto-mode-for-multilingual-pipelines). 
* **Command-line interface** is now available to use. This helps users who are not familiar with Python programming language can use Trankit more easily.  Check out the command-line tutorials on this [page](https://trankit.readthedocs.io/en/latest/commandline.html). 

**Trankit is written in Python** and can be easily installed via pip. Our code and pretrained models are publicly available at: [https://github.com/nlp-uoregon/trankit](https://github.com/nlp-uoregon/trankit)

We also created a documentation page and a demo website for Trankit.

Documentation page: [https://trankit.readthedocs.io/en/latest/index.html](https://trankit.readthedocs.io/en/latest/index.html)

Demo website: [http://nlp.uoregon.edu/trankit](http://nlp.uoregon.edu/trankit)

Technical details about Trankit can be found in our paper: [https://arxiv.org/pdf/2101.03289.pdf](https://arxiv.org/pdf/2101.03289.pdf)

Thank you for your time reading this post!

Hope you enjoy Trankit!",2599,LanguageTechnology,1
Reformulating Unsupervised Style Transfer as Paraphrase Generation | Research Paper Walkthrough ,96,LanguageTechnology,1
"A Python library to boost T5 models speed up to 5x &amp; reduce the model size by 3x. I wanted to share this new library I've been working on and that I open-sourced!.

here are some links to the library:

💻 [GitHub Repository](https://github.com/Ki6an/fastT5)

🐍 [PyPi project](https://pypi.org/project/fastt5/)

as the title suggests, you can increase the inference speed of any pretrained T5 model and also decrease the models' size, in a single line of code.

The library can be installed with `pip install fastt5`. This code snippet from the repository's README gives a concise overview:

    from fastT5 import export_and_get_onnx_model
    from transformers import AutoTokenizer
    
    model_name = 't5-small'
    model = export_and_get_onnx_model(model_name)
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    t_input = ""translate English to French: The universe is a dark forest.""
    token = tokenizer(t_input, return_tensors='pt')
    
    tokens = model.generate(input_ids=token['input_ids'],
                   attention_mask=token['attention_mask'],
                   num_beams=2)
    
    output = tokenizer.decode(tokens.squeeze(), skip_special_tokens=True)
    print(output)

The fastT5 library exports the T5 model to onnx with `past_key_values,` then quantizes it and runs it on onnxruntime.

The exported onnx models support the `generate()` method of huggingface transformers for inferencing.

for more information on the project refer to the repository [here](https://github.com/Ki6an/fastT5#reduce-t5-model-size-by-3x-and-increase-the-inference-speed-up-to-5x).",1603,LanguageTechnology,1
Sentence Transformers: Sentence-BERT - Sentence Embeddings using Siamese BERT-Networks | arXiv demo ,100,LanguageTechnology,1
"Euphemism Corpora? Hello,

I am a Computational Linguistics graduate student, and my peers and I are searching for corpora dealing with euphemisms/dysphemisms. We are interested in automatic detection of euphemisms (and other non-literal speech usage), but have yet to find any substantial collections. 

We are willing to create a corpus if none exists, but would like to have a comprehensive idea of previous work before embarking on creating our own. Any help would be appreciated!",484,LanguageTechnology,1
"Google AI Proposes A Machine Learning Algorithm For Teaching Agents To Solve New Tasks By Providing Examples Of Success Most reinforcement learning algorithms work on a ‘reward’ function to teach the agents in an unknown environment. The reward is given if the action taken results in a good outcome. But it’s a difficult task to define rewards for situations lacking clear objectives. For example, whether a room is clean or if a door is sufficiently shut. In such scenarios, the user cannot describe the task in words or numbers; however, he can readily provide examples of how the world would look like if it were solved.

Thus, Google AI suggests an alternative, example-based control, which aims at teaching agents how to solve new tasks by providing examples of success. This is termed as **recursive classification of examples (RCE)**. It does not rely on formulated reward functions, distance functions, or features. It instead just uses the examples of success. RCE performs better than the prior approaches based on [imitation learning](https://arxiv.org/pdf/1811.06711.pdf) on simulated robotics tasks.

Summary: https://www.marktechpost.com/2021/03/31/google-ai-proposes-a-machine-learning-algorithm-for-teaching-agents-to-solve-new-tasks-by-providing-examples-of-success/

Summary: [https://www.marktechpost.com/2021/03/31/google-ai-proposes-a-machine-learning-algorithm-for-teaching-agents-to-solve-new-tasks-by-providing-examples-of-success/](https://www.marktechpost.com/2021/03/31/google-ai-proposes-a-machine-learning-algorithm-for-teaching-agents-to-solve-new-tasks-by-providing-examples-of-success/) 

Source: https://ai.googleblog.com/2021/03/recursive-classification-replacing.html

GitHub: https://github.com/google-research/google-research/tree/master/rce

Related videos: https://ben-eysenbach.github.io/rce/",1833,LanguageTechnology,1
"Counting the length between syntactic elements / clauses This might be a little tricky, or even impossible without a previously tagged corpus but is there anyway to count the length between syntactic elements? 

For example, I may want to count the length between the matrix verb in a clause and up until the embedded verb as in 'I knew that last week I saw my friend' - so the count here would be 'knew that last week I' = 4. Is this possible at all using Python? Ideally I'd like to set more params but you get the idea.",522,LanguageTechnology,1
"Hawking Date Time Parser is Open-Source Now It's a great pleasure to announce our Natural Language Date Time Parser using Stanford coreNLP in the backend is open-source now. Do Check and Let us Know the Feedback.  
Github: [https://github.com/zoho/hawking](https://github.com/zoho/hawking)

Blog : [https://www.zoho.com/blog/general/zias-nlp-based-hawking-date-time-parser-is-now-open-source.html](https://www.zoho.com/blog/general/zias-nlp-based-hawking-date-time-parser-is-now-open-source.html)  
Tweet from [Stanford University](https://twitter.com/stanfordnlp) :  [https://twitter.com/stanfordnlp/status/1376914683127492614?s=20](https://twitter.com/stanfordnlp/status/1376914683127492614?s=20)

\#nlp #stanfordnlp #datetimeparser",734,LanguageTechnology,1
"How would you preprocess human names?  I have a dataset of names, full names which consist of three names, and they are labeled with their gender.

I am using Sklearn for this task. This is not a coding question I  guess, it theoretical. How I would represent or preprocess those names to predict their gender after the training of the model. I am aware of the many ways for this task such as vectorization and embedding but I am not sure if they are the best for data consists of names and gender.

I can't show any progress in my work since I am still in the data collection stage.

What I am trying to do is similar to what is going on in this paper: [https://arxiv.org/abs/2010.10852](https://arxiv.org/abs/2010.10852)

Appreciate the help, thank you",754,LanguageTechnology,1
"Interactive Research Graph for New Paper ""Approximating How Single Head Attention Learns"" I am sharing our [new interactive graph](https://crossminds.ai/graphlist/approximating-how-single-head-attention-learns-605e84761fb2cdbb7230ffcf/) project that maps out knowledge-paper connections for the paper ""Approximating How Single Head Attention Learns"" published earlier this month by Berkeley NLP. You can click the graph nodes to explore the most relevant papers with videos and key knowledge areas. This page also includes a list of related papers without videos. [You can try it here.](https://crossminds.ai/graphlist/approximating-how-single-head-attention-learns-605e84761fb2cdbb7230ffcf/)

I'd be curious to learn your thoughts. Do you find it helpful? How can we make it more useful for understanding new research papers/areas? Thank you!",843,LanguageTechnology,1
Med7: A Clinical Named Entity Recognition Model | Paper Explained | #NLproc | #spaCy ,85,LanguageTechnology,1
"[Q] How to truncate text to max. permissible tokens within Huggingface Pipeline? At the moment, I am having to manually correct for tokens in order to force limit them to 512 tokens -- the limit for distilBERT

Can some please guide me to a solution which works with Feature Extraction Pipeline?

`tokenizer = AutoTokenizer.from_pretrained(""distilbert-base-uncased"")`

`model = AutoModel.from_pretrained(""distilbert-base-uncased"")`

`model_use = pipeline('feature-extraction', model=model, tokenizer=tokenizer) `

`embedding = model_use( text )`



Thanks",555,LanguageTechnology,1
"BA Ling - Exchange Program Hei mates, I’m currently studying a BA in Linguistics and planning doing an international exchange program offered by my local faculty. I’m quite interested in the biolinguistics framework (yes, chomskyan), evolution of language experimental linguistics and language technologies. Since these fields are not deeply investigated at my university, I am looking for institutions whose programs include the topics mentioned above. Hope I can get a review/advise from any of you!! These are my options:

1. University College London - UK (not my best bc london is $$$$ but if u got any info i will rlly appreciate it)
2. Newcastle University - UK
3. Lund University - Sweden
4. Copenhagen University - Denmark
5. University of Bergen - Norway
6. Tübingen Universität - Germany 
7. University of Helsinki - Finland

Txs &lt;3",846,LanguageTechnology,1
"XLM Roberta - Maximum File Size Hi guys. I’m playing about with XLM Roberta Large XNLI and Python and I’m conscious it has a maximum file size of 512 tokens. 

Can anybody advice me how they are handling inputs that are larger than this? Any suggestions on code edit to only query 512 tokens to the model and ignore any more? Thanks all",336,LanguageTechnology,1
"Update on my project to debug and visualize Python code by using a combination of conventional static analysis tools and the attention based AI model. - Please ask me any questions! I am sorry if my post doesn't sound like an innovation to you, but would like you to take a look as it evolved out of a research project! I thought people in this subreddit might be interested :) Oh and yes! Anyone can use it!

The model has been trained on bug fixes in open source Github projects, and the tool itself is largely written in Python and hoping to help python coders!

The repository I visualized as an example is: [https://metabob.com/gh/galt2x/fastapi](https://metabob.com/gh/galt2x/fastapi?utm_source=Reddit&amp;utm_medium=Reddit%20Post&amp;utm_campaign=Reddit%20Post%20(r%2FLanguageTechnology)%203-29)

The program works best on Google Chrome, If you would like to check out the website, I linked it [here](https://www.metabob.com/?utm_source=Reddit&amp;utm_medium=Reddit%20Post&amp;utm_campaign=Reddit%20Post%20(r%2FLanguageTechnology)%203-29).",1046,LanguageTechnology,1
deep-significance: Easy and Better Significance Testing for Deep Neural Networks ,81,LanguageTechnology,1
Unit Test Case Generation with Transformers (Research Paper Walkthrough) ,73,LanguageTechnology,1
"NeMo Getting Started. Prototyping Conversational AI Application Hey all! Hope everyone is having a great Monday, I know I am. I want to let everyone know about a NVIDIA AI platform piece I use daily, NeMo.

[NVIDIA NeMo](https://github.com/NVIDIA/NeMo) is a toolkit for building new State-of-the-Art Conversational AI models. NeMo has separate collections for Automatic Speech Recognition (ASR), Natural Language Processing (NLP), and Text-to-Speech (TTS) models. Each collection consists of prebuilt modules that include everything needed to train on your data. Every module can easily be customized, extended, and composed to create new Conversational AI model architectures.

Conversational AI architectures are typically large and require a lot of data and compute for training. NeMo uses PyTorch Lightning for easy and performant multi-GPU/multi-node mixed-precision training.

[Cool collab notebook for getting started with NeMo](https://colab.research.google.com/github/NVIDIA/NeMo/blob/r1.0.0rc1/tutorials/NeMo_Getting_Started.ipynb) shows how we can construct a toy demo showing the ability to translate Russian audio files into English. That's what is so neat about NeMo is the pipeline is so diverse; we can translate these audio files into English, then if we wanted to, use BERT for the NLP task for the translated English as well. And so on and so on.

Anyways, I hope you enjoy this collab book, and to have a poke around NeMo afterwards.",1453,LanguageTechnology,1
"Error after adding Embedding Layer to my ANN Hello,

After adding embedding layer to my ANN I'm facing this issue. 

InvalidArgumentError:  indices[6,1] = -11 is not in [0, 6505)
	 [[node model_10/embedding_15/embedding_lookup (defined at &lt;ipython-input-125-fcde7e47b9e9&gt;:3) ]] [Op:__inference_train_function_11619]

Errors may have originated from an input operation.
Input Source operations connected to node model_10/embedding_15/embedding_lookup:
 model_10/embedding_15/embedding_lookup/10457 (defined at C:\Users\erdiv\.conda\envs\python-tf2.0\lib\contextlib.py:81)

Function call stack:
train_function",613,LanguageTechnology,1
"Embedding Layer to XGBClassifier I have an Embedding Matrix resultant from the Word2Vec model. How can I use that in my XGBClassifier? How can I add an Embedding Layer to my XGBClassifier? 
(I'm new to NLP, Plese correct me if I'm wrong)",237,LanguageTechnology,1
"Emotion detection question I have a data which is already labeled different type of emotions based on text context:

Some of the label types: 

    funny, anger, boredom, empty, fun relief, sadness, happines

when I use TFIDF alongside Logistic Regression to predict it gives me a shitty result.

Here is the code:

    ### HERE GOES PREPROCESSING FIRST
    and then
    
    #Encoding output labels 'sadness' as '1' &amp; 'happiness' as '0'
    lbl_enc = preprocessing.LabelEncoder()
    y = lbl_enc.fit_transform(data.sentiment.values)
    
    # Splitting into training and testing data in 90:10 ratio
    X_train, X_val, y_train, y_val = train_test_split(data.content.values, y, stratify=y, random_state=42, test_size=0.1, shuffle=True)
    
    # Extracting TF-IDF parameters
    tfidf = TfidfVectorizer(max_features=1000, analyzer='word',ngram_range=(1,3))
    X_train_tfidf = tfidf.fit_transform(X_train)
    X_val_tfidf = tfidf.fit_transform(X_val)
    
    # Model 3: logistic regression
    logreg = LogisticRegression(C=1,solver='lbfgs', max_iter=3000)
    logreg.fit(X_train_tfidf, y_train)
    y_pred = logreg.predict(X_val_tfidf)
    print('log reg tfidf accuracy %s' % accuracy_score(y_pred, y_val))

this is giving me a very bad result something like 0.2342355%

however, if I remove all types of emotions but leave only two. For example Happiness and Sadness,

it gives me a better result of 0.782342342%

&amp;#x200B;

Why is this?

How can i make it so that the model can predict not only 2 types of emotions but also other types f emotions such as *""Excitement, Fun, Relief""* and so on?

'",1609,LanguageTechnology,1
"Tunning GPT2 I have a specific task which i would like to fine tune a pre-trained GPT2 model. I know those models require a lot of data to train, but what about fine tuning? And how can i measure if my model is actually working for this specific task? I’ve tried to fine tune with about 1600 samples, but after the third epoch the model starts to overfit",354,LanguageTechnology,1
"AI based search engine { Startup idea } Obviously AI based search engine is the next step . Because of too much data available today on internet . We need this tool . I think it will be more interactive version of google . Where you can search based on size or similarity to other website , article or post . Or specify some part of text and find similar parts from other data sources .  To rate the quality of the search after search query . And there could be a lot more features . 

Idea is quite simple , it's all about execution . That's why I am searching for cofounders . Feel free to DM me  .  Potential investors are welcome too . Thank you for reading this ! Stay hungry , stay foolish )",697,LanguageTechnology,1
"Google AI Introduces a New System for Open-Domain Long-Form Question Answering (LFQA) Open-domain long-on answering (LFQA) form questions a fundamental challenge in natural language processing (NLP) that involves retrieving documents relevant to a given query and using them to generate a detailed paragraph-length answer. 

Recently, there has been significant progress in factoid open-domain question answering (QA). In this technique, a short phrase or entity is enough to answer a question, but significantly less work has been done in long-form question answering (LFQA). LFQA is an important task, primarily because it provides a testbed to measure generative text models’ factuality. But, the current benchmarks and evaluation metrics aren’t quite suitable for making progress on LFQA.

In a recent paper, [“Hurdles to Progress in Long-form Question Answering”,](https://arxiv.org/abs/2103.06332) that is set to appear at NAACL 2021, Google.ai present a new system for open-domain long-form question answering that utilizes two recent advances in NLP: One is the state-of-the-art sparse attention models, such as Routing Transformer (RT), which allows attention-based models to scale to long sequences, and other is the retrieval-based models, like REALM, that can facilitate retrievals of Wikipedia articles related to a given query.

Short Summary: [https://www.marktechpost.com/2021/03/27/google-ai-introduces-a-new-system-for-open-domain-long-form-question-answering-lfqa/](https://www.marktechpost.com/2021/03/27/google-ai-introduces-a-new-system-for-open-domain-long-form-question-answering-lfqa/)

Google blog: [https://ai.googleblog.com/2021/03/progress-and-challenges-in-long-form.html](https://ai.googleblog.com/2021/03/progress-and-challenges-in-long-form.html)

Paper: [https://arxiv.org/abs/2103.06332](https://arxiv.org/abs/2103.06332)",1856,LanguageTechnology,1
"Best description of attention? Where can I find the best description of the attention mechanism for NLP?  Blog post or video is good, but both are great!",153,LanguageTechnology,1
"Linguistic Data Science at University of Eastern Finland, how is it? Is there anyone who has any information about this program? I could not find much info online. I have a linguistics background with little computer knowledge, what is the state of the work prospects I might get after the completion of this program? What languages are prominently being worked upon here? Especially from the point of view of an international (non EU) student, is the university nice? Are the job prospects there for international applicants? 

Also, anything you could tell me about the place, accommodation and part time jobs that I might take to sustain myself while studying? I've heard Nordic countries are very expensive to live in. Any info would be helpful at this point.",763,LanguageTechnology,1
"Running Transformer model nearly kills my machine I have just a list of 43 texts. I have encoded them via [Bert Base](https://huggingface.co/bert-base-uncased) model from Huggingface, limited to 512 tokens.

```py
tokenized = df.text.apply(lambda text: tokenizer.tokenize(text)).to_list()
inputs = tokenizer(tokenized, is_split_into_words=True, truncation=True, max_length=512, padding='max_length', return_tensors='pt')
outputs = model(**inputs)  # this line fills my ram
```

I have 16 GB of RAM and another 16 GB of swap. When the third line is being run, the RAM usage goes beyond my machine's limitations. I have tried many different ways but didn't helped.

What could be done? My texts have long sentences, some of them may be consist of no sentences at all; could this be the problem?",792,LanguageTechnology,1
"Emotions extraction how to measure emotions while retrieving data from the text?

I know about the keyword-based methods, now i am more interested in the **Vector space model ( TF-IDF )** method. However, I can't find a working example of it, only research papers without any code.",281,LanguageTechnology,1
"Thoughts on MSc Language Science &amp; Tech at Saarland University? I was recently accepted to the program and I was wondering if anyone had any comments on it. Also, is it really free? Even for international students?
I appreciate any info you can provide. There’s not much info on their website. Thanks!",305,LanguageTechnology,1
"Computing Words per Error of an N-Gram tagger using NLTK in Python? I've got a question - I'm trying to evaluate some sets of ngrams using a training set and a data set from a corpus and write some statistics about it. I have to include two measures: accuracy (which is easy using the "".evaluate"" module in NLTK. But I also have to find out the ""words/error"" rate of it. So for example I'm:

- using the Brown corpus' subcorpus ""news""
- this supcorpus has 100,554 words in total 
- I've split the corpus in to a training set of 500 sentences, and the rest are the testing set (the suborpus has 4,623 phrases in all). 
- I have a partially filled chart with an example:
 -  The default ngram tagger has an accuracy of 30.41% (when comparing the training and testing sets) and a error rate of **1.4 words/error**

But I can't figure out how this 1.4 words/error was calculated. Anyone have any ideas? I think, and I may be wrong here, that it's calculated as a function of the rest of the corpus that is NOT accurate - that is, 100%-30.41% = **69.95%** of the corpus. That number is then related to the total number of words somehow I think?",1139,LanguageTechnology,1
"Feedback on post - Overview of reading comprehension systems I wrote an overview of QA systems that covers early heuristic methods to modern neural net methods. I would appreciate any feedback you give! Thank you :)

[https://www.wittwise.com/blog/reading\_comprehension/](https://www.wittwise.com/blog/reading_comprehension/)",326,LanguageTechnology,1
"Existing Datasets/ Corpus of Word Families Hello! Hope this question makes sense. I'm new to NLP and Python in general. I am using NLTK. I'm getting a Masters in Strategic Communications, and want to do some NLP for a couple of my projects. Specifically, I've been researching Political-based speech.

I want to evaluate pieces of text based on keywords in the text, and match those keywords to broader concepts. As a very rough example, ""Heartland"", ""Farmer"", ""Towns"" might all be classed under ""Small City"" while ""Army"", ""War"", ""Defense"", might all be classed under ""Strength"". I'm pretty open to what style of concept groups are being used (issue based ideas vs. economic based ideas, etc...)- I really just want to see if databases like this already exist so I don't have to start from square one. I'm gathering a lot of ideas based off of the Diction software program, if anyone is familiar. Ideally the concept groups would be housed in a dictionary, with the specific terms being listed as keys, but I'm not picky and if I have to re-format existing work, so be it. 

Also if you know of databases that exist that are not Political-based, I would be happy to see those as well. 

Any ideas or directions? I appreciate it!",1228,LanguageTechnology,1
"Looking for a Dataset study Hello everyone, I've been looking for an article for some time now that I can't for the life of me find anymore...

It was about an NLP dataset (SQuAD, SNLI, or something like that). I mean, you had a text and three theses and the system was supposed to recognise which thesis was covered. One was surprised how well systems performed on this. Then they found out that a large part of the theses was recognised by the wording alone, without the system even knowing the background text. Does anyone know the corresponding article on this?",565,LanguageTechnology,1
"Computing perplexity For language models, perplexity is defined as the inverse probability of the test set, normalized by the number of words. So far, so clear.  
What confuses me is the following section:  


""What we generally use for word sequence (...) is the entire sequence of words in some test set. Since this sequence will cross many sentence boundaries, we need to include the begin- and end-sentence markers &lt;s&gt; and &lt;/s&gt; in the probability computation. We also need to include the end-of-sentence marker &lt;/s&gt; (but not the beginning-of-sentence marker &lt;s&gt;) in the total count of word tokens *N*."" 

\- Speech and Language Processing. Daniel Jurafsky &amp; James H. Martin.   


Why do we not include the beginning-of-sentence marker?",767,LanguageTechnology,1
"Doing an ML project and feel others would benefit from what you're doing? If you're doing an ML project and you think others would benefit from what you're doing, please consider sharing in Learn Data Science with Expert Guidance: [https://discord.gg/Gg3EJrn3](https://discord.gg/Gg3EJrn3) I created a place to share my project publicly for others and I came to realize that people entering data science really need expert guidance, so this server has a focus on bringing together experts and novices so that new entrants can build relationships with more experienced data scientists and feel confident in what they're studying and working on. Consider joining if you feel you have something to teach",700,LanguageTechnology,1
"Extract name:value relationships from plain text I have a microservice that receives plain-text messages from an external API like:

""... You have received 1254.56 from Thomas Paine. Transaction id FGR412512 at 11:02 ...""

Using Regex I can get:

`double amount = 1254.56;`

`String transactionId = FGR412512;`

`String time = 11.02;`

However, I would like to make my code more resilient to possible changes in text to, say, ""... at 11.02, Thomas Pain sent you 1254.56 ... "" in the future.

Is there a Natural Language Processing library or framework that I can use to extract these relationships and convert them to variables like I do with Regex?",649,LanguageTechnology,1
"Current Sota for Multiclass Text Classification? Does anyone know where i can find data on the best performing multiclass text classifiers? [This](https://github.com/sebastianruder/NLP-progress/blob/master/english/text_classification.md) is the only info i could find and it seems it hasn't been updated since 2019.

I'm looking to use one of these for 3 class sentiment classification, negative, neutral, positive. Looking for data comparing the likes of:  

Mpnet

Electra

RoBERTa

BERT

ALBERT

Or any other better models i haven't heard of. 

&amp;#x200B;

On a side note i see a lot of benchmarks such as SQUAD have ensembles of 2 or more models. How is this done? Do they get predictions from both and then take the highest output vector score between the two of them as the prediction?",793,LanguageTechnology,1
"SOTA for Topic Modeling Hi everyone,

Does anyone know what is the state of the art for Topic Modeling and what kind of models do they use for production at Facebook, Amazon, Google etc.? I found a couple of recent papers on NTMs, but not sure how well they work and how well they would scale.

There are many approaches that are quite popular (like LDA, Neural Topic Models, topic models + BERT etc), but I was quite interested in something that is quite scalable for large datasets and is easy to use for production. Also, majority of the models are unsupervised and used on ""exploratory"" basis, is there something that you would recommend when looking for a particular narrative e.g. ""PC Gaming"" or something that will give you ""pure"" topics?

Any comments appreciated, thanks!",780,LanguageTechnology,1
"Libraries/tools to determine if same authorship of short text I'm fairly new to OSINT and would like to try and detect similarities in two different short-paragraph texts that I think are coming from the same author.

For example, a person or bot sends an English-language text message with about 100 words. Then I get another text message with about 150 words, but the words used and the abbreviations are common (e.g. ""St."" with a period used for street).

Is there an NLP library/tool that you can recommend that I can use to analyze 2 text corpus to get a percentage confidence that the two texts were created by the same author (authorship identification?)? Sort of like a plagiarism detector or a library/tool that can check for semantical identicalities (not sure if the manner of use of period, commas, etc. are also detected).

I would love to use a library/environment that uses python but if not, please suggest any other (even if its a hosted website that allows you to paste in texts to compare).

Thanks!",1018,LanguageTechnology,1
"Goodreads user interaction dataset Hi All,

I came across this exciting data set, follow the link: [https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home?authuser=0](https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home?authuser=0)

I want to try my hand at user interaction data for building a book recommendation system, but its too big for my machine, even Google Colab and Kaggle kernels. Anyone with ideas how to go about it?

&amp;#x200B;

cheers",458,LanguageTechnology,1
"Named Entity Recognition (Novice here)
What's the best method for NER? What do big companies like Google use? Can those methods used by big tech be used in a small research project?

 I am thinking about taking the most common /or the most efficient NER system used for English NLP and applying that to my native language (with modifications, off course). To see how it well it performs on my language.

And then maybe further improve on it... :)",446,LanguageTechnology,1
"Uppsala University VS University of Tübingen Hello everyone,

My intention is to pursue a MA in computational linguistics/language technology in one of the two universities mentioned in the tittle.

Is there anybody who studies or have studied in one of the two programmes mentioned above? What are the pros and cons of each choice in terms of academic quality, international student's life etc?

Thank you in advance",417,LanguageTechnology,1
"Production-Ready Machine Learning NLP API with FastAPI and spaCy Hey,

FastAPI has been a nice addition to the Python ecosystem. In my opinion it makes API creation easier, and less error-prone. It also comes with great performances that make it perfectly suited for machine learning APIs.

The [NLPCloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=j80a8332-aaaf-11eb-bcbc-0242ac130002) API has been developed using FastAPI, so I thought it would be interesting to write a concrete article about how to set up an NLP API with FastAPI that is serving spaCy models for NER:

[https://juliensalinas.com/en/machine-learning-nlp-api-production-fastapi-nlpcloud/](https://juliensalinas.com/en/machine-learning-nlp-api-production-fastapi-nlpcloud/)

I'd love to have your feedback on this guys. Are you also FastAPI users? Did you notice caveats I'm not aware of? Or can you think of better tools for machine learning APIs?

Thanks!",941,LanguageTechnology,1
"Compare my word embedding models (Count based, PMI, SPPMI) I am going to build some models over wiki-dump dataset and then try to compare the results to WS353 (for word similarity). So, I need to **check** whether my understanding is correct or not. Firstly, I need to read the text from wiki file and tokenize it, so that I am able to build the co-occurrence matrix. I am going to build the co-occurrence matrix in 3 ways : based on count, based on PMI and based on SPPMI. Then, I am going to build the embedding matrix using SVD. So, after that, I can have the word embedding matrix and I can compare the results to WS353.

So, the way is correct ? Thanks",657,LanguageTechnology,1
"Computer Scientist wants to Parse Meaning from Language I want to create a database of written works that parses each work individually to determine subjects, authoritativeness, accuracy, intention and overall reliability.  I know this is a massive challenge and I'm looking for collaborators.  I think having a linguist or three onboard early is mandatory.  I can parse text all day long, but understanding how to classify words, phrases, sentences, etc is beyond my pay grade.",478,LanguageTechnology,1
"NLP model with more than one dataset Hi I am new on nlp and need little bit help. I would like to train a model for filtering comments

I want to filter: Spam, offensive and toxic political comments. Also I have three dataset: spam messages, offansive words and comments approval status (all website comment data ).

How can I train a model for filtering comments?

Can I use three models for for each separate transaction ?Would this be efficient?

Or can I solve this problem in a one model?

&amp;#x200B;

Thank you.",519,LanguageTechnology,1
"BERT MLM Hi everyone,

I was wondering about the masked word prediction task of BERT and how exactly it is carried out.

In the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf) the authors wrote that the masked tokens are used to predict the real tokens using a cross entropy loss. 

Furthermore they wrote ""\[...\]  the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM.""

So from my understanding I would just use a fully connected layer with a softmax at the end of the BERT model to obtain a output matrix of shape `[vocab_size, input_length]` and then use the actual position of the masked words (15% words masked) to obtain the output matrix of shape `[vocab_size, input_length*0.15]`. This matrix can then be used for the (categorical) cross entropy loss.

Is my way of reasoning correct or am I missing something important here?

&amp;#x200B;

Thank you in advance!",951,LanguageTechnology,1
"stanza's Arabic language model doesn't tokenize sentences properly I'm trying to take Arabic text (e-mail messages, each of which are a few sentences long) and segment it all into their individual sentences.

It's not working. Most of the time I'm getting the entire e-mail message as my output, meaning it thinks the entire thing is one sentence, but really there are 3-5 different sentences in there.

Why is this not working? The stanza language models are working properly for like 7 other languages I've tried. It's not working for Arabic. Occasionally it does separate real sentences, but most of the time it just prints out 3-5 sentences as if it's one tokenized sentence. Does anyone know why the Arabic language model isn't tokenizing these e-mail messages properly?",775,LanguageTechnology,1
"Join an experienced data scientist in learning a new specialization I’m a data scientist with a background in image and natural language processing, and I’m stoked to get my feet wet with AI for audio. I’ll be taking on a paid audio processing project in the next couple months, so I decided to be very public about my learning process to allow others to gain the same knowledge and also observe my learning process. Plus we can all add a python project to our portfolios.

I've never tried sharing this process publicly before, so I'm going to start by sharing my journey on a few different mediums and I'll see what sticks. Please let me know if there's a platform you'd prefer I share this on. Here's how to follow along:

Twitter: ""@yacov\_lewis""

Whatsapp: [https://chat.whatsapp.com/HN3p0llOF9nJY0mQPX5WLe](https://chat.whatsapp.com/HN3p0llOF9nJY0mQPX5WLe)

Facebook group: [https://www.facebook.com/groups/dswithexperts](https://www.facebook.com/groups/dswithexperts)

Excited to share this journey!",1006,LanguageTechnology,1
"How to compare a template sentence with a real sentence ? Hi all, does anyone knows what is the best algorithm to do something like that:

I have a template sentence: 

**""declare a {memType:constant|variable} called {name:term} with value {value:term}""**

That when compared with real life sentences such as: 

1. declare a variable called A with value 32
2. declare variable called XB with value ZA
3. declare a constant called C with **the** value 42

Should all return true.

I tried using regex and it works fine until it does not, because of small variations in the real life sentence such as articles (see the article ""**the""** in sentence number 3).

My question is: is there a better way to do that comparison ?",720,LanguageTechnology,1
"question RoBERTa doc sentances. Hi everyone,

in the Roberta paper ([https://arxiv.org/pdf/1907.11692.pdf](https://arxiv.org/pdf/1907.11692.pdf) page 5) they say they use doc-sentences formatted inputs (512 tokens taken contiguously across multiple documents, documents are separated by a sep token). 

e.g. if I understand well: tokenized-doc 1 : 220 tokens tokenized-doc 2: 350 tokens

\-&gt; corresponding Roberta pre-training inputs: 220 tokens from doc 1 + sep token + 291 first tokens of doc 2 

my question is as follows:

Is there a way to obtain this format of input from a corpus of documents in a convenient way using pytorch or hugging face and a custom BPE tokenizer?",680,LanguageTechnology,1
"A linguist pursuing career im tech industry! Hi!

I hold a BA and an MA in English language and literature.
Since I finished BA in 2008, I worked in translation, interpreting, NGOs and Arabic language lecturing. 
Even though I worked with big comlanies, I feel I need to improve my career and focus on a specific path, after I recently decided to not pursue PHD and academa. 
TBH, I am astonished at how IT and tech industry is defining the future with tech careers growing steadily, as other careers are dying. Unfortunately, my math skills suck and programming is not my thing.

So I noticed jobs posted as data linguist, QA linguist, linguist engineer and linguist posted in tech companies such as Apple and Amazon, where some of Arabic language major firends I know landed similar jobs.

I think these positions are in high demand, where I can be paid well, and work in something I enjoy! 
Esp. that I am fluent in English and a native Arabic speaker. 

Do I need to take courses or internships to imorove my chances to get these jobs? What sort of courses? Provided by which platforms?
Any advice on writing cv and preparing for interviews? 

And is it late too late make such a change in my mid career with no experience in tech industry? 

Thank you so much for for your advice, it means a lot to me!",1307,LanguageTechnology,1
"What's the point of pre-trained tokenizers? I've come across the idea of pre-trained tokenizers, but I'm struggling to understand why and when these would be useful. Isn't it better if you train a tokenizer on your own training data? What if the pretrained tokenizer has only a small vocabulary or is domain-specific? Wouldn't that be more detrimental to your model?",366,LanguageTechnology,1
SpaCy Loss Reduction I've been training my model on a huge corpus. Loss right now is pretty high (100000ish) and I noticed that it does drop when I increase the number of iterations. I wanted to know is there something I'm missing out on or if there's any way to find out the optimal number of iterations without overtraining.,326,LanguageTechnology,1
"EleutherAI releases gpt-neo models trained on GPT3, GPT2 ",57,LanguageTechnology,1
University of Helsinki language technology professor Jörg Tiedemann has released a dataset with over 500 million translated sentences in 188 languages ,151,LanguageTechnology,1
Be patient ,11,LanguageTechnology,1
"Interpreting the topic of a streaming text-based group chat in real-time. Is this possible?  

This may be a simple question for those with a better understanding. I’m trying to figure out if it is possible to use NLP to describe the topic of conversation in a group chat (text). For example, people in a chat are giving advice on surfing for beginners, and using machine learning we would be able to generate something like ‘learning to surf’ or ‘beginner tips for surfing’. My question for you all is (1) is this possible in real-time for an ongoing chat where the topic of conversation evolves/changes and (2) if possible, how specific can the generated topic be? Would it just be ‘surfing’ or can it be more specific such as ‘advice on learning to surf for beginners?

Any help would be much appreciated! Even a resource or guidance would be great if this is too simple of a question. Just haven’t been able to find the answer on google searches.",950,LanguageTechnology,1
"How does Reddit show ""Trending Today"" topics? What algorithm does it use? The trending Today topics show up when you click search bar, and below the recent searches, appear the Trending topics. I would like to know how reddit clusters these sub-reddits, how it ranks various topics, and how it identifies various topics in the first place. The names of the algorithms used would be helpful. Thanks in advance!",409,LanguageTechnology,1
"John Snow Labs Spark-NLP 3.0.0: Supporting Spark 3.x, Scala 2.12, more Databricks runtimes, more EMR versions, performance improvements &amp; lots more ",152,LanguageTechnology,1
"Masters in Voice Technology (Europe) online webinar this Wed/Thursday There’s also a Voice Tech MSc at the University of Groningen (in the Netherlands) which could be interesting for you

The basics:

- It is only one year
- There is no NLP/NLU — only speech synthesis and speech recognition
- It is  very interdisciplinary, students from Linguistics, CS, AI, digital humanities, etc. are welcome, so long as they are not scared to learn programming!
- It is an English language program


Check it out here 

https://www.rug.nl/masters/voice-technology/

There are two (free!) online webinars next week, if you want to lean more! Register at the links below to get the link to the event. 

- March 23rd: https://www.rug.nl/cf/campus-fryslan/cf-masters/events/online-master-week-voice-technology
- March 24th: https://www.rug.nl/cf/campus-fryslan/cf-masters/events/online-master-week-voice-technology",899,LanguageTechnology,1
A Directory of Online Newspaper Sources for 70+ Languages ,58,LanguageTechnology,1
"[Article] Survey of Visual Question Answering A task that has grasped the attention of the AI community recently is that of visual question answering.

Visual question answering involves answering questions about images in natural language. It is an interesting problem, as it combines aspects of both computer vision and natural language processing.

This article explores the problem of visual question answering, different approaches to solve it, associated challenges, datasets, and evaluation methods. Topics such as image featurization, question featurization, joint feature representation, and answer generation will be explained, along with a survey of recent research efforts tackling each of these problems.

Article link: [https://blog.paperspace.com/introduction-to-visual-question-answering/](https://blog.paperspace.com/introduction-to-visual-question-answering/)",877,LanguageTechnology,1
Handwritten Recognition and Analysis ,37,LanguageTechnology,1
Help understanding a NLP methodology in a research paper ,57,LanguageTechnology,1
"Kindly give constructive feedback on my first NLP blog post. Hi Guys,

I am getting started with my blogging journey in NLP and written my first blog post. Would really appreciate it if you guys give constructive feedback on content /quality-wise so that I can improve and provide more useful content to the NLP community. Thanks!!

[https://raghavakotala.medium.com/how-to-choose-embedding-effectively-for-task-specific-use-cases-a-developer-notes-f60f707ac162](https://raghavakotala.medium.com/how-to-choose-embedding-effectively-for-task-specific-use-cases-a-developer-notes-f60f707ac162)",591,LanguageTechnology,1
Ex2: Neural Data Augmentation via Example Extrapolation (Research Paper Walkthrough) ,85,LanguageTechnology,1
"Integrating Ray Tune, Hugging Face Transformers and W&amp;B ",60,LanguageTechnology,1
"Where can I find a dataset of annotated Arabic to Arabizi dataset? I am currently working on an NLP project that deals with dialectal Arabic. Resources are scarce for most dialects, so I want a dataset that has annotated words from Arabizi to Arabic in whichever Arabic dialect. Problem is I don't speak Arabic, so I can't create the dataset myself.

Ps: Arabizi is Arabic written in latin characters.",401,LanguageTechnology,1
"Superior tools than Gensim's Similarity module We  have a project that with a given of arbitrary number of phrases, we  want to find to best fitting document in a bunch of documents.

Consider this as, with a given set of phrases of skillsets, we want to find the best candidate for a job.

So Gensim's Similarity module seems like a good fit for this problem, especially [soft cosine similarity](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb) checking. But inside I can't get comfortable, because transformers are very popular lately.

I  can't be sure if Gensim's similarity is the best fit but I'm not an  expert on transformers either to tell how it can be done with  transformers or do I even need them. Any suggestions on this? Thanks.",794,LanguageTechnology,1
A Transformers Tutorial in Plain English To Get You Thinking ,61,LanguageTechnology,1
"NLP Pipeline Completely on the GPU Hey all, I had to take a day off from posting our DSotD. I think sometimes the amount of information we through daily at everyone is exhausting, and it's better to not overload and instead digest.

So today we have an [awesome blog post](https://forums.developer.nvidia.com/t/nlp-is-not-just-a-domain-solved-by-deep-learning-this-is-also-in-the-ml-wheelhouse/165337?u=kasmith), which explains how we can do an entire NLP pipeline on a GPU... creating incredible fast lightening speeds for NLP solutions. Now, this is not diving into the transformer craze like BERT or GPT, so don't get your hopes up (though you can see easily where those models can be placed in the pipeline).

What's interesting is the majority feel that Deep Transformers are the way to solve all NLP, but it's not always the case. [Here](https://www.kaggle.com/cdeotte/rapids-cuml-tfidfvectorizer-and-knn) is an example of a Nvidia Kaggle Grandmaster (and absolute Kaggle tank!) using the described pipeline above to Kaggle's Shopee - Price Match Guarantee, determining if two products are the same by their images and product description. Also, a even more streamlined Kaggle memory efficient book can be seen [here](https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700) .  


Let me know what you think, and we can get a great discussion going. Anyone want to vent about transformers being the solution to all NLP? *(laughing face emoji, I think of Walid Saba and his many posts on LinkedIn. Give him a follow, it's amazing pure linguist information).* ",1578,LanguageTechnology,1
"Typo correction using NLP 
Hello everyone! We are studying NLP in the university, and I have been given a task to implement a typo correction using NLP. Are there guides and examples that could help me in my work?

Also wonder about NLP-based punctuation and grammar correction.",278,LanguageTechnology,1
"What Are Some Open Source NLP Framework Pipelines For QA Task I am trying to build a QA pipeline. My end goal is that :  
There are total X documents in database. given a query fetch top Y documents using some fast model (tfidf/bm25 etc) where Y is a subset of X. Then run a deep learning model (bert, longformer etc) to fetch and rank Z number of documents where Z is a subset of Y. 

I am using haystack and although I heard good things about it but for some reason its not working for me. its slower than my own pipeline, its retriever (the first/recall module which runs keyword model) is buggy (returning duplicate documents even though my data did not have any duplicate). 

So I was wondering if there are any other tried and tested frameworks that you guys know of. 

Thank you in advance.",797,LanguageTechnology,1
Build ASR for Any Language with Hugginface Transformers ,56,LanguageTechnology,1
"NLP in Finance Hey guys, I have been following this sub for quite some time now and I am always excited to learn about the new technologies that people post here. I wanted to some advice/guidance on what topics are there in NLP that are related to Finance. I obviously know about stock prediction and sentiment analysis of financial documents but I wanted to know if there are any other ideas/tasks that I could pursue for my undergraduate thesis. I have been reading papers on Finance related NLP tasks but I have been unable to narrow down a topic for which I can start collecting data and formulating potential models. Could you guys please help me out?",656,LanguageTechnology,1
"Anyone know where I could find a dataset that contains song lyrics and genre tags? I’ve been searching around but haven’t had much luck on kaggle, anywhere else i could look?",174,LanguageTechnology,1
Open-source libraries for machine translation ,46,LanguageTechnology,1
"Introduction to Named Entity Recognition (NER) Hey!

I made a quick article about Named Entity Recognition (NER). What is it and why is it a useful NLP technique?

[https://nlpcloud.io/nlp-named-entity-recognition-ner-api.html](https://nlpcloud.io/nlp-named-entity-recognition-ner-api.html)

Hope some of you will find it useful.

If you have any comment, please feel free!",373,LanguageTechnology,1
"Corpus Specific Vectors with BeRT I want to create BeRT embeddings for a corpus representing a specific subject or topic. I know how to extract embeddings from the general BeRT model and how to work with those embeddings, but I'm curious to see if I can finetune BeRT on a corpus and then extract embeddings that are more attuned to the subject that corpus represents.

Would this involve creating my own language model by applying transfer learning to BeRT?

Basically, I want to know if I can use BeRT to create/fine-tune custom embeddings similar to how this [tutorial shows how to create custom W2V embeddings using gensim](https://machinelearningmastery.com/develop-word-embeddings-python-gensim/).

Thanks.",712,LanguageTechnology,1
"Recommendations for structure for AIML + Python bot So I am trying to make a rules based bot using AIML + Python that can perform task like fill in forms, execute queries, and do a couple simple things like look up the time.

I have trouble finding the docs to the Python-AIML library but the best tutorial is [here](https://www.devdungeon.com/content/ai-chat-bot-python-aiml) 

To execute code from python you put it in as conditional statements inside a while loop, you can see the example for quitting and saving the brn file in the example. I have two questions on how I should organize and structure my code:

1. What would be the best way to structure the following task: 
Match the correct user inputs to run specific task? I can do this by using regex to match words or spacy's matching system for matching intents using cosine similarities. 

2. Create dialog trees for filling in forms? How do I structure file and code for a dialog tree where a user has to answer questions for a specific input like a query or filling a form? Adding a bunch of code inside the while loop can get messy.",1097,LanguageTechnology,1
"When I vectorize words, what are vector dimensions actually? Word co-occurrencies? What are these hundreds of dimensions?",121,LanguageTechnology,1
"[D] What is currently the best SENTENCE level tokenizer ? Hello ! Spacy isn't that good for that, nltk works but it's quite old. What do you use for sentence tokenization in english ?",183,LanguageTechnology,1
Do ASIC and FPGA have any use in NLP? ,38,LanguageTechnology,1
"Automatic poll generation with transformers similar to guestion generation? Hi,

I would like to generate questions out of a text corpus. However, I can only find models (based on T5) that generate exam like questions.

Does anybody know question generation models that create opionion questions that are useable for polls? e.g. ""How do you feel about NLP?""  


Thanks in advance!",380,LanguageTechnology,1
"AI app to help you read your book smartly Greetings folks,

4 months ago, I started working on a ***machine learning based website which helps readers***

***find exiting content from a book*** or a pdf file without reading all of it.

And now after working hard on it for the past few months, I have come up with a

prototype version. You can upload any pdf and ask questions to get to the interesting part quickly.

Here is the link :

[https://read-what-you-need.firebaseapp.com/](https://read-what-you-need.firebaseapp.com/)

I hope the app saves your time, and fuels your love for reading.

Have a great day ahead!

**ps: here's an account to try the app quickly, set up just for our reddit users**

username: reddit

password: reddit2021

Enjoy !",752,LanguageTechnology,1
"Healthcare NLP Summit, April 8-9 Those in the NLP community interested in building language understanding applications used in the healthcare industry should check out this NLP summit event. Learn from and engage with leading experts and a growing community in the space. 

The tickets are free.",295,LanguageTechnology,1
"BERT pre-training with only masked words Hi everyone, 

i am new to the world of NLP. For an upcoming project I would like to use BERT in order to do sentiment analysis with just the headlines of news articles. But BERT has two training tasks 1. Predict masked words and 2. Next sentence prediction. But since I will use only the headlines of an article the second training task becomes obsolet. 

My question : Is it possible to just train BERT with the 'predict the masked words' task ?

&amp;#x200B;

And maybe an off-topic question: Do you know how difficult it is to make a tokenizer from scratch ?

&amp;#x200B;

Thank you in advance!",640,LanguageTechnology,1
"My side project: Cloud GPUs for 1/3 the cost of AWS/GCP \[cross posting from /r/MachineLearning\]

I’ve just finished building a little side project of mine - [https://gpu.land/](https://gpu.land/).

**What is it?** Cheap GPU instances in the cloud.

**Why is it awesome?**

* It’s dirt-cheap. You get a Tesla V100 for $0.99/hr, which is 1/3 the cost of AWS/GCP/Azure/\[insert big cloud name\].
* It’s dead simple. It takes 2mins from registration to a launched instance. Instances come pre-installed with everything you need for Deep Learning, including a 1-click Jupyter server.
* It sports a retro, MS-DOS-like look. Because why not:)

I’m a self-taught ML engineer. I built this because when I was starting my ML journey I was totally lost and frustrated by AWS. Hope this saves some of you some nerve cells (and some pennies)!

The most common question I get is - how is this so cheap? The answer is because AWS/GCP are charging you a huge markup and I’m not. In fact I’m charging just enough to break even, and built this project really to give back to community (and to learn some of the tech in the process). 

AMA!",1123,LanguageTechnology,1
"preprocessing reddit submissions in python (html tag stripping etc...) Hi there, 

I am preprocessing some reddit submission downloaded from Pushshift, which I'll later feed to a Bert sentiment encoder.

Is there any predefined pipeline (possibly in Python) for which preprocessing steps to perform (e.g., stripping html tags, etc.)?

Thank you in advance!",356,LanguageTechnology,1
"Tutorial on how to create categorical variables based on integers and numerical ranges Hey, I've created a tutorial on how to create categorical variables based on integers and numerical ranges using the R programming language: [https://statisticsglobe.com/create-categories-based-on-integer-and-numeric-range-r](https://statisticsglobe.com/create-categories-based-on-integer-and-numeric-range-r)",396,LanguageTechnology,1
"Extracting related keywords from a specific topic/class Hi all,

With only a little experience on NLP, I was wondering if you guys could help me out.  
From a list of keywords/subclasses/objects, I have to extract all relevant elements that are closely related to a certain topic/class.

So, let's say we have the following topic/class as input: ""Human"".  
Then, ideally, I would like to extract the following keywords:

1. Synonyms, i.e. {Person, Individual, Child} (This should be relatively simple with NLTK/WordNet)
2. Subclass and/or profession, i.e. {Man, Girl, Boy, Grandma, Cousin, Chef, Professor}
3. Body parts, i.e. {Arm, Leg, Knee, Eyes}

One way to deal with this, is to hard code each different subclass or body part and obtain their synonyms using synset. But for obvious reasons, this is not desireable.

Therefore, I was wondering if there are any tips/sources how to effectively approach this operation (preferably in Python)?

Thanks in advance.",964,LanguageTechnology,1
"How to aggregate overall text sentiment from sentences? Since I am new to NLP, I would like to run something by this community. I appreciate in advance any pointers you may provide.

I am using Stanza to compute the sentiment of a tweet. As far as I can tell, Stanza can only compute the sentiment of one sentence at a time. So I devised the following simple computation to aggregate the sentiment of a whole tweet:

1. Get list of sentiments for each sentence as -1 (negative), 0 (neutral), and 1 (positive).

2. Sum up the sentiments.

3. If it’s &lt;= -1, the overall sentiment is negative, if it’s &gt;= 1, it’s positive, otherwise it’s neutral.

It’s a straightforward calculation where neutral sentences don’t affect the outcome, and whichever sentiment appears on the largest number of sentences will determine the overall sentiment of a tweet. Otherwise it’s a tie and the tweet is neutral. Does this make sense?

In case this method is not suitable, I was wondering if anyone had any suggestions on tried-and-tested methods to aggregate the sentiment of a text from its sentences.",1089,LanguageTechnology,1
"Do we really need to Dstill Language Models? Joint loss is all we need - Albert-Joint . Hi NLP folks,

I have create a new model focused mainly on smaller size with decent performance. And my experiments with GLUE proved that Joint Loss is all we need. With just  **14 million parameters and half the computation ( 6 layers instead of 12 layers** , we got a **GLUE score of 81.0**, which is **4 points** ahead of **DistillBERT** which has **60 million parameters** and **requires more training ( may be few days in a single GPU )**. The **Albert-Joint** model is even better than **MobileBERT.**

You can read more about that 

[https://www.reddit.com/r/LanguageTechnology/comments/m5j2hi/tftransformers\_state\_of\_the\_art\_faster\_nlp\_in/](https://www.reddit.com/r/LanguageTechnology/comments/m5j2hi/tftransformers_state_of_the_art_faster_nlp_in/)

For Code and Models

[https://github.com/legacyai/tf-transformers](https://github.com/legacyai/tf-transformers)

[Benchmarks](https://github.com/legacyai/tf-transformers/blob/main/src/tf_transformers/notebooks/tutorials/joint_loss_experiments/glue/glue_benchmark.png)

&amp;#x200B;

Please share feedback, comments, and raise if any issues in Github

                                                    glue_score
    Abert-Joint-layer_0	                        0.504815
    Abert-Joint-layer_1	                        0.682751
    Abert-Joint-layer_2	                        0.743739
    Abert-Joint-layer_3	                        0.773670
    Abert-Joint-layer_4	                        0.798181
    Abert-Joint-layer_5	                        0.810039
    Abert-Joint-layer_6	                        0.813973
    Abert-Joint-layer_7	                        0.822181
    Abert-Joint-layer_8	                        0.823916
    Abert-Joint-layer_9	                        0.823932
    Abert-Joint-layer_10	                        0.827925
    Abert-Joint-layer_11	                        0.821628
    DistillBert	                                0.776625",2009,LanguageTechnology,1
"Edinburgh MSc Speech &amp; Language Processing Hi everyone, 

I'm considering applying for this masters for September entry this year. I've had it on my mind for a few months, and really can't decide if I should focus on studying for my undergrad finals right now and wait to apply next year, or apply now so I can start asap. 

My questions: how long did it take you to write your personal statement? What did it contain? I would really appreciate any guidance! Were you interviewed?

And do you think this course will be much harder coming from a linguistics background rather than STEM? I haven't done any programming beyond tinkering in my spare time.

thank you - any help is really appreciated",699,LanguageTechnology,1
"An end-to-end NLP Pipeline Tutorial Applied to Landscape Restoration in Kenya An open-source project, where you can learn more about NLP pipelines while looking at an impactful use case.  The project was done in collaboration with a group at Stanford University, Code for Africa, WRI, and 40+ individual collaborators. 

From collecting and preparing more than 32.000 notices, legal entities, and court documents to build a web-based dashboard displaying land ownership in Kenya. The purpose of this project is to boost Kenya’s efforts to restore degraded land in an equitable way.  
https://omdena.com/blog/identifying-land-ownership/",635,LanguageTechnology,1
"Coreference resolution with spaCy 3.0 I was wondering if there are any solutions to using coreference resolution in spaCy 3.0 currently as neuralcoref has not been updated for it yet. I unfortunately can’t wait as I it is for a class project.

It is just a small part of the pipeline and I am using it to help with with a question generation system. So if there is a paragraph like “John went to the cleaners. He also ate dinner.” I could link He to John to create the question “Did John eat dinner”. I want to use 3.0 as it sounds like it has improved dependency parsing in English which is essential. Any help is appreciated thanks!",634,LanguageTechnology,1
"Free GPU alternatives to Google Colab for ML/DL Google Colab is an undeniably popular choice for free GPU-backed Jupyter notebooks for deep learning projects, but it's not without its drawbacks.

This article discusses alternate sources of free GPUs in cloud-hosted Jupyter environments:

[https://blog.paperspace.com/best-google-colab-alternatives/](https://blog.paperspace.com/best-google-colab-alternatives/)

Really curious what others' experiences with Google Colab are like, and if any of these issues resonate. Also interested in what the community has to say about the alternates presented here!",603,LanguageTechnology,1
"Optional Entities in Named Entity Recognition? I'm new to the field of natural language processing so please forgive me if I say something incorrectly. I've been reading information and running some basic experiments on the task of Named Entity Recognition. I've seen a wide range of definitions on what constitutes a ""named entity"" including the following list: [https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119268567.app5#:\~:text=BUSINESS%20ANALYTICS%3A%20%E2%80%9CIn%20data%20mining,phone%20numbers%2C%20companies%20and%20addresses](https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119268567.app5#:~:text=BUSINESS%20ANALYTICS%3A%20%E2%80%9CIn%20data%20mining,phone%20numbers%2C%20companies%20and%20addresses).

Since there is not exact agreement on what constitutes a named entity, have there been any studies on optional entities? 

In my basic experiments I wanted to build familiarity with some of the different tools that are available so I tagged some text data that I have and compared the results of the different tools. But as I was tagging I had some difficulty deciding whether certain noun phrases should be considered named entities or not. For the time being, I created an OPTIONAL tag and used that for entities where they fit some definitions of named entity but not others. I wrote a simple script that would score the list of named entities returned by each tool against my taggings where OPTIONAL entities are skipped - neither counted as correct or incorrect. And I can imagine real scenarios where I might not care whether a tool counts certain noun phrases as entities or non-entities. 

I was wondering if there are more sophisticated analyses of these kinds of scenarios? Has this been explored? If so, can anyone point me to resources where I can learn more about it?

Thanks!",1814,LanguageTechnology,1
"VAEs for text generation Hi, I'm very interested in Variational Autoencoders (VAEs) for text generation but struggle to find the current state of the art method. Does someone know a reference?

At the moment all I can find for basically any nlp task are these huge uninterpretable models from Google. Is any research towards anything else dead?

Is there an example of transformers being used for the encoder-decoder parts of a VAE?",432,LanguageTechnology,1
"With GPUs, K-nearest Neighbor Algorithm Crosses the Finish Line When Others are in the Starting Blocks Good morning all! I'm going to try and keep up these Data Science of the Day posts. I think they are fun to share and the feedback has been great so far.

Yesterday I talked to [u/drekalo](https://www.reddit.com/u/drekalo/) in my [r/datascience](https://www.reddit.com/r/datascience/) [post](https://www.reddit.com/r/datascience/comments/m6axq4/embed_your_sql_query_into_your_python_code_and/)(I think might have gotten removed) but I posted it in a few other subs too, here is the [post](https://www.reddit.com/r/MachineLearning/comments/m6b047/p_embed_your_sql_query_into_your_python_code_and/) in [r/MachineLearning](https://www.reddit.com/r/MachineLearning/). We talked a bit about RAPIDS, which is an Nvidia library for accelerated data engineering/science (it's quit awesome). Today I want to shed light on another [RAPIDS](https://rapids.ai/) ability and that's doing classic ML algorithms **FAST**. This article talks about K-Nearest Neighbors (KNN) probably the most well known ML algorithm there is. One of Nvidia's Kaggle Grandmasters wrote this great [article](https://forums.developer.nvidia.com/t/with-gpus-k-nearest-neighbor-algorithm-crosses-the-finish-line-when-others-are-in-the-starting-blocks/168785?u=kasmith) on how RAPIDS accelerates KNN **600x** versus CPU. This speed up might be what many of you need to help get a benchmark on large or complicated data sets, OR get a medal in Kaggle! It's rudimentary, and not as attractive as a DL Algo like BERT, but you can use KNN for text classification to get that first step.

&amp;#x200B;",1660,LanguageTechnology,1
"Language Identification using XGBoost. Code for training and application of a language identification model. Trained on the WiLI-2018 database, the classifier achieves an accuracy of 85.97% on the WiLi test dataset for 235 languages. ",234,LanguageTechnology,1
"Why do we need a [SEP] token needed for the end of the input for a transformer model? I can understand if there are two inputs, like in next sentence prediction, where the inputs need to be separated. But why do we have a 2nd separator token at the end. 

I am especially curious about cases where there is a single input. And in that case, is a CLS token even needed?",368,LanguageTechnology,1
"Anyone have experience doing TTS with Mozzilla TTS or Ossian (especially for low-resource languages)? Apologies, I realize this is tangentially related to this subreddit, but the TTS and NLP community is small and (perhaps I am wrong) fairly overlapped. Just looking to hear about experiences using one over another. Trying to use this on very low resource languages with a very small linguistics team for commercial purposes, so apologies for being vague. We initially tried Festival/FestVox before realizing that calling that 'a hot mess' is a serious understatement.",569,LanguageTechnology,1
Confusion choosing between two CS PhD programs! Please help! 🙏 ,63,LanguageTechnology,1
"Finding the distance between two sentences that that share mostly the same words. I am trying to find the difference between the following examples:

&gt;I am writing this sentence on a Reddit thread.  
&gt;  
&gt;I am right in this sentence on a ready thread.

The output should be something like

&gt;I am --- this sentence on a --- thread

or

&gt;{""writing"" : ""right in"", ""Reddit"" : ""ready""}

I had a very uneducated idea of looping each word from one of the sentences against the other one. Then define a threshold of how many words to go forward in hope of finding the exact word in the second sentence (3-4 words for example) and give up if there is no occurrence. However this method falls short on many points.


Is this a known NLP task? If so, I would appreciate to know what is the keyword for this problem.

I am wondering if someone know/used/developed some algorithm/package/functionality for a similar task? I try to keep my development in Python, so I would appreciate Python suggestions as well as any algorithmic suggestions.


Thanks!

Edit: Wow people, thanks a lot! All of the responses were helpful and creative. I have found my solution and leaving this post here for others who are looking for similar solutions

Edit 2: Come on, guys. What's up with down voting all these people's comments? All these people were trying to help and I can tell they spend time on it. Please let's respect it, let's discuss if there is wrong information and let's be supportive to each other. We are here to share and grow, right?",1537,LanguageTechnology,1
"Facebook comments to build a language model Hello, fellow NLPers,  


I am currently working on a low-resource language that practically has no raw corpus whatsoever.  The language in question is an Arabic dialect and it's mainly used on social media or for chatting. Consequently, I was forced to collect data from public Facebook pages in order to build a raw corpus for such dialect. The purpose behind this is to build a general transformer language model (such as BERT), that can then be used/fine-tuned on other specific tasks. The issue I have is the legality and ethicality of publishing such corpus on a scientific paper. In short here are few questions that I need some help with:  
1. Is it legal/ethically correct to just publish the corpus as is for research purposes?  
2. If not, is it possible to instead apply some preprocessing techniques to the corpus in a way that makes this ethical?  
3. Are there any publications that you know of that have done something similar or related to collecting data from Facebook and so forth?  


Any information you can provide is very appreciated.

Thanks a lot and have a great day.",1137,LanguageTechnology,1
"Computational Linguistics study shows how Metaphors, Figurative Framings and Sentiments are changing in the Covid-19 discourse over the course of the Pandemic. The War-Terminology is increasingly literal with the onset of the BlackLivesMatter Protests in 2020. ",261,LanguageTechnology,1
"Embed Your SQL Query Into Your Python Code and Let It Rip on a GPU - Data Science of the Day *Okay! Thanks to /u/*[*themajesticcalf*](https://www.reddit.com/user/themajesticcalf) *for pointing out the google doc to me on the original post. This community is great, and thank you all for not doing crazy things to it.*

Back to the original post. It's great to get as much information and resources possible out to our community. Here at Nvidia, a cool thing we do is ""Data Science of the Day"" (DSotD). It's tips and information from subject matter experts to better equip anyone doing data science/machine learning to perform their job to the maximum potential.

Today's DSotD is about embedding SQL code into python to query tables at blazing speeds, with the help of GPUs. Click the link (and then the picture) to check out the blog post on how to perform this task. [Embed SQL into Python for Blazing Speeds](https://forums.developer.nvidia.com/t/embed-your-sql-query-into-your-python-code-and-let-it-rip-on-a-gpu/170506?u=kasmith) .

Also, don't forget to register for our **FREE** conference, [GTC 2021 FREE REGISTRATION](https://www.nvidia.com/en-us/gtc/?ncid=GTCS21-NVKASMITH) , to have access to a week of great talks and presentations that show real world scenarios/applications where data science/machine learning skills like these above can help solve complicated problems.",1384,LanguageTechnology,1
"tf-transformers : State of the art faster NLP in Tensorflow 2.0 . 80 % faster to existing TF based libraries. Hi NLP folks,

I have developed a library for NLP with different transformer architectures in Tensorflow 2.0 . The natural question will be, how it is different compared to hugging Face. Hugging face is amazing, no doubt in that, But the Tf 2.0 implementation of Hugging face is way slower compared to PT implementation. Which actually made me think why? 

The lack of proper serialization of models is the main reason. To do so, we actually needed to re-design the code. tf-transformers if serialization and more.

As per the benchmarks for **text generation using GPT2 and t5 models, it is 80 % faster than HF TF implementations and even faster than PT implementations.** All the codes + benchmarks + tutorials released. 

## Unique Features

&amp;#x200B;

* **Faster Auto Regressive Decoding** using Tensorflow2. Faster than PyTorch in most experiments (V100 GPU). **80%** faster compared to existing TF based libraries (relative difference) Refer [benchmark code](https://github.com/legacyai/tf-transformers/blob/main/tests/notebooks/benchmarks).
* Complete **TFlite** support for **BERT, RoBERTA, T5, Albert, mt5** for all down stream tasks except text-generation
* **Faster sentence-piece alignment** (no more LCS overhead)
* **Variable batch text generation** for Encoder only models like GPT2
* No more hassle of writing long codes for **TFRecords. minimal and simple**.
* Off the shelf support for auto-batching **tf.data.dataset** or **tf.ragged** tensors
* Pass dictionary outputs directly to loss functions inside tf.keras.Model.fit  
 using **model.compile2** . Refer [examples](https://github.com/legacyai/tf-transformers/blob/main/src/tf_transformers/notebooks/tutorials) or [blog](https://legacyai-org.medium.com/tf-transformers-f7722536ba61)
* Multiple mask modes like **causal**, **user-defined**, **prefix** by changing one argument . Refer [examples](https://github.com/legacyai/tf-transformers/blob/main/src/tf_transformers/notebooks/tutorials) or [blog](https://legacyai-org.medium.com/tf-transformers-f7722536ba61)

&amp;#x200B;

[https://github.com/legacyai/tf-transformers](https://github.com/legacyai/tf-transformers)

&amp;#x200B;

Please share your thoughts, comments and feedback .",2320,LanguageTechnology,1
"Complaint redressal system Hey guys! Im planning to develop a complaint redressal system. So I have continuously flowing in complaints. The aim is to identify a issue that is being written in majority of the complaints. Is there any paper/ article that can help me achieve this?

I have found many solutions, that go well with static data, that is when you have no Incoming data, and are computing on whatever's available, but here I have continuously incoming data, so I will need an appropriate ML/NLP algorithm to find the common issues faced in all the complaints.
Thanks in advance for your help",600,LanguageTechnology,1
Into NLP - Tokenization ,24,LanguageTechnology,1
"NVIDIA Grandmaster Series – Building World-Class NLP Models with Transformers and Hugging Face So I want to be more active in all the subs and let everyone know the best available material out there. NVIDIA has been steam rolling the Kaggle competitions using some subject matter expertise and the NVIDIA platform. Here are four stellar Kaggle Grandmasters from NVIDIA talking a short history of state-of-the-art NLP models and best practices using Hugging Face.

It's an hour long on no speed up, but legitimately a great source for anyone new or experienced in NLP.

[Building World-Class NLP Models with Transformers and Hugging Face](https://bit.ly/2Nq5yqs)",661,LanguageTechnology,1
"Are there other supervised ml tasks in NLP besides text classification? I’m doing some internet sleuthing but I’m mostly seeing text classification, are there other applications?",178,LanguageTechnology,1
"COMPARISION OF DUC DATASETS ACCURACY which Duc dataset gives good evaluation rouge score in text summarization? like is it DUC 2001 or DUC 2002,...?can anyone provide me a link to refer it.",189,LanguageTechnology,1
"How to parse a sentence by writing grammar rules in NLTK CFG ?  Below is the original code : 

 `import nltk`

&amp;#x200B;

`# flight grammar rules`

`flight_grammar = nltk.CFG.fromstring(""""""`

  `S -&gt; NP VP | VP`

  `VP -&gt; V NP | V NP PP`

  `PP -&gt; P NP`

  `NP -&gt; Prop | Det N | Det N PP`

  `V -&gt; ""walked"" | ""book"" | ""prefer"" | ""gave"" | ""want""`

  `Prop -&gt; ""Jack"" | ""John"" | ""I"" | ""Houston""`

  `Det -&gt; ""a"" | ""an"" | ""the"" | ""my"" | ""that""`

  `N -&gt; ""dog"" | ""bone"" | ""flight""`

  `P -&gt; ""in"" | ""on"" | ""by"" | ""with"" | ""to"" | ""through""`

  `"""""")`

&amp;#x200B;

`# make a recursive descent parser and parse the sentence`

`rd_parser = nltk.RecursiveDescentParser(flight_grammar)`

&amp;#x200B;

`#define first sentence`

`senttext = ""I prefer a flight through Houston""`

`#tokenize sentence by splitting on white space`

`sentlist = senttext.split()`

`# run the parse function on the tokenized sentence and print the tree strucutre`

`for tree in rd_parser.parse(sentlist):`

	`print (tree)`

It is able to parse the sentence mentioned in the code, but fails to do parse this sentence : """"Jack walked with the dog""

Modifying the rule to : `VP -&gt; V NP | V NP PP | V PP`, can parse that as well. However, I am unable to parse the following sentences : 

""John gave the dog a bone"" , ""I want to book that flight""",1340,LanguageTechnology,1
"Alternate approaches to TF-IDF? I'm trying to rank words in a corpus of political speeches. TF-IDF seems to work really nice to identify ""important"" words, much better than raw frequency at least.

I'm wondering if there are any alternate or similar techniques to TF-IDF to rank the importance of words in a corpus.

Thanks!",324,LanguageTechnology,1
"Best approach for automatic key word extraction Hi all,

I'm wondering if there are SOTA approach for automatic keyword extraction. I have following questions, I hope someone answer them.

1 are there any good data set to work on keyword extraction

2 what are the best ML and DL techniques used for it

3 are there any tools to prepare data for keyword extraction

4 is it possible to solve this problem using unsupervised approach

5 what are the key performance metrics to measure performance

Ps: my question is not limited to NER

Cheers",542,LanguageTechnology,1
Structured Nearest Neighbour Learning for Few-Shot NER | Research Papers Summary 012 ,85,LanguageTechnology,1
"Learning resources for chatbots / conversational AI? I'm looking to learn the theory, latest research, and development best practices around chatbots and conversational AI. I'm open to all mediums (e.g., books, lecture videos, courses, articles). 

So far I'm aware of the Jurafsky &amp; Martin [chapter](https://web.stanford.edu/~jurafsky/slp3/) on it. I know Rasa should have some material floating around somewhere, but my feeling is that it'll be too specific to the Rasa library. 

Any resources y'all have really enjoyed/found useful?",540,LanguageTechnology,1
"Is there a python based speaker diarization system you would recommend? I have audio files with two speakers and I want to have speech to text conversation. For this I plan on using Huggingface. But I also want to separate text from the two speakers so I need diarization as well.

Any tips or suggestions based on your experience so I don't make the same mistakes. 

I see pyannote and Bob from idiap as potential options but I haven't used them before.",454,LanguageTechnology,1
"When batch size = n, does this mean parameter updates are performed after processing n characters, words, or sentences? [mini-batch stochastic gradient descent] When performing mini-batch stochastic gradient descent in an NLP setting, what does the batch size represent? If I am training a character-level LSTM language model with batch size 100, does this mean I process 100 characters, 100 words or 100 sentences? 

&amp;#x200B;

Suppose it meant characters. And so I process 100 characters, average the gradients, update parameters, but what happens next? Do I re-initialize the model and input a new sequence of 100 characters sampled randomly from a corpus? Or do I just input the 100th to 200th characters that continue the first sequence? If the latter, at what point do I sample an entirely new sequence of characters from the corpus?",842,LanguageTechnology,1
"LARA implementation Hi all

I am wondering if there's any good implementation of latent aspect rating analysis (LARA). I googled it but surprisingly no libraries support it. I'm too dumb to go through paper and implement it myself. I don't even know how to prepare data for it.

Also now I'm curious how does the guys at Amazon do it?

Cheers",342,LanguageTechnology,1
"Seeking help in NLP assignment I am actively seeking help somebody who is good at NLP in finishing my assignment. The raw data is in an XML file. I need to build probably a LSTM.model. if you are interested, please ping me for more details. Thanks.",248,LanguageTechnology,1
DUC DATASET can anyone send me DUC dataset for text summarization??,67,LanguageTechnology,1
"MA in CompLing with a Linguistic background &amp;#x200B;

&amp;#x200B;

Hello everyone,

I am a senior student of English Language and Literature with a specialization in Linguistics. I am currently interested in pursuing a MA degree in Computational Linguistics but I am afraid that I am not qualified enough given that I lack basic programming skills.

I have already applied to the Language Technology MA degree in Uppsala University and I am planning to apply to both Stuttgart and Tuebingen Computational Linguistics programmes.

Do you think that is possible to be accepted with solely a linguistic background?

Do you have any other similar MA programmes in mind?

Thank you in advance",692,LanguageTechnology,1
"Systematically Exploring Redundancy Reduction in Summarizing Long Documents (Research Paper Walkthrough) Text Summarization is the task of shortening a given document while maintaining the most important information. In general, a good summarizer should generate a summary that is syntactically accurate, semantically correct, coherent, and non-redundant. While extractive methods tend to have better performance on the first two aspects, they are typically less coherent and more redundant than abstractive ones, where new sentences are often generated by sentence fusion and compression, which helps detecting and removing redundancy. This work talks about existing and also proposes new techniques to deal with redundancy in long document summarisation.

Paper walkthrough: https://youtu.be/GFUwKDGYkuI",805,LanguageTechnology,1
What are the ways to handle out of domain inputs for text classification? Out-domain random inputs unwantedly giving very high confidence value. What are the available ways to get rid of it? What to do when we have no negative class data or the negative class is too big to cover?,280,LanguageTechnology,1
"Will NLP have a good future even if we reach AGI? I am an aspiring NLP student and I had this bizarre or some may say amateurish question. I want to know whether this field of AI has good future or not. I know prediction about AGI is like asking when will Avatar sequels come. I just wanted to know if we achieved human level in language, will the field of NLP die or will it move towards Super Intelligence?

So in short I want to know whether NLP will have a good future in terms of research or job opportunities in this century. If this question sounded like non sense, ignore this post, or if you are interested in this question please respond. I am struck at this question for days, so please help me with this.",716,LanguageTechnology,1
"Date extraction from text code/API's Does anyone have experience with extracting dates from text? An example follows. Any pointers to code, papers, API's will be highly appreciated. I've tried out a few API's and tools already, and they did not work. Please reply only if you know that your tool/API of choice will work. Thanks!

Text: ""I signed contract with a vendor to provide a dance floor for my wedding. The Wedding was planned for March 21, 2020. The contract was signed in February.""

Desired output: ""March 21, 2020; February 1, 2020"".",544,LanguageTechnology,1
"[Tutorial] How to Implement Seq2Seq Models for Text Summarization with Keras Seq2seq models are advantageous for their ability to process text inputs without a constrained length. This tutorial covers how to build, train, and test a seq2seq model for text summarization using Keras. 

Article link: [https://blog.paperspace.com/implement-seq2seq-for-text-summarization-keras/](https://blog.paperspace.com/implement-seq2seq-for-text-summarization-keras/)

You can also run the full code on a free GPU: [https://ml-showcase.paperspace.com/projects/text-summarization-with-seq2seq-models](https://ml-showcase.paperspace.com/projects/text-summarization-with-seq2seq-models)

For more information on the theory behind seq2seq models (architecture, applications, and the data loading/processing steps for the model implemented here), check out [Part 1](https://blog.paperspace.com/introduction-to-seq2seq-models/). 

Comments and questions welcome!",942,LanguageTechnology,1
"Constraint Detection Hello,

I am new to NLP so please bear with me. I am wondering if there is a common solution/framework to extracting constraint phrases from a string.


Examples and what I wanted to detect:

Shirt not more than $10. (More than $10)

Topic published  Before January first.  (before January first)

Pants that are exactly $10. (exactly $10)

I did find a library called LexNLP but I cannot use this framework.

https://lexpredict-lexnlp.readthedocs.io/en/docs-0.1.6/modules/extract_en_constraints.html

Thanks in advance.",541,LanguageTechnology,1
"Beginner questions about NER model evaluation. Hi all! I'm new in NLP and still learning, so I'm sorry, this is a noob post. I'm a bit confused with all the information on the internet and I could really use some advice from people with experience.

The problem is that I haven't worked with other people's trained models yet. I get the process of training and evaluating my own ML/deep learning models, but I'm not sure how to proceed with this task and I don't have an expert hotline to ask for help (except for Reddit).

Task: For a course, I need to evaluate the performance of an NER ML model, and compare that to the performance of a deep learning NER model.

At my disposal I have:

* The training data used to train these models + a test and a dev set.
* A large labelled NER corpus of which parts were used to train both of these models.
* The deep learning model works with HuggingFace Transformers (which I'm completely new to, and I don't know if this changes anything).
* Both models have a command-line interface that provide bite-sized NER-tagged output when you load in a .txt file. (e.g.:  ""my name is Rob"" would return the token such as ""Rob"" + label ""B-PER"" for ""person"")

My current plan of action:

* Stripping the test data .txt file from all tags to get ""unseen data"" to test the trained models on.
* Let the command-line tools of both the deep learning model and the ML model generate labelled output on this test data.
* Calculate F1, confusion matrix, etc. on 1) the labelled output of these command-line tools and 2) the labelled ""golden standard"" test data set.

My questions:

* Main question: I don't need to retrain these models to evaluate them, right? Am I correct to assume that I can just work with the labelled output the models provide on the unseen test data set + the golden standard labelled test data, and calculate evaluation metrics on that?
* Is there a better way to evaluate these models beside metrics such as F1?  Can I also do other things to evaluate and compare these models?
* Is there a way to evaluate the performance of the models for each of the NER tags separately?

Thank you, any input is most welcome! ",2162,LanguageTechnology,1
"State of the Art Spelling Correction I am trying to build a really good spell corrector for our search engine. We have a lot of domain specific terms so using an off the shelf one is not going to work well. I found the Peter Norvig spelling corrector post but am having trouble finding more sophisticated (and also scalable) spelling correctors. I need a model that can smartly pick between a set of generated corrections, as the most common one with the lowest edit distance is often not the best (it's right about 65% of the time). I switched to using a contextual corrector based on bigrams which made a small improvement (68% accurate), but am looking for something better. The type of error affects the likelihood of a correction being right as some errors are more likely than others. There's also the issue of speed, as doing an edit distance algo is inherently slow. AFAICT this doesn't seem to be actively being researched, unless i am looking in the wrong places. Is this problem seen as 'solved' by the broader NLP community?",1036,LanguageTechnology,1
"Gradient symbolic Computation... Do you know concrete implementations? I'm reading a couple of papers by Smolensky and Goldrick on their Gradient Symbolic Computation and similar models unifying neural networks and symbolic computation. (Ex. [Cho, Goldrick &amp; Smolensky (2017)](https://faculty.wcas.northwestern.edu/matt-goldrick/publications/pdfs/GSC_LV_final3.pdf) or [Goldrick &amp; Smolensky (2016)](http://roa.rutgers.edu/content/article/files/1552_smolensky_1.pdf)
Really awesome! 

Does anyone of you know where to find a concrete example implemented in Python or R?",576,LanguageTechnology,1
"Gpt-2 simple isnt good enough for the chatbot I want. So I'm running gpt-2 simple in a google colab and I have two simple problems.

Firstly, the text generation speed is far too slow. With minimal input for the medium model I get around 12+ seconds to generate.

So to try and solve this problem I downgraded the model to 124M but the model wasnt capable of a basic conversation.

I need a model that's not only capable of generating text fast but is also just as effect as gpt-2 355M if not more effective.

I'm wondering if any of you may know a gpt-2 alternative or a way to customize gpt-2 simple to generate faster?",621,LanguageTechnology,1
"SpaCy VS Transformers for NER It seems that both spaCy and transform based models (like [https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)) are suite for good entity extraction in English. So it's hard for me to know which one I should use...

Do you have an opinion on this in terms of accuracy and performance ?

Thanks!",418,LanguageTechnology,1
On Generating Extended Summaries of Long Documents (Research Paper Walkthrough) ,80,LanguageTechnology,1
"Libraries/tools to determine if same authorship of short text I'm fairly new to NLP and would like to try and detect similarities in two different short-paragraph texts that I think are coming from the same author.

For example, a person or bot sends an English-language text message with about 100 words.  Then I get another text message with about 150 words, but the words used and the abbreviations are common (e.g. ""St."" with a period used for street).  

&amp;nbsp;

Is there an NLP library/tool that you can recommend that I can use to analyze 2 text corpus to get a percentage confidence that the two texts were created by the same author (authorship identification?)? Sort of like a plagiarism detector or a library/tool that can check for semantical identicalities (not sure if the manner of use of period, commas, etc. are also detected).

&amp;nbsp;

I would love to use a  library/environment that uses python but if not, please suggest any other (even if its a hosted website that allows you to paste in texts to compare).

Thanks!",1044,LanguageTechnology,1
Paraphrase Dataset for Slovak looking for a paraphrase dataset for Slovak. Any link ?,85,LanguageTechnology,1
"Sentiment Analysis with Transformers vs. Textblob Hi,

&amp;#x200B;

I'm pretty new to NLP and was experimenting with spacy to do serveral basic NLP tasks. Now I wanted to do some sentiment analysis.

First I tried Textblob, but it only works for English. After some research I thought using Hugging/Face Transformers might be a better solution.

What do you think is the best option for doing a sentiment analysis? Preferably I would like to use pretrained models.

&amp;#x200B;

Thanks!",488,LanguageTechnology,1
"Topic Modeling using Reddit jokes Has anyone ever tried to do NLP Topic Modeling using Reddit jokes? Currently I'm trying to figure out the best topics for reddit jokes but I'm having a hard time how as the scores using LDA, LSI and HDP are very low. Scores are only around 0.3, 0.2 and the words are just son, man, beer. It just doesn't make sense. My dataset contains jokes from the subreddit dadjokes, 3amjokes, antijokes, darkjokes and jokes.

Here's my code - [https://github.com/ZL63388/c6\_sprint4/blob/main/Reddit\_Joke\_Classifier.ipynb](https://github.com/ZL63388/c6_sprint4/blob/main/Reddit_Joke_Classifier.ipynb)",624,LanguageTechnology,1
"Topic Modelling (LDA) on DUC 2004 dataset Hi, Has anyone here has done topic Modelling on the DUC 2004 plain text dataset? 

I'm not having good results. If you don't know about the dataset, it is similar to a news dataset. 

Can someone guide me to a quality tutorial as i have gone through every video and article and it doesn't seem cover my problem.",353,LanguageTechnology,1
"Next models on nlpcloud.io? I'm asking the question here because I know that some of you liked the idea behind [nlpcloud.io](https://nlpcloud.io), so here's the question: which NLP models should we now add to NLP Cloud?

We initially proposed all the spaCy pre-trained models for NER and POS tagging, and we then heard a lot of users asking for transformer-based models. That's why we added the following transformer-based models last week:

* Facebook's Bart Large MNLI model, for **text classification**
* Facebook's Bart Large CNN model, for **text summarization**
* Deepset's Roberta Base Squad 2 model, for **question answering**
* DistilBERT Base Uncased Finetuned SST-2 English model, for **sentiment analysis**

We are now wondering which models we should add next. Should we add more transformer-based models? Or pre-trained models from another framework? What would you find the most useful for your own situation that we are lacking for the moment?

Thanks a lot!",974,LanguageTechnology,1
"Video introduction to the join functions of the dplyr package in R programming. The tutorial provides programming examples and explains the difference between inner, left, right, full, semi, and anti joins ",206,LanguageTechnology,1
"Semantic Search and Fuzzy string matching I know that bert has been used extensively for semantic search; getting similar documents, articles, etc. Word embeddings capture the context better; but is there a way to implement fuzzy string matching with bert's word embeddings, or is tfidf the better solution for that?",316,LanguageTechnology,1
NAACL 2021 results are out I've heard on twitter that NAACL 2021 final decisions are out. Anyone got the news yet?,114,LanguageTechnology,1
Paper Implementation Suggestions Trying to practice my NLP and was hoping for some good paper implementation suggestions. Any ideas? I could simply pick papers related exactly to what my research focuses on but I guess I am wondering what papers people think would provide general valuable skills.,297,LanguageTechnology,1
"Turing Award Winners Yoshua Bengio, Geoffrey Hinton, and Yann LeCun to Speak at GTC21  

Hey all, I'm an NVIDIA Senior Data Scientist and have a activate personal account on the sub here and across many ML/AI/DL sub-reddits. I think some of the best discussions and debates I've been apart of or read have come on these subs. I'd love to just throw in a plug here and have you all join NVIDIA virtually for our GTC conference this year. Like the title says, the ""granddaddy's"" of modern AI are going to be speaking at the conference! This is a great opportunity to get online and listen to some quality talks and science across many domains and product talks etc.

Here's a sign up link, I get zero commission for this, just want to share the love.[https://www.nvidia.com/en-us/gtc/?ncid=GTCS21-NVKASMITH](https://www.nvidia.com/en-us/gtc/?ncid=GTCS21-NVKASMITH)

Also here are some bullets of what GTC has in store.

* GTC is a **free** online conference and happening **April 12-16** with live sessions across the world.
* **Keynote** requires **no registration** and happens **Monday, April 12, at 8:30 AM PST**, with a second broadcast at **6:00 PM PST** for APAC audiences.
* The conference will have live webinars, on-demand sessions, posters. Connect With Experts, DLI (with a fee), and multiple panels that include industry pioneers, researchers, developers, start-ups, venture capitalists, and more.
* There is an amazing line-up of speakers including Gordon Bell award winners, AI pioneers, Oscar-winning artists, and thought leaders from every industry.",1564,LanguageTechnology,1
"Tokenizing / picking words out of non-english languages Greetings!

I'm a CompSci major interested in NLP - however I'm very new to NLP. I currently know Spanish and Japanese - and am making a tool that allows users to learn Japanese easier. It's very similar to the windows snipping tool, once you've made a selection, it uses OCR to scan a bitmap for text. My main question is: 

How can I tokenize sentences into individual words, when Japanese doesn't have spaces? 

In English we can simply split up sentences based on spaces, but Japanese text does not have this.

Example: この文にスぺースがありません。 (English: ""In this sentence, there are no spaces"")

The goal I'm working towards is being able to click each word within the sentence, allowing for the program to do SQL queries to a Japanese datasbe containing linguistic information. This will then display back to the user things like the word's meaning, and how to read it. I have the data and database part set up, it's specifically the process of breaking Japanese sentences into easily tokenized strings. 

&amp;#x200B;

Apologies for any mistakes I might have made in writing this, I'm still relatively new to NLP and Data Science in general!",1195,LanguageTechnology,1
"[Tutorial] Introduction to Encoder-Decoder Sequence-to-Sequence Models (Seq2Seq) Seq2seq models are advantageous for their ability to process text inputs without a constrained length. This tutorial covers encoder-decoder sequence-to-sequence models (seq2seq) in-depth. Topics covered include:

1. Seq2seq architecture
2. Applications
3. Implementing seq2seq for text summarization with Keras

Note that the third point only goes up until the data loading and text processing steps. Training and inference are covered in Part 2, which is coming out on Friday. :)

Article link: [https://blog.paperspace.com/introduction-to-seq2seq-models/](https://blog.paperspace.com/introduction-to-seq2seq-models/)

You can also run the full code on a free GPU: https://ml-showcase.paperspace.com/projects/text-summarization-with-seq2seq-models",829,LanguageTechnology,1
How to predict a word in a corpus given its description Idk what this task is called in NLP. But is there any dataset that allows me to do this?,144,LanguageTechnology,1
"Got Confused going through a documentation. I have a doubt someone explain please: we have seen this classic example of why we use bert and other sentence level embedding rather than glove or doc2vec etc. for sentence level task because the same word ""bank"" used in different context will carry different meaning.

But if we are using it in two sentence :
1) man went to bank to withdraw money.
2) man went to bank for fishing.

Wouldn't adding bank embedding with deposit for 1st sentence and fishing for 2nd sentence be well enough to differentiate between sentences...so why do we do this sentence embedding when avg. of word embedding are just good enough, I don't see this reason justify the meaning behind it well enough.",727,LanguageTechnology,1
"Custom Word Embeddings or Word2Vec Hi friends,

I have a huge documents(tickets) raised by users in company.But most of those are not more than 50 words and after preprocessing it came down to hardly 20-30 words
If I use Word2vec it is increasing the dimension to 300 
Still is it advisable to go for word2vec for proper relation between words or Custom word embeddings of less dimension",387,LanguageTechnology,1
"Question About ACL2018 Tutorial5 Hey guys!

Hope you are all well.

I found the attractive tutorial in ACL 2018 '*Beyond Multiword Expressions: Processing Idioms and Metaphors',* and am very interested in this topic about modeling metaphors.  
But it was unfortunate that the link provided in ACL2018 ([https://www.angl.hu-berlin.de/department/staff-faculty/other/kordoni/acl18\_tutorial)](https://www.angl.hu-berlin.de/department/staff-faculty/other/kordoni/acl18_tutorial)) was down. 

Is there anyone willing to share a copy or a reachable link?

Thank you in advance!",571,LanguageTechnology,1
Fun failure compilation from our gesture synthesis research ,60,LanguageTechnology,1
"How to fine-tune BERT with Spacy 3: tutorial Looking to train an NER model using your favorite transformer?

Checkout this new article [https://walidamamou.medium.com/how-to-fine-tune-bert...](https://walidamamou.medium.com/how-to-fine-tune-bert-transformer-with-spacy-3-6a90bfe57647?fbclid=IwAR0MrqwSjdlkJq3F58F_rGUqGRPHwaKnAzUyv-87Z6jzPtZMgRR2Bwfhqaw) on how to fine tune BERT transformer for NER using the newly released spaCy 3.

On a different note, we are looking for NLP experts to join our team (https://ubiai.tools), if anyone is interested please DM me!",563,LanguageTechnology,1
"NLU learning path Hi all,

Hope everyone is keeping safe in these tough times. I'm a noob in NLP related technologies, I'm working on a lot of projects to consolidate my learnings. I want to understand the lifecycle of process of making a chat bot for that I want to learn NLU concepts can anyone suggest me some good resource for that. I'll really appreciate it",362,LanguageTechnology,1
"Intent and Action Classification, analyze Chinese News and the Crypto market, train a classifier that understands 100+ languages, translate between 200 + languages, answer questions, summarize text and much more on NLU 1.1.3 # Intent and Action Classification,  analyze Chinese News and the Crypto market, train a classifier that understands 100+ languages, translate between 200 + languages, answer questions, summarize text, and much more in NLU 1.1.3 

## NLU 1.1.3 Release Notes
We are very excited to announce that the latest NLU release comes with a new pretrained Intent Classifier and NER Action Extractor for text related to
music, restaurants, and movies trained on the SNIPS dataset. Make sure to check out the models hub and the easy 1-liners for more info!

In addition to that, new NER and Embedding models for Bengali are now available

Finally, there is a new NLU Webinar with 9 accompanying tutorial notebooks which teach you  a lot of things and is segmented into the following parts :

- Part1: Easy 1 Liners 
  - Spell checking/Sentiment/POS/NER/ BERTtology embeddings
- Part2: Data analysis and NLP tasks on [Crypto News Headline dataset](https://www.kaggle.com/kashnitsky/news-about-major-cryptocurrencies-20132018-40k)
  - Preprocessing and extracting Emotions, Keywords, Named Entities and visualize them
- Part3: NLU Multi-Lingual 1 Liners with [Microsoft's Marian Models](https://marian-nmt.github.io/publications/)
  - Translate between 200+ languages (and classify lang afterward)
- Part 4: Data analysis and NLP tasks on [Chinese News Article Dataset](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/4_Unsupervise_Chinese_Keyword_Extraction_NER_and_Translation_from_Chinese_News.ipynb)
  - Word Segmentation, Lemmatization, Extract Keywords, Named Entities and translate to english
- Part 5: Train a sentiment Classifier that understands 100+ Languages
  - Train on a french sentiment dataset and predict the sentiment of 100+ languages with [language-agnostic BERT Sentence Embedding](https://arxiv.org/abs/2007.01852)
- Part 6: Question answering, Summarization, Squad and more with [Google's T5](https://arxiv.org/abs/1910.10683)
  - T5 Question answering and 18 + other NLP tasks ([SQUAD](https://arxiv.org/abs/1606.05250) / [GLUE](https://arxiv.org/abs/1804.07461) / [SUPER GLUE](https://super.gluebenchmark.com/))


### New Models

#### NLU 1.1.3 New Non-English Models

| Language | nlu.load() reference                                         | Spark NLP Model reference                                    | Type                  |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ | --------------------- |
| Bengali  | [bn.ner.cc_300d](https://nlp.johnsnowlabs.com/2021/02/10/bengali_cc_300d_bn.html) | [ bengaliner_cc_300d](https://nlp.johnsnowlabs.com/2021/02/10/bengali_cc_300d_bn.html) | NerDLModel    |
| Bengali  | [bn.embed](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html) | [bengali_cc_300d](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html) | NerDLModel            |
| Bengali  | [bn.embed.cc_300d](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html) | [bengali_cc_300d](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html) | Word Embeddings Model (Alias)    |
| Bengali  | [bn.embed.glove](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html) | [bengali_cc_300d](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html) |  Word Embeddings Model (Alias)|





#### NLU 1.1.3 New English Models

|Language | nlu.load() reference | Spark NLP Model reference | Type |
|---------|---------------------|----------------------------|------|
| English | [en.classify.snips](https://nlp.johnsnowlabs.com/2021/02/15/nerdl_snips_100d_en.html) |[nerdl_snips_100d](https://nlp.johnsnowlabs.com/2021/02/15/nerdl_snips_100d_en.html)     | NerDLModel |
| English | [en.ner.snips](https://nlp.johnsnowlabs.com/2021/02/15/classifierdl_use_snips_en.html) |[classifierdl_use_snips](https://nlp.johnsnowlabs.com/2021/02/15/classifierdl_use_snips_en.html)|ClassifierDLModel|




### New NLU Webinar
#### [State-of-the-art Natural Language Processing for 200+ Languages with 1 Line of code](https://events.johnsnowlabs.com/state-of-the-art-natural-language-processing-for-200-languages-with-1-line-of-code)


##### Talk Abstract 
Learn to harness the power of 1,000+ production-grade &amp; scalable NLP models for 200+ languages - all available with just 1 line of Python code by leveraging the open-source NLU library, which is powered by the widely popular Spark NLP.

John Snow Labs has delivered over 80 releases of Spark NLP to date, making it the most widely used NLP library in the enterprise and providing the AI community with state-of-the-art accuracy and scale for a variety of common NLP tasks. The most recent releases include pre-trained models for over 200 languages - including languages that do not use spaces for word segmentation algorithms like Chinese, Japanese, and Korean, and languages written from right to left like Arabic, Farsi, Urdu, and Hebrew. All software and models are free and open source under an Apache 2.0 license.

This webinar will show you how to leverage the multi-lingual capabilities of Spark NLP &amp; NLU - including automated language detection for up to 375 languages, and the ability to perform translation, named entity recognition, stopword removal, lemmatization, and more in a variety of language families. We will create Python code in real-time and solve these problems in just 30 minutes. The notebooks will then be made freely available online.

You can watch the [video here,](https://events.johnsnowlabs.com/state-of-the-art-natural-language-processing-for-200-languages-with-1-line-of-code) 

### NLU 1.1.3 New Notebooks and tutorials


#### New Webinar Notebooks

1. [NLU basics, easy 1-liners (Spellchecking, sentiment, NER, POS, BERT](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/0_liners_intro.ipynb)
2. [Analyze Crypto News dataset with Keyword extraction, NER, Emotional distribution, and stemming](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/1_NLU_base_features_on_dataset_with_YAKE_Lemma_Stemm_classifiers_NER_.ipynb)
3. [Translate Crypto News dataset between 300 Languages with the Marian Model (German, French, Hebrew examples)](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/2_multilingual_translation_with_marian_intro.ipynb)
4. [Translate Crypto News dataset between 300 Languages with the Marian Model (Hindi, Russian, Chinese examples)](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/3_more_multi_lingual_NLP_translation_Asian_languages_with_Marian.ipynb)
5. [Analyze Chinese News Headlines with Chinese Word Segmentation, Lemmatization, NER, and Keyword extraction](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/4_Unsupervise_Chinese_Keyword_Extraction_NER_and_Translation_from_Chinese_News.ipynb)
6. [Train a Sentiment Classifier that will understand 100+ languages on just a French Dataset with the powerful Language Agnostic Bert Embeddings](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/5_multi_lingual_sentiment_classifier_training_for_over_100_languages.ipynb)
7. [Summarize text and Answer Questions with T5](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/6_T5_question_answering_and_Text_summarization.ipynb)
8. [Solve any task in 1 line from SQUAD, GLUE and SUPER GLUE with T5](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/7_T5_SQUAD_GLUE_SUPER_GLUE_TASKS.ipynb)
9. [Overview of models for various languages](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/8_Multi_lingual_ner_pos_stop_words_sentiment_pretrained.ipynb)





#### New easy NLU 1-liners in NLU 1.1.3

####  [Detect actions in general commands related to music, restaurant, movies.](https://nlp.johnsnowlabs.com/2021/02/15/nerdl_snips_100d_en.html)


```python
nlu.load(""en.classify.snips"").predict(""book a spot for nona gray  myrtle and alison at a top-rated brasserie that is distant from wilson av on nov  the 4th  2030 that serves ouzeri"",output_level = ""document"")
```

outputs :

|                                               ner_confidence | entities                                                     | document                                                     | Entities_Classes                                             |
| -----------------------------------------------------------: | :----------------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| [1.0, 1.0, 0.9997000098228455, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9990000128746033, 1.0, 1.0, 1.0, 0.9965000152587891, 0.9998999834060669, 0.9567000269889832, 1.0, 1.0, 1.0, 0.9980000257492065, 0.9991999864578247, 0.9988999962806702, 1.0, 1.0, 0.9998999834060669] | ['nona gray myrtle and alison', 'top-rated', 'brasserie', 'distant', 'wilson av', 'nov the 4th 2030', 'ouzeri'] | book a spot for nona gray myrtle and alison at a top-rated brasserie that is distant from wilson av on nov the 4th 2030 that serves ouzeri | ['party_size_description', 'sort', 'restaurant_type', 'spatial_relation', 'poi', 'timeRange', 'cuisine'] |

####  [Named Entity Recognition (NER) Model in Bengali (bengaliner_cc_300d)](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html)


```python
# Bengali for: 'Iajuddin Ahmed passed Matriculation from Munshiganj High School in 1947 and Intermediate from Munshiganj Horganga College in 1950.'
nlu.load(""bn.ner.cc_300d"").predict(""১৯৪৮ সালে ইয়াজউদ্দিন আহম্মেদ মুন্সিগঞ্জ উচ্চ বিদ্যালয় থেকে মেট্রিক পাশ করেন এবং ১৯৫০ সালে মুন্সিগঞ্জ হরগঙ্গা কলেজ থেকে ইন্টারমেডিয়েট পাশ করেন"",output_level = ""document"")
```

outputs :

| ner_confidence                                                                                                                                                                                                                                                                                                                                                                                                                       | entities                                                                           | Entities_Classes   | document                                                                                                                         |
|---------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------|:--------------------------------------|
| [0.9987999796867371, 0.9854000210762024, 0.8604000210762024, 0.6686999797821045, 0.5289999842643738, 0.7009999752044678, 0.7684999704360962, 0.9979000091552734, 0.9976000189781189, 0.9930999875068665, 0.9994000196456909, 0.9879000186920166, 0.7407000064849854, 0.9215999841690063, 0.7657999992370605, 0.39419999718666077, 0.9124000072479248, 0.9932000041007996, 0.9919999837875366, 0.995199978351593, 0.9991999864578247] | ['সালে', 'ইয়াজউদ্দিন আহম্মেদ', 'মুন্সিগঞ্জ উচ্চ বিদ্যালয়', 'সালে', 'মুন্সিগঞ্জ হরগঙ্গা কলেজ'] | ['TIME', 'PER', 'ORG', 'TIME', 'ORG'] | ১৯৪৮ সালে ইয়াজউদ্দিন আহম্মেদ মুন্সিগঞ্জ উচ্চ বিদ্যালয় থেকে মেট্রিক পাশ করেন এবং ১৯৫০ সালে মুন্সিগঞ্জ হরগঙ্গা কলেজ থেকে ইন্টারমেডিয়েট পাশ করেন |

#### [Identify intent in general text - SNIPS dataset](https://nlp.johnsnowlabs.com/2021/02/15/classifierdl_use_snips_en.html)


```python
nlu.load(""en.ner.snips"").predict(""I want to bring six of us to a bistro in town that serves hot chicken sandwich that is within the same area"",output_level = ""document"")
```

outputs :


| document | snips | snips_confidence|
|----------|------|------------------|
| I want to bring six of us to a bistro in town that serves hot chicken sandwich that is within the same area | BookRestaurant |                  1 |


#### [Word Embeddings for Bengali (bengali_cc_300d)](https://nlp.johnsnowlabs.com/2021/02/10/bengali_cc_300d_bn.html)




```python
# Bengali for : 'Iajuddin Ahmed passed Matriculation from Munshiganj High School in 1947 and Intermediate from Munshiganj Horganga College in 1950.'
nlu.load(""bn.embed"").predict(""১৯৪৮ সালে ইয়াজউদ্দিন আহম্মেদ মুন্সিগঞ্জ উচ্চ বিদ্যালয় থেকে মেট্রিক পাশ করেন এবং ১৯৫০ সালে মুন্সিগঞ্জ হরগঙ্গা কলেজ থেকে ইন্টারমেডিয়েট পাশ করেন"",output_level = ""document"")
```

outputs :

|                                                     document | bn_embed_embeddings                                          |
| -----------------------------------------------------------: | :----------------------------------------------------------- |
| ১৯৪৮ সালে ইয়াজউদ্দিন আহম্মেদ মুন্সিগঞ্জ উচ্চ বিদ্যালয় থেকে মেট্রিক পাশ করেন এবং ১৯৫০ সালে মুন্সিগঞ্জ হরগঙ্গা কলেজ থেকে ইন্টারমেডিয়েট পাশ করেন | [-0.0828      0.0683      0.0215     ...  0.0679     -0.0484...] |



### NLU 1.1.3 Enhancements
- Added automatic conversion  to Sentence Embeddings of Word Embeddings when there is no Sentence Embedding Avaiable and a model needs the converted version to run.


### NLU 1.1.3 Bug Fixes
- Fixed a bug that caused `ur.sentiment` NLU pipeline to build incorrectly
- Fixed a bug that caused `sentiment.imdb.glove` NLU pipeline to build incorrectly
- Fixed a bug that caused `en.sentiment.glove.imdb` NLU pipeline to build incorrectly
- Fixed a bug that caused Spark 2.3.X environments to crash.

### NLU Installation

```bash
# PyPi
!pip install nlu pyspark==2.4.7
#Conda
# Install NLU from Anaconda/Conda
conda install -c johnsnowlabs nlu
```

### Additional NLU ressources

- [NLU Website](https://nlu.johnsnowlabs.com/)
- [All NLU Tutorial Notebooks](https://nlu.johnsnowlabs.com/docs/en/notebooks)
- [NLU Videos and Blogposts on NLU](https://nlp.johnsnowlabs.com/learn#pythons-nlu-library)
- [NLU on Github](https://github.com/JohnSnowLabs/nlu)
- [Suggestions or Questions? Contact us in Slack!](https://join.slack.com/t/spark-nlp/shared_invite/zt-lutct9gm-kuUazcyFKhuGY3_0AMkxqA)",14979,LanguageTechnology,1
"NLP for a general object classification? Hey there, I'm interested in experimenting with natural language processing to see if it's possible to perform some sort of object classification with a large dataset, most likely wikipedia. An example scenario would be where a user can formulate questions about what can and cannot be done with an object, and the model can answer these questions.

&amp;#x200B;

An example of this could be:  
User: ""can a cup be filled with water?""

Model: ""yes""

User: ""does a cat bark?""

Model: ""no""

&amp;#x200B;

I am aware that projects like GPT-3 and similar are \*very\* complex and can (to some degree) reason, however my intention with this is \*not\* to perform advanced language processing that requires advanced analysis. Also, the example provided above is not representative of what my goals are with this; the conversation may be represented with some simple class or structure that conveys the object you're asking about (cup/motorcycle), and the subject in question (can it be filled with water/does it bark). Therefore the purpose of this question does not pertain to the question asked by the user but the ability to parse text from an available database and extract useful facts and key information from the text.

&amp;#x200B;

Any advice? Thanks",1294,LanguageTechnology,1
"Using own POS Tagger with a UD parser I have created a POS tagger with a decent accuracy (Ukrainian only, if that matters). I need to apply it in an existing UD parser. I know C# and also I have briefly looked at Python syntax, in case I need it. I can change my tagger to be compatible with a specific parser.

Could you suggest specific parsers that would allow me to use my own part-of-speech tagger?",403,LanguageTechnology,1
Embeddings of Label Components for Fine-grained NER | Research Papers Summary 011 ,82,LanguageTechnology,1
Start Japanese text processing without installing any tokenizer on your local environment (tokenizers are on docker container) ,127,LanguageTechnology,1
"HyperBand and BOHB: understanding hyperparameter optimization algorithms If you want to learn about state-of-the-art hyperparameter optimization algorithms (HPO), in this article I’ll tell you what they are and how they work.

We cover:
- A bit about HPO Approaches 
- What is Bayesian Optimization, and why is this method effective? 
- How do state-of-the-art Hyperparameter Optimization algorithms work? 
- **Hyperband vs BOHB comparison**

[HyperBand vs. BOHB](https://neptune.ai/blog/hyperband-and-bohb-understanding-state-of-the-art-hyperparameter-optimization-algorithms?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=blog-hyperband-and-bohb-understanding-state-of-the-art-hyperparameter-optimization-algorithms&amp;utm_content=languagetechnology)",761,LanguageTechnology,1
"Generator for related words/ ideas/ concepts? Not sure where else to ask this, but I figured people who traffic in machine learning circles might have some idea.

I'm  certain tools like this must exist in some corner of the internet. I'm looking for some sort of algorithm-generated word cloud tool that expands upon ideas/concepts/words fed to it. Preferably, the more words fed into it, the more refined/related the suggestions. Anybody know of any tools that may work like this, or of any other subs that might help find such a tool?",537,LanguageTechnology,1
"PMI for WordClouds Hello!

I'm currently developing an NLP project and I'm stuck on something. I'm a NLP beginner (using Python) and I have some questions I'm hoping the community might help me address.

I've built a dataset of several hundred political speeches and I'm building Word Clouds out of the content of these speeches. To make the story short, I want to tokenize ""phrases"" or ""concepts""; for example the name of an institution or a commonly used phrase by politicians, so that my word cloud will reflect the actual ""phrase"" or ""concept"" instead of showing me the individual words.

I'm aware of the concept of PMI (Pointwise Mutual Information) and how it can help identify these patterns in my data. I haven't been able to find any resources that show a code pipeline to do this.

Any suggestions would be gladly appreciated.

Thanks in advance!",857,LanguageTechnology,1
"ANNOY and Semantic Search I am trying to build a semantic search algorithm using Bert embeddings and using annoy for indexing. AnnoyIndex() is built to take in pooled embeddings (I think); but what about word embeddings?  For example a word embedding of 100 docs of shape\[100, 256, 768\].  An example of working with pooled embeddings shape (100, 768): - 

from annoy import AnnoyIndex

\# Bert emits 768 dim vectors

D = 768

n\_trees = 300

ann = AnnoyIndex(D, 'angular')

pooled

for index, embed in enumerate(pooled\_embeddings):

ann.add\_item(index, embed)

[ann.build](https://ann.build)(n\_trees)

&amp;#x200B;

Should I flatten the word vectors? should I reduce the dimensionality in some other way?",709,LanguageTechnology,1
"Is there a way to deploy NLP models into a Chrome extension? I would like to be able to process text data from newspaper sites, tweets, or whatever piece of a text an user can select from a web site and run different NLP inferences (sentiment, polarity, entity recognition, etc.) Within the client side. 

I'm relatively new and ,I don't know, may be Tensorflow could be used? 

Any thoughts?",392,LanguageTechnology,1
"How to add OOV words into an already pre-trained embedding If one has a pre-trained embedding (e.g. fasttext, glove) What is the best way to create embedding vectors for a new word that was not originally in the vocabulary of the pretrained embedding. There is an interesting approach called ""a la carte embeddings"" ([https://arxiv.org/abs/1805.05388](https://arxiv.org/abs/1805.05388)) but seems to be only limited to GLoVe. Anyone aware of any other published or *ad hoc* methods for doing this for any flavour of embedding models?",533,LanguageTechnology,1
tNodeEmbed: Node Embeddings with Temporal Graphs | ML with Graphs (Research Paper Walkthrough) ,95,LanguageTechnology,1
"Is it possible to test whether a tokenizer can losslessly tokenize and detokenize a given corpus solely from its vocabulary? I am trying to test whether a given vocabulary list contains the tokens necessary to reconstruct a corpus of text losslessly. That is, if a tokenizer trained on a corpus of text were to attempt to tokenize the training corpus according to its associated vocabulary, would it be able to tokenize and detokenize the entire training corpus without losing any text to \[unk\] tokens? Are there ways to test this?

Suppose you had a corpus of text - I’ll use a small one for this example

    corpus = ‘the cat in the hat’

Suppose you had a trained tokenizer which tokenizes according to its vocab list where each token id is simply the index of the token in the list.

    vocab = [‘t’, ‘h’, ‘e’, ‘ ‘, ‘c’, ‘a’, ‘i’, ‘n’]
    print(len(vocab))
    8
    tokens = tokenize(corpus, vocab))
    print(tokens)
    [0, 1, 2, 3, 4, 5, 0, 3, 6, 7, 3, 0, 1, 2, 3, 2, 5, 0]
    print(“Tokenized length: {}”.format(len(tokens)))
    Tokenized length: 18

If I then detokenize from here I can obviously reconstruct the corpus as it originally was. However, this process is suboptimal as we can combine tokens to reduce the length of the tokenized representation.

    vocab = [‘t’, ‘h’, ‘e’, ‘ ‘, ‘c’, ‘a’, ‘i’, ‘n’, ‘th’]
    print(len(vocab))
    9
    tokens = tokenize(corpus, vocab))
    print(tokens)
    [8, 2, 3, 4, 5, 0, 3, 6, 7, 3, 8, 2, 3, 2, 5, 0]
    print(“Tokenized length: {}”.format(len(tokens)))
    Tokenized length: 16

This is obviously still lossless, though it adds an extra token to the vocabulary list, which will increase the possibility space of a model trying to predict the next token in a sequence. But if I remove a necessary token, it becomes lossy. Supppose our tokenizer outputs ‘\[unk\]’ for all tokens not in the vocabulary and that the ‘\[unk\]’ token is always the last token in the vocabulary regardless of the content of the vocabulary.

    vocab = [‘t’, ‘h’, ‘e’, ‘ ‘, ‘c’, ‘a’, ‘i’, ‘th’]
    print(len(vocab))
    8
    tokens = tokenize(corpus, vocab))
    print(tokens)
    [7, 2, 3, 4, 5, 0, 3, 6, 8, 3, 7, 2, 3, 2, 5, 0]
    print(“Tokenized length: {}”.format(len(tokens)))
    Tokenized length: 16

The length of the representation does not change but we can tell that it is lossy just from the vocabulary, therefore the compression is not lossless.

    print(detokenize(tokens, vocab))
    [‘the cat i[unk] the hat’]

This example makes it easy to tell the compression is lossy because our vocabulary is small and made of suboptimally combined tokens. We can go further with the combination of the vocabulary and combine ‘t’, ‘h’, ‘e’, and ‘ ‘ to form a single token covering all instances of “the “. Since ‘e’ doesn’t occur outside of its place in ‘the’ we can delete it from the vocabulary list and still maintain the same level of loss in compression, ditto ‘i’ and ‘n’.

    vocab = [‘t’, ‘h’, ‘the  ‘, ‘ ‘, ‘c’, ‘a’, ‘in’]
    print(len(vocab))
    7
    tokens = tokenize(corpus, vocab)
    print(tokens)
    [2, 4, 5, 0, 3, 6, 3, 2, 1, 5, 0]
    print(len(tokens)
    11

Thus far this is the most optimally compressed form of the sentence (though I don’t claim for it to be the most optimally compressed) and it has the smallest vocabulary size, making it easier for a model to guess the next token. In all of these examples we have been able to tell unambiguously whether or not the compression is lossless just by eyeballing it.

However, there is more than one way to tokenize a sentence. If our tokenizer sees the ‘t’ and ‘h’ without the context of the ‘e ‘, it will run into a problem: \*\*there is no ‘e’ in our vocabulary list\*\* and, if not properly trained, it will be forced to replace that ‘e’ with an ‘\[unk\]’ token, making it lossy while also expanding the length of its representation significantly.

Therefore, even though we have just proven lossless compression is possible given this corpus and the last vocabulary list, if the recognition of the sequence by the tokenizer is poor, it won’t be capable of losslessly tokenizing even though the vocabulary list is suited for it. As corpus size increases, it becomes harder and harder to tell just by looking that the vocabulary list can losslessly reconstruct the corpus.

With something like SentencePiece, which defaults to a vocabulary size in the thousands, trying to reconstruct the Wiki-sentences corpus, which is millions and millions of sentences long, it becomes untenable to pore over every output vocabulary size and manually check if it can reconstruct the corpus.

Thus my question: given a vocabulary list and a corpus, is there an automatic way to tell that that it is \*possible\* to reconstruct the given corpus losslessly using the given vocabulary assuming a well trained tokenizer? In other words is there a function to determine if lossless tokenization is possible which returns true or false given an input corpus and vocabulary?",4991,LanguageTechnology,1
"Linguistic resources? Hi all. I'm looking for resources that computer science students can use to learn more about linguistics. As my professor likes to put it, we're good at the P part of NLP but aren't the best at the NL part. Any help is greatly appreciated!",261,LanguageTechnology,1
"Looking for En-Zh bilingual medical corpora Hi everyone 

I am building an MT engine and looking for En-Zh bilingual medical corpora to download. If anyone can share it here with me will be much appreciated!",207,LanguageTechnology,1
"How NLP might revolutionize scholarship, collaboration, and reasoning. The most recent episode of the [Futurati Podcast](https://www.youtube.com/channel/UCRSov16ZLE2UgekgBTgnrjw/videos) is a big one. [We had Jungwon Byun and Andreas Stuhlmüller on](https://www.youtube.com/watch?v=Lef5q1xPTU0) to talk about their startup ['Ought'](https://ought.org/) and, to the best of my knowledge, this is the first public, long-form discussion of their work around.

(It's also probably our funniest episode.)

Their ambition is to wrap a sleek GUI around advanced language models to build a platform which could transform scholarship, education, research, and almost every other place people think about stuff.

The process is powered by GPT-3, and mostly boils down to teaching it how to do something you want it to do by showing it a couple of examples. To complete a list of potential essay topics you'd just show it 3-4 essay topics, and it'd respond by showing you a few more.

The more you interact with it, the better it gets.

There's all sorts of subtlety and detail, but that's the essence of it.

This may not sound all that impressive, but consider what it means. You can have Elicit (a separate spinoff of Ought) generate counterarguments to your position, brainstorm failure modes (and potential solutions) to a course of action, summarize papers, and rephrase a statement as a question or in a more emotionally positive tone.

The team is working on some integrations to extend these capabilities. Soon enough, Elicit will be able to connect to databases of published scientific papers, newspapers, blogs, or audio transcripts. When you ask it a research question, it'll be able to link out to millions of documents and offer high-level overviews of every major theme; it'll be able to test your comprehensions by asking you questions as you read; it'll be able to assemble concept hierarchies; it'll be able to extract all the figures from scientific papers and summarize them; it'll be able to extract all the proper names, find where those people are located, get their email addresses where available, and write them messages inviting them on your podcast.

We might one day be able to train a model on Einstein or Feynman and create lectures in their style.

What's more, people can share workflows they've developed. If I work out a good approach to learning about the subdisciplines of a field, for example, I can make that available to anyone to save them the effort of discovering it on their own.

There will be algorithms of thought that can make detailed, otherwise inaccessible aspects of other people's cognitive processes available.

And this is just researchers. It could help teachers dynamically adjust material on the basis of up-to-the-minute assessments of student performance. It could handle rudimentary aspects of therapy. It could help people retrain if they've been displaced by automation. It could summarize case law. It could help develop language skills in children.

I don't know if the future will look the way we hope it will, but I do think something like this could power huge parts of the knowledge work economy in the future, making everyone dramatically more productive.

It's tremendously exciting, and I'm honored to have been able to learn about it directly.",3304,LanguageTechnology,1
"NLP Cloud now serves transformers-based models We just released several important additions to the NLP Cloud API.

Many users were asking us for transformers-based models in addition to our existing spaCy models. So here we go, NLP Cloud now serves some of the best transformers-based models with PyTorch from [Hugging Face](https://huggingface.co/models):

* Facebook's Bart Large MNLI model, for **text classification**     
* Facebook's Bart Large CNN model, for **text summarization** 
* Deepset's Roberta Base Squad 2 model, for **question answering**
* DistilBERT Base Uncased Finetuned SST-2 English model, for **sentiment analysis**

Of  course our spaCy models are still available for Named Entity Recognition and POS tagging. And you can even upload your own spaCy  models. Important change: from now on, all the **large** spaCy models will be available for free.

All our models are available **for free** for a limited amount of requests per minute, and pricing for paid plans is very fair.

Hope you will like it, and can't wait for your feedbacks and suggestions!

Website: [https://nlpcloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=3ec10c28-ab0d-11eb-bcbc-0242ac130002)

Documentation: [https://docs.nlpcloud.io](https://nlpcloud.io)",1267,LanguageTechnology,1
"Help with tf-idf for job description extraction Hi, this isn't really a question asking for help with code syntax, more so on the general theory / workflow side of things. 

I'm doing a small Python project where I'm trying to extract relevant job 'requirements' from UX Researcher job listings on Google. I've scraped a load of jobs, cleaned them using NLTK, Spacy etc and I have saved each job description as a text file respectively. I also made one large text file corpus with all the cleaned job descriptions in. 

I've heard that tf-idf is a good way to go about extracting just the employers' desired 'requirements' from these job descriptions (i.e., 'PhD in Psychology' or 'Agile and Scrum experience'). 

My problem however is i'm not sure in what format to load the documents in to process using tf-idf. Should I load in all of the documents which each contain one job description, or use the one text file which has all of the documents in itself? My thinking is I need to load each individual document given that tf-idf uses document frequency right? Any help would be appreciated, thanks!",1101,LanguageTechnology,1
"NLP - Summarization - SeekingAlpha - interesting observation Hello guys.

I am trying to summarize (compare approaches) articles at seekingalpha. To compare various approaches I've created oracle summaries (5 sentences) using greedy algorithm as described in [Bertsumm](https://arxiv.org/pdf/1903.10318.pdf) . As an input to create oracle summaries I have used the author conclusion (can be seen here in section [Conclusion Thoughts and Outlook](https://seekingalpha.com/article/4411395-project-1m-higher-yields-bite-in-february)). 

Fully excited I have jumped into trying novel approaches and I have used for example [PreSumm](https://github.com/nlpyang/PreSumm) (BertSumExt). The problem is that the articles are much longer than PreSumm can process. I solved this problem by dividing the article into blocks of length of 512 tokens, then summarized each block by itself and merged the blocks together. I repeated this process until the final output was between 100 - 200 subword tokens. Allright, it works but...

Then I have tried [TextRank](https://radimrehurek.com/gensim_3.8.3/summarization/summariser.html) that has no length constraints. I selected the ratio of the output to be in the length of 5 sentences which is usually between 100 - 200 subword tokens. 

Finally I have compared the outputs using ROUGE F R1,R2 and RL score and to my suprise the TextRank yielded the score aroung 50 at R1 and RL, and around 39 at R2. On the other hand the PreSumm (or [facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn)) showed score only around 23 at R1 or 14 at R2.

I have also tried to implement trigram blocking for TextRank which havent significanlty lowered the results.

Do you have any suggestions why the current models compared to TextRank perform so much worse? Might it be because of the ""block by block summarization""? I have also tried to find out if creating oracle summaries as described in Bertsumm favors graph based summarization algorithms but I was not able to come up with any reasonable conclusion (yes or not). Do you guys have any hints what I can improve (besides training new model) when using huge language models to summarize longer inputs? Or is it normal that the current approaches fails at longer inputs (at least in my case using above mentioned approach)

Thanks for your suggestions :-)",2345,LanguageTechnology,1
"Dialogflow CX - Issues with Enabling Voice Input Hello,

I am trying to test my Dialogflow CX agent with voice, but I am having difficulties understanding where to start. I followed the instructions on this github page:

[https://github.com/googleapis/python-dialogflow-cx](https://github.com/googleapis/python-dialogflow-cx)

But when I try to run my python script to test my voice file, it gives me an error.

    grpc._channel._MultiThreadedRendezvous: &lt;_MultiThreadedRendezvous of RPC that terminated with: 
    status = StatusCode.UNKNOWN 
    details = ""Exception iterating requests!"" 
    debug_error_string = ""None""

I've also tried following the instructions on this page which gives me a script similarly to the one related to the github:

[https://cloud.google.com/dialogflow/cx/docs/how/detect-intent-stream](https://cloud.google.com/dialogflow/cx/docs/how/detect-intent-stream)

Has anyone successfully worked with Dialogflow and if so, what steps did you take to enable voice input?

I need to make a demonstration and I decided to use the ES agent, but also having difficulties with it recognizing what I want to say. When I say ""Hello LuPal"", it'll give me a response of ""Hello loophole"" (??). Thank you for your response, I'd really appreciate any suggestions!",1280,LanguageTechnology,1
"Hosting PDF for public view: how to make it as much OCR-friendly as possible? Hello!

&amp;#x200B;

 My company is going to make available many PDF files for the public.

&amp;#x200B;

 Which settings/encoding should I recommend them to use in order for the software to be as much OCR-friendly as possible? For example, saving a MS Word file in PDF instead of scanning a printed image. (but that's a non-technical example)",422,LanguageTechnology,1
"How does event extraction differ from information extraction/ retrieval? Based on some additional research I am revising a question I asked yesterday. So new question, how does event extraction differ from information extraction? Could I use information extraction type code to accomplish event extraction from a written corpus? Any help would be much appreciated!",364,LanguageTechnology,1
"Is rule-based NLP officially dead? Machine learning i taking over everything, including training text, speech, and language prediction models to do what they need to do. What's the need for rules in the NLP space anymore? Rules are for non-technical linguists and grammar writers, us NLP people are long past that and are doing it all with ML and neural nets.

Rule-based NLP is dead. Am I wrong? Prove me wrong, please. What USE is there for rule-based models in this field when we have machine learning models trained on mountains of meticulously-labeled data? Maybe if you didn't have any annotated labeled data, you might want to use rules in a pinch, but that's all ad hoc bullshit that will have to keep building up more and more as you find more and more things you didn't think of that will force you to make new rules. With ML all of those little things you don't think of are picked up in training so it knows how to deal with them right off the bat.",960,LanguageTechnology,1
"Visualizing the Distribution of Several $Billions of #NGO Grants through Natural Language Processing (Case Study) I thought this could be of interest. A team of 40+ changemakers built an NLP analysis pipeline for feature extraction, scraping Twitter, Google, and 1200 PDF files through automated APIs.   


https://omdena.com/blog/nlp-analysis/",344,LanguageTechnology,1
How long did it take to get access to gpt3? I heard it takes around a month. I haven't heard back in 6 months.,110,LanguageTechnology,1
"How does event extraction differ from topic modeling? Can anyone explain how event extraction coding techniques differ from topic modeling? A lot of the code examples I’ve looked at seem to be using topic modeling then organizing discerned topics by date. Is there more to event extraction than that? I assume there is. I feel like I must be missing something and I hope someone here has some insight. Also, if anyone has any materials regarding event extraction that are helpful I would love to hear about them. Thanks all!",524,LanguageTechnology,1
"Attention over Ground-Truth Image Caption? I'm a bit of a newbie when it comes to image captioning and was wondering if it would make sense to feed an image and a *ground-truth* *caption* to a pre-trained image-captioning model and visualize the attention scores over the image for every word? I.e. in order to see how much the model *would have had* to rely on the image features as opposed to the language model in order to generate such a caption?  If so, does anyone have experience implementing this or could point me to a repo that does?",543,LanguageTechnology,1
King -Man +Woman = King ? ,26,LanguageTechnology,1
How to best determine changing news event coverage— semantic similarity or no? I am looking to figure out a way to show how news events unfold over time : ie how early articles described an event like the us capitol riot versus what they say now. I’m guessing semantic similarity is the best way to achieve this? Does anyone have other/ better ideas? I’m fairly new to nlp and appreciate any help anyone can provide. I have gotten great help from this subreddit in the past.,474,LanguageTechnology,1
"Tutorial on how to perform a Shapiro-Wilk normality test Hey, I've created a tutorial on how to perform a Shapiro-Wilk normality test in the R programming language: [https://statisticsglobe.com/shapiro-wilk-normality-test-in-r](https://statisticsglobe.com/shapiro-wilk-normality-test-in-r)",289,LanguageTechnology,1
"Using bag or words, bigrams and tf-idf together I’m trying to build a model that will decide if a review is positive or negative. I’d like to use the above methods but am unsure if they can all be used together. I think my order of operations would be clean data(case, punctuation, stop words etc) then get bigrams, as a bag of bigrams instead of just words? And then get the tf-idf scores for these bigrams. Use those scores with with something like naive bayes classifier to make my predictions? Is this doable? Or Will I mangle some methods that aren’t really meant to be used together?",589,LanguageTechnology,1
Combining BERT with Static Word Embedding for Categorizing Social Media | Research Paper Walkthrough ,101,LanguageTechnology,1
"""All terrorists are [MASK]"": Self-Diagnosis and Self-Debiasing for Pretrained Language Models We recently wrote a paper that investigates whether pretrained language models can use their internal knowledge to detect🩺 and discard🩹 undesired behaviors and reduce biases in their own outputs: [https://arxiv.org/abs/2103.00453](https://arxiv.org/abs/2103.00453)

This is still a very early draft. We plan to extend the paper along several axes and to conduct extensive further experiments, so I'd be happy to hear your thoughts 😊",526,LanguageTechnology,1
"For a bag of bigrams, do you tag things positive or negative? I have two corpora, one with 5k positive review and one with 5k negative reviews. If use a bag of words or bag of bigrams method to do sentiment analysis, should I keep the the corpora separate or do I rage each review in each corpus as pos/neg and merge them?",322,LanguageTechnology,1
Is LIWC machine learning? I am reading a paper and it uses the LIWC to estimate risk of certain mental health disorders. I am confused if LIWC is an example of machine learning. Or whether it is part of processing the data to be used in a ML algorithm. Thanks in advance,270,LanguageTechnology,1
"Is a corpus of 5million reviews large or small or midsized for a corpus? Like if you were a data scientist working at an actual company Doing nlp, what would your average corpus size be? I’m writing a paper on serializing classifiers and trimming down/preprocessing data but only have exposure to smaller data sets we would use in a class room and was wondering what the typical load would look like. Thank you!",411,LanguageTechnology,1
"Best/good practices for evaluating deep learning NER models? Hi all, I'm supposed to evaluate a Dutch DL NER model (Huggingface) for a course, and compare it to a rule-based model. I'm feeling a bit lost and don't know where to start. Does anyone have any tips/good practices? What metrics should I use, and are there other evaluation techniques besides metrics? I appreciate all input!",386,LanguageTechnology,1
"Labeling adjective+noun pairs with themes I'm a hobbyist with no background in machine learning. Lately, I have been entertaining the idea of creating a very minimalist story prompt generator for use in solo role playing games (the tabletop sort). I want it to generate only an adjective and a noun, such as ""open cage"", ""endless parade"", or ""timely arrival""; so I data-mined some books for adjectives and nouns that are commonly used together. Now my problem is that these pairs that I have extracted cover a wild range of themes, and in most cases are not applicable to the situation the game is in. I at least want to be able to separate them into some general categories like ""objects"", ""events"", ""threats"" etc. I was wondering if I can somehow do this with machine learning. 

Since I don't fully know the capabilities of the current models, I'm not sure about the doability of what I want to do (i.e. labeling two-word strings). So far, I've tried using Logistic Regression with only a single category and 200 datapoints (half belonging to the category, half not). The model was only able to label 2 out of 7 new pairs correctly, not very successful. I haven't tried doing the same with more data, but my gut feeling tells me that what the model is learning is the relatedness of each token in a pair to the category label (this might be an obvious thing, but like I said, I'm a noob). If that is the case, it is not very ideal for me because I have tens of thousands of pairs and I cannot cover each single word related to a specific category by coding a part of the data by hand.  

Is there a good way to tackle this problem? Is it doable? Can you give me some pointers about where to look before I invest tens of hours in a fruitless endeavor?",1753,LanguageTechnology,1
"Named Entity Recognition: What's the best way to handle entity labeling for variations in HTML encoding for symbols like trademarks(™) and registered trademarks(®)? I'm using Google AI Platform. When importing a JSONL file with various HTML entities, the encoding changes during the import. So for example, a registered name like **Nintendo®** is appearing in my dataset as **NintendoÂ®** and a trademarked name like **Animal Crossing™** is appearing as **Animal Crossingâ¢**.

When it comes to training an accurate model, should I be selecting the longer and stranger encoded version for the label since technically that's all part of the name (encoded differently)? Or am I missing a way to better encode my Google datasets?

**Edit:** More information... I'm uploading it in UTF-8 format and it appears correctly when viewed in Google Cloud Storage but during labeling it's appearing differently (ISO-8859-1 format). I guess my main question is only is it okay to proceed with labeling in this format (since I assume all data coming in will be converted in the same fashion)?",1079,LanguageTechnology,1
"Beginner Research Papers to Read I've been wanting to spend some time reading through research to gain technical computational linguistics knowledge. Are there any more basic papers I can explore in the realm of research? I'm beginning my CS degree, so my knowledge base is pretty sparse at this point, but I'm exploring topics that interest me like Universal Grammar or Phonology/Prosody.",389,LanguageTechnology,1
"Best way to do unsupervised long document similarity? Do you think it would be splitting it up into sentences and using a BERT to generate a single average doc embedding from which you could calculate cosines distance on, or should it be employing some kind of long document transformer?",287,LanguageTechnology,1
"Academic Course Question Hello all!

I apologize of this is the wrong place to ask this question, but I am starting a PhD program in ECE next year focusing on SLP. I mainly have experience with NLP and was wondering if different ial equations are useful for Speech? In my final quarter of undergrad I can either take ODE or Multivariate stats and was wondering if I could get some insight on which one may be more useful?

Thank you!",433,LanguageTechnology,1
"How to benefit from Q&amp;A dataset I have a large db with questions and answers but the problem is that there is no context. So the question is, how can I benefit from this data to train a model? Which type of a model (downstream task) could I train with it? I think I am onto something but can't find out what and how.",320,LanguageTechnology,1
"Help with dissertation survey - Automatic Quiz Qeneration Hey all, 
Hope it’s okay to post link to surveys here.

I’m currently writing my final year dissertation on Automatic Quiz generation  and need data about the quality of the generated questions.

[form](https://forms.gle/TMYcr63HjhVN33P77) 

Thank you for your time!
Any feedback would be appreciated!",359,LanguageTechnology,1
"Why and how to set up a machine-learning pipeline from the home office Hello, community! We are running a machine-learning startup with a focus on #LanguageTechnology ([https://reason.al/](https://reason.al/?utm_source=reddit.com&amp;utm_medium=referral&amp;utm_campaign=BL-share-comm-RD-001)) and started to publish very practical and hands-on blog posts on machine-learning and broader coding insights.

Our latest one is on why and how to set up a machine-learning pipeline from home:

[https://reason.al/blog/ml-from-home-1](https://reason.al/blog/ml-from-home-1/?utm_source=linkedin.com&amp;utm_medium=referral&amp;utm_campaign=BL-share-comm-LI-001)

I hope that's of interest for you here, would be great to see you subscribe to the (monthly) newsletter, too!",765,LanguageTechnology,1
"How are the Q,K &amp; V matrices learned in Self-Attention networks? Apologies for the super-beginner's level question -- I've tried looking around, and there's not a ton out there. 

I understand at least vaguely what's meant when we talk about what the Queries, Keys, and Values in networks with Self-Attention. What I'm not fully understanding is how these Matrices are learned during training. How does the LM converge on these final Matrices?",447,LanguageTechnology,1
"NLU subreddit search app Hi all, I just finished my subreddit browser that organizes subs by tags.  It also queries wikipedia for pages for each tag, analyzes those pages with Watson, and color codes them based on the main emotion.  I'm having fun learning relationships between concepts.  Hope you do too.  www.dao.af",318,LanguageTechnology,1
Does anyone know Chinese version for otter.ai? ,47,LanguageTechnology,1
"Researchers From Stanford, UCI and UC Santa Barbara Conducted a Study to Understand How The mBERT Model Encodes Grammatical Features Over the past few decades, Deep neural network-based models have been developed to complete a broad range of tasks. Some of them are mainly designed to process and generate coherent texts in multiple languages, answer questions about a text, translate texts, and create summaries of the online content.

Several Deep learning systems are already available with linguistic capabilities, for instance, text analysis tools, in the form of applications for real-time translation, and virtual assistants such as Alexa, Bixby, Siri, Google Assistant, and Cortana. Some of the above systems use a specific deep-learning model called Multilingual BERT (mBERT). mBERT is released by Google and is trained on approximately 100 languages simultaneously. 

Paper summary: [https://www.marktechpost.com/2021/03/01/researchers-from-stanford-uci-and-uc-santa-barbara-conducted-a-study-to-understand-how-the-mbert-model-encodes-grammatical-features/](https://www.marktechpost.com/2021/03/01/researchers-from-stanford-uci-and-uc-santa-barbara-conducted-a-study-to-understand-how-the-mbert-model-encodes-grammatical-features/) 

Paper: [https://arxiv.org/abs/2101.11043](https://arxiv.org/abs/2101.11043) 

Github: [https://github.com/toizzy/deep-subjecthood](https://github.com/toizzy/deep-subjecthood)",1418,LanguageTechnology,1
"Conference for Truth and Trust Online Calls for Paper and Talk Proposal Submissions Are you working on fact-checking, detection of misinformation, hate speech or other problems related to trust and truth online? You can now **submit a paper or talk proposal to the Truth and Trust Online 2021 (TTO 2021). https://truthandtrustonline.com/call-for-papers-2/**

The annual Conference for Truth and Trust Online is organised as a unique collaboration between practitioners, technologists, academics and platforms, to share, discuss, and collaborate on useful technical innovations and research in the space. This year’s **TTO is virtual and will take place online Oct 7-8 2021.** 

We welcome technical papers of the following types: surveys, methods, reproduction papers, resource papers, case studies. 

**Topics of interest include:**

* Misinformation and disinformation 
* Trustworthiness of COVID-19 news and guidance
* Hate speech
* Online harassment and cyberbullying
* Credibility and fake reviews
* Hyper-partisanship and bias
* Image/video/audio verification
* Fake amplification, polarization, and echo chambers
* Transparency in content and source moderation 
* Privacy and anonymity requirements

We encourage wide participation from all interested parties and stakeholders on online media, including academics, startups and large industry, non-profit organizations and governmental institutions.

**Technical paper submission deadline: July 30, 2021**

**Talk proposal submission deadline: August 13, 2021**

More details can be found: https://truthandtrustonline.com/call-for-papers-2/",1597,LanguageTechnology,1
"NLP course Hi,

I've peripherally worked with NLP in the past mainly through my work with Python and machine learning. I'd love to get more in-depth knowledge and so would my company, so much so they've offered to pay for any NLP course I want.

Does anyone have any good recommendations for NLP courses?",304,LanguageTechnology,1
Why a YouTube Chat About Chess Got Flagged for Hate Speech ,59,LanguageTechnology,1
"[D] Annotation tool for entity sentiment analysis Hi everyone, we are a marketing company and going to start an annotation project for entity sentiment analysis. Can you please share what are best practices for starting an NLP annotation project? What is most efficient approach? What techniques are mostly used to automate the annotation process?",347,LanguageTechnology,1
"Paper ""M6: A Chinese Multimodal Pretrainer"". Dataset contains 1900GB of images and 292GB of text. Models contain 10B parameters and 100B (Mixture-of-Experts) parameters. Images shown are text-to-image examples from the paper. Paper link is in a comment. ",254,LanguageTechnology,1
Corpus Research Study Advice ,29,LanguageTechnology,1
"LAMA AI's weekly news, updates, and events. Hey guys!

This week, LAMA ([https://lamaai.io](https://lamaai.io)) has a couple of updates. Let's start with this weeks AI news!

You can find the video [here](https://www.lamaai.io/posts/ai-360-01-03-2021-unified-transformer-sebastian-ruder-openais-dall-e-glom-and-studiogan), but as for the key highlights:

* [Facebook AI Research](https://ai.facebook.com/) announce a new multi-modal Transformer architecture, [UniT](https://arxiv.org/abs/2102.10772)
* [Sebastian Ruder updates](https://ruder.io/recent-advances-lm-fine-tuning/) us on the latest advances in language model fine-tuning
* [OpenAI](http://openai.com/) have news about DALL-E
* Geoffrey Hinton [proposes an idea paper](https://arxiv.org/abs/2102.12627) he dubs GLOM
* [StudioGAN](https://github.com/POSTECH-CVLab/PyTorch-StudioGAN) is introduced: A PyTorch library for SoTA GAN models

&amp;#x200B;

Would you like to know how we can use Machine Learning to detect COVID symptoms? Imperial College's **Björn Schuller** is going to be presenting  his recent and topical work on detecting COVID symptoms through the use of Computer Audition (think Computer Vision but for audio instead!). As a little introduction, Björn is a Full Professor at the University of Augsburg in Germany, where he is also Chair of Embedded Intelligence for Health Care and Wellbeing. He is also a Professor of Artificial Intelligence at Imperial College London and heads GLAM (Group for Language, Audio and Music). He has over 1000 publications which feature his name (🤯) and his recent research interests focus on audio and multi-modal approaches to emotion detection. Björn will be discussing his paper: [COVID-19 and Computer Audition](https://arxiv.org/abs/2003.11117) which was written during the outbreak last year. In this paper, he overviews the usage of speech and sound analysis by artificial intelligence/machine learning to detect a presence of COVID. If you're interested in attending the talk, register on the eventbrite: [https://www.eventbrite.com/e/bjorn-schuller-lama-ai-covid-19-and-computer-audition-tickets-143203512561](https://www.eventbrite.com/e/bjorn-schuller-lama-ai-covid-19-and-computer-audition-tickets-143203512561)

&amp;#x200B;

Finally, last week we had a paper presentation on the current state of AI's progress towards Natural Language Understanding. You can find the video/talk [here](https://www.lamaai.io/posts/progress-towards-natural-language-understanding)! As for some key points from the talk:

* (Bender and Koller, 2020) discuss the question whether a system exposed only to the form of language in its training data, can in principle learn its meaning
* They underline their arguments with multiple thought experiments and a comparison to human children language acquisition which is grounded in the real world and in interaction with others
* The NLP research community is called to reflect on the current research trends and to take a more top-down approach by asking “whether the hill we are climbing so rapidly is the right hill”
* (Linzen, 2020) discusses common evaluation practices in NLP research and their limitations
* He proposes a new evaluation paradigm which takes into consideration pre-training corpora of different sizes, as well as normative and efficiency attributes while comparing ML models to each other.",3361,LanguageTechnology,1
"[Need Advice]PhD in NLP @ reputed US institute/Prof. Worth it? Hi, 

I recently got admitted to a good PhD program in the US to work on natural language processing. The advisor is great too. A question to this reddit community - do you think going for a PhD in this domain is worth it? 

Background: Im already working at a great organization where my learning curve is increasing for the last two years. Im getting to work on state of the art things, I get to read papers, implement them, come up with my own architectures etc. My peers are very supportive, and Im assuming this is paving path for great industry opportunities in general. 

However, I applied for a PhD for two reasons: I want to learn more about the domain and stick to an area to get more expertise, understand the theory etc. And eventually I want to be a research lead, for which I think a PhD will provide me with immense credibility. However the idea of starting a PhD at 27, and going back to school and that lifestyle for another five years is very very scary. Im starting to have cold feet. 

Any words of wisdom from someone in the process, or someone who has been through this? 

Thank you so much!",1177,LanguageTechnology,1
"Any pre-trained longformers for this task? My goal is to have a network I can feed large amounts of text and complex questions to receive full length through answers.

Firstly is such a thing even possible with current models?  


Here is an example of what I want to be done

Input: the entire book of animal farm

Question: How does the book animal farm depict the issues with communism.

Full length answer. Animal farm depicts the failure of communism in several ways. The book shows us that people working with equal pay results in laziness among certain individuals and people having less motivation to work harder jobs. ETC ETC

Answer length should be anywhere from sentence s to paragraphs.

Also Id prefer that the model would have a higher bias to include reasoning for the questions.

For example Bob is 20 years old. Is Bob 30 years old?

Answer: No, Bob cannot be 30 years old because he is 20 years old.

This might be ambitious but the goal is something that I could use continuously through high school/college to save a ton of time. Maybe ill have to wait for bigger models like GPT-4 Im not sure but im hoping anyone here can give me some pointers?",1167,LanguageTechnology,1
"Where can I find an online resource of voice cloning? Is there a website where I can upload a snippet of speech and clone a voice? Just for demo purposes. 

I tried the method in this link but it didn’t work. https://youtu.be/CkZJ1knpZmo

Do you know of a good resource?",270,LanguageTechnology,1
"Extracting information from text Hello,

I am a NLP noob so please forgive my naive questions. I have a problem and looking for pointers to find a solution.

I have a list of keywords and have loads of unstructured data. Based on the keywords in my list, I want to extract information like values from the text. For example I have a sentence like 'His age is 15'. and my keyword is 'AGE'. Now I want to extract the age and the number from the text. THe thing is since the text is unstructured, the exact semantics of the sentence would differ in different texts. So I am looking to find/implement some solution that identifies the keyword (Simple enough) and extracts the associated information. I have tried using regexp, they work sometimes and sometimes start getting too complex to extract the info. Any help would be appreciated.

Thanks",842,LanguageTechnology,1
Into NLP - Fuzzy String Matching and the Edit Distance ,55,LanguageTechnology,1
"Training a Multi-Label Emotion Classifier with Tez and PyTorch to detect +20 different emotions Hello everyone!

I built an end-to-end tutorial to train an emotion classifier using SqueezeBERT, a state-of-the-art lightweight version of BERT.  
I got +0.9 on test data in a few lines of code.  
I thought I'd share with you my approach and the code so that you can maybe apply it to your projects.

The trained model is also available so that you can use it on your own data without retraining from scratch.

* Here's the code: [https://github.com/ahmedbesbes/multi-label-sentiment-classifier](https://github.com/ahmedbesbes/multi-label-sentiment-classifier)
* and the blog post: [https://medium.com/@ahmedbesbes/training-a-multi-label-emotion-classifier-with-tez-and-pytorch-af04c899a63a](https://medium.com/@ahmedbesbes/training-a-multi-label-emotion-classifier-with-tez-and-pytorch-af04c899a63a)

Please let me know if you have an issue

Feedbacks more than welcome :)

Best!",977,LanguageTechnology,1
SummPip: Multi-Document Summarization with Sentence Graph Compression | Research Paper Walkthrough ,99,LanguageTechnology,1
An effective joint sentence and token labelling method for Low-Resource NER | Research Papers Summary 010 ,106,LanguageTechnology,1
"I have an issue with GPT-2 Reading comprehension for a task. Scroll down to bottom for TLDR.

So ive been working on fine-tuning GPT2 to be capable of reading comprehension using CoQa dataset. It works good but my goal is to create a system that can read an entire book and then answer questions given about the book. The problem is there is too much data being sent into the bot within a single input. It can handle multiple paragraphs quite well and perhaps even an essay but a book is just too much data. There are a few solutions ive thought of such as using reading comprehension to create a pre-generated data set with each paragraph of the book followed by generated questions. However, even using google colab such data generation would take an excruciatingly long time. If there are 400 paragraphs it would take nearly 3 hours to finish the data generation. I also thought I could perhaps remove words like ""You, but or"" etc those kind of conjunctions and pronouns that don't at to the story. But I feel as if its comprehension capabilities may be diminished. Perhaps such a task would not be feasible until I can get my hands on GPT-3?

Anyways ill break down the methods that I have thought of

Method 1: Take the specified book and allow GPT-2 to generate reading comprehension dataset from it. Then feed that data back into the model so that it learns specific things about said book. ETA 4+ hours

Method 2: Take the book and remove conjunctions and pronouns from the text. then do the same as method 1

ETA &lt;2 hours

Method 3: No clue how this would work but perhaps I could just throw the whole book into the training data along with reading comprehension data to see if it might pick up on some of the questions Eta &lt; 1 hour

Method 4: Find a dataset that specifically answers questions from a variety of books and add that to CoQa. Eta &lt; 1 hour + how long to find said dataset.

The end goal is to create a system where I can have an AI answer questions about a book. Which would completely remove any need for me to read said book. Also be just as capable of reading pages from a text book and completing questions. Such a system would greatly enhance my ability to complete school work and free up hours of wasted time on learning about 50 year old books.

TLDR:

Book is too big to feed into GPT-2 reading comprehension model. How do I design/train my model to quickly answer questions about entire books?

My Question to you:

If you are reading, my questions to you are exactly which methods I should approach and is this challenge even feasible using the current GPT-2 model.",2608,LanguageTechnology,1
"Struggling With Terminology From Linguistics Hi all, 

Starting to make a switch toward NLP research in my Ph.D. and find I struggle to read many papers which use terminology from linguistics. Does anyone have any book suggestions that might help me better understand grammar / linguistic terminology? I am a native English speaker, I just am not familiar, for example, with Chomsky's surface vs deep structures, what frame semantics are, etc. 

Thanks!",453,LanguageTechnology,1
"Does anyone know of any domain-specific pretrained language models? Hi everyone. The title is the question. I'm just wondering whether there may be any language models like BERT that were pretrained on domain-specific corpora.

For example, the BioBERT model is known to have been pretrained on biomedical text, and hence achieves better performance than BERT on biomedical NLP tasks.

Would there be anything else along the same lines? Thanks.",444,LanguageTechnology,1
"University of Wisconsin-Madison, UC Berkeley, and Google Brain Introduce Nystromformer: A Nystrom-based Algorithm for Approximating Self-Attention Early days of research in Natural language processing established long-term dependencies. It also brought the vanishing gradient problem in front of us because nascent models were handling the input sequences one by one, without parallelization. Recently, revolutionary transformer-based architectures and the self-attention mechanisms have enabled token pairs’ interactions across complete sequences resulting in modeling arbitrary dependencies in a constant number of layers. The above helped achieve state-of-the-art performance across many Natural language processing tasks.

However, these advantages came to a high-cost barrier because the transformer-based networks’ memory and computational requirements grow quadratically with sequence length. The above results in significant efficiency bottlenecks when dealing with long sequences. Researchers from the University of Wisconsin-Madison, American Family Insurance, UC Berkeley, and Google Brain propose Nyströmformer. Nyströmformer is an O(n) approximation in both memory and time for self-attention. The above is designed to rescue us from the[ quadratic cost](https://syncedreview.com/2020/07/31/applying-linearly-scalable-transformers-to-model-longer-protein-sequences/) associated with the long input sequences

Paper Summary: https://www.marktechpost.com/2021/02/27/university-of-wisconsin-madison-uc-berkeley-and-google-brain-introduce-nystromformer-a-nystrom-based-algorithm-for-approximating-self-attention/

Paper: https://arxiv.org/pdf/2102.03902.pdf

Github: https://github.com/mlpen/Nystromformer",1714,LanguageTechnology,1
"How to generate sentences from a set of keywords? like this: 

[https://towardsdatascience.com/data-to-text-generation-with-t5-building-a-simple-yet-advanced-nlg-model-b5cce5a6df45](https://towardsdatascience.com/data-to-text-generation-with-t5-building-a-simple-yet-advanced-nlg-model-b5cce5a6df45) 

Sample Input: ""Teacher"", ""Great"".

Output: ""He is a great teacher and everyone needs to learn from him""

&amp;#x200B;

I want to do this for a university project and  I don't have a lot of time. do you think this is a good idea or should I change the topic. 

I have a dataset of sample outputs. I am new to this. Any help is appreciated thanks.",647,LanguageTechnology,1
"Apache Superset for textual data ? The github description of Apache Superset is:

&gt; Apache Superset is a Data Visualization and Data Exploration Platform 

ref: https://github.com/apache/superset

The data I am looking at is semi-structured text (html), and I can reach to some structured data (knowledge base) related to the texts.

Questions:

A) Do you use Superset for exploring textual data? How?

B) Is there another tool similar to Superset that is specifically crafted to visualize / explore / analyze text such a html?

C) If there is no existing library / app / api / service, what would be the bare minimal features you would need in such tool?",658,LanguageTechnology,1
"Is it just me, or are the majority of research papers dealing with relation extraction written by Chinese institutions? So...yeah this has really been on the back of my mind and it may be more appropriate for r/TooAfraidToAsk but yeah...

I'm working on a relation extraction research project, and I've noticed that almost every single paper that I read is written by something related to China (school, institution, author, etc.).

Is there some sort of reason why this might be? Or have I just not read enough papers?",519,LanguageTechnology,1
"Applying gensim topic model on new document Hi,

I am currently working on a topic modelling project with gensim and i have some issues, regarding the tf\_idf vectors for adding a new document.

&amp;#x200B;

The preprocessed and tokenized data is stored in a pandas dataframe (using spaCy). I splitted the data into 2 parts train and test. (2 seperate dataframes). 

Gensim requires to first create a dictionary. The dictionary is expandable, so i first created it with only the train data and afterwards expand it with the test data, if there are possibly some new tokens.

Afterward the corpus is created ( which are basically just BOW vectors). The tf-idf vectorizer is created on top of this corpus and then applyed on the corpus:

""""""

dataset = corpora.Dictionary()

dataset.add\_documents(dataframe\['tokens'\].tolist())

corpus = \[dataset.doc2bow(text) for text in dataframe\['tokens'\].tolist()\]

tfidf\_vectorizer = TfidfModel(corpus)

tfidf = tfidf\_vectorizer\[corpus\]

""""""

&amp;#x200B;

Question 1: For applying the model to a new document, i would need the tfidf vectors of the new document. Do i have to just add the document to the old corpus, create a new tfidf\_vectorizer with that corpus and then just apply the vectorizer to only the new document? Or am i missing something?

Question 2: As i saw in the documentation, the model is applied to a new document with the following code( i am using LsiModel):

""""""

model = LsiModel(tfidf\_vect\[corpus\], id2word=dataset)

vector = model\[tfidf\_vect\[new\_corpus\]

""""""

Print(vector) prints :  &lt;gensim.interfaces.TransformedCorpus at 0x245468431&gt;. 

How can i visualize / just print the resulting vector ( i guess it should be the topic distribution of the new document)? Or am i doing something wrong here ?",1788,LanguageTechnology,1
Paper: Investigating the Limitations of the Transformers with Simple Arithmetic Tasks ,86,LanguageTechnology,1
"Best MOOCs/Projects for MS in CL/NLP (Linguistics Background) Hi everyone! 

I come from a language/linguistics background. I got my BA in Spanish with a minor in Linguistics. I’ve taken classes like Syntax, phonetics, phonology, language technology, Hispanic linguistics, and “understanding machine learning.”

I’m looking to apply to masters in Voice Tech, comp ling or NLP this year and I would like to boost the technical side of my knowledge and CV so that the universities see that I could be a good fit for their programs. 

I’ve already completed two courses on Python on Coursera and Codecademy, and I trained a language detection model using naive bayes to distinguish between Spanish and Portuguese texts. I am really eager to keep learning, I just don’t know where to start or what direction to go in.

Are there any online courses or portfolio projects you would recommend to show admissions that I have some background knowledge? Unfortunately I can’t re-enroll at my university at the moment as they are already half way through the semester and it would cost thousands. 

Thanks in advance!",1106,LanguageTechnology,1
"New Masters program in Voice and Speech Tech (in Europe) There is a new 1-year Master’s program at the University of Groningen (the Netherlands) dedicated exclusively to voice/speech tech. Focus is on speech recognition and voice synthesis. 

This is an interdisciplinary program for students from CS, AI,  Linguistics, or similar. 

https://youtu.be/297BY6uTHB8
https://www.rug.nl/masters/voice-technology/",407,LanguageTechnology,1
"Contextual matching of category names Hi everyone!  


I am trying to find a solution for following problem:  
I have a large data frame where documents are labeled with &gt;400 different category names.   
I built a simple ""data explorer"" that takes a free-text query as user input and runs fuzzywuzzy string matching over those category names.

For example, user searches for ""climate change"" and the fuzzy string matching returns the top 10 most similar category names - let's say ""climate\_change\_mitigation"", ""climate\_change\_programme"", etc...Then the user can select all the desired categories and the data-frame is filtered accordingly. So this is quite simple but works well.

Now, I would like to also take some contextual information into account. For example, when user searches for ""plastic"" the fuzzy string matching, of course, only matches with category names that contain the string ""plastic"" or very similar. However, I would like to also obtain categories such as ""Clean Oceans"", ""Marine Ecosystem"", etc.

I was thinking of training a basic word/doc2vec model on the documents, but I am not sure how to use it to match similarity between a string and another lists of strings (the category names), also perhaps this could help in some cases but usually the simple fuzzy string matching would outperform it still I suppose.

Any ideas?

Thanks!",1364,LanguageTechnology,1
"Some questions regarding word embeddings Hey! I have a bunch of questions for a project of mine.

1. For word embeddings, do I need to perform other similarity measures like Tf-Idf? Cause I read somewhere about BoW and Skip-Gram methods used for creating embeddings. 


2. What do we do with word embeddings? Like once we get the word vectors, what is the next step in the process? I'm trying to make a sentiment analysis project with LSTM and word embeddings but I'm not sure how to use the word embeddings. 


3. Are embeddings, features? If not, how do I extract features from data? 

I'm a noob so I'll probably be asking more questions here.",646,LanguageTechnology,1
"Researchers From UC Berkeley, University of Maryland, and UC Irvine Introduce A new Contextual Calibration Approach That Significantly Improves GPT-3 Accuracy Up to 30% A team of researchers at UC Berkeley, University of Maryland, and UC Irvine conducted a study to identify that can cause instability in the GPT-3 language model. The team proposes a contextual calibration procedure that consistently improves GPT-3 accuracy across diverse prompt format choices and examples.

GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. Large language models have significantly improved their few-shot performance, with top models like GPT-3. Few-shot learning allows users to prototype NLP models swiftly. It enables non-technical users to create NLP systems and efficiently reuse models reducing system memory and complexity.

Paper Summary: [https://www.marktechpost.com/2021/02/26/researchers-from-uc-berkeley-university-of-maryland-and-uc-irvine-introduce-a-new-contextual-calibration-approach-that-significantly-improves-gpt-3-accuracy-up-to-30/](https://www.marktechpost.com/2021/02/26/researchers-from-uc-berkeley-university-of-maryland-and-uc-irvine-introduce-a-new-contextual-calibration-approach-that-significantly-improves-gpt-3-accuracy-up-to-30/) 

Paper Source: [https://arxiv.org/pdf/2102.09690.pdf](https://arxiv.org/pdf/2102.09690.pdf) 

Github: [https://github.com/tonyzhaozh/few-shot-learning](https://github.com/tonyzhaozh/few-shot-learning)",1515,LanguageTechnology,1
"How to extract keywords important to a text classification problem? Hi.

I have a text multi-class text classification problem, in which I'm trying to classify different subreddits' comments using a very simple TFIDF + PCA + SVM pipeline. What I'm really keen to know is that how different keywords in each class contribute to this classification problem. How can I do this? I have around 10 classes each having 5000 comments with 30 words in average.",451,LanguageTechnology,1
Intro to AI Bias ,17,LanguageTechnology,1
"Bringing deep learning into our work tools: Waitlist open &amp; Feedback welcome! We're building an ML-(graph)-based tool for file &amp; knowledge management and are just onboarding our first alpha &amp; beta users 🙆🏼‍♀️ !

(Just building our little community here [r/reasonal](https://www.reddit.com/r/reasonal/))

You probably know how difficult it is to keep track of knowledge snippets, files, and links. **In fact,** [**30%**](https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/the-social-economy) **of the average ""knowledge worker's"" time is spent searching and locating files, as well as communicating &amp; collaborating internally 🤯**

We are happy to finally bring some ""intelligence"" into our work tools and combine it with an intuitive UX. If you would like to see how our tool can bring structure, transparency, and insights to your work content across your Google Drive, Dropbox, Slack, MS Teams, and browser bookmarks, add yourself to the waitlist and get early access to our beta version: ➡️ [https://reason.al](https://reason.al/?utm_source=reddit.com&amp;utm_medium=referral&amp;utm_campaign=WL-comm-rd-003-LT)

❗ If you fill in the survey and are onboarded on the closed beta, you'll receive 5 premium accounts for a lifetime (to be used by yourself or given to your friends 😻). The premium account gets all features and integrations and includes one workspace with up to 10 members 👍🏼

💡 In a nutshell, the underlying ML surfaces the right content among work files or highlights outdated/unreliable/faulty files before you open them. It is based on (combined 😇 ) more than 10 years of our founders' [/u/btabibian](https://www.reddit.com/u/btabibian/) [/u/musically\_ut](https://www.reddit.com/u/musically_ut/) research:

[https://dl.acm.org/doi/abs/10.1145/3038912.3052672](https://dl.acm.org/doi/abs/10.1145/3038912.3052672)

[https://arxiv.org/abs/1905.05305](https://arxiv.org/abs/1905.05305)

[http://proceedings.mlr.press/v124/tabibian20a](http://proceedings.mlr.press/v124/tabibian20a)

[https://www.pnas.org/content/116/10/3988.short](https://www.pnas.org/content/116/10/3988.short)

[https://dl.acm.org/doi/abs/10.1145/3018661.3018685](https://dl.acm.org/doi/abs/10.1145/3018661.3018685)

[https://dl.acm.org/doi/abs/10.1145/2939672.2939875](https://dl.acm.org/doi/abs/10.1145/2939672.2939875)

[https://arxiv.org/abs/1805.09360](https://arxiv.org/abs/1805.09360)",2430,LanguageTechnology,1
"Information Retrieval in Rust Hello everyone,

  
I wrote an article on Information Retrieval and implemented a basic search engine in Rust.

Feedback is much appreciated :)

[https://luc-sydney-georges.medium.com/how-to-never-lose-your-stuff-again-pt-1-b06a9d4e0040](https://luc-sydney-georges.medium.com/how-to-never-lose-your-stuff-again-pt-1-b06a9d4e0040)",359,LanguageTechnology,1
"Debugging Transformer based classification model for its behavior Hi,

I have fined tuned a based-based classification model. I want to understand why model predicting a certain tag. Are there any tools for checking this behavior or study  what's going on inside the hood.",272,LanguageTechnology,1
"Is there a bare-bones text rank package for python? I'm working on an unsupervised summary extraction problem for an under-resourced language.

I would prefer to use an out-of-the-box solution in the following form - I provide sentence embeddings (e.g. using a trained BERT model), and the text rank package uses them to find the most relevant ones.

However, a google search made me realize that existing solutions mostly rely on popular packages, e.g. PyTextRank works on top of spacy, and when I run it I get an error that 'noun\_chunks' syntax iterator is not implemented for my language.

Any suggestions how to do it better?",630,LanguageTechnology,1
[R] AAMAS 2021: A framework for integrating gesture generation models into interactive conversational agents ,109,LanguageTechnology,1
"Want to extract semantic meaning from implied context and norms, I need direction of what to search for I’m curious about determining semantic meaning in cases where context implies more than what is available in the sentence. 

For example: “I only wear that shirt because she gave it to me”. The implied meaning is that “I” don’t like the shirt. 

Is there any dataset or tool to extract these implied or norm based meanings? Thank you!",438,LanguageTechnology,1
"Emotion extraction from subtitles of the movies Hi everyone, 

i'm trying to implement a system that from the subtitles of movies/tv series extracts emotion that film causes. I found dataset named DailyDialog that contain 103K sentences labelled with 6 based Ekman's emotion plus no\_emotion label.

I'm trying to use BERT and i reached an 85% of accuracy. The problem is that dataset is very unbalanced. I'm trying to use BERT for data augmentation using it as masked language modelling (MLM) for generate new sentences with new words that fit in the context of the sentence. For every sentence the data aug generates ten sencentes. The problem is  that the word outputed by BERT it is not sure it is conforms to the emotion of the sencente. I can try to see with SenticNet if the word is related to the emotion of sentence.

Another thing is that Bert immediately goes into overfitting.

\- Anyone have any suggestions if the project is feasible?  
\- Do you know any datasets related to the described task?  
\- Do you have any suggestions on how to do a good data augmentation?",1081,LanguageTechnology,1
"Given the vast amount of unstructured data captured in e-Health Records, there's an immediate high demand for NLP to facilitate automatic extraction &amp; structuring of clinical data for decision support ",205,LanguageTechnology,1
"Train Multi-Lingual classifier for 100 languages in 1 Line, Hindi Word Embeddings, Bengali NER NYC/DC Meetup Webinar, in NLU 1.1.2 
## NLU 1.1.2 Release Notes

We are very happy to announce NLU 1.1.2 has been released with the integration of 30+ models and pipelines Bengali Named Entity Recognition, Hindi Word Embeddings,
and state-of-the-art transformer based OntoNotes models and pipelines from the [incredible Spark NLP 2.7.3 Release](https://github.com/JohnSnowLabs/spark-nlp/releases/tag/2.7.3) in addition to a few bugfixes.  
In addition to that, there is a [new NLU Webinar video](https://www.youtube.com/watch?t=2141&amp;v=hJR9m3NYnwk&amp;feature=youtu.be) showcasing in detail 
how to use NLU to analyze a crypto news dataset to extract keywords unsupervised and predict sentimential/emotional distributions of the dataset and much more!

### [Python's NLU library: 1,000+ models, 200+ Languages, State of the Art Accuracy, 1 Line of code - NLU NYC/DC NLP Meetup Webinar](https://www.youtube.com/watch?t=2141&amp;v=hJR9m3NYnwk&amp;feature=youtu.be)
Using just 1 line of Python code by leveraging the NLU library, which is powered by the award-winning Spark NLP.

This webinar covers, using live coding in real-time,
how to deliver summarization, translation, unsupervised keyword extraction, emotion analysis,
question answering, spell checking, named entity recognition, document classification, and other common NLP tasks. T
his is all done with a single line of code, that works directly on Python strings or pandas data frames.
Since NLU is based on Spark NLP, no code changes are required to scale processing to multi-core or cluster environment - integrating natively with Ray, Dask, or Spark data frames.

The recent releases for Spark NLP and NLU include pre-trained models for over 200 languages and language detection for 375 languages.
This includes 20 languages families; non-Latin alphabets; languages that do not use spaces for word segmentation like
Chinese, Japanese, and Korean; and languages written from right to left like Arabic, Farsi, Urdu, and Hebrew.
We'll also cover some of the algorithms and models that are included. The code notebooks will be freely available online.

 

### NLU 1.1.2 New Models  and Pipelines

#### NLU 1.1.2 New Non-English Models

|Language | nlu.load() reference | Spark NLP Model reference | Type |
|---------|---------------------|----------------------------|------|
|Bengali | [bn.ner](https://nlp.johnsnowlabs.com/2021/01/27/ner_jifs_glove_840B_300d_bn.html) |[ner_jifs_glove_840B_300d](https://nlp.johnsnowlabs.com/2021/01/27/ner_jifs_glove_840B_300d_bn.html) | Word Embeddings Model (Alias) |
| Bengali  | [bn.ner.glove](https://nlp.johnsnowlabs.com/2021/01/27/ner_jifs_glove_840B_300d_bn.html) | [ner_jifs_glove_840B_300d](https://nlp.johnsnowlabs.com/2021/01/27/ner_jifs_glove_840B_300d_bn.html) | Word Embeddings Model (Alias) |
|Hindi|[hi.embed](https://nlp.johnsnowlabs.com/2021/02/03/hindi_cc_300d_hi.html)|[hindi_cc_300d](https://nlp.johnsnowlabs.com/2021/02/03/hindi_cc_300d_hi.html)|NerDLModel|
|Bengali | [bn.lemma](https://nlp.johnsnowlabs.com/2021/01/20/lemma_bn.html) |[lemma](https://nlp.johnsnowlabs.com/2021/01/20/lemma_bn.html) | Lemmatizer                    |
|Japanese | [ja.lemma](https://nlp.johnsnowlabs.com/2021/01/15/lemma_ja.html) |[lemma](https://nlp.johnsnowlabs.com/2021/01/15/lemma_ja.html) | Lemmatizer                    |
|Bihari | [bh.lemma](https://nlp.johnsnowlabs.com/2021/01/18/lemma_bh.html) |[lemma](https://nlp.johnsnowlabs.com/2021/01/18/lemma_bh.html) | Lemma                    |
|Amharic | [am.lemma](https://nlp.johnsnowlabs.com/2021/01/20/lemma_am.html) |[lemma](https://nlp.johnsnowlabs.com/2021/01/20/lemma_am.html) | Lemma                    |

#### NLU 1.1.2 New English Models and Pipelines

|Language | nlu.load() reference | Spark NLP Model reference | Type |
|---------|---------------------|----------------------------|------|
| English | [en.ner.onto.bert.small_l2_128](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L2_128_en.html) |[onto_small_bert_L2_128](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L2_128_en.html)     | NerDLModel |
| English | [en.ner.onto.bert.small_l4_256](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L4_256_en.html) |[onto_small_bert_L4_256](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L4_256_en.html)     | NerDLModel |
| English | [en.ner.onto.bert.small_l4_512](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L4_512_en.html) |[onto_small_bert_L4_512](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L4_512_en.html)     | NerDLModel |
| English | [en.ner.onto.bert.small_l8_512](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L8_512_en.html) |[onto_small_bert_L8_512](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L8_512_en.html)     | NerDLModel |
| English | [en.ner.onto.bert.cased_base](https://nlp.johnsnowlabs.com/2020/12/05/onto_bert_base_cased_en.html) |[onto_bert_base_cased](https://nlp.johnsnowlabs.com/2020/12/05/onto_bert_base_cased_en.html)     | NerDLModel |
| English | [en.ner.onto.bert.cased_large](https://nlp.johnsnowlabs.com/2020/12/05/onto_bert_large_cased_en.html) |[onto_bert_large_cased](https://nlp.johnsnowlabs.com/2020/12/05/onto_bert_large_cased_en.html)     | NerDLModel |
| English | [en.ner.onto.electra.uncased_small](https://nlp.johnsnowlabs.com/2020/12/05/onto_electra_small_uncased_en.html) |[onto_electra_small_uncased](https://nlp.johnsnowlabs.com/2020/12/05/onto_electra_small_uncased_en.html)     | NerDLModel |
| English  | [en.ner.onto.electra.uncased_base](https://nlp.johnsnowlabs.com/2020/12/05/onto_electra_base_uncased_en.html) |[onto_electra_base_uncased](https://nlp.johnsnowlabs.com/2020/12/05/onto_electra_base_uncased_en.html)     | NerDLModel |
| English | [en.ner.onto.electra.uncased_large](https://nlp.johnsnowlabs.com/2020/12/05/onto_electra_large_uncased_en.html) |[onto_electra_large_uncased](https://nlp.johnsnowlabs.com/2020/12/05/onto_electra_large_uncased_en.html)     | NerDLModel |
| English | [en.ner.onto.bert.tiny](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_tiny_en.html) | [onto_recognize_entities_bert_tiny](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_tiny_en.html) | Pipeline |
| English | [en.ner.onto.bert.mini](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_mini_en.html) |[onto_recognize_entities_bert_mini](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_mini_en.html)     | Pipeline |
| English | [en.ner.onto.bert.small](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_small_en.html) | [onto_recognize_entities_bert_small](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_small_en.html) | Pipeline |
| English | [en.ner.onto.bert.medium](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_medium_en.html) |[onto_recognize_entities_bert_medium](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_medium_en.html)     | Pipeline |
| English | [en.ner.onto.bert.base](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_base_en.html) |[onto_recognize_entities_bert_base](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_base_en.html)     | Pipeline |
|English|[en.ner.onto.bert.large](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_large_en.html)|[onto_recognize_entities_bert_large](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_large_en.html)|Pipeline|
|English|[en.ner.onto.electra.small](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_small_en.html)|[onto_recognize_entities_electra_small](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_small_en.html)|Pipeline|
|English|[en.ner.onto.electra.base](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_base_en.html)|[onto_recognize_entities_electra_base](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_base_en.html)|Pipeline|
|English|[en.ner.onto.large](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_large_en.html)|[onto_recognize_entities_electra_large](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_large_en.html)|Pipeline|



### New Tutorials and Notebooks

- [NYC/DC NLP Meetup Webinar video analyze Crypto News, Unsupervised Keywords, Translate between 300 Languages, Question Answering, Summerization, POS, NER in 1 line of code in almost just 20 minutes](https://www.youtube.com/watch?t=2141&amp;v=hJR9m3NYnwk&amp;feature=youtu.be)
- [NLU basics POS/NER/Sentiment Classification/BERTology Embeddings](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/NYC_DC_NLP_MEETUP/0_liners_intro.ipynb)
- [Explore Crypto Newsarticle dataset, unsupervised Keyword extraction, Stemming, Emotion/Sentiment distribution Analysis](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/NYC_DC_NLP_MEETUP/1_NLU_base_features_on_dataset_with_YAKE_Lemma_Stemm_classifiers_NER_.ipynb)
- [Translate between more than 300 Languages in 1 line of code with the Marian Models](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/NYC_DC_NLP_MEETUP/2_multilingual_translation_with_marian.ipynb)
- [New NLU 1.1.2 Models Showcase Notebooks, Bengali NER, Hindi Embeddings, 30 new_models](https://colab.research.google.com/github/JohnSnowLabs/nlu/blob/master/examples/release_notebooks/NLU1.1.2_Bengali_ner_Hindi_Embeddings_30_new_models.ipynb)


### NLU 1.1.2 Bug Fixes

- Fixed a bug that caused NER confidences not beeing extracted
- Fixed a bug that caused nlu.load('spell') to crash
- Fixed a bug that caused Uralic/Estonian/ET language models not to be loaded properly


### New  Easy NLU 1-liners in 1.1.2


#### [Named Entity Recognition for Bengali (GloVe 840B 300d)](https://nlp.johnsnowlabs.com/2021/01/27/ner_jifs_glove_840B_300d_bn.html)


```python
#Bengali for :  It began to be widely used in the United States in the early '90s.
nlu.load(""bn.ner"").predict(""৯০ এর দশকের শুরুর দিকে বৃহৎ আকারে মার্কিন যুক্তরাষ্ট্রে এর প্রয়োগের প্রক্রিয়া শুরু হয়'"")
```
output :

|   entities             | token     | Entities_classes   |   ner_confidence |
|:---------------------|:----------|:----------------------|-----------------:|
| ['মার্কিন যুক্তরাষ্ট্রে'] | ৯০        | ['LOC']               |           1      |
| ['মার্কিন যুক্তরাষ্ট্রে'] | এর        | ['LOC']               |           0.9999 |
| ['মার্কিন যুক্তরাষ্ট্রে'] | দশকের     | ['LOC']               |           1      |
| ['মার্কিন যুক্তরাষ্ট্রে'] | শুরুর       | ['LOC']               |           0.9969 |
| ['মার্কিন যুক্তরাষ্ট্রে'] | দিকে      | ['LOC']               |           1      |
| ['মার্কিন যুক্তরাষ্ট্রে'] | বৃহৎ       | ['LOC']               |           0.9994 |
| ['মার্কিন যুক্তরাষ্ট্রে'] | আকারে     | ['LOC']               |           1      |
| ['মার্কিন যুক্তরাষ্ট্রে'] | মার্কিন    | ['LOC']               |           0.9602 |
| ['মার্কিন যুক্তরাষ্ট্রে'] | যুক্তরাষ্ট্রে | ['LOC']               |           0.4134 |
| ['মার্কিন যুক্তরাষ্ট্রে'] | এর        | ['LOC']               |           1      |
| ['মার্কিন যুক্তরাষ্ট্রে'] | প্রয়োগের   | ['LOC']               |           1      |
| ['মার্কিন যুক্তরাষ্ট্রে'] | প্রক্রিয়া   | ['LOC']               |           1      |
| ['মার্কিন যুক্তরাষ্ট্রে'] | শুরু        | ['LOC']               |           0.9999 |
| ['মার্কিন যুক্তরাষ্ট্রে'] | হয়        | ['LOC']               |           1      |
| ['মার্কিন যুক্তরাষ্ট্রে'] | '         | ['LOC']               |           1      |


#### [Bengali Lemmatizer](https://nlp.johnsnowlabs.com/2021/01/20/lemma_bn.html)


```python
#Bengali for :  One morning in the marble-decorated building of Vaidyanatha, an obese monk was engaged in the enchantment of Duis and the milk service of one and a half Vaidyanatha. Give me two to eat
nlu.load(""bn.lemma"").predict(""একদিন প্রাতে বৈদ্যনাথের মার্বলমণ্ডিত দালানে একটি স্থূলোদর সন্ন্যাসী দুইসের মোহনভোগ এবং দেড়সের দুগ্ধ সেবায় নিযুক্ত আছে বৈদ্যনাথ গায়ে একখানি চাদর দিয়া জোড়করে একান্ত বিনীতভাবে ভূতলে বসিয়া ভক্তিভরে পবিত্র ভোজনব্যাপার নিরীক্ষণ করিতেছিলেন এমন সময় কোনোমতে দ্বারীদের দৃষ্টি এড়াইয়া জীর্ণদেহ বালক সহিত একটি অতি শীর্ণকায়া রমণী গৃহে প্রবেশ করিয়া ক্ষীণস্বরে কহিল বাবু দুটি খেতে দাও"")

```
output :

| lemma                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | document                                                                                                                                                                                                                                                                                                                                          |
|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ['একদিন', 'প্রাতঃ', 'বৈদ্যনাথ', 'মার্বলমণ্ডিত', 'দালান', 'এক', 'স্থূলউদর', 'সন্ন্যাসী', 'দুইসের', 'মোহনভোগ', 'এবং', 'দেড়সের', 'দুগ্ধ', 'সেবা', 'নিযুক্ত', 'আছে', 'বৈদ্যনাথ', 'গা', 'একখান', 'চাদর', 'দেওয়া', 'জোড়কর', 'একান্ত', 'বিনীতভাব', 'ভূতল', 'বসা', 'ভক্তিভরা', 'পবিত্র', 'ভোজনব্যাপার', 'নিরীক্ষণ', 'করা', 'এমন', 'সময়', 'কোনোমত', 'দ্বারী', 'দৃষ্টি', 'এড়ানো', 'জীর্ণদেহ', 'বালক', 'সহিত', 'এক', 'অতি', 'শীর্ণকায়া', 'রমণী', 'গৃহ', 'প্রবেশ', 'বিশ্বাস', 'ক্ষীণস্বর', 'কহা', 'বাবু', 'দুই', 'খাওয়া', 'দাওয়া'] | একদিন প্রাতে বৈদ্যনাথের মার্বলমণ্ডিত দালানে একটি স্থূলোদর সন্ন্যাসী দুইসের মোহনভোগ এবং দেড়সের দুগ্ধ সেবায় নিযুক্ত আছে বৈদ্যনাথ গায়ে একখানি চাদর দিয়া জোড়করে একান্ত বিনীতভাবে ভূতলে বসিয়া ভক্তিভরে পবিত্র ভোজনব্যাপার নিরীক্ষণ করিতেছিলেন এমন সময় কোনোমতে দ্বারীদের দৃষ্টি এড়াইয়া জীর্ণদেহ বালক সহিত একটি অতি শীর্ণকায়া রমণী গৃহে প্রবেশ করিয়া ক্ষীণস্বরে কহিল বাবু দুটি খেতে দাও |


#### [Japanese Lemmatizer](https://nlp.johnsnowlabs.com/2021/01/15/lemma_ja.html)


```python
#Japanese for :  Some residents were uncomfortable with this, but it seems that no one is now openly protesting or protesting.
nlu.load(""ja.lemma"").predict(""これに不快感を示す住民はいましたが,現在,表立って反対や抗議の声を挙げている住民はいないようです。"")

```
output :

| lemma                                                                                                                                                                                                                                                          | document                                                                                         |
|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------|
| ['これ', 'にる', '不快', '感', 'を', '示す', '住民', 'はる', 'いる', 'まする', 'たる', 'がる', ',', '現在', ',', '表立つ', 'てる', '反対', 'やる', '抗議', 'のる', '声', 'を', '挙げる', 'てる', 'いる', '住民', 'はる', 'いる', 'なぐ', 'よう', 'です', '。'] | これに不快感を示す住民はいましたが,現在,表立って反対や抗議の声を挙げている住民はいないようです。 |

#### [Aharic Lemmatizer](https://nlp.johnsnowlabs.com/2021/01/20/lemma_am.html)


```python
#Aharic for :  Bookmark the permalink.
nlu.load(""am.lemma"").predict(""መጽሐፉን መጽሐፍ ኡ ን አስያዛት አስያዝ ኧ ኣት ።"")

```
output  :

| lemma                                                | document                         |
|:-----------------------------------------------------|:---------------------------------|
| ['_', 'መጽሐፍ', 'ኡ', 'ን', '_', 'አስያዝ', 'ኧ', 'ኣት', '።'] | መጽሐፉን መጽሐፍ ኡ ን አስያዛት አስያዝ ኧ ኣት ። |

#### [Bhojpuri Lemmatizer](https://nlp.johnsnowlabs.com/2021/01/18/lemma_bh.html)


```python
#Bhojpuri for : In this event, participation of World Bhojpuri Conference, Purvanchal Ekta Manch, Veer Kunwar Singh Foundation, Purvanchal Bhojpuri Mahasabha, and Herf - Media.
nlu.load(""bh.lemma"").predict(""एह आयोजन में विश्व भोजपुरी सम्मेलन , पूर्वांचल एकता मंच , वीर कुँवर सिंह फाउन्डेशन , पूर्वांचल भोजपुरी महासभा , अउर हर्फ - मीडिया के सहभागिता बा ।"")
```

output :

| lemma                                                                                                                                                                                                                               | document                                                                                                                      |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------|
| ['एह', 'आयोजन', 'में', 'विश्व', 'भोजपुरी', 'सम्मेलन', 'COMMA', 'पूर्वांचल', 'एकता', 'मंच', 'COMMA', 'वीर', 'कुँवर', 'सिंह', 'फाउन्डेशन', 'COMMA', 'पूर्वांचल', 'भोजपुरी', 'महासभा', 'COMMA', 'अउर', 'हर्फ', '-', 'मीडिया', 'को', 'सहभागिता', 'बा', '।'] | एह आयोजन में विश्व भोजपुरी सम्मेलन , पूर्वांचल एकता मंच , वीर कुँवर सिंह फाउन्डेशन , पूर्वांचल भोजपुरी महासभा , अउर हर्फ - मीडिया के सहभागिता बा । |

#### [Named Entity Recognition - BERT Tiny (OntoNotes)](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L2_128_en.html)
```python
nlu.load(""en.ner.onto.bert.small_l2_128"").predict(""""""William Henry Gates III (born October 28, 1955) is an American business magnate,
 software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft,
  Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect,
   while also being the largest individual shareholder until May 2014.
    He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico;
     it went on to become the world's largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect.
     During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time
      role at Microsoft and full-time work at the Bill &amp; Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.
 He gradually transferred his duties to Ray Ozzie and Craig Mundie.
  He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella."""""",output_level = ""document"")
```

output  :

| ner_confidence | entities | Entities_classes                                          |
| :------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| [0.8536999821662903, 0.7195000052452087, 0.746...] | ['PERSON', 'DATE', 'NORP', 'ORG', 'ORG', 'PERSON', 'DATE', 'CARDINAL', 'DATE', 'DATE', 'GPE', 'GPE', 'PERSON', 'DATE', 'GPE', 'GPE'] | ['William Henry Gates III', 'October 28, 1955', 'American', 'Microsoft Corporation', 'Microsoft', 'Gates', 'May 2014', 'one', '1970s', '1980s', 'Seattle', 'Washington', 'Paul Allen', '1975', 'Albuquerque', 'New Mexico'] |

####  [Named Entity Recognition - BERT Mini (OntoNotes)](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L4_256_en.html)
```python
nlu.load(""en.ner.onto.bert.small_l4_256"").predict(""""""William Henry Gates III (born October 28, 1955) is an American business magnate,
 software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft,
  Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect,
   while also being the largest individual shareholder until May 2014.
    He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico;
     it went on to become the world's largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect.
     During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time
      role at Microsoft and full-time work at the Bill &amp; Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.
 He gradually transferred his duties to Ray Ozzie and Craig Mundie.
  He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella."""""",output_level = ""document"")
```

output :

|  ner_confidence	  | entities                                                                                                                                                                                                                                           | Entities_classes                                                                                                                     |
|---------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------|
|          [0.835099995136261, 0.40450000762939453, 0.331...] | ['William Henry Gates III', 'October 28, 1955', 'American', 'Microsoft Corporation', 'Microsoft', 'Gates', 'May 2014', 'one', '1970s and 1980s', 'Seattle', 'Washington', 'Gates', 'Microsoft', 'Paul Allen', '1975', 'Albuquerque', 'New Mexico'] | ['PERSON', 'DATE', 'NORP', 'ORG', 'ORG', 'ORG', 'DATE', 'CARDINAL', 'DATE', 'GPE', 'GPE', 'ORG', 'ORG', 'PERSON', 'DATE', 'GPE', 'GPE'] |




#### [Named Entity Recognition - BERT Small (OntoNotes)](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L4_512_en.html)

```python
nlu.load(""en.ner.onto.bert.small_l4_512"").predict(""""""William Henry Gates III (born October 28, 1955) is an American business magnate,
 software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft,
  Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect,
   while also being the largest individual shareholder until May 2014.
    He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico;
     it went on to become the world's largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect.
     During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time
      role at Microsoft and full-time work at the Bill &amp; Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.
 He gradually transferred his duties to Ray Ozzie and Craig Mundie.
  He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella."""""",output_level = ""document"")
```
output :

|   ner_confidence | entities                                                                                                                                                                                                                                               | Entities_classes                                                                                                                           |
|---------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------|
|              [0.964900016784668, 0.8299000263214111, 0.9607...]| ['William Henry Gates III', 'October 28, 1955', 'American', 'Microsoft Corporation', 'Microsoft', 'Gates', 'May 2014', 'one', 'the 1970s and 1980s', 'Seattle', 'Washington', 'Gates', 'Microsoft', 'Paul Allen', '1975', 'Albuquerque', 'New Mexico'] | ['PERSON', 'DATE', 'NORP', 'ORG', 'ORG', 'PERSON', 'DATE', 'CARDINAL', 'DATE', 'GPE', 'GPE', 'PERSON', 'ORG', 'PERSON', 'DATE', 'GPE', 'GPE'] |


#### [Named Entity Recognition - BERT Medium (OntoNotes)](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L8_512_en.html)

```python
nlu.load(""en.ner.onto.bert.small_l8_512"").predict(""""""William Henry Gates III (born October 28, 1955) is an American business magnate,
 software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft,
  Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect,
   while also being the largest individual shareholder until May 2014.
    He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico;
     it went on to become the world's largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect.
     During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time
      role at Microsoft and full-time work at the Bill &amp; Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.
 He gradually transferred his duties to Ray Ozzie and Craig Mundie.
  He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella."""""",output_level = ""document"")
```
output :

| ner_confidence   | entities                                                                                                                                                                                                                           | Entities_classes                                                                                                        |
|---------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|
|        [0.916700005531311, 0.5873000025749207, 0.8816...] | ['William Henry Gates III', 'October 28, 1955', 'American', 'Microsoft Corporation', 'Microsoft', 'Gates', 'May 2014', 'the 1970s and 1980s', 'Seattle', 'Washington', 'Gates', 'Paul Allen', '1975', 'Albuquerque', 'New Mexico'] | ['PERSON', 'DATE', 'NORP', 'ORG', 'ORG', 'PERSON', 'DATE', 'DATE', 'GPE', 'GPE', 'PERSON', 'PERSON', 'DATE', 'GPE', 'GPE'] |



#### [Named Entity Recognition - BERT Base (OntoNotes)](https://nlp.johnsnowlabs.com/2020/12/05/onto_bert_base_cased_en.html)

```python
nlu.load(""en.ner.onto.bert.cased_base"").predict(""""""William Henry Gates III (born October 28, 1955) is an American business magnate,
 software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft,
  Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect,
   while also being the largest individual shareholder until May 2014.
    He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico;
     it went on to become the world's largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect.
     During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time
      role at Microsoft and full-time work at the Bill &amp; Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.
 He gradually transferred his duties to Ray Ozzie and Craig Mundie.
  He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella."""""",output_level = ""document"")
```
output :

|   ner_confidence | entities                                                                                                                                                                                                                                               | Entities_classes                                                                                                                           |
|---------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------|
|              [0.504800021648407, 0.47290000319480896, 0.462...] | ['William Henry Gates III', 'October 28, 1955', 'American', 'Microsoft Corporation', 'Microsoft', 'Gates', 'May 2014', 'one', 'the 1970s and 1980s', 'Seattle', 'Washington', 'Gates', 'Microsoft', 'Paul Allen', '1975', 'Albuquerque', 'New Mexico'] | ['PERSON', 'DATE', 'NORP', 'ORG', 'ORG', 'PERSON', 'DATE', 'CARDINAL', 'DATE', 'GPE', 'GPE', 'PERSON', 'ORG', 'PERSON', 'DATE', 'GPE', 'GPE'] |





### NLU Installation

```bash
# PyPi
!pip install nlu pyspark==2.4.7
#Conda
# Install NLU from Anaconda/Conda
conda install -c johnsnowlabs nlu
```

### Additional NLU ressources
- [NLU Website](https://nlu.johnsnowlabs.com/)
- [All NLU Tutorial Notebooks](https://nlu.johnsnowlabs.com/docs/en/notebooks)
- [NLU Videos and Blogposts on NLU](https://nlp.johnsnowlabs.com/learn#pythons-nlu-library)
- [NLU on Github](https://github.com/JohnSnowLabs/nlu)",33401,LanguageTechnology,1
"OpenAI has released the paper associated with DALL-E: ""Zero-Shot Text-to-Image Generation"" ",91,LanguageTechnology,1
MBart Language Translation (English to Multilanguage) using Hugging Face ,73,LanguageTechnology,1
"Retrain GPT-2 model with tensorflow2 I was trying to catch up with transformer and gpt-2 implementation detail. So I have created some scripts to support retraining gpt-2 model using tensorflow2: [https://github.com/wenxichen/gpt-2](https://github.com/wenxichen/gpt-2)

Have fun!",279,LanguageTechnology,1
"How do you make a model for gender recognition of speech in an audio file? I have short audio segments, lasting 5-10 seconds each, of a single speaker speaking a single sentence. I need gender recognition on this. I need an algorithm that I feed an audio file into, and it spit out a result of if this is a male speaker, or this is a female speaker. And I need to run this on tens of thousands of audio files in a dataset.

How do I do this? What features do I use to determine this? Do I need machine learning or can I use some fixed numbers to figure it out? How do I access these stats/numbers that I need to figure this out?",628,LanguageTechnology,1
"Parsing with statistical method Hello, I'm quite new to parsing in NLP and I'm still confuse about some stuff.

1) I saw that CKY algorithm only work with CNF grammar, but on some research paper i saw they use CKY with PCFG and CCG. Can PCFG and CCG, be transformed into CNF?

2) I would like to make a parser from scratch, using my own lexicon and grammar extracted from a corpus in my language, with probabilistic method. I wanted to start with probabilistic CKY is it a good idea?

Thanks in advance Have a nice day!",519,LanguageTechnology,1
"Text-to-image Google Colab notebook ""Aleph-Image: CLIPxDAll-E"" has been released. This notebook uses OpenAI's CLIP neural network to steer OpenAI's DALL-E image generator to try to match a given text description. ",213,LanguageTechnology,1
"Looking for machine translation papers in other languages mt-archive.info is a really neat site, it's an archive of papers on machine translation. However, they're only in English. I know such research has been going on outside the Anglosphere as well since the beginning, do you know where I can find similar archives in other languages?",338,LanguageTechnology,1
"Tutorial on how to calculate summary statistics by group Hey, I've created a tutorial on how to calculate summary statistics by group in the R programming language: [https://statisticsglobe.com/summary-statistics-by-group-in-r](https://statisticsglobe.com/summary-statistics-by-group-in-r)",289,LanguageTechnology,1
Swearing in programming ,24,LanguageTechnology,1
Does anyone know where I could get some tagged pos/neg/neutral restaurant reviews? I want to create a sentiment analysis for a class using NLTK but not on movie reviews or Twitter data which seem to be the two most popular. Does anyone know where I could find a large set of sentiment tagged restaurant reviews? Thank you,321,LanguageTechnology,1
Recent Advances in Language Model Fine-tuning ,46,LanguageTechnology,1
"How do cross-lingual encoders work? Hello! I have a small question about cross lingual encoders (after having read numerous papers and going through so much code, I am still a little lost about something pretty basic lol)

Once we train cross lingual word embeddings (using a mapping method like [VecMap](https://github.com/artetxem/vecmap)), we get two resultant word embeddings: source_mapped and target_mapped. So when we use a ""cross-lingual"" encoder and we just copy the source_mapped embedding parameters into the encoder, how are we utilising the cross-lingual signal exactly? I would understand if the initial encoder mapping is done like source_mapped-&gt;target_mapped-&gt;decoder hidden state-&gt;backward pass-&gt;....-&gt; trained model, but that doesn't seem to be the case. So if someone has experience with cross-lingual encoders, could you explain how exactly it would work/be implement after getting the source and target embeddings mapped (similarly, how it would work if they were trained in a joint fashion)? Thank you so much!!",1049,LanguageTechnology,1
Paper: Calibrate Before Use: Improving Few-Shot Performance of GPT-3 ,69,LanguageTechnology,1
"Whats the state of art for anaphora resolution? I'm trying to think of a fun project to do for a class, and I found the problem of anaphora resolution interesting. 

Is this a problem that is essentially ""solved"" or is it still being actively researched? 

What are some state-of-the-art methods? Could I apply BERT? What are some papers I could read about?  Thank you!",369,LanguageTechnology,1
"Question: What is the commercial value of NLP? I am very interested in this topic but learning and getting academic certifications takes time and effort, as topic, is it worth it?",179,LanguageTechnology,1
"How to convert pre-processed text into feature vectors I have used pos tagging, stop words removal and lemmatization to pre process the given sentence. After that I've got the output as list of lists. 

    Somethin like this :   [['explain', 'VERB'], ['briefly', 'ADV'], ['working', 'NOUN'], ['merge', 'NOUN'], ['sort', 'NOUN'], ['.', 'PUNCT']] 

Now I have to convert this into feature vectors so that I could pass these vectors as an input to an classification model. But I'm not sure how to use pre processed text as an input to feature vectorizers.

If anybody has any inputs regarding this, it would be helpful. Any links that address the same problem would be appreciated. If it's not the sub to ask these questions, please guide me to the proper sub  
Thanks a lot in advance :)",786,LanguageTechnology,1
"BERT-QE: Contextualized Query Expansion for Document Re-ranking (Research Paper Walkthrough) Query expansion is the process of reformulating a given query to improve retrieval performance in information retrieval operations, particularly in the context of query understanding. 🤠
This paper uses BERT to find relevant chunks from the documents as possible augmentations to the given query. 🔥 

Paper Walkthrough: https://youtu.be/WAv6LsIJZbs",440,LanguageTechnology,1
"Any interest in BERT-related (or LDA) acceleration (5x)? &lt;5-min Survey&gt; I'm a graduate student on the east coast working with a postdoc researcher focused on ML acceleration. We are trying to see what pain points researchers and engineers experience with their ML project to help direct our future research projects.

We have the belief that the team is capable of developing a solution towards accelerating BERT through training times and model sizes.

We are hoping to get some opinions from the community on whether this would be something for us as researchers worthwhile to pursue. We also would be interested in finding teams looking to test this solution in the future.

If you can spare 5 minutes, please help us by filling out this survey: [https://tufts.qualtrics.com/jfe/form/SV\_bI9I7IDELoJveC2](https://tufts.qualtrics.com/jfe/form/SV_bI9I7IDELoJveC2)",870,LanguageTechnology,1
"Should you standardize/normalize embeddings when using them with a classifier? Standardizing/normalizing is known to help learning when fitting a model using gradient descent.

When using a mix of embeddings and non-text features, how should you approach standardization/normalization? 

a) Standardize/normalize everything per usual.

b) Standardize/normalize non-text features, but leave the word embeddings alone.

c) Don't standardize/normalize anything.

So far I'm leaning towards option c. My reasoning:

\- For embeddings extracted from transformers-based models, there's no sensible notion of mean and standard deviation (in the case of standardization) or maximum or minimum (in the case of normalization).

\- For a fixed vocabulary of word vectors (such as from word2vec), gensim supports norming all the vectors, but I worry that this procedure would lose useful information contained in the embeddings.

\- If you standardize/normalize non-text features only, the model would likely update weights associated with the word embeddings faster due to them likely being on a larger scale, which would hurt learning.",1125,LanguageTechnology,1
Master Thesis: Matching companies that are potential cooperation partners ,74,LanguageTechnology,1
Have you ever had the opportunity to be in Europe and learn Slovak? 😄 ,70,LanguageTechnology,1
A unique self-assessment algorithm App that simulates changes taking place in personality over time based [https://youtu.be/IeACb0nqK24](https://youtu.be/IeACb0nqK24),166,LanguageTechnology,1
Hierarchical Transformers for Long Document Classification (Research Paper Walkthrough) ,88,LanguageTechnology,1
"NLP generated Zork-like text adventure games I forgot where I saw it, and now I can't find it, so now I was hoping someone could tell me where it was.

Someone had been using NLP to create text adventure games.  I remember trying it, selecting a genre, and it was a mess but it was pretty interesting / good.  I wanted to try it again but I can't find it.  They were using some sort of neural net.  Everytime I checked my inventory it was different but still made for a novel game experience.",492,LanguageTechnology,1
"Looking for a code base to implement multi-task learning in NLP I am looking for library or code base to implement MTL. Any ideas on this 

Things on my checklist.

1. Should be able to mix different datasets.
2. Able to accumulate gradients for separate task and propagate.
3. Ability to define custom layers on pretrained models.
4. Single head or specific heads for MTL.
5. Implement my custom loss",401,LanguageTechnology,1
How to use Instance-based Learning to improve the INTERPRETABILITY of NER models | Research Papers Summary 009 ,111,LanguageTechnology,1
torch.nn.Embedding explained (+ Character-level language model) ,64,LanguageTechnology,1
"NLP Real World Challenge to Detect Bias &amp; Misinformation in Articles  

In case you are looking for a real-world project (not a hackathon) to make an impact while building up your project portfolio, here is a collaborative two-month project where 50 engineers from all around the world will work together. 

You can apply here [https://omdena.com/projects/bias/](https://omdena.com/projects/bias/?fbclid=IwAR2WIXjaZuY8vNTMm8uUDqE57cqHftC8NlWQzSLnSiTLrLBV4y6IgnuYd_Q)",470,LanguageTechnology,1
"Future advancements and unsolved problems I am curious to hear about your opinions regarding future advancements and possible solutions to the issues that exist now in the field of language technology.

What do you see as the main issues and do you think something radically has to change in the way we process language data in order to solve those issues? 

Do you think that new paradigms will emerge in the years to come? I guess that's impossible to predict but do you see a tendency towards new ways of processing languages?

What are you excited about the most?",567,LanguageTechnology,1
"What are some classification tasks where BERT-based models don't work well? In a similar vein, what are some generative tasks where fine-tuning GPT-2/LM does not work well? I am looking for problems where BERT has been shown to perform poorly. Additionally, what are some English to English NLP (or any other - same language to the same language)  tasks where fine-tuning GPT-2 is not helpful at all?",400,LanguageTechnology,1
"I was wondering if there were any new insights for text representation using transformers, did a lit search, here are a few highlights 

&gt;tween 0 and 1 by a sigmoid activation function.
Our proposed model (model (b) in Fig. 1) uses additional information from
final hidden states of input tokens t1, t2,...tN . The BERT’s sequence output
is fetched into time-distributed fully connected dense layer with the number of
neurons equal to the number of labels. The output of this layer is pooled in
two different ways using the max-pooling and the average-pooling. Max-pooling
outputs the maximum of each feature across all tokens, thus it reacts on strong
class-related keywords. The average-pooling, on the other hand, outputs the
average of each feature over the sequence and thus attends to all tokens in
the sequence evenly. To compute average-pooling, we clipped all features into
interval [−1, 1] to intentionally suppress the influence of strong keywords. When
computing pooled values, we ignored all padding tokens (denoted as [PAD]).
Finally, the features generated from the pooled output and the output for the
[CLS] token are summed together. 

https://link.springer.com/chapter/10.1007%2F978-3-030-58323-1_23

From this paper, mean pool is best, followed by max pool, cls token rep, and sep token rep. 

https://arxiv.org/pdf/1910.07973.pdf",1352,LanguageTechnology,1
"How do you download data from the Linguistic Data Consortium? Hi. I'm currently trying to download the TACRED dataset. According the Stanford NLP's website, you can download it for free from LDC if you're an LDC member or pay $25.

At the TACRED page on LDC (https://catalog.ldc.upenn.edu/LDC2018T24) I see at the bottom ""Available Media"" and nothing under ""View Fees."" I don't see any way to download the data though.

On my account page it says that the administrator for my school hasn't approved my account yet, but would this matter? I'm still an LDC member even if I'm not officially verified to be a member of my school, right?

Any tips are appreciated. Thanks.",669,LanguageTechnology,1
"Does anybody know of relation extraction datasets that are at the document level? Hey guys. Working on a document-level relation extraction (DocRE) task and I'm wondering what kind of datasets there may be.

In the general domain I've only been able to come across DocRED, and there are also the BC5CDR and GDA datasets in the biomedical domain. I'm wondering what else may be out there that I've missed.

Thanks!",413,LanguageTechnology,1
Nucleus Sampling: The Curious Case of Neural Text Degeneration (Research Paper Walkthrough) ,92,LanguageTechnology,1
"Finding good NLP developers? Hi everyone, any idea as to where I can find a good NLP developer? I tried Upwork but it seems like there are more AI generalists than NLP specialists.

Specifically, 

I need a AI/NLP Developer with AWS Lambda experience to train and deploy GPT-2 model. 

End goal to make a chatbot that will be used as a ‘friend’ for lonely people (see www.replika.ai [advanced] and chatty-app https://apps.apple.com/ca/app/chatty-your-robot-friend/id1020581003 [less advanced]). The chatty-app is based on GPT-2 architecture and is what the first iteration should be similar to.

The first iteration of this project will be a multi user web app that will communicate back and forth with a machine learning API (accessible via high performance gPRC protobufs) running on AWS lambda endpoint. The final product should be maintained 100% by amazon so I won’t need a system engineer/admin.

The GPT-2 model itself will need to be fine tuned on an empathetic dialogue corpus (which I will provide). To keep server costs low, the training should be done via GPU but once trained, the model should use CPU. I am open to any suggestions that will help keep on-going operational server costs lows (for ex. Imposing a slight delay 1-2 seconds between user questions and chatbot response). Ideally the architecture should allow for:

- multi-user support;
- up/down voting of responses so that the model can adapt to each user (via beam search [ex. You could create a tensor of weights assigned to multiple outputs from each question, have the model generate multiple output sequences via beam search, and then promote or penalize the weight associated with that response based on the upvote/downvote]); and
- allow for transfer learning so that the model output can be biased towards end user’s topical and stylistic preferences via Q&amp;A script (or another method).

Any suggestions regarding the overall app and NLP architecture are welcome.",1951,LanguageTechnology,1
Do you think OpenAI's GPT3 is good enough to pass the Turing Test? / The world's largest scale Turing Test ,107,LanguageTechnology,1
"Some questions about Spacy vs Hugging face transformers, fine-tuning and wav2vec. I am new to the NLP game and exploring the available options. I have stumbled across both Spacy and Hugging Face Transformers as python packages that seem applicable to my use cases. However, I am having a surprisingly hard time differentiation between the two packages. I would like to hear your input on the differences between Spacy and Hugging Face and perhaps some use cases in which you would prefer on over the other.

A second question relates to the fine-tuning of the models. It is my understanding that both Spacy and Hugging Face typically require fine-tuning before reasonable accuracy can be expected on domain-specific use cases. Could anyone give an estimate of the number of labeled text files one should expect to need for fine-tuning a model? Again, I am having a hard time finding an estimate for these numbers as most blogs use pre-existing datasets with large amounts of data.

Finally, a third question relates to the Wav2Vec 2 model, which can transcribe audio into text. It is my understanding that this model was trained on multiple languages. However, on [huggingface.co/models](https://huggingface.co/models), I am only finding english models at the moment. Is there some way in which I could use Wav2Vec (preferably with the hugging face package) to transcribe for example French texts?

I would very much appreciate it if you could share your expertise and help me to navigate the woods here.",1504,LanguageTechnology,1
"Can I apply for a PhD after a Master's if I don't have Research Experience? Help!  

Hey Reddit community, 

I am a recent graduate from Columbia University with a masters in financial engineering and a minor in machine learning and I'm looking for advice as I am sort of in a dilemma. I have been actively involved in data science and machine learning since 2017 and have loved working in the space and applying concepts to real-world problems (Worked Full time 2018-2019). In order to build on my knowledge and attain expertise applying ML to financial problems, I took up a masters in financial engineering hoping to leverage the program to work in FinTech roles and startup in the US. Through the course of my masters, I actively took up courses from the CS &amp; Data Science department taking a total of 5 courses (NLP, Applied Deep Learning, Cloud Computing &amp; Big Data, Artificial Intelligence and Recommender Systems) and realized that more than the financial engineering degree wasn't exactly what I wanted to do as I was in love with the field of NLP and the possible applications it can have across the spectrum rather than just limiting myself to applications of AI in finance. 

Unfortunately due to the course load and relatively short term course (3 semesters with 36 credits = 12 courses over 3 semesters) as well as the impact of COVID with classes going virtual since March 2020 I never really got the opportunity to take up roles in research during my masters despite the fact that I was extremely keen on it. The usual process would be to take a class with a professor, excel in the class, approach them for research and work with them after. However, due to the structure of my program, this never panned out. 

Currently, I feel capable enough to take up roles in the industry and work as a data scientist or machine learning engineer but have this burning desire to continue exploring NLP and topics in the space. I am extremely passionate about Abstractive Summarization, Search and Information Retrieval, and also keen to explore Speech Recognition systems. I want to be involved in both developing better algorithms and systems in these areas as well as finding robust inter-disciplinary applications for current systems. As I don't have any research experience I am highly doubtful of my chances as a PhD student. I would love to take up a program like the CMU MLT which gives you the chance to start out as a masters student and then transition to a full-time PhD student. 

I would be grateful for any advice you could provide to me. Would it be better to work for a year and try to be involved in projects in NLP before applying for a PhD and how can an individual with my profile successfully get into a PhD program? (Hopefully Top 10 eventually but more than anything I want to work with a driven like-minded advisor who can guide me in my development)",2888,LanguageTechnology,1
"What are some good open problems in/applications of Paraphrase Generation? The most pertinent use-case I have found for Paraphrase Generation models deals with data-augmentation and adversarial example generation. However, I am looking for other potential applications and would be interested to dive deep into one topic.  Broadly I am also interested in open problems in Controlled Text Generation. Any leads would be helpful.",427,LanguageTechnology,1
"NLP Infrastructure with Docker Swarm, Docker Compose, and Traefik Hey, I know this is more of a devops thing, but as more and more people are asking questions about how to deploy their NLP models to production and which kind of infrastructure they should set up, I thought I would share 2 articles I wrote about that recently:

Container orchestration with Docker Swarm: [https://juliensalinas.com/en/container-orchestration-docker-swarm-nlpcloud/](https://juliensalinas.com/en/container-orchestration-docker-swarm-nlpcloud/)

Routing requests to the right NLP model with Traefik: [https://juliensalinas.com/en/traefik-reverse-proxy-docker-compose-docker-swarm-nlpcloud/](https://juliensalinas.com/en/traefik-reverse-proxy-docker-compose-docker-swarm-nlpcloud/)

I'm basically talking about how we're doing things behind the hood at [NLP Cloud](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=2fc10c28-ab0d-11eb-bcbc-0242ac130002), where each spaCy NLP model is running inside its own container.

I hope some of you will find these posts useful.",1053,LanguageTechnology,1
"How do you make a text normalizer NOT based on rules but based on TRAINING DATA? I need a good text normalization algorithm. Every single thing I've looked up on the subject just has a bunch of ad hoc rules that are blanket regex-replacements and they're frankly horrible. I want human-corrected text to a normalized format, using HUMAN INTELLIGENCE to normalize the text, and then I want to use it to actually train a normalizer. Example of how garbage rule-based normalization is is here:

The team is 7-0 and took a 7-0 lead in the first quarter.

What the normalization SHOULD be (if done with human intelligence and full knowledge of context):

The team is seven and O and took a seven nothing lead in the first quarter.

What most garbage rule-based normalizers would do with this sentence:

The team is seven minus zero and took a seven minus zero lead in the first quarter.

So obviously, you can see why I need human intelligence to do this properly, and if I do it by machine, I need it TRAINED on normalizations done with human intelligence. The issue is I have no idea how to do that, does anyone know how this might be done? What library, algorithm etc. is best for this? I REFUSE to use a rule-based model to do this, I've just proven how stupid it is to do that.",1277,LanguageTechnology,1
"GPT-3, Esq? Evaluating AI Legal Summaries Hi all,

As an attorney &amp; ML enthusiast alarmed by the prospect of losing my profession to AI, I ran a few experiments of legal summaries using GPT-3 that I thought the community might find interesting. My comments follow. I would love to hear your thoughts, particularly anyone who has had success using GPT-3 for high-accuracy summarization.

[http://www.davidvictorrodriguez.com/uploads/2/6/4/2/26420847/gpt-3\_esq\_-\_evaluating\_ai\_legal\_summaries.pdf](http://www.davidvictorrodriguez.com/uploads/2/6/4/2/26420847/gpt-3_esq_-_evaluating_ai_legal_summaries.pdf)",613,LanguageTechnology,1
"Is classifying word categories with word embeddings a thing? I'm new to NLP and I'd like to get some of your input on a random idea I had. 

I'd like to know if there's any use to the idea of using word embeddings to predict types of words. Let's say I download 50k words and convert them to some set of labels (POS tags, LIWC categories, etc) and then I use the N-dimensional vectors to classify those labels using a standard ML algorithm.  If this is successful then I could point a subset of those features/dimensions and show they have strong correlations with certain types of words. And I could use my model to appraise texts based on the selected categories.

Please let me know if this makes sense or if this is a totally nonsensical idea. 

Thanks!!",758,LanguageTechnology,1
"Going into DevOps? Hey all,

I graduated from uni with a master's in linguistics and worked as a PhD researcher in NLP (had some knowledge of ML beforehand). After a few years I decided research wasn't for me and quit. Now I find myself in the (European) job market with decent experience as a researcher, but little to no development experience -- my coding work was all academic. I've done a number of interviews for NLP engineer roles and was always passed over for my lack of development/company experience. I love coding, though, and so I'd like to transition to more of a DevOps role.

Trouble is, I don't know where to start, and the companies I found aren't willing to take on a junior right now. Not even sure what extra skills I need. How do I get into DevOps? Are there resources floating around for that? Good courses that focus on the scaling &amp; production side of NLP work? Any personal experience you can share?",929,LanguageTechnology,1
"Best OCR converter for magazine images? My grandma has a Canadian pen pal but she doesn’t speak English so I translate for her. She recently received magazines from her pen pal and asked me if I could translate them for her. She really enjoys the images and the overall magazine style so I wanted to keep that format. I figured converting it would do the trick but all online converters just scramble and mess up the resulting word document. Here is a sample of the scanned pdf file of the magazine [here](https://imgur.com/a/7DcIAIo)
Thanks for any help you can give!

P.S: I would really prefer to have a free converter since I don’t normally use them for such a hard task.",675,LanguageTechnology,1
"Best pratice for pdf classification/clustering Hi y’all, I have a large volume of searchable PDFs. Based on their structure (and text) I’d like to cluster or classify them into 3 categories: A, B and other (C). I 

What is best practice for this task? I’ve experimented with extracting the structure and the text content as hOCR and plain text respectively. However, I’m not sure how to use this information efficiently. 

I could simply do a text-based model using a tf-idf matrix but I’m interested in how I can use the structure to classify or cluster. 

Any suggestions? Thanks 

Btw, I love this subreddit! It seems I bookmark every other post.",649,LanguageTechnology,1
Split audio based on text tokenization I have a long audio message and the text. I want to split the sentence in the sentence bases on full stop and also split the relevant audio path. Can anyone recommend the relevant source,225,LanguageTechnology,1
"Europe Boot camp/Courses on Natural Language Processing? Hi,

Does somebody have first-hand experience from bootcamps/short courses (strongly preferred in-person)  to get into the NLP space?",190,LanguageTechnology,1
"Question about cross lingual pretraining I was consulting the following paper about cross lingual language models (XLMs): https://arxiv.org/pdf/1901.07291.pdf

I am having a bit of trouble understanding the cross lingual aspect in the MLM and CLM objectives. I have read the paper quite a few times, but I don’t see how they use any cross lingual signal in the monolingual case. In the TLM+MLM objective it’s pretty clear to me that the encoder is a cross lingual space but in the others I don’t see where the cross lingual aspect is taking place. I hope someone can clear this for me :) thanks!!",596,LanguageTechnology,1
Anyone know of a Farsi TTS? ,28,LanguageTechnology,1
"Dehyphenation Hi,

Are there any available libraries for the task of word dehyphenation (removing unwanted hyphens within words, when they are a result of document hyphenation)? This occurs especially when converting PDFs to TXTs.

Thank you!

&amp;#x200B;

EDIT: I'm not working with PDFs; i just have plain text which is already in the hyphenated format (without newline or space-newline after the hyphen). Therefore words like ""bag-of-words"" and ""fu-ture"" are not differentiated in any way. I've now just tried a vocabulary-based approach, which is basically: given a hyphenated word in the ""x-y"" form, if it is more frequent in the corpus in its ""xy"" form, dehyphenate to ""xy"". Seems to work. Any other suggestions?",719,LanguageTechnology,1
"Multiple Frameworks in the same projekt? Hi,

is it okay to use multiple frameworks/libraries in the same project? 

i am currently writing my bachelorthesis about analyzing online lectures and would like to use gensim for it. But i already used spacy for data preperation like filtering stopwords e.g. 

Is it okay to mix these tools or better use the same tool over the span of the project?",392,LanguageTechnology,1
"What proper method to aggregate embeddings? Hi,

I am working on NLP projects where I usually extract context embeddings for some given tokens. As an example, when I am performing a filling-mask task using hugging face transformers, I always wonder how to properly aggregate context embedding vectors for each \[MASK\] token in the same sentence. Usual methods to proceed are :

\- Averaging them all (e.g using np.mean())

\- Summing over embedding space dimensions

\- Concatenates them (and probably apply a PCA to get a more compact representation)

&amp;#x200B;

Is there any better way to perform multidimensional vector aggregation while preserving most of the information?",680,LanguageTechnology,1
"[R] SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning This paper digs into a new algorithm called SpAtten, an efficient algorithm-architecture co-design that leverages token sparsity, head sparsity, and quantization opportunities to reduce attention computation and memory access. 

\[[Paper Video Presentation](https://crossminds.ai/video/602b30c3d8d2f96ed18b0d23/)\] \[[arXiv Link](https://arxiv.org/abs/2012.09852)\]

&amp;#x200B;

**Abstract:** The attention mechanism is becoming increasingly popular in Natural Language Processing (NLP) applications, showing superior performance than convolutional and recurrent architectures. However, general-purpose platforms such as CPUs and GPUs are inefficient when performing attention inference due to complicated data movement and low arithmetic intensity. Moreover, existing NN accelerators mainly focus on optimizing convolutional or recurrent models, and cannot efficiently support attention. In this paper, we present SpAtten, an efficient algorithm-architecture co-design that leverages token sparsity, head sparsity, and quantization opportunities to reduce the attention computation and memory access. Inspired by the high redundancy of human languages, we propose the novel cascade token pruning to prune away unimportant tokens in the sentence. We also propose cascade head pruning to remove unessential heads. Cascade pruning is fundamentally different from weight pruning since there is no trainable weight in the attention mechanism, and the pruned tokens and heads are selected on the fly. To efficiently support them on hardware, we design a novel top-k engine to rank token and head importance scores with high throughput. Furthermore, we propose progressive quantization that first fetches MSBs only and performs the computation; if the confidence is low, it fetches LSBs and recomputes the attention outputs, trading computation for memory reduction. Extensive experiments on 30 benchmarks show that, on average, SpAtten reduces DRAM access by 10.0x with no accuracy loss, and achieves 1.6x, 3.0x, 162x, 347x speedup, and 1,4x, 3.2x, 1193x, 4059x energy savings over A3 accelerator, MNNFast accelerator, TITAN Xp GPU, Xeon CPU, respectively.

&amp;#x200B;

Authors: Hanrui Wang, Zhekai Zhang, Song Han (MIT)",2308,LanguageTechnology,1
"Anything to be doing alongside Jurafsky &amp; Martin SLP? Apologies if this is a broken record post-- I tried looking around and couldn't find anything on this. 

I've heard in here multiple times that Speech &amp; Language Processing is sort of the Bible for NLP stuff. I've been going through it, doing the exercises, and trying to implement the algorithms it talks about where possible. I'm also trying to follow along with Relevant Papers for each chapter (like the original Word2Vec &amp; Negative Sampling papers for Word Vectors, etc)

That said, I'm not certain I'm really holding onto information in any manner where I'm actually going to implement it in my own projects. Are there any other supplements to this? Or is it sort of just drill this until you have it down?",778,LanguageTechnology,1
Paper: 65 Million Probably-Asked Questions and What You Can Do With Them ,73,LanguageTechnology,1
"Paper: ""Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"" ",84,LanguageTechnology,1
"Question Regarding LSTM-CRF architecture for NER Hello. I'm reading the paper titled [_Neural Architectures for Named Entity Recognition (Lample et al., 2016)_](https://www.aclweb.org/anthology/N16-1030/) and had a question regarding some details of the paper.

In section 2.3 Parameterization and Training, the paper claims:

&gt; These representations (i.e., the hidden representations $h$ for left- and right-going LSTM) are concatenated and linearly projected onto a layer **whose size is equal to the number of distinct tags**. Instead of using the softmax output from this layer, we use a CRF as previously described to take into account neighboring tags, yielding the final predictions for every word.

The part that's confusing me is in bold. My understanding is that if we have an input sequence of length _n_, then the output of a CRF should also be of size _n_ since we're essentially predicting the labels for the entire sequence. If we linearly project the hidden representations onto a layer whose size is the number of distinct tags (say, _k_) then wouldn't there be a size mismatch?",1098,LanguageTechnology,1
CLIP - Learning Transferable Visual Models From Natural Language Supervision ,77,LanguageTechnology,1
Shortformer: Better Language Modeling using Shorter Inputs (Paper Explained) ,77,LanguageTechnology,1
"Single Sentence Simplification I've seen some posts on this in the far past (half a decade or more ago). Looking to see if anyone has any up to date ideas on the subject. Process of simplifying a single sentence. I know there are a lot of tools and models out there for summarizing multi-sentence corpora but I'm looking for one that can operate on a single sentence. Bonus points if it doesn't require training, domain-specific knowledge, or tuning.",450,LanguageTechnology,1
Has anyone here studied Computational Linguistics at Tübingen University (Germany) or at UAM university ( Poland) ? I would just like to know more about what the student experience is like at those universities for CL.,218,LanguageTechnology,1
"Computational linguistics in Stuttgart university and Saarland university struggling to choose Saarland or Stuttgart university.
How about work opportunity after graduating from both universities?",196,LanguageTechnology,1
"How to source support for a project Hi all, I am looking to source some support to do some text summarization and sentiment analysis from some transcripts of interviews. 

I have a pretty small budget but I was wondering if this community could guide me to somewhere or someone who could take this on?",301,LanguageTechnology,1
"Is AIML worth considering in 2021? I work in a company and was recently assigned to an NLP team (it's a startup). Recently, my superior asked me to research technology that can be used to solve a particular problem he described that sounds a lot like a QnA chat bot.   


For example if the system is meant to be used by tenants to ask question about the building they live in, and the building has an automated locking system that uses keycards (and RFID) to get in, one use case may be: 

  
Tenant: I can't unlock the door

Bot: Is there a green light on the lock? 

Tenant: Yes

Bot: Is there any video feed on the screen? 

Tenant: No

Bot: Solution 1

&amp;#x200B;

I work in a company and was recently assigned to an NLP team (it's a startup). Recently, my superior asked me to research technology that can be used to solve a particular problem he described that sounds a lot like a QnA chatbot.   
 

Is AIML still used to build systems like these?",956,LanguageTechnology,1
"How do fake news video have audio matching almost perfectly to chosen hosts voice? Or are those audio snippets collection from host characters real videos

If not what technology is used to create them.",202,LanguageTechnology,1
"Did spaCy Used to Allow LEMMA exceptions? *edit:* formatting

Hello everyone. I'm working my way through *Natural Language Processing with Python and spaCy* by Yuli Vasilev and have hit an error in chapter 2.

The chapter wants me to add a special case to set the `LEMMA` of ""Frisco"" to ""San Francisco."" I have written the following code:

    import spacy
    from spacy.symbols import ORTH, LEMMA
    
    nlp = spacy.load('en_core_web_sm')
    doc = nlp(u'I am flying to Frisco')
    print([w.text for w in doc])
    
    special_case = [{ORTH: u'Frisco', LEMMA: u'San Francisco'}]
    nlp.tokenizer.add_special_case(u'Frisco', special_case)
    print([w.lemma_ for w in nlp(u'I am flying to Frisco')])

but when I run this, I get the following exception:

    Traceback (most recent call last):
      File "".\lemmatization2.py"", line 6, in &lt;module&gt;
        nlp.tokenizer.add_special_case(u'Frisco', special_case)
      File ""spacy\tokenizer.pyx"", line 601, in spacy.tokenizer.Tokenizer.add_special_case
      File ""spacy\tokenizer.pyx"", line 589, in spacy.tokenizer.Tokenizer._validate_special_case
    ValueError: [E1005] Unable to set attribute 'LEMMA' in tokenizer exception for 'Frisco'. Tokenizer exceptions are only allowed to specify ORTH and NORM.

I've tried to look up how to change the `LEMMA` value of a token and there's some Stack Overflow answers where people are doing the same thing the book is telling me to do. I can't find any information on this outside of GitHub issues that seem related at first but then aren't.

So I figure maybe when this book was written (looks like 2020?), spaCy let you add exceptions for `LEMMA`, but now it doesn't. Is that accurate? Should I use `NORM` instead? Is there a different problem that I might be overlooking?",1778,LanguageTechnology,1
How we have been evaluating Knowledge Graph Completion models INACCURATELY | Research Papers Summary 008 ,105,LanguageTechnology,1
Most influential ACL papers by year (1980-2020) [https://www.paperdigest.org/2021/02/most-influential-acl-papers/](https://www.paperdigest.org/2021/02/most-influential-acl-papers/),180,LanguageTechnology,1
"Happy Valentine's, everyone! This year, I used NLP to solve me being single by programming an autocomplete program using Markov Chains and RNNs trained on romantic movies! The results were REALLY unexpected so I turned that whole experience into a video. I hope y'all like it! ",277,LanguageTechnology,1
"How to implement self supervision in GCN [D]  Q1: where is self supervision implemented in this code ([https://github.com/Shen-Lab/SS-GCNs/blob/master/SS-GCNs/main.py](https://github.com/Shen-Lab/SS-GCNs/blob/master/SS-GCNs/main.py)) . please mention the line numbers and give a small explanation

Q2:Also how to implement self super vision if i want to implement in this code [https://github.com/tkipf/pygcn](https://github.com/tkipf/pygcn)",441,LanguageTechnology,1
"Discovering topics in NEW document using Mallet I've been playing around with Mallet and I know how to discover the per-document topic distributions, per-topic word distributions, and how to find similar documents within an existing corpus.

&amp;#x200B;

But what I'm not clear on and can't find anywhere is how to deal with NEW documents without running the entire topic model again.

&amp;#x200B;

So given a new document, discovering what topics are within in and also similar documents within the base corpus.

&amp;#x200B;

Can anyone offer some light on this?",566,LanguageTechnology,1
"How to train large models on a normal laptop? Hi, so if you've seen a previous post of mine, I mentioned a class project where we were designing a new model. The problem here is, that our advisory board wants us to show a comparison between the results of this model and existing models in the field.

The Transformer and the Reformer are two such models, among others. The problem is that I assume it would be impossible to train them for an extended period of time, on a dataset on the scale of enwik9 or so. My laptop specs are 8GB RAM with 256 GB SSD and I'm not really sure about the GPU but I think there is some sort of NVidia GeForce.

Colab crashed the last time I tried to train the model. And we can't really afford to shell out a lot for cloud instances, being undergrads on a budget.

Does anybody have any suggestions about what to do?",849,LanguageTechnology,1
"Best Masters/PhD programs in Computational Linguistics in Europe I have been exploring some options like Gothenburg and Saarbrucken. Wanted to hear some thoughts from current or past students, about these and other such programs.

If it helps, I am quite interested in common sense reasoning and explainability so if the Unis have research labs focused on such topics will be an added bonus.",391,LanguageTechnology,1
"MS in CL at Montclair? Has anyone here done the MS in Computational linguistics at Montclair? Or is currently in the program? I’m considering applying but I would like to know if anyone would recommend it/what they think about it. 
Thanks!",239,LanguageTechnology,1
"How to create a model for Question Answering on the SQuAD Dataset? Hi guys! I'm new here and I have to create a model for the Question Answering task using only as training data the SQuAD dataset. It's very difficult find something that match my requests, cause all the model used in this task are pretained model like BERT or LUKE and I want to trained the model by myself. 

I would be really grateful to any of you who know how or where to find tutorials or resources.",471,LanguageTechnology,1
Any standard textbooks on Bangla Natural Language Processing or other Indian languages? --Where data preprocessing is nicely explained. ,136,LanguageTechnology,1
"New multilingual models, Spark 2.3 support, new tutorials for Bengali, Bhojpuri, Japanese, T5, and more in 1 line of Python code with NLU 1.1.1! # John Snow Labs NLU 1.1.1 : New multilingual models, Spark 2.3 support, new tutorials and more! 

## NLU 1.1.1 Release Notes
We are very excited to release NLU 1.1.1!
This release features 3 new tutorial notebooks for Open/Closed book question answering with Google's T5, Intent classification, and Aspect Based NER.
In Addition, NLU 1.1.0 comes with  25+ pre-trained models and pipelines in Amharic, Bengali, Bhojpuri, Japanese, and Korean languages from the [amazing Spark2.7.2 release](https://github.com/JohnSnowLabs/spark-nlp/releases/tag/2.7.2)
Finally, NLU now supports running on Spark 2.3 clusters.


### NLU 1.1.0 New Non-English Models
|Language | nlu.load() reference | Spark NLP Model reference | Type |
|---------|---------------------|----------------------------|------|
|Arabic | [ar.ner](https://nlp.johnsnowlabs.com/2020/12/05/aner_cc_300d_ar.html) |[arabic_w2v_cc_300d](https://nlp.johnsnowlabs.com/2020/12/05/aner_cc_300d_ar.html) | Named Entity Recognizer                    |
|Arabic | [ar.embed.aner](https://nlp.johnsnowlabs.com/2020/12/05/aner_cc_300d_ar.html) |[aner_cc_300d](https://nlp.johnsnowlabs.com/2020/12/05/aner_cc_300d_ar.html) | Word Embedding                    |
|Arabic | [ar.embed.aner.300d](https://nlp.johnsnowlabs.com/2020/12/05/aner_cc_300d_ar.html) |[aner_cc_300d](https://nlp.johnsnowlabs.com/2020/12/05/aner_cc_300d_ar.html) | Word Embedding (Alias)                    |
|Bengali | [bn.stopwords](https://nlp.johnsnowlabs.com/2020/07/14/stopwords_bn.html) |[stopwords_bn](https://nlp.johnsnowlabs.com/2020/07/14/stopwords_bn.html) | Stopwords Cleaner                    |
|Bengali | [bn.pos](https://nlp.johnsnowlabs.com/2021/01/20/pos_msri_bn.html) |[pos_msri](https://nlp.johnsnowlabs.com/2021/01/20/pos_msri_bn.html) | Part of Speech                    |
|Thai | [th.segment_words](https://nlp.johnsnowlabs.com/2021/01/11/ner_lst20_glove_840B_300d_th.html) |[wordseg_best](https://nlp.johnsnowlabs.com/2021/01/11/ner_lst20_glove_840B_300d_th.html) | Word Segmenter                    |
|Thai | [th.pos](https://nlp.johnsnowlabs.com/2021/01/13/pos_lst20_th.html) |[pos_lst20](https://nlp.johnsnowlabs.com/2021/01/13/pos_lst20_th.html) | Part of Speech                    |
|Thai |   [th.sentiment](https://nlp.johnsnowlabs.com/2021/01/14/sentiment_jager_use_th.html) |[sentiment_jager_use](https://nlp.johnsnowlabs.com/2021/01/14/sentiment_jager_use_th.html) | Sentiment Classifier                     |
|Thai |    [th.classify.sentiment](https://nlp.johnsnowlabs.com/2021/01/14/sentiment_jager_use_th.html) |[sentiment_jager_use](https://nlp.johnsnowlabs.com/2021/01/14/sentiment_jager_use_th.html) | Sentiment Classifier (Alias)                    |
|Chinese | [zh.pos.ud_gsd_trad](https://nlp.johnsnowlabs.com/2021/01/25/pos_ud_gsd_trad_zh.html) |[pos_ud_gsd_trad](https://nlp.johnsnowlabs.com/2021/01/25/pos_ud_gsd_trad_zh.html) | Part of Speech                    |
|Chinese | [zh.segment_words.gsd](https://nlp.johnsnowlabs.com/2021/01/25/wordseg_gsd_ud_trad_zh.html) |[wordseg_gsd_ud_trad](https://nlp.johnsnowlabs.com/2021/01/25/wordseg_gsd_ud_trad_zh.html) | Word Segmenter                    |
|Bihari | [bh.pos](https://nlp.johnsnowlabs.com/2021/01/18/pos_ud_bhtb_bh.html) |[pos_ud_bhtb](https://nlp.johnsnowlabs.com/2021/01/18/pos_ud_bhtb_bh.html) | Part of Speech                    |
|Amharic | [am.pos](https://nlp.johnsnowlabs.com/2021/01/20/pos_ud_att_am.html) |[pos_ud_att](https://nlp.johnsnowlabs.com/2021/01/20/pos_ud_att_am.html) | Part of Speech                    |



### NLU 1.1.1 New English Models and Pipelines
|Language | nlu.load() reference | Spark NLP Model reference | Type |
|---------|---------------------|----------------------------|------|
| English | [en.sentiment.glove](https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html) |[analyze_sentimentdl_glove_imdb](https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html)     | Sentiment Classifier |
| English | [en.sentiment.glove.imdb](https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html) |[analyze_sentimentdl_glove_imdb](https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html)     | Sentiment Classifier (Alias) |
| English | [en.classify.sentiment.glove.imdb](https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html) |[analyze_sentimentdl_glove_imdb](https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html)     | Sentiment Classifier (Alias) |
| English | [en.classify.sentiment.glove](https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html) |[analyze_sentimentdl_glove_imdb](https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html)     | Sentiment Classifier (Alias) |
| English | [en.classify.trec50.pipe](https://nlp.johnsnowlabs.com/2021/01/08/classifierdl_use_trec50_pipeline_en.html) |[classifierdl_use_trec50_pipeline](https://nlp.johnsnowlabs.com/2021/01/08/classifierdl_use_trec50_pipeline_en.html)     | Language Classifier |
| English | [en.ner.onto.large](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_large_en.html) |[onto_recognize_entities_electra_large](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_large_en.html)     | Named Entity Recognizer |
| English | [en.classify.questions.atis](https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html) |[classifierdl_use_atis](https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html)     | Intent Classifier |
| English  | [en.classify.questions.airline](https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html) |[classifierdl_use_atis](https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html)     | Intent Classifier (Alias) |
| English | [en.classify.intent.atis](https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html) |[classifierdl_use_atis](https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html)     | Intent Classifier (Alias) |
| English | [en.classify.intent.airline](https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html) |[classifierdl_use_atis](https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html)     | Intent Classifier (Alias) |
| English | [en.ner.atis](https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html) |[nerdl_atis_840b_300d](https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html)     | Aspect based NER |
| English | [en.ner.airline](https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html) |[nerdl_atis_840b_300d](https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html)     | Aspect based NER (Alias) |
| English | [en.ner.aspect.airline](https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html) |[nerdl_atis_840b_300d](https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html)     | Aspect based NER (Alias) |
| English | [en.ner.aspect.atis](https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html) |[nerdl_atis_840b_300d](https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html)     | Aspect based NER (Alias) |

### New Easy NLU 1-liner Examples : 

#### Extract aspects and entities from airline questions (ATIS dataset)
```python
	
nlu.load(""en.ner.atis"").predict(""i want to fly from baltimore to dallas round trip"")
output:  [""baltimore"","" dallas"", ""round trip""]
```



#### Intent Classification for Airline Traffic Information System queries (ATIS dataset)


```python

nlu.load(""en.classify.questions.atis"").predict(""what is the price of flight from newyork to washington"")
output:  ""atis_airfare""	
```



#### Recognize Entities OntoNotes - ELECTRA Large


```python

nlu.load(""en.ner.onto.large"").predict(""Johnson first entered politics when elected in 2001 as a member of Parliament. He then served eight years as the mayor of London."")	
output:  [""Johnson"", ""first"", ""2001"", ""eight years"", ""London""]	
```

#### Question classification of open-domain and fact-based questions Pipeline - TREC50


```python
nlu.load(""en.classify.trec50.pipe"").predict(""When did the construction of stone circles begin in the UK? "")
output:  LOC_other
```

#### Traditional Chinese Word Segmentation

```python
# 'However, this treatment also creates some problems' in Chinese
nlu.load(""zh.segment_words.gsd"").predict(""然而，這樣的處理也衍生了一些問題。"")
output:  [""然而"","","",""這樣"",""的"",""處理"",""也"",""衍生"",""了"",""一些"",""問題"",""。""]

```


#### Part of Speech for Traditional Chinese

```python
# 'However, this treatment also creates some problems' in Chinese
nlu.load(""zh.pos.ud_gsd_trad"").predict(""然而，這樣的處理也衍生了一些問題。"")
```
Output:

|Token |  POS   |
| ----- | ----- |
| 然而  | ADV   |
| ，    | PUNCT |
| 這樣  | PRON  |
| 的    | PART  |
| 處理  | NOUN  |
| 也    | ADV   |
| 衍生  | VERB  |
| 了    | PART  |
| 一些  | ADJ   |
| 問題  | NOUN  |
| 。    | PUNCT |

#### Thai Word Segment Recognition


```python
# 'Mona Lisa is a 16th-century oil painting created by Leonardo held at the Louvre in Paris' in Thai
nlu.loadnlu.load(""th.segment_words"").predict(""Mona Lisa เป็นภาพวาดสีน้ำมันในศตวรรษที่ 16 ที่สร้างโดย Leonardo จัดขึ้นที่พิพิธภัณฑ์ลูฟร์ในปารีส"")

```
Output:

| token |
| --------- |
| M         |
| o         |
| n         |
| a         |
| Lisa      |
| เป็น       |
| ภาพ       |
| ว         |
| า         |
| ด         |
| สีน้ำ       |
| มัน        |
| ใน        |
| ศตวรรษ    |
| ที่         |
| 16        |
| ที่         |
| สร้าง      |
| โ         |
| ด         |
| ย         |
| L         |
| e         |
| o         |
| n         |
| a         |
| r         |
| d         |
| o         |
| จัด        |
| ขึ้น        |
| ที่         |
| พิพิธภัณฑ์    |
| ลูฟร์       |
| ใน        |
| ปารีส      |

#### Part of Speech for Bengali (POS)

```python
# 'The village is also called 'Mod' in Tora language' in Bengali 
nlu.load(""bn.pos"").predict(""বাসস্থান-ঘরগৃহস্থালি তোড়া ভাষায় গ্রামকেও বলে ` মোদ ' ৷"")
```
Output:

| token             | pos  |
| ----------------- | ---- |
| বাসস্থান-ঘরগৃহস্থালি | NN   |
| তোড়া              | NNP  |
| ভাষায়             | NN   |
| গ্রামকেও           | NN   |
| বলে               | VM   |
| `                 | SYM  |
| মোদ               | NN   |
| '                 | SYM  |
| ৷                 | SYM  |



#### Stop Words Cleaner for Bengali


```python
# 'This language is not enough' in Bengali 
df = nlu.load(""bn.stopwords"").predict(""এই ভাষা যথেষ্ট নয়"")

```
Output:

| cleanTokens | token |
| :---------- | :---- |
| ভাষা        | এই    |
| যথেষ্ট       | ভাষা  |
| নয়          | যথেষ্ট |
| None        | নয়    |


#### Part of Speech for Bengali
```python

# 'The people of Ohu know that the foundation of Bhojpuri was shaken' in Bengali
nlu.load('bh.pos').predict(""ओहु लोग के मालूम बा कि श्लील होखते भोजपुरी के नींव हिल जाई"")
```
Output:

| pos   | token   |
| :---- | :------ |
| DET   | ओहु     |
| NOUN  | लोग     |
| ADP   | के      |
| NOUN  | मालूम   |
| VERB  | बा      |
| SCONJ | कि      |
| ADJ   | श्लील   |
| VERB  | होखते   |
| PROPN | भोजपुरी |
| ADP   | के      |
| NOUN  | नींव    |
| VERB  | हिल     |
| AUX   | जाई     |


#### Amharic Part of Speech (POS)
```python
# ' ""Son, finish the job,"" he said.' in Amharic
nlu.load('am.pos').predict('ልጅ ኡ ን ሥራ ው ን አስጨርስ ኧው ኣል ኧሁ ።""')
```

Output:

| pos   | token   |
|:------|:--------|
| NOUN  | ልጅ      |
| DET   | ኡ       |
| PART  | ን       |
| NOUN  | ሥራ      |
| DET   | ው       |
| PART  | ን       |
| VERB  | አስጨርስ   |
| PRON  | ኧው      |
| AUX   | ኣል      |
| PRON  | ኧሁ      |
| PUNCT | ።       |
| NOUN  | ""       |


#### Thai Sentiment Classification
```python
#  'I love peanut butter and jelly!' in thai
nlu.load('th.classify.sentiment').predict('ฉันชอบเนยถั่วและเยลลี่!')[['sentiment','sentiment_confidence']]
```

Output:

| sentiment   |   sentiment_confidence |
|:------------|-----------------------:|
| positive    |               0.999998 |


#### Arabic Named Entity Recognition (NER)
```python
# 'In 1918, the forces of the Arab Revolt liberated Damascus with the help of the British' in Arabic
nlu.load('ar.ner').predict('في عام 1918 حررت قوات الثورة العربية دمشق بمساعدة من الإنكليز',output_level='chunk')[['entities_confidence','ner_confidence','entities']]
```

Output:

| entity_class   | ner_confidence                                                                                                                                                                  | entities            |
|:----------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------|
| ORG                   | [1.0, 1.0, 1.0, 0.9997000098228455, 0.9840999841690063, 0.9987999796867371, 0.9990000128746033, 0.9998999834060669, 0.9998999834060669, 0.9993000030517578, 0.9998999834060669] | قوات الثورة العربية |
| LOC                   | [1.0, 1.0, 1.0, 0.9997000098228455, 0.9840999841690063, 0.9987999796867371, 0.9990000128746033, 0.9998999834060669, 0.9998999834060669, 0.9993000030517578, 0.9998999834060669] | دمشق                |
| PER                   | [1.0, 1.0, 1.0, 0.9997000098228455, 0.9840999841690063, 0.9987999796867371, 0.9990000128746033, 0.9998999834060669, 0.9998999834060669, 0.9993000030517578, 0.9998999834060669] | الإنكليز            |



### NLU 1.1.0 Enhancements : 
-  Spark 2.3 compatibility

### New NLU Notebooks and Tutorials 
- [Open and Closed book question Ansering](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_question_answering.ipynb)
- [Aspect based NER for Airline ATIS]New multilingual models, Spark 2.3 support, new tutorials for Bengali, Bhojpuri, Japanese, T5  and more in 1 line of Python code with NLU 1.1.1!(https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/classifiers/intent_classification_airlines_ATIS.ipynb)
- [Intent Classification for Airline emssages ATIS](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/named_entity_recognition_(NER)/NER_aspect_airline_ATIS.ipynb)

### Installation

```bash
# PyPi
!pip install nlu pyspark==2.4.7
#Conda
# Install NLU from Anaconda/Conda
conda install -c johnsnowlabs nlu
```

### Additional NLU ressources
- [NLU Website](https://nlu.johnsnowlabs.com/)
- [All NLU Tutorial Notebooks](https://nlu.johnsnowlabs.com/docs/en/notebooks)
- [NLU Videos and Blogposts on NLU](https://nlp.johnsnowlabs.com/learn#pythons-nlu-library)
- [NLU on Github](https://github.com/JohnSnowLabs/nlu)",14777,LanguageTechnology,1
"Any recommendations for NLP learning material in 2021? Could someone recommend some updated NLP learning material? I am looking specifically for NLG and text summarisation. 

One of the most recent books I found is: [Hands-On Python Natural Language Processing: Explore tools and techniques to analyze and process text with a view to building real-world NLP applications](https://www.goodreads.com/book/show/54412967-hands-on-python-natural-language-processing)

by [Aman Kedia](https://www.goodreads.com/author/show/20480128.Aman_Kedia), [Mayank Rasu](https://www.goodreads.com/author/show/20480129.Mayank_Rasu)

but couldn't find any reviews or opinions.. 

Looking forward to your recommendations.",700,LanguageTechnology,1
"Neural Query Expansion for Code Search Searching repositories of existing source code for code snippets is a key task in software engineering. Earlier techniques like Neural Code Search(NCS), takes in a natural language query and outputs relevant code snippets, often suffers incase of short queries or query that have vague intent. Researchers propose NQE for expanding search queries to improve results. 🤠

Blog- https://link.medium.com/uFDypem4Pdb",450,LanguageTechnology,1
"NLP work in politics/intelligence Hi! I will be starting grad school to get my MS in CS this fall and am planning on specializing in NLP and working as an NLP engineer afterwards. I have always been very interested in politics + government work as well and I imagine there's need for NLP work in intelligence. Does anyone in this sub have any experience in this field? Or knows how much opportunity there is, or anything else worth sharing?

Also a bit more generally, how prevalent are CS/NLP jobs related to the humanities? For example, working in a technical role on investigative work for a news company. So roles working in politics/social/advocacy/news (esp if they require writing and communication alongside the CS). Maybe this is more applicable to leadership positions? But is it plausible that I could get a job in these fields/roles after grad school?",863,LanguageTechnology,1
Semantic Viz: a semantic distance visualizer written in Haskell - let me know what you think! ,94,LanguageTechnology,1
How should I go about creating an API to summarize text automatically? I would like to take the abstractive approach. Eager to learn more about this.,149,LanguageTechnology,1
"Which are some good papers on political science applications of NLP? I'm looking for some papers applying NLP to politics.

It doesn't matter if it is new or old, groundbreaking or not, as long as you enjoyed it, I'd like to know about it!",239,LanguageTechnology,1
"Recommendations for OpenSource NLP Projects to contribute at Hello everyone! I would like to ask you all for recommendations on some interesting NLP projects that are running at the moment and need contributors. I do not have extensive experience, but I'd love a starting point. **Feel free to advertise your open source project here**!",336,LanguageTechnology,1
"Try to improve a DL model with Attention Hello everybody,

I've been working with a deep learning model (a parser) for some time to make dependency parsing.

In brief, the model is composed of several embeddings that are passed to a BiLSTM and finally to two classifiers (one for the labels and one for the positions). An idea to improve the model is to integrate a self-attention mechanism to BiLSTM. In particular, the attempts I have made are:

* add self-attention to BiLSTM using this [gist](https://gist.github.com/cbaziotis/94e53bdd6e4852756e0395560ff38aa4)
* add multi head (self) attention using the [function](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) provided by pytorch

The results obtained are not very comforting ... with self-attention I have a very slight improvement in performance, while with multi head (self) attention I have no improvements.

Being self-attention mechanisms, in the multi head the vectors corresponding to Q (query), K (key) and V (value) are the same and correspond to the BiLSTM output. What I did was to vary the number of heads (2, 4, 6, 8, 10, 12: no improvement). 

The only other attempt I can think of is to use a [BatchNormalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) layer after the BiLSTM and before the attention or after the attention, but honestly I doubt it will do anything ...

Obviously, adding an attention mechanism does not mean that the model necessarily improves. But I don't know exactly how to justify it. Anyone have any ideas? Or, does anyone have any ideas on how I might try to insert self attention / multi head self attention into BiLSTM?

If you have read this far, thank you very much!",1726,LanguageTechnology,1
[N] ICMI 2020 Best Paper | Gesticulator: A framework for semantically-aware speech-driven gesture generation ,109,LanguageTechnology,1
"Text-generation frontiers Hello all. Im a data science graduate student and NLP research assistant looking to write my thesis about generative language models. 

As I understand it, the current field is largely concerned with large scale pre-trained transformer-based models such as the GPT and BERT projects. 

It seems that the scale and computational cost of such models sets very high barriers of entry for researchers aspiring to challenge state-of-the-art or do interesting research in this sub-field. 

So, I’m curious as to what you may consider interesting frontiers within text-generation/generative models. In your opinion, is there anything that would be worth exploring in this subfield, that do not require access to GPT-level resources?

So far I have considered a comparative study of traditional models (word2vec etc) and these pre-trained models on the new Allen Institute [GENIE testbank](https://leaderboard.allenai.org/genie-anlg/submissions/get-started). However, I’d also very much like to build my own model somehow. 

In relation, are there any other NLP frontiers that you might suggest for a thesis project? 

Thanks for reading and for you time. Any inputs would be appreciated and I would be happy to discuss with further with anyone interested. Naturally, I don’t mind sharing credit either.",1321,LanguageTechnology,1
English Speech-to-Text Transcript with Hugging Face ,52,LanguageTechnology,1
"Looking for a job!! 
I want to get a job as a freelancer

Interests 

 - deeplearning
  - NLP
  - Recommender System

Additionally, i am also looking for teammate to do any project with deeplearning.",199,LanguageTechnology,1
"Rebuilding the spellchecker: Well, akchualy... ",47,LanguageTechnology,1
"Need Reviews Of Saarland and Stuttgart’s Masters NLP Program Saarland nlp program = language science and technology
Stuttgart’s nlp program= computational linguistics.

I have got admission in stuttgart nlp program and have applied in saarland’s and will also probably get an admit from there too. So i will be deciding between them for masters.

Both programs “look” great (by module book)  but i need review of real experience of studying there. I guess since saarland is more like a university town hence there will be less work opportunities in the CS or NLP field outside university, compared to stuttgart. But on the other hand saarland is cheaper than stuttgart.

Any other thing i should know about before making the decision. Also, note that i think first 1 or 2 semesters will be online because of covid.

TIA.


Edit: I am unable to find people from stuttgart university in this program (not just on this post, but generally). so if you know someone from this program, can you kindly ask them to help me out here. thanks",1031,LanguageTechnology,1
"Are there more practical tools for KNN searches and storing documents/embeddings? I have a semantic searcher that retrieves the most similar documents, and then the similarity for each segment (which are sentences). I have been using FAISS for the KNN search over the documents, then I retrieve the sentences to do a dot product operation and highlight the most similar ones.

I also need to keep my data either pickled, in hdf5, or in json, to be able to build the searcher, but after a while, it takes too long to rebuild. I need to rebuild the searcher because I don't have the option to neither update nor delete from a FAISS index, and I don't think I can save the FAISS index either - can't pickle because it yields an error related to using C++ objects.

I also have to have the searcher object in memory (plus the embedding model itself, which is unavoidable). If you have tried Whoosh (probably is similar to Elastic\_Search), it creates an index file which opened/closed per search/data\_manipulation. I can update and delete documents easily, and besides being fast, doesn't have concurrency problems either, I'd love an option like this.

EDIT: another issue I have with FAISS (and other KNN I have tried at the time), is that they don't handle IDs, instead, they retrieve positions and I need to keep a mapping to the original data.",1345,LanguageTechnology,1
"Researchers from the University of Sheffield &amp; Beihang University Introduce a New Approach Based on Transfer Learning to Automate Historical Text Summarization The researchers at the University of Sheffield, Beihang University, and Open University’s Knowledge Media Institute have introduced historical text summarization task, where documents in historical forms of a language are summarised in the corresponding modern language.

The process of text summarization is a fundamentally important routine to historians and digital humanities researchers. Historical text summarization is regarded as a particular case of cross-lingual summarization. However, summarizing and interpreting historical documents can cost a lot of time and effort, even for experts. This is due to the limited historical and modern language corpora and cultural and linguistic variations over time.

Paper summary: https://www.marktechpost.com/2021/02/10/researchers-from-the-university-of-sheffield-beihang-university-introduce-a-new-approach-based-on-transfer-learning-to-automate-historical-text-summarization/

Paper: https://arxiv.org/pdf/2101.10759.pdf

Github: https://github.com/Pzoom522/HistSumm",1185,LanguageTechnology,1
Large archive of papers on machine translation starting from the beginning of the field ,88,LanguageTechnology,1
"NLP Cloud for spaCy NLP models in production So many ML projects are failing because teams don't have the skills to deploy their new model to production... [NLP Cloud](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=7fc10c28-ab0d-11eb-bcbc-0242ac130002) wants to solve this problem by providing spaCy users with an easy way to deploy their models to production.

More and more companies are using [spaCy](https://spacy.io/) for their NLP projects as it's a modern and production-ready framework for NLP.

Successfully  serving spaCy NLP models in production is a tough job that has nothing  to do with data science, but that is nevertheless critical to the  success of a project. The goal of [https://nlpcloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=7fc10c28-ab0d-11eb-bcbc-0242ac130002)  is to take care of this DevOps task: developers or data scientists only  have to upload their spaCy models to NLP Cloud and their models are  then served through a RESTful API. NLP Cloud ensures high availability  of the models by automatically scaling the instances, adding redundancy,  managing memory consumption, etc.

All the pre-trained spaCy models are also available and small pre-trained models are actually completely free.

For more details here is the API documentation: [https://docs.nlpcloud.io](https://docs.nlpcloud.io/)

If any question or feedback, don't hesitate to answer this post!",1419,LanguageTechnology,1
"I see mean pool and max pool output of transformer models as representations of passages of text, why not concatenate the two? I actually tried this and the representation seems to perform worse than using just one of these. Is there a theoretical reason why this might be so?",276,LanguageTechnology,1
Creating a domain specific part of speech tagger I tried using parser that are readily available but for some domain specific tasks the parsers do not work well. Could you please recommend me a way to create my own parser or fine tune available ones with some supervision? How do I jump start?,293,LanguageTechnology,1
LibreTranslate - Free and Open Source Machine Translation API ,62,LanguageTechnology,1
"Intro to Embodied AI: How to combine NLP, CV, and RL ",53,LanguageTechnology,1
"Simple question about attention in NLP Attention is all the rage (at least, it gets a lot of press) in NLP DL. I have been trying to wrap my head around how and why it works so well. I have embarked on a journey of reading papers and watching YouTube videos (e.g., [https://distill.pub/2016/augmented-rnns/](https://distill.pub/2016/augmented-rnns/) and [https://www.youtube.com/watch?v=KN3ZL65Dze0](https://www.youtube.com/watch?v=KN3ZL65Dze0))

I have one nagging question. One of the key ideas in attention is that the model learns a weight from each item in the input sequence to each item in the output sequence. And I can see that for a single input/output pair (e.g., the input sequence ""The agreement on the European Economic Area was signed in August 1992."" and the corresponding output sequence ""l' accord sur la zone economique europeene a ete signe en aout 1992.""), how the weights are very helpful and important. In this example, the 5th input token ""European"" should/will have a high weight to the 7th output token ""europeene"". Paper figures indeed show nice examples of the weights for single input/output pairs, and those figures make sense to me. But my understanding is in these attention models, there is only *one* set of weights to be learned (and later used) for all input/output pairs. So my question is: how does that work, given the enormous variations possible in sentence structure? For example, does the model expect that there should always be a strong weight between the 5th input token and the 7th output token?  It seems like the weights should be drastically different depending on the specific input/output pair, and that a single, global set of weights won't work well.

Any help or guidance would be much appreciated.",1753,LanguageTechnology,1
Unit testing neural networks (BERT example) ,44,LanguageTechnology,1
"Commercial data scientist to NLP-specialist? Hello there,

&amp;#x200B;

Long time lurking, never posting, but I'll break the habit here. I saw every now and then there's a post about NLP-focused jobs in the real world, and I wanted to get a bit of discussion going about the future of DS/ML jobs that focus on NLP.

&amp;#x200B;

I'm mainly interested as that is a future I wish to build for myself. Worked in academia in scientific research for some time (physics and psychology) but been a data scientist for the past 2 years-ish, although since Covid started I pretty much became a BI engineer as my firm needed that gap to be filled. I guess the skills of building and monitoring ETL processes, containerisation, as well as plugging in analytics to integrate with other tools or BI frontend can be useful in the long run, so that's why I didn't mind the slight change. 

&amp;#x200B;

Ever since I have worked as a data scientist, I really enjoyed reading and studying about NLP (thank you Dan Jurafsky and Chris Manning for the courses!) , I feel like it hits close to my kinds of challenges. Even though I have not really used it in many projects at work, apart from occasional named entity recognition and lots of regex usage and stuff like that, it still excites me more than other tasks I had to tackle before. I am doing some basic projects on my own every now and then (news for sentiment analysis for trading, decomposing TedX transcripts for some basic insights) but would hardly call that a portfolio. 

&amp;#x200B;

So kind of as a bundle of questions, I was wondering what's a general consensus on someone entering the NLP field without specific research background, ML engineering experience, or computational linguistics studies. To get to a stage where most of my work is with NLP, is it wiser to apply for junior NLP roles given I have experience in data science and some engineering too, or build a stronger NLP portfolio and apply for mid-level role and demonstrate experience and skills in language tech through that rather than work? Plus, projecting for the next 2-3-5 years, what trends should we envisage happening in the NLP workforce/community? 

&amp;#x200B;

Thanks, and take care!",2214,LanguageTechnology,1
"How important is classical (non-Neural) ML for NLP? My introduction to NLP was from the Stanford lectures on **NLP with Deep Learning** (available freely online - see [here](http://web.stanford.edu/class/cs224n/) for links to the course pages from different years).

I was wondering how much is missed by having jumped straight into neural methods and if I should supplement this with something more classical (e.g. the book **Natural Language Processing with Python**). I also saw there's an older [course](https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html) from Stanford (also available freely online) focusing on more classical methods.",647,LanguageTechnology,1
INTERLINGO: Mi nueva APP para aprender idiomas ,47,LanguageTechnology,1
"Interesting models for character prediction? Hi, I'm taking a course on NLP and the class' final project is about character prediction. The task is to predict the next character given a sequence of characters.

I'm trying to find an interesting model I can use. The default answer would probably be LSTMs, but I want to try using something more interesting.

Any suggestions for a model architecture I could use that's more interesting than LSTMs but still doable by an undergrad without much experience?",504,LanguageTechnology,1
Microsoft spelling correction for 100 languages ,48,LanguageTechnology,1
Detect hate speech using a Transformer with only a few lines of code. ,70,LanguageTechnology,1
"Are there any methods for extracting tokens that make a category unique in a dataset? For example, if I have a text classification problem that's trying to determine if a document belongs to ""politics"" or ""sports"", is there a way that I could ""pull out"" a list of the tokens and/or n-grams in ""politics"" that set it apart from ""sports"" as a category? I assume it would be some process of comparing the difference in word frequency across each corpus, and then finding the terms that have the biggest difference in word frequency, but I'm wondering if there is a more efficient or smarter way to do this. Thanks.",611,LanguageTechnology,1
"Looking for dataset with empathic language Does anyone know of publicly available data which includes labels for levels of empathy/kindness/compassion in text?

I am trying to reproduce old results with new data.",212,LanguageTechnology,1
"Help with Named Entity Recognition (I think?) project Hey all! Im new to ML and Im working on an iOS project that I am needing a little advice on. I am trying to extract information from advertisements and categorize them. On top of that, I need to associate certain categories with each other so that when they are processed by my computer vision algorithm, they are associated together and presented as a single item. I am kind of stumped on how to approach this problem. I think I need to use some kind of vision algorithm to recognize the relevant parts of the advertisement and then either a vision or NLP algorithm to associate the particular categories within each ad together correctly. Any advice would be greatly, greatly appreciated!",744,LanguageTechnology,1
INTERLINGO: Tu nueva APP para aprender INGLES ,46,LanguageTechnology,1
"Where should I start? Hi, I'd like to learn NLP by using python3. What should I do to learn? What do I have to know before start? Should I firstly check AI? It's related? Could you share documents or complete course videos. Thanks. 😊",233,LanguageTechnology,1
A Robust and Domain-Adaptive Approach for Low-Resource NER | Research Papers Summary 007 ,89,LanguageTechnology,1
"Studying linguistic features in emotion analysis with deep learning Hello every one 

I am new in NLP , and I want to perform emotion analysis especially (worry) emotion. I have collected tweets related to COVID-19  to classify into 2 classes with deep learning. Someone said you have to make a linguistic study related to the Worry concept to use it as linguistic features and get good results. Does anyone know any resources that can help me or any linguistic resources? Thanks",479,LanguageTechnology,1
"I'm looking for nice resources on information retrieval I need to retrieve information from papers and I found few tools, but still I don't know how a workflow should be. Can anyone post resources from data preprocessing to model training for information retrieval? Any pre-trained models that I can use (I checked pegasus for summarization, T5 and few others) Help is appreciated.",381,LanguageTechnology,1
"Amazon Alexa Team -- Data Linguist Roles Alexa team is hiring ""Data Linguists"", are these literal manual annotation roles? Is Alexa literally hand-tuned based manual decision trees?

&gt; Sample JD Description

&gt; Amazon is seeking a Data Linguist-II to join our Amazon Devices Team. This role focuses on language data, primarily in the areas of text annotation and general data analysis deliverables.

&gt;The Data Linguist-II must have a passion for data, efficiency, accuracy, and should be capable of:

&gt;• Handling unique data analysis requests from a range of data customers

&gt;• Providing data quality expertise to other team members and coaching improvements

&gt;• Delivering high quality work under aggressive deadlines

&gt;• Working autonomously with minimum direction

&gt;• Building a thorough understanding of conventions and providing support to global sites

&gt;• Understanding changes to conventions deployed in response to customers’ requests and modifying workflows accordingly

&gt;• Contributing to process improvements to reduce handling time and improve output

&gt;• Improving software tools by identifying bugs and suggesting enhancements

&gt;• Diving deep into issues and implementing solutions independently

&gt;• Proactively addressing issues and problems

&gt;• Keeping up with changing project conventions and priorities

&gt;Basic Qualifications

&gt;• Bachelor’s degree

&gt;• 2+ years of experience working with written language data, including experience with annotation and other forms of data markup

&gt;• 1+ year(s) of experience working with command line interfaces and basic UNIX commands

&gt;• Near-native level fluency in one or more non-English language

&gt;• Business level fluency in English

&gt;• Working knowledge of Microsoft Office products (e.g. Word, Excel, Access, etc.)


&gt;Preferred Qualifications

&gt;• Bachelor’s degree in Linguistics or Computational Linguistics

&gt;• Interest in semantics and related areas

&gt;• Comfortable working with text from various languages and dialects

&gt;• Ability to quickly grasp technical concepts and learn in-house data processing tools

&gt;• Strong analytical, problem-solving, and critical-thinking skills

&gt;• Practical knowledge of data processing needs and trade-offs

&gt;• Detail-oriented with excellent communication and strong organizational skills

&gt;• Team player with exceptional interpersonal skills and a solution-oriented attitude

&gt;• Comfortable working in a fast-paced, highly-collaborative, dynamic work environment

&gt;• Willingness to support several projects at one time and to accept reprioritization as necessary",2655,LanguageTechnology,1
How can I look up for early-stage NLP companies? ,49,LanguageTechnology,1
"Obsei: OBserve SEgment and Inform I created an open source tool to collect textual information from various sources (Social media, app stores and Reddit), then applying NLP models (Sentiment, NER and Classification)  on it and later report them back to various places (Slack, ES, Jira etc). My main aim to add many integrations and use existing opensource NLP models (instead of reinvent the wheel) so user can create practical workflow as fast as possible.

Please try it and gives feedback. [Obsei repo](https://github.com/lalitpagaria/obsei)

![Image](https://raw.githubusercontent.com/lalitpagaria/obsei/master/images/Obsei-flow-diagram.png)",645,LanguageTechnology,1
"Small Neural Text Generator Models Hi.

I have a very specific dataset with around 20K sentences, that I want to use for a neural text generation pipeline. I'm looking for really lightweight models, which can train fast and give decent (not at all perfect) result. I'm currently using Markov chain models, but while they are decent, they aren't that original, so I'm trying to experiment with NN models.

I've tested textgenrnn, it's good but the model I think is pretty heavy still, not as heavy as XLNet or other transformer models, but still pretty heavy. Any suggestions would be greatly appreciated.",604,LanguageTechnology,1
"NLP model in Python Hey everyone! 

I am currently trying to do a sentiment analysis of different text documents. I am using pyhton 3 and jupyter Notebook. The underlying issue is that I am completely new to programming and have probably taking too much on. I was wondering if someone could explain how I can read a .csv file into [textblob.de](https://textblob.de) and actually get a sentiment analysis 

&amp;#x200B;

PS: sorry for all the terrible simplifications but I am a total noob in this field",502,LanguageTechnology,1
"Need help with NLU task of getting insights into a real short story Hello, I have experimented with an Natural Language Understanding task on a real short story.  The original text is as follows:

&amp;#x200B;

&gt;""The Doctor's Word"", Short Story From ""Maguldi Days"", by R.K. Narayan (1972):  
&gt;  
&gt;People came to him when the patient was on his last legs. Dr Raman often burst out, 'Why couldn't you have come a day earlier?' The reason was obvious - visiting fee twenty-five rupees, and more than that, people liked to shirk the fact that the time had come to call in Dr Raman; for them there was something ominous in the very association. As a result, when the big man came on the scene it was always a quick decision one way or another. There was no scope or time for any kind of wavering or whitewashing. Long years of practice of this kind had bred in the doctor a certain curt truthfulness; for that very reason his opinion was valued; he was not a mere doctor expressing an opinion but a judge pronouncing a verdict. The patient's life hung on his words. This never unduly worried Dr Raman. He never believed that agreeable words ever saved lives. He did not think it was any of his business to provide comforting lies when as a matter of course nature would tell them the truth in a few hours. However, when he glimpsed the faintest sign of hope, he rolled up his sleeve and stepped into the arena: it might be hours or days, but he never withdrew till he wrested the prize from Yama's hands.  
&gt;  
&gt;Today, standing over a bed, the doctor felt that he himself needed someone to tell him soothing lies. He mopped his brow with his kerchief and sat down in the chair beside the bed. On the bed lay his dearest friend in the world: Gopal. They had known each other for forty years now, starting with their kindergarten days. They could not, of course, meet as much as they wanted, each being wrapped in his own family and profession. Occasionally, on a Sunday, Gopal would walk into the consulting room and wait patiently in a corner till the doctor was free. And then they would dine together, see a picture and talk of each other's life and activities. It was a classic friendship, which endured untouched by changing times, circumstances and activities.  
&gt;  
&gt;In his busy round of work, Dr Raman had not noticed that Gopal had not called in for over three months now. He only remembered it when he saw Gopal's son sitting on a bench in the consulting hall one crowded morning.  Dr Raman could not talk to him for over an hour. When he got up and was about to pass on to the operating room, he called up the young man and asked, 'What brings you here, sir?' The youth was nervous and shy. 'Mother sent me here.'  
&gt;  
&gt;'What can I do for you?'  
&gt;  
&gt;'Father is ill...'  
&gt;  
&gt;It was an operation day and he was not free till three in the afternoon. He rushed off straight from the clinic to his friend's house, in Lawley Extension.  
&gt;  
&gt;Gopal lay in bed as if in sleep. The doctor stood over him and asked Gopal's wife, 'How long has he been in bed?'  
&gt;  
&gt;'A month and a half, Doctor.'  
&gt;  
&gt;'Who is attending him?'  
&gt;  
&gt;'A doctor in the next street. He comes down once in three days and gives him medicine.'  
&gt;  
&gt;'What is his name?' He had never heard of him. 'Someone I don't know, but I wish he had had the goodness to tell me about it. Why, why couldn't you have sent me word earlier?'  
&gt;  
&gt;'We thought you would be busy and did not wish to trouble you unnecessarily.' They were apologetic and miserable. There was hardly any time to be lost. He took off his coat and opened his bag. He took out an injection tube, the needle sizzled over the stove. The sick man's wife whimpered in a corner and essayed to ask questions.  
&gt;  
&gt;'Please don't ask questions,' snapped the doctor. He looked at the children, who were watching the sterilizer, and said, 'Send them all away somewhere, except the eldest.'  
&gt;  
&gt;He shot in the drug, sat back in his chair and gazed at the patient's face for over an hour. The patient still remained motionless. The doctor's face gleamed with perspiration, and his eyelids drooped with fatigue. The sick man's wife stood in a corner and watched silently. She asked timidly, 'Doctor, shall I make some coffee for you?' 'No,' he replied, although he felt famished, having missed his midday meal. He got up and said, 'I will be back in a few minutes. Don't disturb him on any account.' He picked up his bag and went to his car. In a quarter of an hour he was back, followed by an assistant and a nurse. The doctor told the lady of the house, ' I have to perform an operation.'  
&gt;  
&gt;'Why, why? Why?' she asked faintly.  
&gt;  
&gt;'I will tell you all that soon. Will you leave your son here to help us, and go over to the next house and stay there till I call you?'  
&gt;  
&gt;The lady felt giddy and sank down on the floor, unable to bear the strain. The nurse attended to her and led her out.  
&gt;  
&gt;At about eight in the evening the patient opened his eyes and stirred slightly in bed. The assistant was overjoyed. He exclaimed enthusiastically, 'Sir, he will pull through.' The doctor looked at him coldly and whispered, ' I would give anything to see him pull through but, but the heart...'  
&gt;  
&gt;'The pulse has improved, sir.'  
&gt;  
&gt;'Well, well,' replied the doctor. 'Don't trust it. It is only a false flash-up, very common in these cases.' He ruminated for a while and added, 'If the pulse keeps up till eight in the morning it will go on for the next forty years, but I doubt very much if we shall see anything of it at all after two tonight.'  
&gt;  
&gt;He sent away the assistant and sat beside the patient. At about eleven the patient opened his eyes and smiled at his friend. He showed a slight improvement, he was able to take in a little food. A great feeling of relief and joy went through the household. They swarmed around the doctor and poured out their gratitude. He sat in his seat beside the bed, gazing sternly at the patient's face, hardly showing any signs of hearing what they were saying to him. The sick man's wife asked, 'Is he now out of danger?' Without turning his head the doctor said, 'Give glucose and brandy every forty minutes; just a couple of spoons will do.' The lady went away to the kitchen. She felt restless. She felt she must know the truth whatever it was. Why was the great man so evasive? The suspense was unbearable.. Perhaps he could not speak so near the patient's bed. She beckoned to him from the kitchen doorway. The doctor rose and went over. She asked, 'What about him now? How is he?' The doctor bit his lips and replied, looking at the floor, 'Don't get excited. Unless you must know about it, don't ask now.' Her eyes opened wide in terror. She clasped her hands together and implored, 'Tell me the truth.' The doctor replied, 'I would rather not talk to you now.' He turned round and went back to his chair. A terrible wailing shot through the still house; the patient stirred and looked about in bewilderment. The doctor got up again, went over to the kitchen door, drew it in securely and shut off the wail.  
&gt;  
&gt;When the doctor resumed his seat the patient asked in the faintest whisper possible, 'Is that someone crying?' The doctor advised, 'Don't exert yourself. You mustn't talk.' He felt the pulse. It was already agitated by the exertion. The patient asked, 'Am I going? Don't hide it from me.' The doctor made a deprecating noise and sat back in his chair. He had never faced a situation like this. It was not in his nature to whitewash. People attached great value to his word because of that. He stole a look at the other. The patient motioned a finger to draw him nearer and whispered, 'I must know how long I am going to last. I must sign the will. It is all ready. Ask my wife for the despatch box. You must sign as a witness.'  
&gt;  
&gt;'Oh!' the doctor exclaimed. 'You are exerting yourself too much. You must be quieter.' He felt idiotic to be repeating it. 'How fine it would be,' he reflected, 'to drop the whole business and run away somewhere without answering anybody any question!' The patient clutched the doctor's wrist with his weak fingers and said, 'Ramu, it is my good fortune that you are here at this moment. I can trust your word. I can't leave my property unsettled. That will mean endless misery for my wife and children. You know all about Subbiah and his gang. Let me sign before it is too late. Tell me...'  
&gt;  
&gt;'Yes, presently,' replied the doctor. He walked off to his car, sat in the back seat and reflected. He looked at his watch. Midnight. If the will was to be signed, it must be done within the next two hours, or never. He could not be responsible for a mess there; he knew the family affairs too well and about those wolves, Subbiah and his gang. But what could he do? If he asked him to sign the will, it would virtually mean a death sentence and destroy the thousandth part of a chance that the patient had of survival. he got down from the car and went in. He resumed his seat in the chair. The patient was staring at him appealingly. The doctor said to himself, 'If my word can save his life, he shall not die. The will be damned.' He called, 'Gopal, listen.' This was the first time he was going to do a piece of acting before a patient, simulate a feeling and conceal his judgement. He stooped over the patient and said, with deliberate emphasis, 'Don't worry about the will now. You are going to live. Your heart is absolutely sound.' A new glow suffused the patient's face as he heard it. He asked in a tone of relief, 'Do you say so? If it comes from your lips it must be true...' The doctor said, 'Quite right. You are improving every second. Sleep in peace. You must not exert yourself on any account. You must sleep very soundly. I will see you in the morning.' The patient looked at him gratefully for a moment and then closed his eyes. The doctor picked up his bag and went out, shutting the door softly behind him.  
&gt;  
&gt;On his way home he stopped for a moment at his hospital, called out his assistant and said, 'That Lawley Extension case. You might expect the collapse any second now. Go there with a tube of --- in hand, and give it in case the struggle is too hard at the end. Hurry up.'  
&gt;  
&gt;Next morning he was back at Lawley Extension at ten. From his car he made a dash for the sick bed. The patient was awake and looked very well. The assistant reported satisfactory pulse. The doctor put his tube to his heart, listened for a while and told the sick man's wife, 'Don't look so unhappy, lady. Your husband will live to be ninety.' When they were going back to the hospital, the assistant sitting beside him in the car asked, 'Is he going to live, sir?'  
&gt;  
&gt;'I will bet on it. He will live to be ninety. He has turned the corner. How he has survived this attack will be a puzzle to me all my life,' replied the doctor.

&amp;#x200B;

Next, I have prepped the data by removing punctuation marks and capital letters etc, resulting in a cleaned up version like this:

&amp;#x200B;

&gt;people came to him when the patient was on his last legs dr raman often burst out why couldnt you have come a day earlier the reason was obvious - visiting fee twenty-five rupees and more than that people liked to shirk the fact that the time had come to call in dr raman for them there was something ominous in the very association as a result when the big man came on the scene it was always a quick decision one way or another there was no scope or time for any kind of wavering or whitewashing long years of practice of this kind had bred in the doctor a certain curt truthfulness for that very reason his opinion was valued he was not a mere doctor expressing an opinion but a judge pronouncing a verdict the patients life hung on his words this never unduly worried dr raman he never believed that agreeable words ever saved lives he did not think it was any of his business to provide comforting lies when as a matter of course nature would tell them the truth in a few hours however when he glimpsed the faintest sign of hope he rolled up his sleeve and stepped into the arena: it might be hours or days but he never withdrew till he wrested the prize from yamas hands today standing over a bed the doctor felt that he himself needed someone to tell him soothing lies he mopped his brow with his kerchief and sat down in the chair beside the bed on the bed lay his dearest friend in the world: gopal they had known each other for forty years now starting with their kindergarten days they could not of course meet as much as they wanted each being wrapped in his own family and profession occasionally on a sunday gopal would walk into the consulting room and wait patiently in a corner till the doctor was free and then they would dine together see a picture and talk of each others life and activities it was a classic friendship which endured untouched by changing times circumstances and activities in his busy round of work dr raman had not noticed that gopal had not called in for over three months now he only remembered it when he saw gopals son sitting on a bench in the consulting hall one crowded morning dr raman could not talk to him for over an hour when he got up and was about to pass on to the operating room he called up the young man and asked what brings you here sir the youth was nervous and shy mother sent me here what can i do for you father is ill it was an operation day and he was not free till three in the afternoon he rushed off straight from the clinic to his friends house in lawley extension gopal lay in bed as if in sleep the doctor stood over him and asked gopals wife how long has he been in bed a month and a half doctor who is attending him a doctor in the next street he comes down once in three days and gives him medicine what is his name he had never heard of him someone i dont know but i wish he had had the goodness to tell me about it why why couldnt you have sent me word earlier we thought you would be busy and did not wish to trouble you unnecessarily they were apologetic and miserable there was hardly any time to be lost he took off his coat and opened his bag he took out an injection tube the needle sizzled over the stove the sick mans wife whimpered in a corner and essayed to ask questions please dont ask questions snapped the doctor he looked at the children who were watching the sterilizer and said send them all away somewhere except the eldest he shot in the drug sat back in his chair and gazed at the patients face for over an hour the patient still remained motionless the doctors face gleamed with perspiration and his eyelids drooped with fatigue the sick mans wife stood in a corner and watched silently she asked timidly doctor shall i make some coffee for you no he replied although he felt famished having missed his midday meal he got up and said i will be back in a few minutes dont disturb him on any account he picked up his bag and went to his car in a quarter of an hour he was back followed by an assistant and a nurse the doctor told the lady of the house i have to perform an operation why why why she asked faintly i will tell you all that soon will you leave your son here to help us and go over to the next house and stay there till i call you the lady felt giddy and sank down on the floor unable to bear the strain the nurse attended to her and led her out at about eight in the evening the patient opened his eyes and stirred slightly in bed the assistant was overjoyed he exclaimed enthusiastically sir he will pull through the doctor looked at him coldly and whispered i would give anything to see him pull through but but the heart the pulse has improved sir well well replied the doctor dont trust it it is only a false flash-up very common in these cases he ruminated for a while and added if the pulse keeps up till eight in the morning it will go on for the next forty years but i doubt very much if we shall see anything of it at all after two tonight he sent away the assistant and sat beside the patient at about eleven the patient opened his eyes and smiled at his friend he showed a slight improvement he was able to take in a little food a great feeling of relief and joy went through the household they swarmed around the doctor and poured out their gratitude he sat in his seat beside the bed gazing sternly at the patients face hardly showing any signs of hearing what they were saying to him the sick mans wife asked is he now out of danger without turning his head the doctor said give glucose and brandy every forty minutes just a couple of spoons will do the lady went away to the kitchen she felt restless she felt she must know the truth whatever it was why was the great man so evasive the suspense was unbearable perhaps he could not speak so near the patients bed she beckoned to him from the kitchen doorway the doctor rose and went over she asked what about him now how is he the doctor bit his lips and replied looking at the floor dont get excited unless you must know about it dont ask now her eyes opened wide in terror she clasped her hands together and implored tell me the truth the doctor replied i would rather not talk to you now he turned round and went back to his chair a terrible wailing shot through the still house the patient stirred and looked about in bewilderment the doctor got up again went over to the kitchen door drew it in securely and shut off the wail when the doctor resumed his seat the patient asked in the faintest whisper possible is that someone crying the doctor advised dont exert yourself you mustnt talk he felt the pulse it was already agitated by the exertion the patient asked am i going dont hide it from me the doctor made a deprecating noise and sat back in his chair he had never faced a situation like this it was not in his nature to whitewash people attached great value to his word because of that he stole a look at the other the patient motioned a finger to draw him nearer and whispered i must know how long i am going to last i must sign the will it is all ready ask my wife for the despatch box you must sign as a witness oh the doctor exclaimed you are exerting yourself too much you must be quieter he felt idiotic to be repeating it how fine it would be he reflected to drop the whole business and run away somewhere without answering anybody any question the patient clutched the doctors wrist with his weak fingers and said ramu it is my good fortune that you are here at this moment i can trust your word i cant leave my property unsettled that will mean endless misery for my wife and children you know all about subbiah and his gang let me sign before it is too late tell me yes presently replied the doctor he walked off to his car sat in the back seat and reflected he looked at his watch midnight if the will was to be signed it must be done within the next two hours or never he could not be responsible for a mess there he knew the family affairs too well and about those wolves subbiah and his gang but what could he do if he asked him to sign the will it would virtually mean a death sentence and destroy the thousandth part of a chance that the patient had of survival he got down from the car and went in he resumed his seat in the chair the patient was staring at him appealingly the doctor said to himself if my word can save his life he shall not die the will be damned he called gopal listen this was the first time he was going to do a piece of acting before a patient simulate a feeling and conceal his judgement he stooped over the patient and said with deliberate emphasis dont worry about the will now you are going to live your heart is absolutely sound a new glow suffused the patients face as he heard it he asked in a tone of relief do you say so if it comes from your lips it must be true the doctor said quite right you are improving every second sleep in peace you must not exert yourself on any account you must sleep very soundly i will see you in the morning the patient looked at him gratefully for a moment and then closed his eyes the doctor picked up his bag and went out shutting the door softly behind him on his way home he stopped for a moment at his hospital called out his assistant and said that lawley extension case you might expect the collapse any second now go there with a tube of --- in hand and give it in case the struggle is too hard at the end hurry up next morning he was back at lawley extension at ten from his car he made a dash for the sick bed the patient was awake and looked very well the assistant reported satisfactory pulse the doctor put his tube to his heart listened for a while and told the sick mans wife dont look so unhappy lady your husband will live to be ninety when they were going back to the hospital the assistant sitting beside him in the car asked is he going to live sir i will bet on it he will live to be ninety he has turned the corner how he has survived this attack will be a puzzle to me all my life replied the doctor

&amp;#x200B;

Next, i extracted a list of unique words used in the text, of which there are a total of 653 unique words in this short story:

&amp;#x200B;

&gt;people came to him when the patient was on his last legs dr raman often burst out why couldnt you have come a day earlier reason obvious - visiting fee twenty-five rupees and more than that liked shirk fact time had call in for them there something ominous very association as result big man scene it always quick decision one way or another no scope any kind of wavering whitewashing long years practice this bred doctor certain curt truthfulness opinion valued he not mere expressing an but judge pronouncing verdict patients life hung words never unduly worried believed agreeable ever saved lives did think business provide comforting lies matter course nature would tell truth few hours however glimpsed faintest sign hope rolled up sleeve stepped into arena: might be days withdrew till wrested prize from yamas hands today standing over bed felt himself needed someone soothing mopped brow with kerchief sat down chair beside lay dearest friend world: gopal they known each other forty now starting their kindergarten could meet much wanted being wrapped own family profession occasionally sunday walk consulting room wait patiently corner free then dine together see picture talk others activities classic friendship which endured untouched by changing times circumstances busy round work noticed called three months only remembered saw gopals son sitting bench hall crowded morning hour got about pass operating young asked what brings here sir youth nervous shy mother sent me can i do father is ill operation afternoon rushed off straight clinic friends house lawley extension if sleep stood wife how has been month half who attending next street comes once gives medicine name heard dont know wish goodness word we thought trouble unnecessarily were apologetic miserable hardly lost took coat opened bag injection tube needle sizzled stove sick mans whimpered

&amp;#x200B;

Then, I have searched for all co-occuring pair of words, neighboring incidents of two-words and three-words phrases, and listed out the most common of those:

&amp;#x200B;

&gt;Index \[2, 5\] Co-occurence of phrase ' to the ' = 5  
Index \[2, 9\] Co-occurence of phrase ' to his ' = 6  
Index \[2, 210\] Co-occurence of phrase ' to be ' = 5  
Index \[5, 6\] Co-occurence of phrase ' the patient ' = 15  
Index \[5, 109\] Co-occurence of phrase ' the doctor ' = 23  
Index \[5, 138\] Co-occurence of phrase ' the patients ' = 5  
Index \[5, 623\] Co-occurence of phrase ' the sick ' = 5  
Index \[12, 13\] Co-occurence of phrase ' dr raman ' = 5  
Index \[51, 5\] Co-occurence of phrase ' in the ' = 15  
Index \[51, 9\] Co-occurence of phrase ' in his ' = 6  
Index \[51, 22\] Co-occurence of phrase ' in a ' = 8  
Index \[54, 22\] Co-occurence of phrase ' for a ' = 5  
Index \[75, 7\] Co-occurence of phrase ' it was ' = 7  
Index \[122, 7\] Co-occurence of phrase ' he was ' = 6  
  
&gt;  
&gt;Index \[5, 6, 7\] Co-occurence of phrase ' the patient was ' = 3  
Index \[5, 6, 433\] Co-occurence of phrase ' the patient asked ' = 2  
Index \[5, 6, 607\] Co-occurence of phrase ' the patient opened ' = 2  
Index \[5, 109, 122\] Co-occurence of phrase ' the doctor he ' = 2  
Index \[5, 623, 624\] Co-occurence of phrase ' the sick mans ' = 4  
Index \[6, 607, 9\] Co-occurence of phrase ' patient opened his ' = 2  
Index \[9, 609, 34\] Co-occurence of phrase ' his bag and ' = 2

&amp;#x200B;

My question is that I feel stuck at a dead end here, because those co-occurrences do not seem to provide any meaningful data by which we can gain insight into the story at hand.

For example, taking out filler words like 'to', 'in' and so forth, the most common occurring word-pair is ""The doctor"" at 23 times. The most common word-triples are only used 3 or 4 times, and not very interesting.

What other tools in the Natural Language Processing toolkit can allow us to gain deeper insights into a short story such as this?

Thank you beforehand!

Cheers!",25317,LanguageTechnology,1
How does working on NLP job looks like? ,40,LanguageTechnology,1
"Book recommendations Hi there,

I am completely new to nlp but already fascinated - especially by the concept of word embeddings. 

Since I am novel to programming in general, can someone recommend a book that gives an overview into different nlp models/techniques without getting to technical? I have no idea whether this even exists or if books are the go to way to acquire knowledge in machine learning. Alternatively if someone could explain good websites/youtube channels/podcasts I highly appreciate.

Fyi, I do have a thorough understanding of linear algebra including matrix-operations so if it gets very detailed on the mathematical layer I am happy to explore. 

Thanks in advance",690,LanguageTechnology,1
"Beyond Accuracy: Behavioral Testing of NLP models with CheckList Often in Natural Language Processing(NLP), we see some kind of accuracy measure as the proof of our model’s correctness. But clearly, such automated objective evaluations end-up resulting in overestimating the performance.  This paper exactly talks about that and proposes new framework for evaluation called CheckList. 🎉 

This paper won Best Paper Award at ACL 2020. 🥇 


Blog - https://lnkd.in/dhgFKRE",469,LanguageTechnology,1
Interlingo - App de Ingles para MÓVIL (próximo lanzamiento) ,60,LanguageTechnology,1
"How to match headlines with countries? I've been a professional web developer for a couple of years, but never took on a project that uses any kind of ML. Now I have a problem that I'd like to solve with ML, but am having a hard time figuring out what kind of technology is fit for this purpose.

What I want to do is to tag articles by country, based on the headline. A few examples of expected mappings:

""Interview with Martin Jacques on BBC Coverage of China "" =&gt; ""China""  
""The Senate says no to $15"" =&gt; ""United States""  
""Why Memes Will Never Be Monetized "" =&gt; ""International""  
""Along the Thames "" =&gt; ""United Kingdom""  
""Rising Tides: Diving Into Mumbai’s Flooding Challenges "" =&gt; ""India""  
""US Election: What’s at stake for Brazil "" =&gt; ""United States, Brazil""  


What's a reasonable way to solve this problem? Preferably something I can run myself, without using a paid service",904,LanguageTechnology,1
"[question] What kind of resources are required to train a decoder-only Transformer end-to-end? I would like to train a Transformer to perform a next-word prediction task, using a basic corpus (e.g., a native corpus from Python nltk). Is this doable with the resources I have?

&amp;#x200B;

**Processor**: Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz 3.19 GHz

**RAM:** 16.0 GB (15.8 GB usable)

**GPU:** My Device Manager indicates that I have two: NVIDIA GeForce GTX 1070 &amp;  Intel UHD Graphics 630",498,LanguageTechnology,1
"What are some document-level relation extraction datasets I may have missed? Hey guys. Working on a document-level relation extraction (DocRE) task and I'm wondering what kind of datasets there may be.

In the general domain I've only been able to come across DocRED, and there are also the BC5CDR and GDA datasets in the biomedical domain. I'm wondering what else may be out there that I've missed.

Thanks!",408,LanguageTechnology,1
"I want to find out popular issues faced by consumers Hello everyone!
I'm doing this project where, I'll be taking in consumer complaints. 
The basic aim of my project is to highlight most common issues faced by consumers by using either keyword extraction or document clustering.

Kindly suggest some techniques/tools using which I could implement this.",353,LanguageTechnology,1
"Trained a Markov Chain on a bunch of r/WSB posts and comments. Only 2-word conditional probabilities but honestly, that's all that's necessary 🚀🚀 First two words are the seeds

* your wife didn't marry you because we've understood all along . Deep Fucking Value . Salute !
* gamestop is still a 300% green day . Hold the line , the funds most likely rebought
* the retard you must not be earth shattering dump . Don’t worry everyone always wipes their first
* the retard strength in this nonsense lmao ! Priceless . Good shit Edit: In discovery we could
* stonks can only place I can warm my tendies . Can someone explain how this movie 2
* your wife and her boyfriend extra close tonight . J . Simpson to do another paper trading
* your wife may have covered the shorts to have joined . I can’t buy more shares
* stonks can only hope for the word ""tendies"" . # HOLD HOLD HOLD ! 💎🙌 . Guys
* gamestop is going to $100 Million . DO NOT SELL 💎🙌🏻 , BUY ! ! ! ! Available shares just got bodied ",990,LanguageTechnology,1
"Automatic Glossary Creation and Definition Extraction from Text in Unsupervised way This blog reflects my learnings from a recent project that I wrapped up as a part of my last semester course. I hope this will be helpful to many of you looking forward to solving similar problems. The task is to come up with an unsupervised technique for automatic extraction of glossary and their respective definitions from some input text (could be the book, chapter, etc)

Blog Link: https://link.medium.com/hjgUMtjECdb",508,LanguageTechnology,1
"POS tagging practice NLP enthusiasts - we all know POS tagging is an important task for so many applications. I made a mastery-based assignment to help you practice and learn the Penn Treebank POS tags. Please feel free to share any feedback with me on the assignment. Thank you!

[https://open.openclass.ai/resource/assignment-601c5ae255950e64a8c0d581?code=94s5gGSI1PZKSg](https://open.openclass.ai/resource/assignment-601c5ae255950e64a8c0d581?code=94s5gGSI1PZKSg)",465,LanguageTechnology,1
"[Best Practices] on how to organize deep learning projects In this article you’ll see how to structure work on deep learning projects — from the inception to deployment, and everything in between. You will learn:

- About the lifecycle of the project.
- Importance of defining an objective or goal of the project.
- Collecting data based on the requirements of the project.
- Model training and results exploration including:
    - Establishing baselines for better results.
    -Adopting techniques and approaches from the existing open-source state-of-the-art models research papers and code repositories.
    - Experiment tracking and management management 
- Model refinement techniques to avoid underfitting and overfitting like:
    - Controlling hyperparameters
    - Regularisation
    - Pruning
- Testing and evaluating your project before deployment.
- Model deployment
- Project maintenance

[Structuring deep learning projects](https://neptune.ai/blog/how-to-organize-deep-learning-projects-best-practices?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=blog-how-to-organize-deep-learning-projects-best-practices&amp;utm_content=languagetechnology)",1167,LanguageTechnology,1
The most Spoken Languages in the World - 1900/2021 ,51,LanguageTechnology,1
"Recognition of logical document structures First approach for recognizing logical document structures like texts, sentences, segments, words, chars and sentence/segment depth based on recurrent neural network grammars.

The model is able to recognizing the followig logical document structures

* (t - text start
* (s - sentence start
* (seg - segment start
* (w - word start
* (c - char start
* )- end of logical document structure
* Ti - sentence/segment depth will be measured recursive

 

The sentence

Georg Bendemann, ein junger Kaufmann, saß in seinem Privatzimmer im ersten Stock eines der niedrigen leichtgebauten Häuser, die entlang des Flusses in einer langen Reihe, fast nur in der Höhe und Färbung unterschieden, sich hinzogen.

should be predicted as

(s (seg (w Georg) (w Bendemann) (c ,)) (seg (w ein) (w junger) (w Kaufmann) (c ,)) (seg (w saß) (w in) (w seinem) (w Privatzimmer) (w im) (w ersten) (w Stock) (w eines) (w der) (w niedrigen) (c ,)) (seg (w leichtgebauten) (w Häuser) (c ,)) (seg (w die) (w entlang) (w des) (w Flusses) (w in) (w einer) (w langen) (w Reihe) (c,)) (seg (w fast) (w nur) (w in) (w der) (w Höhe) (w und) (w Färbung) (w unterschieden) (c ,)) (seg (w sich) (w hinzogen) (c .)))

Check out my Github for the full source Code: [https://github.com/Psarpei/Recognition-of-logical-document-structures](https://github.com/Psarpei/Recognition-of-logical-document-structures)",1411,LanguageTechnology,1
"How to stem plural words properly? I'm looking for a way to avoid removing ending s when s isn't a suffix. In order to do that, I first check if a word exists in my index, if it does, I don't remove the ending s but If it doesn't, I go on and remove the ending s and add it to the index. But the problem is what to do when starting to build the index.

Imagine we encounter ""books"", I remove s and add ""book"" to my index. On the other hand, I may encounter  ""dangerous"" for the first time, since it doesn't exists in my index yet, I remove s and add ""dangerou"" which is obviously wrong. What should I do?

Specifically I'm looking for ways to properly detect if suffixes and prefixes are indeed one or part of the original word.

P.S: I'm not working on English docs. Therefore I'm looking for general ideas about these situations not ready to use libraries.",858,LanguageTechnology,1
Folks'Talks human-computer interaction test 11 ,47,LanguageTechnology,1
"I have a question So I'm going through [TensorFlows NLP Zero to Hero](https://www.youtube.com/watch?v=fNxaJsNG3-s) video playlist as an introduction to NLP.

The host shows the word LISTEN and talks about how it may be encoded letter by letter with ASCII.

He then proceeds to show the word SILENT and claims that because it contains the same letters and numbers, it is hard for us to understand the sentiment of the word.

Am I stupid or is it easy as hell to read something in order?",485,LanguageTechnology,1
"How to get an accurate text similarity score between very large documents (dozens or even hundreds of pages of text), when token order matters? Levenshtein woud be ideal, but it's not computationally feasible to compare hundreds of pages long documents with it. Hamming and other edit-based metrics look good but are also way too time consuming.

Cosine similarity is very fast, but it doesn't take into account the order of appearance of tokens, which matters to me. Same goes for Jaccard and other token-based similarity metrics I've ran across.

Is there a fast algorithm out there which takes into account the order of the sequence?

I don't care about semantic stuff btw, I need to compare texts superficially.",715,LanguageTechnology,1
"Master’s in NLP or CompLing in Europe (non CS background) Hello everyone! 
I’m from the US. Recently graduated with BA in Spanish with a minor in Linguistics (3.75 GPA). I have coursework in Spanish, phonology, phonetics, syntax, language and technology and machine learning (only one class).

I’m looking for Master’s in NLP or CL in Europe that are open to applicants from a non CS background. I have intermediate level knowledge of Python and I am taking Python classes through Coursera. I have already completed a few. Are there any programs that are willing to consider applicants from a strictly a language/linguistics background?

Thank you!",648,LanguageTechnology,1
Connection between WIKIDATA and WORDNETS Just a quick question.... Are wordnet synsets linked to opendata in any way like wikidata or dbpedia ?,143,LanguageTechnology,1
"Best module for my project? Please help Hi there NLP friends,
Im pretty new to python and NLP. Please help point me in the right direction.

Im curious about potential causes to an effect. Casual inference but maybe not just directly cause and effect, and even more like potential causes that may lead to effect.
I would want to be able to type in an effect and get a collection of all the potential causes and vice versa.
As well the negative or nullifying forces that may potentially oppose the effect.
My project needs me to type in a sentence not just a bag of words. And would return phrases and words that meet the criteria.

Eg. Forest Fire.
 Potential causes and correlations: cigarette, spark, negligence, firepit, lightening strike, fireworks, spontaneous combustion of mulch, dry weather, dry plant matter.
Potentially causes: burn down homes, fear, 
Potential opposing elements: rain, firefighters, fire extinguisher, dump water on camp fire, careful, etc.

Preferably trained on a blend of General academic subjects, psychology, physics, phrases and quotes, and large web commons like wiki, google, etc.",1116,LanguageTechnology,1
Language acquisition by virtual agent (The Folks’Talks game project)  [https://drive.google.com/file/d/1cWNqnOyowoAr3d\_DJZbdIpKuvlAYu7eu/view?usp=sharing](https://drive.google.com/file/d/1cWNqnOyowoAr3d_DJZbdIpKuvlAYu7eu/view?usp=sharing),239,LanguageTechnology,1
"Pororo: A Deep Learning based Multilingual Natural Language Processing Library Pororo: A Deep Learning based Multilingual Natural Language Processing Library

Hello, we at Kakao Brain made [Pororo](https://github.com/kakaobrain/pororo) open source based Natural Language and Speech Processing library

Pororo is a Python library that implements more than 30 natural language processing models in various languages ​​such as English, Korean, Chinese, and Japanese.

 Even if you don't know anything about artificial intelligence or natural language processing, you can easily perform various tasks such as name entity recognition, machine reading comprehension, machine translation, summarization, and sentiment classification by writing 3 to 4 lines of code.  For example, NER can be performed with the code below.

 If you install it with pip install pororo, you can use it immediately, and we plan to expand models and tasks by language, so please use and feedback a lot.
 More details can be found at https://github.com/kakaobrain/pororo.  Thank you.",1053,LanguageTechnology,1
"How do you explain NLP to someone who's never heard of NLP? I'm a marketing, college student learning about AI and NLP. I have noticed that when I try to explain NLP to my non-technical friends, they have a hard time understanding. I know this group talks about more technical stuff (and I cannot understand 90% of the conversations), but does anyone have any suggestions on how I can explain NLP to someone who's never heard of NLP?",433,LanguageTechnology,1
"What is a good task to demonstrate the power of a new language modeling architecture? So we are doing a class project, which requires use to use NLP. We have chosen to design a model akin to the Transformer. The faculty evaluating us is a tad inexperienced with language modeling, so we would like to demonstrate the performance gains and competency of our model using a simple task, like article generation, for example.
What other tasks would you recommend, especially those that have comprehensive datasets available?",520,LanguageTechnology,1
"[DATASET] Massive multi-turn conversational dataset based on cleaned discord data This is a long-context, anonymized, clean, multi-turn and single-turn conversational dataset based on discord data scraped from a large variety of severs, big and small.

The goal is to use this data to pretrain a small conversational model on a big variety of data.

The raw data for this version contained 51,826,268 messages5103788 (regex) + 696161 (toxic)/51826268, or 0.11% of the messages were removed**The dataset's final size is 46,026,319 messages across 456810 conversations**, which is reduced from 33.06 GB of raw json data to 968.87 MB

[https://www.kaggle.com/jef1056/discord-data](https://www.kaggle.com/jef1056/discord-data)",722,LanguageTechnology,1
"What do you do at work? I’m a student in computational linguistics, wondering what the actuality of the degree is Title explains itself. I’m a sophomore studying linguistics with a concentration in computational linguistics. If you feel so inclined the include salary, I would not mind either!",293,LanguageTechnology,1
Why don't we fine-tune BERT tokenizer? We fine-tune the work representations in BERT to learn information from the task but we don't touch the tokenizer. Why don't we learn better subword representations from the task too?,222,LanguageTechnology,1
"How should I go with training a word decompounder? I would like to look into and train a word decompounder for Nordic languages. However, I have never done it, so I am not really familiar with which methods work best. Would anyone have any advice?",247,LanguageTechnology,1
"Current state of the art for document classification? I have a problem where I need to classify chunks of text as to whether they pertain to the pharmaceutical industry or not. It will be a supervised learning task but I will need to go through the process of manually labeling the text to come up with the training data.

What are the current state-of-the-art text classifiers and are they feasible to use given my constraints?",428,LanguageTechnology,1
"Do companies offer NLP Research Internships for the summer before starting grad school? I have been fortunate enough to be accepted into three schools so far for an MS degree and am waiting on others. Therefore, I have begun searching for internships for this upcoming summer. I have found that most research internships only want Ph.D. students. I have tried cold-emailing, connecting, asking research scientists for a ""coffee chat,"" etc. Unfortunately, I have had no success. Is there any place I should be looking? Is it rare for people in my position not to get internships?

I have almost two years of research experience working and leading NLP research projects. However, I do not have any publications. I am also located in the US if that makes a difference.",766,LanguageTechnology,1
"Is there any difference between sentence embedding and document embedding? I thought sentence and document embeddings are different. I thought one is one that embeds sentences whereas another is one that embeds more then a sentence such as paragraph or whole document text. However, some research paper got me confusing.

[LASER](https://arxiv.org/pdf/1812.10464.pdf) paper describes it as sentence embeddings whereas [T-LASER](https://arxiv.org/pdf/2008.08567.pdf) metions LASER as document embedding. In LASER paper, text classification experiment is done, as well as multiple text classification experiments are carrying using sentence embeddings. If there would be sentence classification, I wouldn't be confused. But don't text usually means a document and instead would be efficient to use document embeddings instead?",824,LanguageTechnology,1
What is topic modeling? ,24,LanguageTechnology,1
What's a good dataset to demonstrate LDA? ,42,LanguageTechnology,1
What is the latest research on NLU and decision making? I would be really interested if anyone a list of links/reading resources in this area as it of particular interest for a project i am working on.,201,LanguageTechnology,1
"How can I summarize text with an unsupervised technique these days? Hi, I have been reading that the state of the art for summarizing, and probably other NLP tasks are done with supervised training, with huge data sets and complex architectures like transformers with pre training. 

That is awesome, but I don't have the GPU power nor access to the datasets to explore that part. Also, I'm OK achieving acceptable results for a start.

That's why I was thinking on unsupervised ways to summarize.

Could you recommend what are the state of the art techniques to do it? Where should I start to dig in?

Thanks!

**Edit for clarification**: I have already read about the abstractive and extractive approaches. What I would like to learn is how to do unsupervised abstractive summarization for a generic text corpora, and the concepts behind this task.",850,LanguageTechnology,1
How To Profit From Alexa Skill Development Using Node JS ,57,LanguageTechnology,1
Do distributed semantic word embeddings such as word2vec have a theoretical foundation in linguistics? ,103,LanguageTechnology,1
"Exactly *how* is the new technology supposed to help improving search engines? Aside from the traditional inverted-index searches, and nearest-neighbor searches (with document/sentence embeddings), I mean.

I am having trouble mixing the two as well, because the ML models fail when it comes to proper nouns, I need an inverted-index search. Sometimes the nearest-neighbor searchs are all over the place too, with tangentially related sentences, and it's hard to adjust a similarity threshold.

Furthermore, I'd like to know how align (highlight) related parts of the text to the query (like Google does). I suspect this is done with expanded queries on inverted-index search, because it would be impractical to store embeddings for each word/wordpiece.

If that's the case, then how am I supposed to expand the searches?",821,LanguageTechnology,1
"Hierarchical Transformers for Long Document Classification (Research Paper Walkthrough) This paper extends BERT for doing long document classification in nlp. They propose BERT variations RoBERT and ToBERT as hierarchical enchancements for the same. They obtained a significant improvement over the baseline models.

Paper Walkthrough: https://youtu.be/3IOl5d9PZeM

Paper Link: https://arxiv.org/abs/1910.10781",410,LanguageTechnology,1
"Metrics to analyze the quality of generated text? If I have a set of synthetic text/articles generated by language models, what metrics can I use to measure the text's quality, in terms of how coherent the context is and how close to human writing? Thank you!",259,LanguageTechnology,1
TriggerNER: Learning with Entity Triggers as Explanations | Research Papers Summary 006 ,88,LanguageTechnology,1
"substitute for tokenizer in torchtext In the pytorch official tutorial for language translation ([https://pytorch.org/tutorials/beginner/torchtext\_translation\_tutorial.html](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html)), tokenizer in torchtext is used. Parts of code are

    from torchtext.data.utils import get_tokenizer
    de_tokenizer = get_tokenizer('spacy', language='de')
    en_tokenizer = get_tokenizer('spacy', language='en')

However, I have difficulties in downloading and installing spacy package. Are there other substitutes for the tokenizer? Thanks.",599,LanguageTechnology,1
"[P] An interactive history of natural language processing, starting a long time ago ",84,LanguageTechnology,1
"Research topics Hi all, I finished my master in nlp two years ago (worked on summarization) and I'm thinking of starting a PhD program.
The thing is, I'm not sure what topics I should work on. 

What I know is that I'd like to work on applicable topics, so nothing too theoretical. Also, leaderboard climbing using Bert in different ways is not that interesting imo.

So I'm basically asking what nlp domains you find interesting and you think should be great research topics.
Thanks!!",485,LanguageTechnology,1
"Partner Up for Learning Hello everyone, hope you doing well. I just wanted share the discord server for the people who search for learning partners. You can join server to find a partner for learning different programming languages or any topics you are interested in.
Here is the link for the server:

https://discord.gg/ayeGrsaSG2",332,LanguageTechnology,1
"How to break a word into syllables? I am new to ML and starting off with what I think is an easy project. My first foray into ML was to predict the number of syllables in a word given its pronunciation, or phonetic transcription. That was pretty straightforward using graph convolutional networks to solve a classification problem. Having succeeded at that, my next goal is to predict the the location of syllable breaks in a word. I have written code to enumerate all possible combinations of *N* syllables for a word, but I don't know which is correct. The core of my problem seems to me to be how to accurately break a graph into subgraphs. This is where I'm running up against my own ignorance about ML approaches. How would I go about doing that? My first thought was link prediction, but I'm open to other ideas. Thanks.",826,LanguageTechnology,1
"using self attention on Word embeddings improved context in only one direction I am trying to implement the explanation for attention in this video using pytorch (https://www.youtube.com/watch?v=yGTUuEx3GkA) . The main idea can be seen on the slide at 12:38

Focusing on just the word ""bank"":

The idea is to get glove embeddings for the words ""Bank of a river"", and then take a dot product of the word embedding of bank with itself and every other word, to get a tensor of 4 weights. Then normalize the weights and use them to get a new word embedding for the word ""bank"" which is now the sum of all the word embeddings scaled by their corresponding normalized weights.

Before, I tried to find the cosine similarity between the words ""bank"" and ""river"" and got 0.33 and after the reweighing I got 0.59 i.e it worked.

I also checked that the similarity between ""water"" and ""bank"" was 0.49 and became 0.57 with the new embeddings.

BUT, the similarity between ""money"" and ""bank"" was 0.57 and before and became 0.64 after! So the ""bank"" vector came closet to ""money"" after I provided context.

So while what I did to the original ""bank"" vector got it closer to things like ""water"" (which is what I was trying to do), it didn't get away from ""money"" as I was expecting.

Why is the change happening in only one direction?

You can have a look at the code and the outputs here https://github.com/VishakBharadwaj94/transformers/blob/master/transformers_tmp.ipynb",1459,LanguageTechnology,1
"Which model should I use to pick the best answer for the TOEIC reading test? I want to build a question answering model that take the context (the reading paragraphs), the question and 4 choices as inputs and output the best answer for that question given that context. I’m new to NLP but willing to learn more so any recommendations will help a lot. Thanks",357,LanguageTechnology,1
Rebuilding the spellchecker: Hunspell and the order of edits ,61,LanguageTechnology,1
"Entity Resolution for Master Data Management Last year, we set out to create a master entity resolution tool. We wanted to identify and resolve multiple occurrences of a single entity to get a clearer picture of the information within data... but in a practical and scaleable way.

Our blog post, **Entity Resolution for Master Data Management** explains the why and how behind er², ThinkData's entity resolution tool.

If you're interested, check it out here: [https://blog.thinkdataworks.com/entity-resolution-for-master-data-management](https://blog.thinkdataworks.com/entity-resolution-for-master-data-management)",617,LanguageTechnology,1
An unexpected use case of word embeddings ,42,LanguageTechnology,1
"Which are top APIs for Indian languages mainly VR, OCR, Speech - Text - Speech? ",80,LanguageTechnology,1
"Chaining BERT multilabel classifiers. Say I have a two layers of labels with a label and sublabel:

    Fruit.Apple
    Fruit.Orange
    Fruit.Banana

    Vegetable:Cucumber
    Vegetable:Squash
    Vegetable:Cabbage

    Meat:Cow
    Meat:Chicken
    Meat:Fish

Then suppose I build a dataset of ""meals"" which are combinations of the above labels:

    [Vegetable:Squash,Meat:Cow]
    [Fruit.Apple,Fruit.Orange]
    [Meat:Fish,Fruit.Orange]

Really each ""meal"" is a document with a category and subcategory. If I were to train a bert classifier for guessing the ingredients in a given meal, would it be better to:

* train 1 classifier with 9 labels
* or build a classier with 3 labels (fruits, vegetables,meats) then build 3 additional classifier to sort the finer sublabels (fruits -&gt; (apple, orange, banana), vegetable -&gt;(cucumber, squash, cabbage), etc)

Now I'd imagine if you had a large enough database, the first would probably be fine. But if you had say 20 labels each with 10 sublabels, then the second option starts to make more sense.",1054,LanguageTechnology,1
"Sales Conversations Datasets Hey everybody,

I'm new to NLP and looking into training tensorflow using sales conversations, i.e. e-mail conversations. The problem of course is getting data... I have not been able to come up with a good possible source yet. Any ideas are appreciated :-)",286,LanguageTechnology,1
"Stylometry library in Python I've written a library for doing forensic stylometry (identifying who wrote an unknown text)

pip install faststylometry

Here's a tutorial [https://freelancedatascientist.net/fast-stylometry-tutorial/](https://freelancedatascientist.net/fast-stylometry-tutorial/)

And here's the library [https://pypi.org/project/faststylometry](https://pypi.org/project/faststylometry)

There are a few cool things I'd like to try with it, such as checking if it could identify Miles Taylor as the staffer who publicly criticised Trump's administration",567,LanguageTechnology,1
"Has anyone seen work investigating the relationship between neural text summarization and relation extraction? Title is the question. I'm working on a relation extraction research project and had a shower thought of whether models that are good at summarization (although this expression is also highly controversial) are also good at extracting relationships between two entities.

Intuitively, it makes sense. However, if I think about the details making the transfer between two tasks seems very difficult (e.g., how to make the transition between structured to unstructured output spaces).

I'd appreciate it if anyone would let me know if they know anything. Thanks!",671,LanguageTechnology,1
Towards Automated Fact-Checking ,32,LanguageTechnology,1
"Hi all! For my side project, I made an AI-based program that predicts what a user will search next for an Online Dictionary. Here is a short article that I wrote about the project. Any thoughts or feedback are greatly appreciate. Thank you! ",241,LanguageTechnology,1
"Tool for Complex Data Labelling Tasks Hi r/LanguageTechnology readers!

We have created a [labelling tool](https://humanlambdas.com/solutions/data-labelling) that can be customized to display all sorts of data models and tasks. Here are a couple of [examples](https://www.humanlambdas.com/templates/nlp-news-article-annotation) of [types](https://www.humanlambdas.com/templates/sec-filing-data-extraction) of complex tasks one can set up.

I hope some of you will find this useful, and if you have any thoughts I would love to hear your feedback!",546,LanguageTechnology,1
"[R] RECCON: A Dataset for Recognizing Emotion Cause in Conversations RECCON is a dataset for causal reasoning of emotions in conversations. It has subtasks of textual entailment aka Natural Language Inference (NLI) and QA.  


**Abstract.** Recognizing the cause behind emotions in text is a fundamental yet under-explored area of research in NLP. Advances in this area hold the potential to improve interpretability and performance in affect-based models. Identifying emotion causes at the utterance level in conversations is particularly challenging due to the intermingling dynamic among the interlocutors. To this end, we introduce the task of recognizing emotion cause in conversations with an accompanying dataset named RECCON. Furthermore, we define different cause types based on the source of the causes and establish strong transformer-based baselines to address two different sub-tasks of RECCON: 1) Causal Span Extraction and 2) Causal Emotion Entailment. The dataset is available at [https://github.com/declare-lab/RECCON](https://github.com/declare-lab/RECCON).

**Paper:** [**https://arxiv.org/pdf/2012.11820.pdf**](https://arxiv.org/pdf/2012.11820.pdf)

**Code:** [**https://github.com/declare-lab/RECCON**](https://github.com/declare-lab/RECCON)

**PwC:** [**https://paperswithcode.com/paper/recognizing-emotion-cause-in-conversations**](https://paperswithcode.com/paper/recognizing-emotion-cause-in-conversations)",1431,LanguageTechnology,1
Does anyone have any experience in Japanese text classification? Or is there anyone who can help me with that? I’ll be really thankful! ,136,LanguageTechnology,1
"Match questions to similar questions? basically we want to help users find similar questions

I've been using https://github.com/hanxiao/bert-as-service#building-a-qa-semantic-search-engine-in-3-minutes but am not very happy with the result. 

Are there any low hanging fruits for it?


Why am I not so happy?

Given this input:
`Someone got stuck in the middle of a donation`


We received these matches:
```
18.805347 How the fiscal sponsorship works
18.803146 What fees are involved in running a campaign on MyApp?
18.800673 Do I have to provide contribution levels in my campaign?
18.682882 Someone could not donate, what is wrong?
```

well it found the match on position 4, but this is a suuuuper small example size to match again, we only had a total of 14 possible questions that it could match. So not very impressed. Are there any better easy approaches?",864,LanguageTechnology,1
"Find sponsors from transcripts (yt/podcast) Hi there!

For videos on YouTube or podcasts on other platforms, I want to find all the brand(s) that are sponsoring that video or that podcast. Right now we are using fuzzy queries on description and transcript texts. But that does not cover all the cases. 

Often content creators use terms like “this video is sponsored/brought to you by {brand name}” but not always. So we want to use nlp. 

Please suggest any approach/idea/library or any direction to further investigation. Any help is appreciated.",548,LanguageTechnology,1
"Is there a way to analyze a sentence to calculate whether A is beneficial to B? For example we use a float value to represent whether Alice is helping Bob:

* In **Alice gives Bob $100**, Alice is helping Bob; (+0.5)
* In **Alice gives Bob a slap**, Bob is hurt by Alice; (-0.5)
* In **Alice gives Bob a slap to keep him from falling sleep in the snow**, Though Bob is hurt, Alice is saving Bob's life so it's still good for him; (+0.7)

So, my idea is to make a corpus based on [FrameNet](https://framenet.icsi.berkeley.edu/fndrupal/WhatIsFrameNet), giving *beneficial* value for frame elements. (Assuming my input is already in the frame data structure) 

Is there any better idea or existing solution for this problem?",721,LanguageTechnology,1
"Is it possible to train a decoder using monolingual data only? I’m working on enhancing low resource translation using cross lingual signals from related languages and I was wondering if it’s somehow possible to train a decoder stack in a seq2seq task using just monolingual data. For example if I have A -&gt; B translation with A having good monolingual data but little or no parallel data with B thus is low resource, my current methodology uses a monolingual embedding space of A to first encode the input sentence, then uses a cross lingual map to map this encoded sentence into a pivot language, and lastly I use a joint cross lingual map of pivot and B to decode the sentence. Now I’m think of employing this method is B -&gt; A direction. The issue is, it’s not possible to learn a joint cross lingual map between A and B (spaces are not isomorphic) so I was thinking of learning a decoder stack for A using monolingual data available. I can clarify more if needed! Apologies if the questions is a bit uninformed haha",1025,LanguageTechnology,1
"Backprop through Time vs Normal for RNN language model Hello people, so I’m experimenting with an RNN language model and normal back-propagation appears to be performing better than bptt even when it’s been truncated to only look back two or five steps (we’ve tested both). Can anyone give me some intuition about why this might be going on? Does BPTT require more training data to perform better? I would have thought the truncation would mean that vanishing gradients shouldn’t be a problem. If anyone knows any articles or blogs I could read to try and understand this better it would be really useful :)",607,LanguageTechnology,1
"Unbalanced document length &amp; topic modeling (lda) [help] Hi all,

I can't seem to form an intuition for the following settings : I would like to perform a LDA on a group of text but some document are many, many time longer than others. 

The average is at 80 tokens, median 40 but there are document reaching up to 6000 tokens. 

My strategy was to discard document with too few words (lower than the 25% quantile ) as they are unlikely to satisfy the assumption that ""documents are a mixture of topics"". 

But I'm not sure how to deal with the outlier with too many words. Would the LDA be robust enough to deal with the imbalance? Or should I split large documents into smaller chunks and randomly select one chunk.

Thanks in advance.",741,LanguageTechnology,1
"NLP Specialization course on coursera worth it? Dear Reddit-Fam

Can anyone tell me whether the NLP Specialization track on coursera is a good starting point for someone who is studying Data Science (Master's) and want's to go deeper into NLP?

Greeets",252,LanguageTechnology,1
"Fine-Tune 11B T5 for Generation Task I'm trying to figure out how to fine-tune the largest version of T5 for a language generation task, but I can't seem to find any notebooks or resources to do so. Any great places to start?",225,LanguageTechnology,1
NLP for Interpretability Can someone point me in the direction of any literature using NLP to understand why a neural network makes the choices it does? Like e.g. a chess bot which can explain why it made the move it did. Humans use natural language to communicate our beliefs and reasons: surely there's a way to make our models do the same.,342,LanguageTechnology,1
"Announcing UBIAI: Easy-to-Use Text Annotation Tool [UBIAI](https://ubiai.tools) was born out of frustration with existing solutions, which either have a low quality/price ratio or are expensive and geared towards large companies. We know that data labeling is the bottleneck for creating custom NLP models (NER, entity relations, classification, etc) and is here to stay. We wanted to create the most accessible, easy-to-use and automated solution at an affordable price.

If you are launching a new NLP project, please explore our [tool](https://ubiai.tools) (we offer free 14 day trial) and give us your feedback at [admin@ubiai.tools](mailto:admin@ubiai.tools)

Here are few blog links:

* [Introducing UBIAI](https://chatbotslife.com/introducing-ubiai-easy-to-use-text-annotation-for-nlp-applications-74a2401fa725?sk=cace5030af4ca42ad4fda0eff4f2a229)
* [Building Job Entity Recognizer using Amazon Comprehend](https://medium.com/swlh/building-a-job-entity-recognizer-using-amazon-comprehend-5dd2c33faa82?sk=e34060de1a9a5b9da6a5bfefc0bff7d4)",1044,LanguageTechnology,1
"[Tutorial] Training, Visualizing, and Understanding Word Embeddings In this post, we will look at different techniques you can use to better understand how well a language model captures the contextual relationship between words. We will do this by: 

1. Looking at the dataset we need to train these models to see if we can come up with a simple one that helps us visualize how these models “learn” the relationship between different words.
1. Looking at the tools and techniques you can use to track the progress of these models and monitor the results while they process our simplified dataset.
1. After that you should hopefully be able to re-use that template for more complex models with some real life datasets.

[understanding word embeddings](https://neptune.ai/blog/word-embeddings-deep-dive-into-custom-datasets?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=blog-word-embeddings-deep-dive-into-custom-datasets&amp;utm_content=languagetechnology)",965,LanguageTechnology,1
What is the most accurate sentiment recognition open source code? ,66,LanguageTechnology,1
What is the best summarisation open source code? ,49,LanguageTechnology,1
"Building a personal corpus based on digital conversation history Has anybody ever done work on this? Or does anybody know of papers/projects that have done this?

With the new personal data availability created by European Union and California laws, I am currently working on a personal project to unify all of my digital conversation history and data into a personal corpus of everything I've said.

I've come across a number of projects that work with Facebook Messenger, whatsapp, WeChat, etc., but all the projects that I have found only work at an individual app level, and almost all of them are limited to shallow linguistic analysis. I want to perform various types of analysis on my data (mostly for novelty, with some practical application in language learning)

While this is not something everyone would be interested in, and in fact, many privacy minded people would be very opposed to doing this, I think this would be very useful to computational linguists and enthusiasts for analyzing data at a level of intimacy that far overextends what you would get from an academic level.

I'll do this project regardless eventually, but I was just wondering if there is previous work or an open source project to contribute to so I don't have to reinvent the wheel.",1271,LanguageTechnology,1
"Demo colab for openAI's CLIP I decided to create a fun playground Colab, where you can dabble with CLIP!
https://colab.research.google.com/drive/1ePZiVXnINfJNTHB6T_qsNlbo8fFWDsk_?usp=sharing 

Upload your own pictures, choose silly captions, and find out which ones CLIP thinks are most likely. I was already impressed, but do test it yourself :wink:

Please share any funny results you might encounter! :nerd_face:",415,LanguageTechnology,1
"Allen Institute launches GENIE, a leaderboard for human-in-the-loop language model benchmarking ",96,LanguageTechnology,1
"ERNIE-M: Multilingual Model Learns 96 Languages from Monolingual Corpora, Tops Google’s XTREME Benchmark [ERNIE-M](http://arxiv.org/abs/2012.15674) is a new multilingual model that understands 96 languages and tops [XTREME](https://arxiv.org/abs/2003.11080), a substantial multilingual multi-task benchmark proposed by Google, Carnegie Mellon University, and DeepMind. The novel pre-training method can learn semantic alignment across multiple languages on monolingual corpora.

Paper: [https://arxiv.org/abs/2012.15674](https://arxiv.org/abs/2012.15674)

Blog: [http://research.baidu.com/Blog/index-view?id=151](http://research.baidu.com/Blog/index-view?id=151)",662,LanguageTechnology,1
"720+ new NLP models, 300+ supported languages, translation, summarization, question answering, and more with T5 and Marian models! - John Snow Labs NLU 1.1.0 
# 720+ new  NLP models, 300+ supported languages, translation, summarization, question answering and more with T5 and Marian models!  - John Snow Labs NLU 1.1.0

##  NLU 1.1.0 Release Notes

We are incredibly excited to release NLU 1.1.0!
This release integrates the 720+ new models from the latest [Spark-NLP 2.7.0 + releases](https://github.com/JohnSnowLabs/spark-nlp/releases)
You can now achieve state-of-the-art results with Sequence2Sequence transformers on problems like text summarization, question answering, translation between  192+ languages, and extract Named Entity in various Right to Left written languages like  Arabic, Persian, Urdu, and languages that require segmentation like Koreas, Japanese, Chinese, and many more in 1 line of code!     
These new features are possible because of the integration of the [Google's T5 models](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) and [Microsoft's Marian models](https://marian-nmt.github.io/publications/)  transformers.

NLU 1.1.0 has over 720+ new pretrained models and pipelines while extending the support of multi-lingual models to 192+ languages such as Chinese, Japanese, Korean, Arabic, Persian, Urdu, and Hebrew.


In addition to this, NLU 1.1.0 comes with 9 new notebooks showcasing training classifiers for various review and sentiment datasets and 7 notebooks for the new features and models.


### NLU 1.1.0  New Features
* **720+** new models you can find an overview of all NLU models [here](https://nlu.johnsnowlabs.com/docs/en/namespace) and further documentation in the [models hub](https://nlp.johnsnowlabs.com/models)
* **NEW:** Introducing MarianTransformer annotator for machine translation based on MarianNMT models. Marian is an efficient, free Neural Machine Translation framework mainly being developed by the Microsoft Translator team (646+ pretrained models &amp; pipelines in 192+ languages)
* **NEW:** Introducing T5Transformer annotator for Text-To-Text Transfer Transformer (Google T5) models to achieve state-of-the-art results on multiple NLP tasks such as Translation, Summarization, Question Answering, Sentence Similarity, and so on
* **NEW:** Introducing brand new and refactored language detection and identification models. The new LanguageDetectorDL is faster, more accurate, and supports up to 375 languages
* **NEW:** Introducing WordSegmenter model for word segmentation of languages without any rule-based tokenization such as Chinese, Japanese, or Korean
* **NEW:** Introducing DocumentNormalizer component for cleaning content from HTML or XML documents, applying either data cleansing using an arbitrary number of custom regular expressions either data extraction following the different parameters

## Translation
[Translation example](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/translation_demo.ipynb)       
You can translate between more than 192 Languages pairs with the [Marian Models](https://marian-nmt.github.io/publications/)
You need to specify the language your data is in as `start_language` and the language you want to translate to as `target_language`.    
The language references must be [ISO language codes](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)

`nlu.load('&lt;start_language&gt;.translate.&lt;target_language&gt;')`

**Translate English to French :**     
```
nlu.load('en.translate_to.fr').predict(""Hello from John Snow Labs"")
&gt;&gt;&gt; Output: Bonjour des laboratoires de neige de John!	 

```
**Translate English to Inukitut :**     
```
nlu.load('en.translate_to.lu').predict(""Hello from John Snow Labs"")
&gt;&gt;&gt; Output: kalunganyembo ka mashika makamankate 
```
**Translate English to Hungarian :**
```
nlu.load('en.translate_to.hu').predict(""Hello from John Snow Labs"")
&gt;&gt;&gt; Output: Helló John hó laborjából.
```
**Translate English to German :**
```
nlu.load('en.translate_to.de').predict(""Hello from John Snow Labs!"")
&gt;&gt;&gt; Output: Hallo aus John Schnee Labors 
```


```python
translate_pipe = nlu.load('en.translate_to.de')
df = translate_pipe.predict('Billy likes to go to the mall every sunday')
df
```

|	sentence|	translation|
|-----------|--------------|
|Billy likes to go to the mall every sunday	| Billy geht gerne jeden Sonntag ins Einkaufszentrum|






## T5
[Example of every T5 task](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more)
### Overview of every task available with T5
[The T5 model](https://arxiv.org/pdf/1910.10683.pdf) is trained on various datasets for 17 different tasks which fall into 8 categories.


1. Text summarization
2. Question answering
3. Translation
4. Sentiment analysis
5. Natural Language inference
6. Coreference resolution
7. Sentence Completion
8. Word sense disambiguation

### Every T5 Task with explanation:

|Task Name | Explanation | 
|----------|--------------|
|[1.CoLA](https://nyu-mll.github.io/CoLA/)                   | Classify if a sentence is gramaticaly correct|
|[2.RTE](https://dl.acm.org/doi/10.1007/11736790_9)                    | Classify whether if a statement can be deducted from a sentence|
|[3.MNLI](https://arxiv.org/abs/1704.05426)                   | Classify for a hypothesis and premise whether they contradict or contradict each other or neither of both (3 class).|
|[4.MRPC](https://www.aclweb.org/anthology/I05-5002.pdf)                   | Classify whether a pair of sentences is a re-phrasing of each other (semantically equivalent)|
|[5.QNLI](https://arxiv.org/pdf/1804.07461.pdf)                   | Classify whether the answer to a question can be deducted from an answer candidate.|
|[6.QQP](https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs)                    | Classify whether a pair of questions is a re-phrasing of each other (semantically equivalent)|
|[7.SST2](https://www.aclweb.org/anthology/D13-1170.pdf)                   | Classify the sentiment of a sentence as positive or negative|
|[8.STSB](https://www.aclweb.org/anthology/S17-2001/)                   | Classify the sentiment of a sentence on a scale from 1 to 5 (21 Sentiment classes)|
|[9.CB](https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/601)                     | Classify for a premise and a hypothesis whether they contradict each other or not (binary).|
|[10.COPA](https://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2418/0)                   | Classify for a question, premise, and 2 choices which choice the correct choice is (binary).|
|[11.MultiRc](https://www.aclweb.org/anthology/N18-1023.pdf)                | Classify for a question, a paragraph of text, and an answer candidate, if the answer is correct (binary),|
|[12.WiC](https://arxiv.org/abs/1808.09121)                    | Classify for a pair of sentences and a disambigous word if the word has the same meaning in both sentences.|
|[13.WSC/DPR](https://www.aaai.org/ocs/index.php/KR/KR12/paper/view/4492/0)       | Predict for an ambiguous pronoun in a sentence what it is referring to.  |
|[14.Summarization](https://arxiv.org/abs/1506.03340)          | Summarize text into a shorter representation.|
|[15.SQuAD](https://arxiv.org/abs/1606.05250)                  | Answer a question for a given context.|
|[16.WMT1.](https://arxiv.org/abs/1706.03762)                  | Translate English to German|
|[17.WMT2.](https://arxiv.org/abs/1706.03762)                   | Translate English to French|
|[18.WMT3.](https://arxiv.org/abs/1706.03762)                   | Translate English to Romanian|

- [Every T5 Task example notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more) to see how to use every T5 Task.
- [T5 Open and Closed Book question answering  notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_question_answering.ipynb)

# `Open book` and `Closed book` question answering with Google's T5
[T5 Open and Closed Book question answering tutorial](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_question_answering.ipynb)

With the latest NLU release and Google's T5 you can answer **general knowledge based questions given no context** and in addition answer **questions on text databases**.      
These questions can be asked in natural human language and answerd in just 1 line with NLU!.




## What is a `open book question`?
You can imagine an `open book` question similar to an examen where you are allowed to bring in text documents or cheat sheets that help you answer questions in an examen. Kinda like bringing a history book to an history examen.

In `T5's` terms, this means the model is given a `question` and an **additional piece of textual information** or so called `context`.

This enables the `T5` model to answer questions on textual datasets like `medical records`,`newsarticles` , `wiki-databases` , `stories` and `movie scripts` , `product descriptions`, 'legal documents' and many more.

You can answer `open book question` in 1 line of code, leveraging the latest NLU release and Google's T5.     
All it takes is :



```python
nlu.load('answer_question').predict(""""""
Where did Jebe die?
context: Ghenkis Khan recalled Subtai back to Mongolia soon afterwards,
 and Jebe died on the road back to Samarkand"""""")
&gt;&gt;&gt; Output: Samarkand
```

Example for answering medical questions based on medical context
``` python
question ='''
What does increased oxygen concentrations in the patient’s lungs displace? 
context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. 
Carbon monoxide poisoning, gas gangrene, and decompression sickness (the ’bends’) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin.
 Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment.
'''


#Predict on text data with T5
nlu.load('answer_question').predict(question)
&gt;&gt;&gt; Output: carbon monoxide	
```

Take a look at this example on a recent news article snippet :
```python
question1 = 'Who is Jack ma?'
question2 = 'Who is founder of Alibaba Group?'
question3 = 'When did Jack Ma re-appear?'
question4 = 'How did Alibaba stocks react?'
question5 = 'Whom did Jack Ma meet?'
question6 = 'Who did Jack Ma hide from?'

# from https://www.bbc.com/news/business-55728338 
news_article_snippet = """""" context:
Alibaba Group founder Jack Ma has made his first appearance since Chinese regulators cracked down on his business empire.
His absence had fuelled speculation over his whereabouts amid increasing official scrutiny of his businesses.
The billionaire met 100 rural teachers in China via a video meeting on Wednesday, according to local government media.
Alibaba shares surged 5% on Hong Kong's stock exchange on the news.
""""""
# join question with context, works with Pandas DF aswell!
questions = [
             question1+ news_article_snippet,
             question2+ news_article_snippet,
             question3+ news_article_snippet,
             question4+ news_article_snippet,
             question5+ news_article_snippet,
             question6+ news_article_snippet,]
nlu.load('answer_question').predict(questions)
```
This will output a Pandas Dataframe similar to this :

|Answer|Question|
|-----|---------|
Alibaba Group founder| 	Who is Jack ma? |        
|Jack Ma	|Who is founder of Alibaba Group? |  
Wednesday	| When did Jack Ma re-appear? | 
surged 5%	| How did Alibaba stocks react? | 
100 rural teachers	| Whom did Jack Ma meet? | 
Chinese regulators	|Who did Jack Ma hide from?|



## What is a `closed book question`?
A `closed book question` is the exact opposite of a `open book question`. In an examen scenario, you are only allowed to use what you have memorized in your brain and nothing else.      
In `T5's` terms this means that T5 can only use it's stored weights to answer a `question` and is given **no aditional context**.        
`T5` was pre-trained on the [C4 dataset](https://commoncrawl.org/) which contains **petabytes  of web crawling data**  collected over the last 8 years, including Wikipedia in every language.


This gives `T5` the broad knowledge of the internet stored in it's weights to answer various `closed book questions`

You can answer `closed book question` in 1 line of code, leveraging the latest NLU release and Google's T5.     
You need to pass one string to NLU, which starts which a `question` and is followed by  a `context:` tag and then the actual context contents.
All it takes is :


```python
nlu.load('en.t5').predict('Who is president of Nigeria?')
&gt;&gt;&gt; Muhammadu Buhari 
```


```python
nlu.load('en.t5').predict('What is the most spoken language in India?')
&gt;&gt;&gt; Hindi
```


```python
nlu.load('en.t5').predict('What is the capital of Germany?')
&gt;&gt;&gt; Berlin
```




## Text Summarization with T5
[Summarization example](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more)

`Summarizes` a paragraph into a shorter version with the same semantic meaning, based on [this paper](https://arxiv.org/abs/1506.03340)

```python
# Set the task on T5
pipe = nlu.load('summarize')

# define Data, add additional tags between sentences
data = [
'''
The belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal’s side currently sit two points clear of liverpool in fourth .
''',
'''  Calculus, originally called infinitesimal calculus or ""the calculus of infinitesimals"", is the mathematical study of continuous change, in the same way that geometry is the study of shape and algebra is the study of generalizations of arithmetic operations. It has two major branches, differential calculus and integral calculus; the former concerns instantaneous rates of change, and the slopes of curves, while integral calculus concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus, and they make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit.[1] Infinitesimal calculus was developed independently in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz.[2][3] Today, calculus has widespread uses in science, engineering, and economics.[4] In mathematics education, calculus denotes courses of elementary mathematical analysis, which are mainly devoted to the study of functions and limits. The word calculus (plural calculi) is a Latin word, meaning originally ""small pebble"" (this meaning is kept in medicine – see Calculus (medicine)). Because such pebbles were used for calculation, the meaning of the word has evolved and today usually means a method of computation. It is therefore used for naming specific methods of calculation and related theories, such as propositional calculus, Ricci calculus, calculus of variations, lambda calculus, and process calculus.'''
]


#Predict on text data with T5
pipe.predict(data)
```

| Predicted summary| Text | 
|------------------|-------|
| manchester united face newcastle in the premier league on wednesday . louis van gaal's side currently sit two points clear of liverpool in fourth . the belgian duo took to the dance floor on monday night with some friends .            | the belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal’s side currently sit two points clear of liverpool in fourth . | 


## Binary Sentence similarity/ Paraphrasing
[Binary sentence similarity example](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more)
Classify whether one sentence is a re-phrasing or similar to another sentence      
This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf) and based on [MRPC - Binary Paraphrasing/ sentence similarity classification ](https://www.aclweb.org/anthology/I05-5002.pdf)

```
t5 = nlu.load('en.t5.base')
# Set the task on T5
t5['t5'].setTask('mrpc ')

# define Data, add additional tags between sentences
data = [
''' sentence1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , "" Rumsfeld said .
sentence2: Rather , the US acted because the administration saw "" existing evidence in a new light , through the prism of our experience on September 11 ""
'''
,
'''  
sentence1: I like to eat peanutbutter for breakfast
sentence2: 	I like to play football.
'''
]

#Predict on text data with T5
t5.predict(data)
```
| Sentence1 | Sentence2 | prediction|
|------------|------------|----------|
|We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , "" Rumsfeld said .| Rather , the US acted because the administration saw "" existing evidence in a new light , through the prism of our experience on September 11 "" . | equivalent | 
| I like to eat peanutbutter for breakfast| I like to play football | not_equivalent | 


### How to configure T5 task for MRPC and pre-process text
`.setTask('mrpc sentence1:)` and prefix second sentence with `sentence2:`

### Example pre-processed input for T5 MRPC - Binary Paraphrasing/ sentence similarity

```
mrpc 
sentence1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , "" Rumsfeld said . 
sentence2: Rather , the US acted because the administration saw "" existing evidence in a new light , through the prism of our experience on September 11"",
```



## Regressive Sentence similarity/ Paraphrasing

Measures how similar two sentences are on a scale from 0 to 5 with 21 classes representing a regressive label.     
This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf) and based on[STSB - Regressive semantic sentence similarity](https://www.aclweb.org/anthology/S17-2001/) .

```python
t5 = nlu.load('en.t5.base')
# Set the task on T5
t5['t5'].setTask('stsb ') 

# define Data, add additional tags between sentences
data = [
             
              ''' sentence1:  What attributes would have made you highly desirable in ancient Rome?  
                  sentence2:  How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?'
              '''
             ,
             '''  
              sentence1: What was it like in Ancient rome?
              sentence2: 	What was Ancient rome like?
              ''',
              '''  
              sentence1: What was live like as a King in Ancient Rome??
              sentence2: 	What was Ancient rome like?
              '''

             ]



#Predict on text data with T5
t5.predict(data)

```

| sentence1 | sentence2 | prediction|
|------------|------------|----------|
|What attributes would have made you highly desirable in ancient Rome?        | How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER? | 0 | 
|What was it like in Ancient rome?  | What was Ancient rome like?| 5.0 | 
|What was live like as a King in Ancient Rome??       | What is it like to live in Rome? | 3.2 | 


### How to configure T5 task for stsb and pre-process text
`.setTask('stsb sentence1:)` and prefix second sentence with `sentence2:`




### Example pre-processed input for T5 STSB - Regressive semantic sentence similarity

```
stsb
sentence1: What attributes would have made you highly desirable in ancient Rome?        
sentence2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?',
```





## Grammar Checking
[Grammar checking with T5 example](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more)
Judges if a sentence is grammatically acceptable.    
Based on [CoLA - Binary Grammatical Sentence acceptability classification](https://nyu-mll.github.io/CoLA/)

```python
pipe = nlu.load('grammar_correctness')
# Set the task on T5
pipe['t5'].setTask('cola sentence: ')
# define Data
data = ['Anna and Mike is going skiing and they is liked is','Anna and Mike like to dance']
#Predict on text data with T5
pipe.predict(data)
```
|sentence  | prediction|
|------------|------------|
| Anna and Mike is going skiing and they is liked is | unacceptable |      
| Anna and Mike like to dance | acceptable | 


## Document Normalization
[Document Normalizer example](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/text_pre_processing_and_cleaning/document_normalizer_demo.ipynb)     
The DocumentNormalizer extracts content from HTML or XML documents, applying either data cleansing using an arbitrary number of custom regular expressions either data extraction following the different parameters

```python
pipe = nlu.load('norm_document')
data = '&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Example&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;This is an example of a simple HTML page with one paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt;'
df = pipe.predict(data,output_level='document')
df
```
|text|normalized_text|
|------|-------------|
| `&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Example&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;This is an example of a simple HTML page with one paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt;`       |Example This is an example of a simple HTML page with one paragraph.|

## Word Segmenter
[Word Segmenter Example](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/multilingual/japanese_ner_pos_and_tokenization.ipynb)     
The WordSegmenter segments languages without any rule-based tokenization such as Chinese, Japanese, or Korean
```python
pipe = nlu.load('ja.segment_words')
# japanese for 'Donald Trump and Angela Merkel dont share many opinions'
ja_data = ['ドナルド・トランプとアンゲラ・メルケルは多くの意見を共有していません']
df = pipe.predict(ja_data, output_level='token')
df

```

|	token|
|--------|
|	ドナルド|
|	・|
|	トランプ|
|	と|
|	アンゲラ|
|	・|
|	メルケル|
|	は|
|	多く|
|	の|
|	意見|
|	を|
|	共有|
|	し|
|	て|
|	い|
|	ませ|
|	ん|


# Named Entity Extraction (NER) in Various Languages 
NLU now support NER for over 60 languages, including Korean, Japanese, Chinese and many more!   
```python

# Extract named chinese entities
pipe = nlu.load('zh.ner')
# Chinese for 'Donald Trump and Angela Merkel dont share many opinions'
zh_data = ['唐纳德特朗普和安吉拉·默克尔没有太多意见']
df = pipe.predict(zh_data, output_level='document')
df
&gt;&gt;&gt; Output : [唐纳德, 安吉拉]

# Now translate [唐纳德, 安吉拉] back to english with NLU!
translate_pipe = nlu.load('zh.translate_to.en')
en_entities = translate_pipe.predict(['唐纳德', '安吉拉'])
&gt;&gt;&gt; Output :
```
|Translation|	Chinese| 
|------|------|
|Donald | 唐纳德 |
|Angela | 	安吉拉|

# New NLU Notebooks


### NLU 1.1.0  New Notebooks for new features
- [Translate between 192+ languages with marian](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/translation_demo.ipynb)
- [Try out the 18 Tasks like Summarization Question Answering and more on T5](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more)
- [T5 Open and Closed Book question answering tutorial](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_question_answering.ipynb)
- [Tokenize, extract POS and NER in Chinese](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/multilingual/chinese_ner_pos_and_tokenization.ipynb)
- [Tokenize, extract POS and NER in Korean](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/multilingual/korean_ner_pos_and_tokenization.ipynb)
- [Tokenize, extract POS and NER in Japanese](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/multilingual/japanese_ner_pos_and_tokenization.ipynb)
- [Normalize documents](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/text_pre_processing_and_cleaning/document_normalizer_demo.ipynb)
- [Aspect based sentiment NER sentiment for restaurants](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/named_entity_recognition_(NER)/aspect_based_ner_sentiment_restaurants.ipynb)

### NLU 1.1.0 New Classifier Training Tutorials
#### Binary Classifier training Jupyter tutorials
- [2 class Finance News sentiment classifier training](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary_text_classification/NLU_training_sentiment_classifier_demo_apple_twitter.ipynb)
- [2 class Reddit comment sentiment classifier training](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary_text_classification/NLU_training_sentiment_classifier_demo_reddit.ipynb)
- [2 class Apple Tweets sentiment classifier training](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary_text_classification/NLU_training_sentiment_classifier_demo_IMDB.ipynb)
- [2 class IMDB Movie sentiment classifier training](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary_text_classification/NLU_training_sentiment_classifier_demo_IMDB.ipynb)
- [2 class twitter classifier training](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary_text_classification/NLU_training_sentiment_classifier_demo_twitter.ipynb)

#### Multi Class text Classifier training Jupyter tutorials
- [5 class WineEnthusiast Wine review classifier training](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/multi_class_text_classification/NLU_training_multi_class_text_classifier_demo_wine.ipynb)
- [3 class Amazon Phone review classifier training](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/multi_class_text_classification/NLU_training_multi_class_text_classifier_demo_amazon.ipynb)
- [5 class Amazon Musical Instruments review classifier training](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/multi_class_text_classification/NLU_training_multi_class_text_classifier_demo_musical_instruments.ipynb)
- [5 class Tripadvisor Hotel review classifier training](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/multi_class_text_classification/NLU_training_multi_class_text_classifier_demo_hotel_reviews.ipynb)



### NLU 1.1.0 New Medium Tutorials

- [1 line to Glove Word Embeddings with NLU     with t-SNE plots](https://medium.com/spark-nlp/1-line-to-glove-word-embeddings-with-nlu-in-python-baed152fff4d)
- [1 line to Xlnet Word Embeddings with NLU     with t-SNE plots](https://medium.com/spark-nlp/1-line-to-xlnet-word-embeddings-with-nlu-in-python-5efc57d7ac79)
- [1 line to AlBERT Word Embeddings with NLU    with t-SNE plots](https://medium.com/spark-nlp/1-line-to-albert-word-embeddings-with-nlu-in-python-1691bc048ed1)
- [1 line to CovidBERT Word Embeddings with NLU with t-SNE plots](https://medium.com/spark-nlp/1-line-to-covidbert-word-embeddings-with-nlu-in-python-e67396da2f78)
- [1 line to Electra Word Embeddings with NLU   with t-SNE plots](https://medium.com/spark-nlp/1-line-to-electra-word-embeddings-with-nlu-in-python-25f749bf3e92)
- [1 line to BioBERT Word Embeddings with NLU   with t-SNE plots](https://medium.com/spark-nlp/1-line-to-biobert-word-embeddings-with-nlu-in-python-7224ab52e131)



## Installation

```bash
# PyPi
!pip install nlu pyspark==2.4.7
#Conda
# Install NLU from Anaconda/Conda
conda install -c johnsnowlabs nlu
```


# Additional NLU ressources
- [NLU Website](https://nlu.johnsnowlabs.com/)
- [All NLU Tutorial Notebooks](https://nlu.johnsnowlabs.com/docs/en/notebooks)
- [NLU Videos and Blogposts on NLU](https://nlp.johnsnowlabs.com/learn#pythons-nlu-library)
- [NLU on Github](https://github.com/JohnSnowLabs/nlu)",28997,LanguageTechnology,1
"New to NLP; Seeking guidance on current project Hello, I am conducting research to identify ""improvisations"" within the healthcare system as a response to COVID (e.g scuba masks in place of normal face masks). I am doing this by analyzing text data of thousands articles taken from various news apis.

What I have done so far: 

1. familiarized myself with concepts (bag of words, word2vec, tfidf)
2. created a testing and training set (testing set I have created a binary field indicating whether or not the article is the type of article I am looking for)
3. familiarized myself with preprocessing (cleaning the data)

This is my first real exposure to NLP and I really fell in love with what I have seen so far. But as with every new thing, there is of course a lot of unknowns. 

I am looking to see if anyone can give me pointers on how to proceed. I thought about summarizing the articles (reduces dimensions of vector for BoW) then using Bag of words to train a model. Not sure if there is a different and better approach. All comments/critiques/criticisms are welcome and help would be most appreciated :)",1113,LanguageTechnology,1
"PhD internship in NLP at industrial research venue Hi NLP community,

I am a second-year PhD student in Computer Science at a prestigious university in Europe (&lt;100 ranking). 
I would want to start planning for a PhD internship at a research institute/industry in 2022/3. However, I am not too sure how to proceed with the process and what are typical requirements. Specifically, I work on the intersection of Structured Prediction and Bayesian Deep Learning, think deriving uncertainty for Named Entity Recognition. In my first year, I published a workshop paper at ICML and a large journal paper at JMLR.

For example, imagine I target one of the FAANG companies, what would they at least expect me to have done, e.g., publish at top ML conferences (ACL, NeurIPS, ICML, ICLR,...)? How can I increase my chances? Can someone maybe share their experiences on this? 

I appreciate your time in replying, cheers!",913,LanguageTechnology,1
"Is there a model for comparing the relevance a piece of text to the GPT-2 model's fine tuning data? I have a couple of GPT-2-backed reddit chatbots (on r/SubSimGPT2Interactive ) that reply to other humans and bots based on random probability. GPT-2 has a pretty broad corpus so it works quite well, generally, except for GPT-2 losing the context as it's well known to do. 

Perhaps choosing which comments to reply to more carefully would help maintain context and in turn keep the bots replying to replies that are closely related to their training material, or 'on-brand' if you want to call it that!

When there is a potential reply, I would like to compare the incoming comment (which is only a few hundred to a thousand characters at most) to the bot's fine tuning material (a 10-30Mb text file) and be able to get some kind of relevance value and if it is over a particular threshold, generate a reply.

Is there some way that I can use a second ML model, or the GPT-2 model itself, to calculate such a relevance value?

Any different ideas appreciated, too! 

Cheers",1073,LanguageTechnology,1
Language Understanding with Knowledge-based Embeddings (LUKE) | Research Papers Summary 005 ,92,LanguageTechnology,1
"Erasmus Mundus MSc LCT Questions Hi, everyone!

I'm applying to the LCT MSc for Fall 2021, and I would really like your opinions on the best combination of universities to take. 

Background: I'm a Computer Science student with experience in NLP at a foundational level. I have done projects using ML and Deep Learning as well. My linguistics background is not very strong, but I would like to learn more about it.

Requirements: I have already settled on Saarland for one of the universities. I would probably choose it for Year 2. For Year 1, I am focusing on Lorraine, Groningen or Trento. I have some rudimentary knowledge of French. I would like to study at a uni that has good research opportunities, with connections to applying NLP to the humanities, if possible. I realize the LCT programme is a bit disconnected, but which of these three unis would have a better transitory experience? I would also prefer that the classes be taught by profs with a good level of competency in English. Since I aim to work in industry research or at a research institute post this masters, ideally a university with a good CS programme would help, hence the reason for Saarland. As for the other, I am open to a more linguistics focused background, but having relevant options that can tie in with my focus area.

&amp;#x200B;

I would also like a ranking based on living environment - things to do, how expensive it is, how complicated the document processes are, how likely it is to find English speakers in the area, conveniences, food, relative ease of finding accommodation. 

&amp;#x200B;

Your help would be greatly appreciated, Reddit community. I will be cross-posting this on r/CompLing too.",1694,LanguageTechnology,1
The Language Interpretability Tool (LIT): Interactive Exploration and Analysis of NLP Models - Google AI ,105,LanguageTechnology,1
"DialoGPT Paper Walkthrough This paper from Microsoft presents a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained transformer) model trained on 147M conversation-like exchanges extracted from Reddit comments. 

Researchers show that the conversational systems that leverage DialoGPT generate more relevant, contentful, and context-consistent responses than strong baseline systems.

Paper Walkthrough: https://youtu.be/Zo679MYoJns

Paper: ⏩ Paper: https://www.aclweb.org/anthology/2020.acl-demos.30.pdf",558,LanguageTechnology,1
What We Found Analyzing 300 Yelp Reviews of a Michelin Reviewed Restaurant with Natural Language Processing ,108,LanguageTechnology,1
"What exists in the way of open source machine translation? In particular, is there any rule-based system that does a deeper analysis than Apertium? I'm interested in making a translator that attempts to preserve the meter and rhyme scheme of verse.",248,LanguageTechnology,1
"AI that generates rap lyrics (Open-Source + Live Demo) I built an AI that generates rap music lyrics using TensorFlow and Keras actually posted this project a while ago but since then lots of improvements have done also at that time servers are not capable of running the tf model. you can check the website(live demo you can give it a seed and I generates rap based on the seed) and the GitHub repos all links down below if you star or fork the repo I would be so happy thanks.

Github: [https://github.com/YigitGunduc/Spectrum](https://github.com/YigitGunduc/Spectrum)

Website: [https://spectrumapp.herokuapp.com/](https://spectrumapp.herokuapp.com/)",653,LanguageTechnology,1
"Announcing DeepPavlov Community Call #5 - January 28, 8am PDT Hi, I'm a CPO of DeepPavlov, an R&amp;D lab that builds a popular open-source NLP &amp; Conversational AI library also called DeepPavlov. It was born in February 2018, has lots of different state-of-the-art and demo NLP components. 

In addition to our dear users it has been battletested in our Socialbot in Amazon Alexa Prize 3, and it is now used in our Socialbot in Amazon Alexa Prize 4.

This January we've started work on refactoring the DeepPavlov Library on it's path to v1.0.  

I want to welcome you to join our DeepPavlov Community Call #5 to learn more next week on Jan 28 at 7pm MSK/8am PDT.  

Here's the link: [https://bit.ly/DPCommunityCall5](https://bit.ly/DPCommunityCall5) 

We are also interested in your feedback. Let us know what you want from DeepPavlov: 

[https://bit.ly/DPLibrary2021Survey](https://bit.ly/DPLibrary2021Survey)

See you next week!",934,LanguageTechnology,1
"Linguistics Reading List for NLP Hi all,

What reading in linguistics would you recommend to someone entering NLP without much of a linguistics background? 

My background: graduated with double BA in International Studies and Japanese a few years ago. Became interested in linguistics at the end of my BA and took a couple lower level courses. I’m currently finishing prerequisites in stats, math, and CS and am applying to data science MS programs with the hope of concentrating on NLP—I originally wanted to apply to computational linguistics programs but am opting for data science for a few reasons. 

I see people argue back and forth for days about whether linguistic knowledge is helpful at all for for NLP. For those of you who think it is worthwhile, what reading would you recommend?",794,LanguageTechnology,1
How to cluster documents using Word2vec and K-means ,52,LanguageTechnology,1
Does anyone know of a certificate program affiliated with a University that teaches NLP? I want to learn from a live instructor instead of a coursera class. I am ok with it being online. I just want something where I can ask a instructor questions right away.,259,LanguageTechnology,1
"From document library to corpus for training BERT Hello. I would like to attempt extending BERT with my own small corpus of technical documents. They include lots of tables, figures, bulleted lists, mixed in footnotes, and other relics that aren’t always part of my domains natural language discourse. Some may even be riddled with errors from being generated from OCRing pdfs (let’s say about 2% of them)

What’s the optimal way to clean up a big pile of garbage like this to improve my attempt to train a language model?",522,LanguageTechnology,1
"withdraw from naacl submit to acl Is it okay to withdraw from naacl submit to acl, looks like my reviewers are being unprofessional, and my paper also has some changes. But I am submiting to the same venue. Would that be ok?",224,LanguageTechnology,1
"Which cloud storage provider for uploading scraped data and analyzing it later on using DL and ML. I have a project of multiple web scrapers running, all fetching text data from online. Once I've collected enough data to draw conclusions from it, I would like to analyse the text data (NLP, ML, DL). The scrapers are currently running on a VPS and storing the scraped text data in a local database.

Since the VPS is not strong enough for high performance NLP, I'm thinking about outsourcing the storing- and analysis-part to another provider. But I'm completely overwhelmed by the endless amounts of providers and their rather abstract descriptions of what they provide.

Are there good and cheap (free?) solutions that allow for uploading and storing data (approx. 1MB per upload) in regular intervals (1 upload per minute) and analyzing that data (preferably python; nltk, tensorflow, scikit etc.)? They can be different providers, but I prefer everything in one household.",976,LanguageTechnology,1
"Meaning of Regex [^.]*+ Hi, I found a document which uses the formula VerbRoot[^.]*+  , I think it's for finding all the forms in which the verb can appear. But I don't really understand how the formula works, or if It's OK

Hope someone can help me! Thanks in advance!",269,LanguageTechnology,1
Composable Named Entities in Apache NLPCraft ,45,LanguageTechnology,1
"Looking for an automatic text summarization method for academic papers Loads of works have been published on automatic summarization in general, and also for summarizing academic papers - it's quite easy to get lost in the literature and start implementing things that do not end up being the right methods. 

I am trying to implement an academic paper summarizer - so going from the full paper to abstract-like sized summary. I would be looking to use abstractive methods, or potentially hybrid solutions (extractive + abstractive). 

Does anyone have experience with a specific paper or method that they recommend for this task? ++ &lt;3 if the paper comes with code 

PS: Honestly, I hope that in the near future, all researchers publishing this kind of work will \*always\* publish code with it. I find it unacceptable that such works are most often without code linked to it.",880,LanguageTechnology,1
"How to introduce named entities into clustering? Hello there.

If I get some embeddings (e.g. Bert based ones) for some article headlines corpus and run some clustering algorithm, the articles about e.g. crime get (understandably) clustered together, but I’d like to cluster by news story/event instead, so I think named entities are the key for this.

When using a more traditional TF-IDF approach I’ve seen people boosting the weight of the words representing named entities, but with sentence embeddings I don’t think that’s possible.

I found some approach in a paper that works reasonably well but I wonder what people are doing about this?",645,LanguageTechnology,1
The field of natural language processing is chasing the wrong goal ,67,LanguageTechnology,1
[R] Finding The Words To Say: Hidden State Visualizations For Language Models ,78,LanguageTechnology,1
GPT-2 How can I use GPT-2 to teach a model how to summarize a piece of random text?,83,LanguageTechnology,1
"Google AI Introduces ToTTo: A Controlled Table-to-Text Generation Dataset Using Novel Annotation Process **The rising field of natural-language generation**

Research in natural language generation (NLG), a subset of artificial intelligence, is rising. [NLG](https://en.wikipedia.org/wiki/Natural-language_generation) is a software process that changes structured data into natural language. Not to be confused with natural language processing (NLP), NLG synthesizes and writes new content, whereas NLP reads and derives analytic insights from content ([Gartner](https://www.gartner.com/en/documents/3388326)).  

* Natural language generation (NLG) creates (or generates) text. It is when computers write language, turning structured data into text.
* Natural language processing (NLP) reads (or processes) text. It is when computers read language and derive insights.

Read Full Summary: [https://www.marktechpost.com/2021/01/18/google-ai-introduces-totto-a-controlled-table-to-text-generation-dataset-using-novel-annotation-process/](https://www.marktechpost.com/2021/01/18/google-ai-introduces-totto-a-controlled-table-to-text-generation-dataset-using-novel-annotation-process/)

Paper: [https://arxiv.org/abs/2004.14373](https://arxiv.org/abs/2004.14373)

GitHub: [https://github.com/google-research-datasets/totto](https://github.com/google-research-datasets/totto)",1371,LanguageTechnology,1
"[Q] Tips/tricks for dataset synthesis? I imagine this is an incredibly common problem to other NLP practitioners, but I find that anytime I need to work on some sort of sequence-sequence model, there is always a shortage of cleaned, tagged data. (For classification tasks, this problem isn't nearly as much of an issue.) And so, my go to is typically using some combination of SpaCy POS tagging, HuggingFace  NER, regular expressions, and rule based logic. In particular, I like that SpaCy will not just tag the parts of speech but also use dynamic programming to find noun phrases that you can drill down on. This, in combination with rules and regex, usually allows me to find the tokens of interest, label them accordingly, save to disk/db, then model using Keras, PyTorch, etc. (which is much more streamlined for a single task.) 

One example: I was trying to extract ""excitors"" and ""aggravators"" for a marketing-related NLP project. The idea was to take""long wait time"" or ""friendly service"" from Yelp reviews. I found using the above pseudo method, I was able to piece together raw documents and an array of tokens for NER modeling. 

My employers have never seemed inclined to spend on a 3rd party data tagging service (overseas or otherwise), though I do understand such a service exists.

Curious, how do others approach the all-familiar, ""I don't have nearly enough data"" conundrum?",1393,LanguageTechnology,1
"[R]: NAACL, reviews are out when will the reviews of NAACL be out?",66,LanguageTechnology,1
"Best approach for content recommendation This is my problem:

I have a set of essays or articles I know have been ""enjoyed"" by a user, but only have data on one user's preferences. Now, based on this how can I assess whether a new piece of content will suit the user or not?

It's some sort of classification issue but the problem is that most things I have been finding online go about this by looking at what other ""similar"" (as in like the same content) users have liked, and I can't really go that route here.

I'm a beginner, so this might be a stupid question but I'd love to learn.

Thanks!",597,LanguageTechnology,1
Are there any companies putting NLP into ethical issues? My question is inspired by this list [here](https://ethical.net/resources/?resource-category=ethical-tech-orgs). Do you know about any tech company that is putting NLP into ethical applications or at least care about it?,277,LanguageTechnology,1
"I made NLPRule: A library for fast grammatical error correction [*Also posted on /r/ML*](https://www.reddit.com/r/MachineLearning/comments/kzuaie/p_i_made_nlprule_a_library_for_fast_grammatical/) *but this subreddit is more fitting :)*

&amp;#x200B;

Hi /r/LanguageTechnology!

I made NLPRule, a library for fast grammatical error correction in English and German by  checking thousands of rules. It is written in Rust and has bindings for Python.

Repository: [https://github.com/bminixhofer/nlprule](https://github.com/bminixhofer/nlprule)

**Synopsis**

    from nlprule import Tokenizer, Rules, SplitOn
    
    tokenizer = Tokenizer.load(""en"")
    rules = Rules.load(""en"", tokenizer, SplitOn([""."", ""?"", ""!""]))
    
    rules.correct(""He wants that you send him an email."")
    # returns: 'He wants you to send him an email.'
    
    rules.correct(""I can due his homework."")
    # returns: 'I can do his homework.'
    
    rules.correct(""It is enough for all intensive purposes."")
    # returns: 'It is enough for all intents and purposes.'
    
    
    suggestions = rules.suggest(""She was not been here since Monday."")
    for s in suggestions:
      print(s.start, s.end, s.replacements, s.source, s.message)
    # prints:
    # 4 16 ['was not', 'has not been'] WAS_BEEN.1 Did you mean was not or has not been?

**Background**

I've been interested in grammatical error correction for a while and came across [LanguageTool](https://github.com/languagetool-org/languagetool) which is based on [thousands of rules for error correction in an XML file](https://raw.githubusercontent.com/languagetool-org/languagetool/master/languagetool-language-modules/en/src/main/resources/org/languagetool/rules/en/grammar.xml).  You can think of the rule syntax as a restricted form of Regex where  the atoms are words annotated with lemmas, part-of-speech tags, and  chunks.

I'm not a big fan of Java,  wanted to improve my Rust and was interested in how these rules are  parsed so I made a proof of concept reverse-engineering the LanguageTool  logic in Rust. I had this lying around for quite some time and decided  to finish it up and make it into a usable library now during the  holidays.

**Relation to more sophisticated GEC approaches**

There's lots of research in using Neural Networks for Grammatical Error Correction and [there are some exciting recent approaches](https://github.com/grammarly/gector) which capture many more errors than a rule-based approach could. Still, for me there are two reasons to use rules:

1. **Speed.** On my machine with an 8th Gen Intel CPU it takes less than 1ms to correct a sentence.
2. **Dealing with extreme data sparsity of some errors.** The above example ""It is enough for all intensive purposes."" contains a [well known error](https://www.merriam-webster.com/words-at-play/usage-for-all-intensive-purposes-intents).  Yet, I would be surprised if a current ML model corrects this error  unless specifically accounted for since it will almost never have  appeared in its training data. This is even more true for similarly rare  errors in other languages where there is less data available than for  English. So I believe rules are especially useful in conjunction with a  more powerful ML model.

I think of NLPRule as a kind of ""sanity-check"" for text.

**Rule-based postprocessing for NLG**

Two  areas where NLPRule might be interesting are preprocessing for NLP and  postprocessing for NLG. I've tried the latter with texts generated from  GPT2. Applying NLPRule yields a significant amount of suggestions:

    Generated 192300 tokens.
    misspelling:    35 suggestions  (0.18 per 1000 tokens)
    style:          53 suggestions  (0.28 per 1000 tokens)
    typographical:  112 suggestions (0.58 per 1000 tokens)
    grammar:        29 suggestions  (0.15 per 1000 tokens)
    none:           3 suggestions   (0.02 per 1000 tokens)
    inconsistency:  2 suggestions   (0.01 per 1000 tokens)

Not all of these are errors, some are just suggestions for improvement. More information [here](https://github.com/bminixhofer/nlprule/tree/master/examples).

I'm happy to discuss anything in the  comments!",4149,LanguageTechnology,1
"Confused about whether I can add special tokens to a pretrained GPT-2 tokenizer I am using transformers/simpletransformers.

I have sifted through the transformers source code, but the loose inheritance it uses (where parameters are all defined at the base and not validated when they are inherited) makes it really hard to decipher what is possible. Some Github issues suggest it's possible to add tokens, some not.

Trial-and-error setting has just run me into more errors of arguments not set properly in simpletransformers, so I'm at the stage of trying to call the base tokenizer functions directly like so:

    lmm = LanguageModelingModel(""gpt2"", ""gpt2"", args=args)
    ## special_tokens is a list of strings
    lmm.tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})
    lmm.model.resize_token_embeddings(len(lmm.tokenizer)) 
    lmm.train_tokenizer(train_files=[training_file, eval_file])
    lmm.train_model(train_file=training_file, eval_file=eval_file, args=args, verbose=True)

Which fails with an unclear cuda error, unfortunately.

I am using special tokens to segment parts of conversational text. The model is used by a reddit chatbot.

Am I wasting my time trying to add special tokens?

Update: I seem to have it working by re-setting the vocab\_size argument with the updated value when calling train\_model. I'll find out soon whether there is any kind of improvement.",1414,LanguageTechnology,1
"How to validate a dataset? I asked a team of data creators to generate a dataset for me to be used in implementing a dialect classification in the Arabic language for specific domains.  
Anyone know a way to validate this data.  
I thought of making an EDA listing the most important features like the distribution of the terms, number of unique words and somethings like that.  
Anyone knows any other features to validate the dataset that it's valuable, or know another way to do so !?",487,LanguageTechnology,1
Temporally-Informed Analysis of Named Entity Recognition | Research Papers Summary 004 ,87,LanguageTechnology,1
"Recommendations for extracting data Dear Reddit,

I'm trying to extract/scrape the following data at scale and was looking for recommendations from the community on the best approach. 

1. News articles from the web (*I've found 'news-please' works well. Are there others?* )
2. e-mail Newsletters
3. Arxiv papers   
4. Financial earnings call transcripts. 
5. Twitter posts (*I think the Twitter API might be the best approach*)
6. Instagram posts and comments

Ideally I'd prefer python libraries but I'm open to any tool that does the job. 

Thank You for your help",568,LanguageTechnology,1
"How to learn (more about) GPT-2 Dear Reddit-Fam

I'm looking for resources about GPT-2 (for natural language generation), especially for training, fine-tuning (maybe in another language also). It seems like documentations or courses that go in-depth are very scarce at the moment (maybe/hopefully i'm false).

Greeeets",318,LanguageTechnology,1
"How to make your NLP system multilingual So you have an NLP system - a chat bot, a search engine, NER, a classifier... - working well for English.

And you want to make it work for other languages, or maybe for all languages.

We see 3 basic approaches:

1. machine-translating at inference (or query) time
2. machine-translating labelled training data (or search indices), and training a multilingual model
3. zero-shot approaches with a multilingual LM like BERT or LASER

When to use which approach?

Machine-translating at inference time [2] is easiest to start with, but it's usually a bad idea.  It's the default at major US tech enterprises, from what I've seen, and even at really smart ML startups like Aylien.  And it's often suggested in this sub.

In Europe, where building a multilingual system is super important, we've even seen researchers *human-labelling* for every language, and ML startups *human-translating* labelled training data, or doing rules-based transliteration with *human post-editing*.

As a guy who thinks around the clock about machine translation risk and automation, all this unscalableness pains me to see.

So we have shared some open guides based on the work of our clients who implemented [multilingual search](https://www.reddit.com/r/LanguageTechnology/comments/k9ui4b/how_to_build_multilingual_search_with_translation/).

Nerses Nersesyan from Polixis and I will give a workshop on this at Applied Machine Learning Days in March.

https://appliedmldays.org/events/amld-epfl-2021/workshops/how-to-make-your-nlp-system-multilingual",1572,LanguageTechnology,1
"How would you analyze text message history from Facebook, WhatsApp, Wechat, etc.? How would grouping work? So, when you get a conversation history, it'll be in the form of messages obviously. But messages aren't interpreted by people in the same way that they're stored.  People will consciously or unconsciously send multiple messages in a row for an overarching message.

If you were to try to analyze conversation chains though, this would make it harder to do LDA topic modelling, because the occurrences of words in a message are split up.  Another problem too, time gaps mean that sometimes a conversation will die, and the messaging following it will be part of a completely separate conversation. Othertimes, it might just be a while before someone gets back to you, or the conversation may be asynchronous due to conflicting schedules or something.

Another issue is revisiions.  


\*revisions.

How do you interpret revisions or clarifications in text?

Personally, I think these problems are too heavily in the area of computational pragmatics to be solvable, but surely there is something more that can be done than simply pretending the problem doesn't exist.

&amp;#x200B;

What would your solution/suggestion be?

Due to the lack of solutions in this area, I think nearly every suggestion that attempts to address any of these problems has merit.

&amp;#x200B;

Another thing too, do you think that this topic is publishable?  This is mostly a personal project for me, but if it comes out as something worthwhile, maybe I would put forth more effort into it for publication.",1590,LanguageTechnology,1
[p] Ecco – See what your NLP language model is “thinking” ,58,LanguageTechnology,1
"What are non-english language missing in order to reach english-like NLP performances? Hi all,
I am currently working on some NLP tasks on Italian corpuses. I am noticing that, in general, Italian language models perform worse than their english siblings on similar tasks. I Indeed found several articles online raising the problem of low performance in non english NLP models.

So my question is: is this only a problem of low quality /small sized training datasets? Or is it something else? How would you address this problem if you had infinite ?money?

Thanks!",564,LanguageTechnology,1
"How to perform cosine similarity when taking into account the importance of the words as well. I have two documents or text data. Document 1 contains information like keywords with its own numeric number (which is the importance of the word):

    *gre (300)    india(290)    art(278)   galleries(257) ...*  

And another document that i have is the tf\*idf matrix. its the extracted keywords from single document with its tf\*idf score. (again, can be interpreted as the importance of the word).

    function   0.6781 art        0.2463 galleries  0.15655 . . ...  

so How do i compute similarities between these two document considering that the similarity between ""**art""** from document 1 and 2 should have higher score (higher similarity) because they are more important keywords as compared to the word ""**galleries""** from document 1 and 2 since they are less important keywords comparatively. How do i do this?",919,LanguageTechnology,1
"First NLP course. Seeking ideas for fun/interesting term project. (Interesting data? Hottest research?) The headline pretty much says it all:

I'm taking my first NLP course and need some inspiration coming up with ideas for my term project. Broad strokes of the project requirements are to find a research paper involving a particular method or somesuch, and then to implement said somesuch and report various metrics on efficiency/f1/etc.

It'll be a lot more fun if the method and or data are new/unique.

Show me what you got! :D",533,LanguageTechnology,1
"Knowledge of linguistics in NLP For those of you who have started out from a traditional ML/DL background, have you ever felt like knowledge of linguistics is necessary to excel in this field?

While I love NLP as a field of study, I've also started to increasingly wonder how non-linguists can break into NLP, when, after all, NLP is a study of the human language. This also makes me think that recent advances in NLP are being driven by hard-core engineering and resource-intensive computing (huge, billion-parameter models trained on terabytes of datasets) rather than refined linguistic approaches that could potentially yield more meaningful insight into how humans learn and understand language. 

But of course, this is coming from someone who has just started to study NLP, so I'm sure there are finer details that I'm missing from the bigger picture.

Thanks for sharing your opinions and experience!",909,LanguageTechnology,1
"Any good model which can be used for fine-tuning I am doing a search NLP engine which takes short query of 6-7 tokens  of  type question or a statement which wants to find a answer from database. Can you guys give me a good starting point to have a good pretrained model for short queries and fine tune different tasks like NER, entity relationship, pos tagging. And also data must be created manually. How Fine-tuning can be done wtih  minimal training data.?",460,LanguageTechnology,1
Bullet point summarizers? Can anyone point me to some literature that would be talking about models where you input a text and it returns bullet point summary? I found a very little number of such. Is this even a thing?,219,LanguageTechnology,1
"Are there any models to do fine-grained POS tagging for Nouns (Concrete noun and Abstract noun )? Can anyone point to NLP models/approaches to do fine-grained POS tagging of Nouns. I'm especially interested to distinguish if a Noun is Concrete or Abstract.

[https://www.grammarly.com/blog/concrete-vs-abstract-nouns/](https://www.grammarly.com/blog/concrete-vs-abstract-nouns/)

A concrete noun is a noun that can be identified through one of the five senses (taste, touch, sight, hearing, or smell).  

 An abstract noun is a noun that cannot be perceived using one of the five senses (i.e., taste, touch, sight, hearing, smelling).",634,LanguageTechnology,1
"Upcoming shared tasks in NLP 2021 Any shared tasks in the year 2021 for NLP happening soon. 

Anyone needs collaborator for shared task. ?",138,LanguageTechnology,1
"Is there any case ELMo can be a better choice than BERT/Transformer? Hi, here's one very simple question.

Can you guys think of any case where ELMo can be a clearly better design choice than BERT/Transformer family? I tried, but I couldn't think of any case.",259,LanguageTechnology,1
"Resource on Multi document summarisation Hey,
I was looking for some good papers on unsupervised multi-document summarisation ?
Any recommendations?

Thanks",156,LanguageTechnology,1
"How can I extract the topic of a sentence and then the adjective related to it in spacy? Hello everyone! I've posted here a couple of times and you guys helped me a lot. The last time, I tried to extract NOUN and ADJECTIVE together, but the output had a lot of errors. Now I want to try a different approach, to first identify the topic of a sentence and then extract the adjective, like this: 

&amp;#x200B;

The teacher is calm and intelligent.

The material is bad.

I want an output that is like:

{teacher calm}, {teacher intelligent} and {material bad}

Can anyone help me? 

Also, if anyone has a different idea about how to solve this problem in a better way, I'll really appreciate your reply too!  


Thanks in advance!   
 

#",737,LanguageTechnology,1
"Why are language modeling pre-training objectives considered unsupervised? Maybe this is stemming from my not-so-great grasp of supervised vs. unsupervised learning, but my understanding is that if we have access to ground-truth labels then it's supervised learning and if not then it's unsupervised.

I'll take the masked language modeling (MLM) that BERT ([Devlin et al., 2019](https://www.aclweb.org/anthology/N19-1423/)) and many other subsequent language models use.

According to the original paper:

&gt; ...we simply mask some percentage of the input tokens at random, and then predict those masked tokens... In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM.

If we just replace a certain percentage of tokens with `[MASK]` randomly, don't we technically have access to the ground-truth labels (i.e., the original unmasked tokens)? Shouldn't this be considered supervised learning?

My argument is analogous for the next sentence prediction (NSP) task.",1055,LanguageTechnology,1
"Introduction to chatbots: what, why and how?[chatbot creation using dialogflow] ",80,LanguageTechnology,1
"Does anyone maybe know (or has an insight) into the things I could not understand from reading the NLP book? As I currently want to self-study NLP (and information retrieval) I have a few questions and I hope that someone can provide me some insight on them :) .

1) If I want to train a learning-to-rank model with imperfect labels, which loss would be better to choose: point-wise or pair-wise? If I choose a pair-wise loss, will it be computationally more expensive, but less prone to overfitting (and learning the exact non-machine learning) ranking function?

2) Does the term-at-a-time algorithm need to parse the entire posting list of at least one query term?

3) Can the Cranfield paradigm (about the testing datasets) be used for the evaluation of an e-mail search system?

4) Does stemming help or hurt the ranking algorithm for the retrieval system of the entity knowledge graph?

This is the things I could not understand from reading the book about it (and searching it later online), so thank you in advance :)",1025,LanguageTechnology,1
"Article: ""Google’s new trillion-parameter AI language model is almost 6 times bigger than GPT-3"" ",97,LanguageTechnology,1
Folks’Talks video game. Graphic Research Interface. Explanatory note for current temporary interface. ,102,LanguageTechnology,1
"Could you please advice if I am using this Marian MT transformer correctly? It runs way too slow. Hello, I am dabbing in NLP transformers, specifically the [Marian MT model released by HuggingFace](https://huggingface.co/transformers/model_doc/marian.html).

I adapted the tutorial example and it does work, but it is super slow. Could you please advise if I am using it correctly? Please note that I wish to use it to translate individual words from a list, not a whole text.


Here is the code:

    from transformers import MarianTokenizer, MarianMTModel
    from typing import List
    mt_model = MarianMTModel.from_pretrained('mt_model/opus-mt-en-es')
    mt_tok = MarianTokenizer.from_pretrained('mt_model/opus-mt-en-es')
    
    def translate(word, model, tok):
        batch = tok.prepare_seq2seq_batch(src_texts=[word], 
                                          return_tensors=""pt"")  
        gen = model.generate(**batch)
        translated_word: List[str] = tok.batch_decode(gen, skip_special_tokens=True)
        return ' '.join(translated_word)",1059,LanguageTechnology,1
Commonsense Reasoning for Natural Language Processing - Vered Shwartz ,70,LanguageTechnology,1
"Measuring Fullness of a Sentence Would love some help with a bit of an odd problem I’m trying to solve… How does one measure the “fullness” of a sentence?

Ex:
Thank you for reaching out about the issue at school
- will score higher than 
Thank you for reaching out

As the first sentence provides more detail, it should score higher. How can I measure the fullness/detail of a sentence?",387,LanguageTechnology,1
"Translation management systems For anyone into language tech, my colleague is hosting a free event, thought I would share here [https://us02web.zoom.us/webinar/register/1916105631007/WN\_ReYW3SRKRXeSQtuxiya1ng](https://us02web.zoom.us/webinar/register/1916105631007/WN_ReYW3SRKRXeSQtuxiya1ng)",292,LanguageTechnology,1
Great papers for distantly supervised learning? I have been searching for papers on distant supervision for named-entity extraction on Google scholar. Does anyone have some suggestions for high impact papers that worked on distant supervision in named-entity extraction?,270,LanguageTechnology,1
"Recommendations for Semantic Search I'm looking for recommendations for a semantic search system that can score thousands of text snippets based on their relevance to a user's question.

`INPUT:`  
`- A question, in natural language English.`  
`- A corpus of thousands of text snippets.`

`OUTPUT:`  
`A score for each of the text snippets based on how relevant it is to the question.`

It should not just be keyword based, but based on meaning / closeness of concept. For example...

`QUESTION: ""Why is LIDAR unneccesary for self driving cars?""`  
`SNIPPET: ""SpaceX uses LIDAR for docking to the ISS"".`

\-&gt; The snippet should get a lower relevance score, because the snippet is about the same subject (LIDAR), but in a different context (space vs. automotive).

It should also be smart enough to rank snippets that use different words for the same concept.

`QUESTION: ""Why is LIDAR unneccesary for self driving cars?""`  
`SNIPPET: ""Tesla's FSD uses cameras to create a 3D vector space representation of the surroundings, which makes costly sensors such as LIDAR unnecessary.""`

\-&gt; The snippet should get a higher relevance score, because the words ""Tesla"" and ""FSD"" in the snippet are semantically linked to the words ""self driving cars"" in the question.

What would you say is the best tool to accomplish this today?",1328,LanguageTechnology,1
"Our new state-of-the-art multilingual NLP Toolkit - Trankit has been released Hi everyone,

We just released our lightweight transformer-based NLP toolkit named **Trankit.**

**Our toolkit outperforms the current state-of-the-art Stanford NLP (Stanza)** in many tasks such as sentence segmentation, part-of-speech tagging, and dependency parsing **over** **56 different languages**. Detailed comparison can be found [here](https://trankit.readthedocs.io/en/latest/performance.html#universal-dependencies-v2-5).

For example, for **English**, **Trankit is significantly better than Stanford NLP (Stanza)** on sentence segmentation (**+7.22%**) and dependency parsing (**+3.92%** for UAS and **+4.37%** for LAS). For **Arabic**, our toolkit substantially improves sentence segmentation performance by **16.16%** while **Chinese** observes **12.31%** and **12.72%** improvement of UAS and LAS for dependency parsing.

**Trankit is written in Python and can be easily installed via pip**. Our code and pretrained models are publicly available at: [https://github.com/nlp-uoregon/trankit](https://github.com/nlp-uoregon/trankit)

We also created a documentation page and a demo website for Trankit.Documentation page: [https://trankit.readthedocs.io/en/latest/index.html](https://trankit.readthedocs.io/en/latest/index.html)

Demo website: [http://nlp.uoregon.edu/trankit](http://nlp.uoregon.edu/trankit)

Technical details about Trankit can be found in our paper: [https://arxiv.org/pdf/2101.03289.pdf](https://arxiv.org/pdf/2101.03289.pdf)

Thank you!",1548,LanguageTechnology,1
"Is there any work in NLP where a neural summarization model is ""forced"" to contain a certain subject and object? Hi guys. I'm wondering if there is any work done in the NLP field regarding neural summarization where the model is given the constraint to contain a certain subject and object.

For example, if we're given a document about Elon Musk and Tesla, I'm wondering if the model can be constrained to output a summarization in the form of, for example, ""Elon Musk is the founder of Tesla.""

Any tips are appreciated. Thanks.",530,LanguageTechnology,1
"Multiclass dataset for sentiment analysis? Hi all

I am looking to fine tune a deep learning model like BERT to classify news sentiment as either negative, neutral or positive. I can find many binary classification datasets used as performance measures by state of the art models, and even [papers where their scores are compared](https://arxiv.org/pdf/2004.03705.pdf),  but i can't seem to find a dataset that is used by  state of the art models to compare 3 class sentiment analysis of positive negative and neutral.

How should i approach this? Should i look at performance metrics for binary classification tasks and make an assumption that the ones that perform best at binary classification will also perform best at 3-class classification? Or should i look at model performances with multi-class labels (which often have 15-30 labels) and use that as a guideline?

Are there any leaderboards that contain rankings of the best models for 3-label negative, neutral and positive sentiment? i can't seem to find any.

My aim is to find good at this task in general, and then fine tune it classifying sentiment of financial news headlines",1140,LanguageTechnology,1
Information extraction and comparison How to extract information from sentences which is present in one excel file and compare it with 700 standard sentences  with unique codes  which is present in another Excel sheet and assign the sentence with that particular unique code of the sentence to which it is almost similar ?,322,LanguageTechnology,1
"Pronunciation Attribution for Japanese Kanji Hello,

I am wondering if there are algorithms out there for attributing syllables of a romaji/furigana/pronunciation to the different Kanji within a word?

eg  東京 (トウキョウ) -&gt; 東 (トウ) 京 (キョウ)

Maybe a lattice-based approach? I implemented an algorithm using a form of A\* search but I was wondering if there were better algorithms that I could look at.

Thanks!",407,LanguageTechnology,1
"[D] Help Document Ordering Hi, I am currently working on a problem to order/rank help documents in the most useful way. The ranking criteria is a bit complex.   
The criteria should be based on :  
1. how useful it was in the past 

2. how popular (or trending) it is amongst the users 

3. also how much information(relative to other documents in the list) does the document have. 

 As of now I was thinking I could use some weighted window method to compute the popularity and trending aspect based on click through rate or likes/dislikes the documents get over the window of time.   
I wanted to understand how I could address the information aspect of the criteria i.e what methods would I use to compare the documents based on the amount of information they would have to rank accordingly.   
I would also appreciate ideas that would help me combine scores from both these aspects to get to a final score to rank the documents.",933,LanguageTechnology,1
"Is there any tool (preferably for python) that helps with extracting all entities and relationships in a path of a RDF graph? I know that property paths in SPARQL can aid you with that. However, you need to be really specific with your nodes and relationships in between two entity nodes that mark the start and the end of the path, if you want to extract them. I am searching for a arbitrary way to do this.

What i want is basically:

Input: start entity, amount of max hops for the path (f.e. 4), RDF knowledge graph

Extract all paths with hops &lt;= 4 beginning from the start entity. A hop is basically a jump between entities via SPO representation.

Output:

\[start entity - relationship 1.1 - entity 1.1 - relationship 2.1 - entity 2.1 - relationship 3.1 - entity 3.1 - relationship 4.1 - entity 4.1\],

\[start entity - relationship 1.2 - entity 1.2 - relationship 2.2 - entity 2.2 - relationship 3.2 - entity 3.2 - relationship 4.2 - entity 4.2\],

etc.

Does not need to be in array representation. All i want is to be able to analyze a path programmatically. Im am currently considering doing this manually via loops and multiple queries on a local RDF TTL-graph.",1177,LanguageTechnology,1
"Create search bar NLP recommendation in Python  Like the title says, I would like to create an NLP project which consists of a search bar and when someone types something domain specific, it can start generating suggestions. The problem is, I want these suggestions to only be domain specific. Can someone guide me to tutorials or frameworks that can help me. Thank you.

Just as an example - I would create a bank and clients would type up specific processes in that bank. So if a client were to type ""I would like"" it would generate some suggestions like ""I would like to request a loan"" and ""I would like to speak to customer service"" and ""I would like to open a bank account""

Thanks again.",694,LanguageTechnology,1
T5: Exploring Limits of Transfer Learning with Text-to-Text Transformer | Research Paper Walkthrough ,101,LanguageTechnology,1
"Is there a method for restyling a sentence given another sentence? I’m working on a problem where I need to take two sentences:

1. Input sentence (this needs to be restyled)
2. Reference sentence (we match the style of this sentence)

I need to rephrase the input sentence to be more similar to the reference sentence.

Very simple example — if the reference sentence is needlessly verbose, has misspellings and lots of commas, we rephrase the input sentence to have commas and misspellings, and add words to make it more verbose.

How should I approach this problem?",568,LanguageTechnology,1
"How to know if my language model saw a phrase during training Is there a way to know if there was a phrase in an LM’s training data w/o looking at the training data itself? It is not as trivial as looking at the vocab: for example if the vocab contains Donald and Trump, doesn’t mean that it the phrase “Donald Trump” was in the training data.",343,LanguageTechnology,1
"[R]: Twitter Data crawling for research Hello All

I am looking to crawl data for academic research (most likely need to release/open-source the dataset).  Do you guys know the license? (I have already read their webpage, terms and condition), however, I don't find too many open source twitter data set, wondering if there is any hidden terms that I am not awared off?",369,LanguageTechnology,1
Simple LSTM to Detect Passive and Active Voice in Your Writing ,63,LanguageTechnology,1
"Is there an NLP technique for rephrasing question-answer pairs as a full sentence? Is there an NLP technique for rephrasing question-answer pairs as a full and grammatically correct sentence? For example:

Question: Where does Joe live?

Answer: Joe lives in Los Angeles.

I’ve looked into Answer Ellipsis, which is essentially the opposite of this issue and what most Machine Reading Comprehension or Question Answering systems already generate. For example, the answer to the above with answer ellipsis would be “Los Angeles”.

For the sake of reader comprehension, I want to turn question-answer pairs into something understandable. What techniques exist to do this?",669,LanguageTechnology,1
GAP: Text2sql with Generation-Augmented Pre-Training ,53,LanguageTechnology,1
Spider: Yale Semantic Parsing and Text-to-SQL Challenge ,56,LanguageTechnology,1
"Making a natural language bot.. that sounds like its having a stroke. What data sets to use? Got sent here from r/learnprogramming! 

&amp;#x200B;

Hi, for a long time I've ""collected"" and made up sentences that \*almost\* make sense, but don't. It's similar to the kinds of things you might see on [r/ihadastroke](https://www.reddit.com/r/ihadastroke/). Such as:

***Appreciate what you what, be are the make you appreciate what you dad.***

or

***Why do they call it oven when you of in the cold food of out hot eat the food?***

or

***Don't think that carrot big because carrot big leaf because small leaf carrot not big leaf sizes.***

As a fun quarantine side project, I wanted to train an AI to generate these almost-sensical sentences for my own amusement. Since I typically only program games, I wanted something simple and I'm currently using Max Woolfe's [GPT-2 simple](https://minimaxir.com/2019/09/howto-gpt2/) since its extremely easy to input data sets and quickly train a model right from a google collab project. I've considered that perhaps using a ""worse"" platform to create a model might be better for my goals though.

Anyway, I'm considering from where I should pull input sets to train the model. Some ideas I have right now are English as second language forums, mass-translating sentences through a bunch of different languages then back to english, bad sentences generated by other bots like on [r/SubredditSimulator](https://www.reddit.com/r/SubredditSimulator/), or mixing proper english sentences with a smattering of ones that are nonsensical. The nuance to this is that I'd want sentences that ***almost*** make sense, but don't. Oftentimes they'll have a proper grammatic opening or ending, but then will start to deviate or repeat verbs when the clause should end. It might also be possible to not use ML but just take fully formed sentences and start swapping around and subbing out words algorithmically. Any and all suggestions are welcome! This is my first time trying any type of model training so I appreciate any tips, but would probably need to keep it simple.",2102,LanguageTechnology,1
"NLP School project Hello everyone !

I'm a beginner in NLP, and as part of a school project, I created a model allowing to automatically generate emails subjects based on the content of the email. I need to assess the quality of the generated objects, so I need the help of many people to fill a questionnaire :  
[https://forms.gle/8cnnNqtyd8Qv1wbU6](https://forms.gle/8cnnNqtyd8Qv1wbU6?fbclid=IwAR0rqHJOHBQ-jHFX8E522UiB3CUORtBqgpmGJ2hXw2HfUDdKR25NkjP1SU0)

In this questionnaire there are ten sentences, and for each sentence you can say, in your opinion, if this could or not be an email subject.

Thank you so much !",620,LanguageTechnology,1
"Any interest in a disk image for running GPT-2 on a Raspberry Pi4? How to make it available - it's 64GB I have created a clean(ish) image with a running implementation of GPT-2 and Tensorflow 1.13.1 that runs on the Pi 4B 8 gig (might also run on the 4G, but possibly slower.)  GitHub limits file uploads to 25MB.  The image is 64 GB......  does anybody have a suggestion of how best to make this available?  Maybe I should sell them for $ on Etsy??? :)

By clean-ish, I mean that I have cleaned up *files I have created* that pertain to other functions the robots have, but there are a LOT of things still *installed* that do not pertain specifically to GPT-2 or Tensorflow 1.13.1 (things like speech recognition, synthesis, zmq &amp; imagezmq, face and object detection &amp; recognition, etc., etc.......)  This image is highly customized, and folks seeking to experiment with GPT-2 on the Pi should be able to do quite a bit with this as a starting point.  Apparently others have had a very hard time (as did I) getting GPT-2 to run on the Pi.  All this needs you to do is burn the image, boot the Pi, open a terminal window, cd ~/Desktop/HOSTCORE/gpt-2 and then python src/speakGPT2.py

You'll get a boatload of tensorflow deprication warnings and memory allocation warnings, but it will eventually offer you the ""Model prompt &gt;&gt;&gt;"" and you can enter your prompt and you're off to the races.

Here's a sample I just ran to test that I didn't break anything when I cleaned up the ~100 files....  Everything after ""SAMPLE 1"" is generated by GPT-2 lol...  Anyway - please let me know if there is interest.

Dave


Model prompt &gt;&gt;&gt; I've worked hard to create this disk image for folks who want to experiment so they can get a working copy of GPT-2.  Now I hope to see some inspiring results, and I hope you'll all share 

============SAMPLE 1 =============

 with your own experience and how it goes for you! Enjoy!
I was fortunate enough to meet a guy at an airport this week who gave me a great introduction to his GPT-2 system. I had never been able to get a working copy of a GPT-2, but his system was working, and he had me work with it. I did a lot of research and spent hours trying to figure out how to get it to work. It was really nice to see someone with experience and knowledge of the system, and how to get it to work. When I went to see him I knew what to expect.

-----------------------------

Model prompt &gt;&gt;&gt;",2454,LanguageTechnology,1
"Probabilistic verbal subcategorisation frames research This is more so on the theoretical side of things but I'm curious to know if anybody is familiar with any research that deals with calculating the likelihood that a verb will take a particular set of arguments as it's complement, in particular the likelihood that a verb will take the overt complementiser *that* as in \[think + \[C\[that\]\]? 

There's a tonne of theoretical linguistic research into this but I'm curious to learn about any different approaches.",518,LanguageTechnology,1
ICLR 2020 Workshop Paper on Distant Supervision for Low-Resource Named Entity Recognition | Research Papers Summary 003 ,120,LanguageTechnology,1
ecco - Visualize &amp; Explain NLP Language Models in Python ,61,LanguageTechnology,1
"If I am processing pdfs and are formatted incorrectly, what algorithm should I use? Most of my pdfs are fine. However, some the text is not formatted correctly.",160,LanguageTechnology,1
"Universal Sentence Encoder large not working? The large universal sentence encoder was updated today and it seems to no longer be working, I'm trying to load it in colab and it doesn't ever stop loading.

&amp;#x200B;

Does anyone know anything about this?",256,LanguageTechnology,1
"How to go about building a word list (subset of english word). With the knowledge that we now have of language embedding like BERT I was wondering if there exist a better way to select for example the top 5000 must useful english word.  


This have traditionally been done by simply sorting by word frequency.  
But this have many problem:  
1- word like ""stick"" have multiple meaning which increase the frequency by grouping them under the same count. ex: a) Popsicle sticks b) ""just stick that sandwich on my desk"" c) up the stick (pregnant) ...  
2- word have synonyms that are not grouped together   
3- word at different tense are not grouped together (is vs has), (go vs going) (be vs been)  


I am curious if anyone have explored tagging word usage in a corpus with its meaning before taking a count the the usage frequency?  
Have anyone used some alternative approach to select a subset of word, for example if you wanted to produce a pocket english dictionary.",972,LanguageTechnology,1
"Extrapolating Materials Science domain knowledge through context-free embeddings. [https://www.sciencedirect.com/science/article/pii/S2589004220311196](https://www.sciencedirect.com/science/article/pii/S2589004220311196)

This paper talks about how context-free word embeddings can capture the domain knowledge in a corpus of papers and can be used to extrapolate the same by finding novel polymers for existing applications.

Let me know what you think!",454,LanguageTechnology,1
"Minimum number of samples for encoder-decoder models? Hi,

Is there any research or empirical rule of thumb for minimum number of text pairs needed to train an encoder-decoder style model (seq2seq LSTM, BART, T5, etc.)?",219,LanguageTechnology,1
"Masters in LT - Sweden - Any and all advice welcome Hello! 

I know this is a little late, but I was looking for some opinions/suggestions on this dilemma I'm facing: 

I'm applying for a masters in language technology and I currently have 2 Swedish programs that I'm confused about: the masters programs at Uppsala and Gothenburg. While the Gothenburg syllabus looks more interesting with fun profs and labs to be around, Uppsala offers more (2-3 as opposed to 1) options in terms of funding. Both require that they be my first choice if I am to be considered for funding, and I need some form of funding if I'm not to drown in student debt. 

My background: I'm from South Asia and currently finishing up a master's in English with an 8-pointer GPA. I've done half a dozen courses in linguistics and I know some C++ (only the basics - functions, data structures, loops, sorting, recursion that sort of thing). I also know some calculus, probability, and basic linear algebra. I do not have related research or internship experience.

I understand that this background is as such kinda poor and that the programs I'm applying to are very competitive, but any and all help is appreciated. Especially because Gothenburg says that only one applicant out of all fee paying applicants will be awarded the Axel Adler scholarship, I wanted to know if by opting for Uppsala as my first choice I might be losing out on much in terms of what Gothenburg has to offer. What I'm hoping to do if I get accepted is to pick up enough math and programming for an industrial career as opposed to an academic one, and I'm looking for a program that has good teachers and facilities as opposed to say, all repute but no teaching. 

Thanks!",1720,LanguageTechnology,1
How to construct a taxonomy from news articles? I am trying to build a taxonomic representation of football related stuffs from football news articles.,151,LanguageTechnology,1
Introduction to Rasa: the NLU chatbot framework ,48,LanguageTechnology,1
"500 AI, Machine learning, Deep learning, Computer vision, and NLP Projects with code ",85,LanguageTechnology,1
"VSCode NLP++ Language Extension Released A VSCode Language Extension for NLP++ has been released in the VisualStudio Marketplace.  NLP++ is a open source computer language specifically dedicated to creating text analyzers that mimic human readers and includes the NLP++ language and knowledge based system called the ""conceptual grammar"". NLP++ is used for any type of text processing from simple tagging or extraction, to full language parsing. There is a full English parser that is free an available for use. More information can be found at [http://visualtext.org](http://visualtext.org).",592,LanguageTechnology,1
"How to Fine-tune / Use GPT-3 Alternatives for Text Generation I'm new to NLP, and I'm trying to find an open-source alternative to GPT-3, which I've been using for text generation. I've been using prompt programming to give GPT-3 some examples of emails and have it generate one for me based on parameters I indicate. I'm looking for a way to do the same in the easiest and cheapest manner. I know that T5, Bert, and GPT-2 are all potential alternatives that require fine-tuning on my use case, but I don't exactly know what that entails (other than the fact I probably need a GPU). Can anyone shed some light?",610,LanguageTechnology,1
"Rank text by structural similarity? I’m trying to solve a problem where I have 10 sentences that all mean the same thing (semantically very similar). 

Given a different, somewhat semantically similar sentence, how can I find the most structurally similar sentence out of the 10?

First thoughts are searching for occurrences of punctuation, words like “the”, etc., but is there a better way (or better yet, an API) to do this?",427,LanguageTechnology,1
"The Curious Case of Neural Text DeGeneration | Research Paper Walkthrough This ICLR 2020 paper introduces a novel text decoding strategy called Nucleus Sampling (Top-p sampling). This strategy overcomes the limitation of generating bland, repetitive and incoherent long text from other decoding strategies like Beam Search, Top-k sampling, etc. 🔥 

Watch Paper Walkthrough: https://youtu.be/dCORspO2yVY


⏩ Paper Title: The Curious Case of Neural Text Degeneration
⏩ Paper: https://arxiv.org/abs/1904.09751
⏩ Author: Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi
⏩ Organisation: Allen School of Computer Science &amp; Engineering, University of Washington, Allen Institute for Artificial Intelligence, University of Cape Town",737,LanguageTechnology,1
"Has anyone deployed a BERT like model across multiple tasks (Multi-class, NER, outlier detection)? Seeking advice. So I have a custom pre-trained RoBERTa model that I want to fine tune with NER, multi class classification, and outlier / new class detection.  Currently using Huggingface Transformers for pre-training and fine-tuning.

I’ve recently realized that naively fine tuning on each task separately would require loading in 3 instances of the pre trained model.  My understanding is that fine tuning makes small adjustments to the embedding layers.  Fine tuning separately but using the initial embedding may take a hit to performance.  

Any thoughts or experience on fine tuning across all 3 tasks simultaneously? Or maybe some hacky approach?  Also, if anyone has anyone insights on parameter tuning (specifically BPE vocab size), scheduling, or noising.  Any help would be much appreciated!",902,LanguageTechnology,1
NLP in 2020: The Year In Review ,32,LanguageTechnology,1
"Free NLP assignments NLP educators-- I made some free mastery-based assignments to reinforce learning in your NLP classes this semester. These assignments leverage cognitive neuroscience principles proven to optimize knowledge retention &amp; adapt to unique student needs:

[https://docs.google.com/document/d/1PJPc8mTdkm-QSwN4FCjjTxEup7npqE-uUNwz4uhdtzM/edit?usp=sharing](https://docs.google.com/document/d/1PJPc8mTdkm-QSwN4FCjjTxEup7npqE-uUNwz4uhdtzM/edit?usp=sharing)",471,LanguageTechnology,1
OpenAI's DALL·E: Creating Images from Text - Explained ,55,LanguageTechnology,1
"Best way to work into an NLP/Computational Linguist career? Hi all, I hope I’ve posted this in the right place! I’ve just graduated with an undergraduate degree in linguistics with hopes to do my masters in computational linguistics, but was ultimately unsuccessful with getting into a graduate program. I have basic experience with python, R, foma, and took computational ling courses during my degree so I’m a bit familiar with CYK, FSTs, etc. I’m just wondering if there’s any entry level job I could do with little experience/no masters that would keep me in the area of computational lingusitics/NLP, or if anyone else has had any success or advice about this sort of situation? Thanks so much :)",701,LanguageTechnology,1
Big tech fails to recognize African languages | DW News ,56,LanguageTechnology,1
"As a part of Pincone, a bookmark manager, we added auto-labeling of websites and wrote a bit about how we did it. ",114,LanguageTechnology,1
[D] Hugging Face has released an official course ,49,LanguageTechnology,1
"HemingwAI, the API writing text with you is looking for feedback Hey fellow NLPlers,

I'd love to ask you for some feedback on our NLG project, as we have just released our documentation to our AI copywriter API!

Now we are inviting developers to test out the current platform. Feedback from you is gold for us as we are trying to better understand where we can generate value in your daily needs! And please do share it with people which would enjoy our work!

**How we got started?**

We are two NLG Enthusiasts in Berlin who wanted to take away the complexity till somebody could leverage some of the newest GPT models. Hence, we built an infrastructure to get your AI copywriter ready in a matter of minutes!

What started as a GUI to validate that individuals and businesses show interest in natural language generation is now also just one API request away.

Please ask me any question in the chat below or on twitter via dom\_does.

Links for accessing HemingwAI API.

Documentation: [https://textcortex.com/documentation/api](https://textcortex.com/documentation/api)

Github: [https://github.com/textcortex/hemingwai](https://github.com/textcortex/hemingwai)",1168,LanguageTechnology,1
How to fine-tune with BertForPretraining ,41,LanguageTechnology,1
"Research project: Free text to excel output Hi all,

I'm new to machine learning so please bear with me and forgive me for my ignorance.

I am working with medical data, specifically radiology reports. One of my potential projects hopes to take free text and be able to capture specific anatomy and their measurements, and output it into an excel in an organized fashion. For instance, would it be possible to identify the thoracic and abdominal aorta within free text, see their maximum measurements, and to output it into an excel report? 

I have already annotated all the free text reports with the anatomy and measurements of interest. How would I go about finding the correct machine learning algorithm and start training it to properly identify what I am looking for? If anyone could help point me in the right direction to get started, I would be so very grateful. Thank you",882,LanguageTechnology,1
"Can't run 124M using transformers I have downloaded gpt 124M on my local machine and i was able to run the [interactivesample.py](https://interactivesample.py) that was provided by them

&amp;#x200B;

But when i try to load 124M using transformers, i get following error:

*\_OSError: Can't load config for 'models\\124M'. Make sure that:*



*- 'models\\124M' is a correct model identifier listed on '*[*https://huggingface.co/models*](https://huggingface.co/models)*'*



*- or 'models\\124M' is the correct path to a directory containing a config.json file\_*

&amp;#x200B;

**\*\*My code:\*\***

tokenizer = AutoTokenizer.from\_pretrained(""models\\\\124M"")

&amp;#x200B;

124M contains following json file : encoder",719,LanguageTechnology,1
"Best learning resources Hi all, I am currently working through a “Build ChatBots with Python” course on Codecademy which obviously spends some time on NLP. However, as a Python beginner, I feel like their explanations on a lot of topics are glazed over leaving me confused. Thus I am looking for some beginner-friendly resources I can use to supplement my course. Ultimately I want to be able to program my own chatbots with Python, and am fascinated by NLP, linguistics, and machine learning. I am open to online courses, projects to work through, YouTube videos, books, etc. Thanks in advance!",595,LanguageTechnology,1
"Open Source Grammar Correction Service with Gramformer- Google Collab, fastapi,pyngrok -Python demo ",100,LanguageTechnology,1
"My first contribution into hugging face I have finetune wav2vec2 large xlsr53 on WOLOF audio data set, for more info visit the [here](https://huggingface.co/kingabzpro/wav2vec2-large-xlsr-53-wolof). You can also check my [Github](https://github.com/kingabzpro/WOLOF-ASR-Wav2Vec2) repo. You can also look at my [Kaggle](https://www.kaggle.com/kingabzpro/fine-tuning-xlsr-wav2vec2-for-wolof-asr-with) notebook.",408,LanguageTechnology,1
"Edinburgh or U Washington NLP Masters? accepted and need help deciding. Hi, I'm into both U of Washington and U of Edinburgh for NLP Master's and Master's of Speech and Language Processing, respectively. Washington is remote and tuition is 40k (and 1 year in duration if full-time) and Edinburgh seems to be 1 year + dissertation at 60k for tuition. I was hoping to get some insight on which path would be more lucrative considering the costs, as well as U wash having an internship option. I am certain I want to continue onwards to industry as opposed to PHD if that helps. My concerns are getting a degree from an overseas uni, as well as slight concern about the tuition. I am a new grad (May 2021) and realized I need further education for most NLP positions in the US. 

Thank you everyone in this community for your help!",828,LanguageTechnology,1
"Fine-tuning the larger GPT Neo models(1.3B and 2.7B) with a Jupyter notebook Normally, only fine-tuning the 125M GPT Neo model is possible due to the fact that even that model uses over 10GB of VRAM, and the 1.3B and 2.7B taking much more. By following this video and the provided Jupyter notebook, it is possible to fine-tune the larger GPT Neo models on high-end consumer hardware or on cheap cloud options

[https://www.youtube.com/watch?v=Igr1tP8WaRc](https://www.youtube.com/watch?v=Igr1tP8WaRc)",500,LanguageTechnology,1
"Should you use MT5 instead of T5 for every non-english task? I was looking at the T5 transformer model and was thinking about using it for a german generation task. This model seems really powerful but it is only trained for English. Then I found the multilingual T5 model which supports many languages. Is this way better for the non-english tasks (for example paraphrase generation in my case)? I am confused because the T5 model is able to translate from English to German, it can be trained on other languages etc.",518,LanguageTechnology,1
"Researchers From Bangladesh University And UCLA Use AI To Develop A Framework (Text2App) To Create Android Apps From Text Descriptions A team of Researchers at BUET (Bangladesh University of Engineering and Technology) and UCLA (University of California- Los Angeles) has created a [framework that can be used to develop Android applications from text descriptions.](https://arxiv.org/pdf/2104.08301.pdf)

According to Masum Hasan, a researcher who carried out the study, their team wondered whether a full-fledged software could be built from natural language specification. Almost all the existing models for creating software based on text descriptions are based on end-to-end neural machine translation (NMT) models, which are similar to the one behind Google Translate. Usually, these models use NMT frameworks to translate human language into a source code.

Summary: https://www.marktechpost.com/2021/06/14/researchers-from-bangladesh-university-and-ucla-use-ai-to-develop-a-framework-text2app-to-create-android-apps-from-text-descriptions/

Paper: https://arxiv.org/pdf/2104.08301.pdf

Github: https://text2app.github.io/",1129,LanguageTechnology,1
"I want to learn about Language, Linguistics, Etymology, and everything in between in one book. Any recommendations. Bonus points if it's computationally relevant. I hate ending up going in google spirals , forgetting terms, confusing terms. I am only talking about books for English and maybe any Arabic letter languages although only Eng is necessary.

Research papers and articles are welcome as well.",403,LanguageTechnology,1
"BERT: How to do sentiment analysis on a custom dataset? Hello,

I have seen his sentiment analysis tutorial here: [https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment?text=I+like+you.+I+love+you](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment?text=I+like+you.+I+love+you)

Now, I was wondering, how do I do such a task using my own dataset? I have list of different sentences and I want to find out the sentiment of each sentence, which can be either ""happy"", ""sad"" or ""angry"". 

Do I need to fine tune BERT with my own data? If  so, how?

Thanks.",592,LanguageTechnology,1
"Gpt 2 124m using transformers I downloaded gpt 124 M and i was able to run the interactive sample file.
How can I run 124M using transformers.
While running:
Auto tokenizer. Frompretrained(""c:\\path\\124M"")

I get an error :configuration not found.

Note:124 M contains encoder. Json, hoarams. Json,... Etc.",307,LanguageTechnology,1
Energy-Based Models for Code Generation under Compilability Constraints ,72,LanguageTechnology,1
"Looking for medical audio/transcription dataset for training STT model with DeepSpeech. I’m working on training a STT model with DeepSpeech that is based on a pre-trained model but will specialize in transcribing medical conversation (diseases, medications, etc). The most frustrating part so far has been a lack of medical audio data for training. I’m looking for high quality audio on the scale of thousands of hours with accurate transcriptions. Does anyone know if such a dataset exists?",491,LanguageTechnology,1
How To Build and Deploy an NLP Model with FastAPI: Part 1 ,58,LanguageTechnology,1
"This Chinese Super Scale Intelligence Model, ‘Wu Dao 2.0’, Claims To Be Trained Using 1.75 Trillion Parameters, Surpassing All Prior Models to Achieve a New Breakthrough in Deep Learning Deep learning is one area of technology where ambitiousness has no barriers. According to a recent announcement by [The Beijing Academy of Artificial Intelligence (BAAI)](https://www.baai.ac.cn/), in China, yet another milestone has been achieved in the field with its “Wu Dao” AI system. The [GPT 3](https://www.marktechpost.com/2020/08/02/gpt-3-a-new-breakthrough-in-language-generator/) brought in new interest for all the AI researchers, the super scale pre training models. By this approach and making use of 175 billion parameters, it managed to achieve exceptional performance results across the natural language processing tasks (NLP). However, the lacking component is its inability to have any form of cognitive abilities or common sense. Therefore, despite the size, even these models cannot indulge in tasks such as open dialogues, visual reasoning, and so on. With Wu Dao, the researchers plan to address this issue. This is China’s first attempt at a home-grown super-scale intelligent model system. 

Article: [https://www.marktechpost.com/2021/06/13/this-chinese-super-scale-intelligence-model-wu-dao-2-0-claims-to-be-trained-using-1-75-trillion-parameters-surpassing-all-prior-models-to-achieve-a-new-breakthrough-in-deep-learning/](https://www.marktechpost.com/2021/06/13/this-chinese-super-scale-intelligence-model-wu-dao-2-0-claims-to-be-trained-using-1-75-trillion-parameters-surpassing-all-prior-models-to-achieve-a-new-breakthrough-in-deep-learning/?_ga=2.13897584.636390090.1623335762-488125022.1618729090)

Reference: [https://syncedreview.com/2021/03/23/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0/](https://syncedreview.com/2021/03/23/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0/)",1946,LanguageTechnology,1
"The NLP challenge: 5 Puzzles in 2 Weeks Anybody participating?   


https://www.aicrowd.com/challenges/ai-blitz-9?utm\_source=reddit&amp;utm\_medium=languagetechnology&amp;utm\_campaign=blitz9",192,LanguageTechnology,1
"Can the task of question answering ever be formulated as classification? Hi. I'm currently wondering if the NLP task of question answering (QA) can be formulated as classification as opposed to span extraction. For example, if I feed a model `[CLS] Where does President Roosevelt live? [SEP] CONTEXT` it seems that almost all (if not all) QA models formulate the task as span extraction and return the correct answer that's contained within the document. However, I'm curious whether we could have labels and formulate it as classification instead.

If anybody knows any papers or resources I could look into, that'd be great. Thanks!",634,LanguageTechnology,1
"Knowledge Graph Hi , i’m working on a project to build knowledge graph , so that the relationships within the document will help us to query documents with those attributes from the entire corpus.
Would like some help what methods can be used to mine relationships from the document.",283,LanguageTechnology,1
