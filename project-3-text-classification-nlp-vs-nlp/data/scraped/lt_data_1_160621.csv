,approved_at_utc,subreddit,selftext,author_fullname,saved,mod_reason_title,gilded,clicked,title,link_flair_richtext,subreddit_name_prefixed,hidden,pwls,link_flair_css_class,downs,top_awarded_type,hide_score,name,quarantine,link_flair_text_color,upvote_ratio,author_flair_background_color,subreddit_type,ups,total_awards_received,media_embed,author_flair_template_id,is_original_content,user_reports,secure_media,is_reddit_media_domain,is_meta,category,secure_media_embed,link_flair_text,can_mod_post,score,approved_by,is_created_from_ads_ui,author_premium,thumbnail,edited,author_flair_css_class,author_flair_richtext,gildings,content_categories,is_self,mod_note,created,link_flair_type,wls,removed_by_category,banned_by,author_flair_type,domain,allow_live_comments,selftext_html,likes,suggested_sort,banned_at_utc,view_count,archived,no_follow,is_crosspostable,pinned,over_18,all_awardings,awarders,media_only,can_gild,spoiler,locked,author_flair_text,treatment_tags,visited,removed_by,num_reports,distinguished,subreddit_id,mod_reason_by,removal_reason,link_flair_background_color,id,is_robot_indexable,report_reasons,author,discussion_type,num_comments,send_replies,whitelist_status,contest_mode,mod_reports,author_patreon_flair,author_flair_text_color,permalink,parent_whitelist_status,stickied,url,subreddit_subscribers,created_utc,num_crossposts,media,is_video,url_overridden_by_dest,crosspost_parent_list,crosspost_parent,media_metadata,author_cakeday,textlen
0,,LanguageTechnology,"[FLORES-101](https://l.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2Ffacebookresearch%2Fflores%3Ffbclid%3DIwAR18XHNF2irnZUwv6ZwTrfX9LTX7NFQW-GHaVPTMffhKWPQcgBjgMD1A8mo&amp;h=AT03MSblPXA3bufCYi2Xy_GR-fxCQFim0iX2w2ZkT5fiiJLjE-Zltua9obJXi4oRaOaqVHEEZ3QNRN8SXrC8lt5lLw3P4Hnlvw6L3ldX5I_z35thn6992Ve6b8H5wYE_JiYd_A), a first-of-its-kind, many-to-many evaluation data set that covers 101 languages from around the world, is now open-sourced. FLORES-101 is a tool that allows researchers to test and refine multilingual translation models such as M2M-100 quickly. To speed work on many-to-many translation systems worldwide, Facebook AI makes [the complete FLORES-101 data set](https://github.com/facebookresearch/flores?fbclid=IwAR1pjSZbSRQhxv9QccaCCApZLS0zltZy0uji24rWt9QxTR0HgWkNCGu_F2M), and associated technical report, and various models freely available for anybody to use.

Full Article: [https://www.marktechpost.com/2021/06/12/facebook-ai-open-source-the-flores-101-data-set-for-better-translation-systems-around-the-world/](https://www.marktechpost.com/2021/06/12/facebook-ai-open-source-the-flores-101-data-set-for-better-translation-systems-around-the-world/) 

Paper: https://arxiv.org/abs/2106.03193",t2_2wsvqwhg,False,,0,False,"Facebook AI Open-Source ‘The FLORES-101 Data Set’, For Better Translation Systems Around The World (Paper included)",[],r/LanguageTechnology,False,6,,0,,False,t3_nypgu0,False,dark,0.95,,public,16,0,{},,False,[],,False,False,,{},,False,16,,False,False,,False,,[],{},,True,,1623590618.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://l.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2Ffacebookresearch%2Fflores%3Ffbclid%3DIwAR18XHNF2irnZUwv6ZwTrfX9LTX7NFQW-GHaVPTMffhKWPQcgBjgMD1A8mo&amp;amp;h=AT03MSblPXA3bufCYi2Xy_GR-fxCQFim0iX2w2ZkT5fiiJLjE-Zltua9obJXi4oRaOaqVHEEZ3QNRN8SXrC8lt5lLw3P4Hnlvw6L3ldX5I_z35thn6992Ve6b8H5wYE_JiYd_A""&gt;FLORES-101&lt;/a&gt;, a first-of-its-kind, many-to-many evaluation data set that covers 101 languages from around the world, is now open-sourced. FLORES-101 is a tool that allows researchers to test and refine multilingual translation models such as M2M-100 quickly. To speed work on many-to-many translation systems worldwide, Facebook AI makes &lt;a href=""https://github.com/facebookresearch/flores?fbclid=IwAR1pjSZbSRQhxv9QccaCCApZLS0zltZy0uji24rWt9QxTR0HgWkNCGu_F2M""&gt;the complete FLORES-101 data set&lt;/a&gt;, and associated technical report, and various models freely available for anybody to use.&lt;/p&gt;

&lt;p&gt;Full Article: &lt;a href=""https://www.marktechpost.com/2021/06/12/facebook-ai-open-source-the-flores-101-data-set-for-better-translation-systems-around-the-world/""&gt;https://www.marktechpost.com/2021/06/12/facebook-ai-open-source-the-flores-101-data-set-for-better-translation-systems-around-the-world/&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/abs/2106.03193""&gt;https://arxiv.org/abs/2106.03193&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nypgu0,True,,ai-lover,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nypgu0/facebook_ai_opensource_the_flores101_data_set_for/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nypgu0/facebook_ai_opensource_the_flores101_data_set_for/,30199,1623561818.0,0,,False,,,,,,1208
1,,LanguageTechnology,,t2_159buvym,False,,0,False,Can anyone please guide me to a video based Automatic speech recognition (ASR) course ?,[],r/LanguageTechnology,False,6,,0,,False,t3_nyw4qu,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1623619057.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nyw4qu,True,,rakshith291,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nyw4qu/can_anyone_please_guide_me_to_a_video_based/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nyw4qu/can_anyone_please_guide_me_to_a_video_based/,30199,1623590257.0,0,,False,,,,,,0
2,,LanguageTechnology,"I am confused about which out of BERT transformer should be fed to dense layer. BERT transformer output is \`batch,sequence length, 768 \`. So I am confused about how to pass it to the dense layer",t2_5owk7j7,False,,0,False,How to pass BERT output to dense layer,[],r/LanguageTechnology,False,6,,0,,False,t3_nyx09y,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1623621795.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am confused about which out of BERT transformer should be fed to dense layer. BERT transformer output is `batch,sequence length, 768 `. So I am confused about how to pass it to the dense layer&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nyx09y,True,,mrtac96,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nyx09y/how_to_pass_bert_output_to_dense_layer/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nyx09y/how_to_pass_bert_output_to_dense_layer/,30199,1623592995.0,0,,False,,,,,,196
3,,LanguageTechnology,,t2_hkv9s,False,,0,False,Detecting Hallucinated Content in Conditional Neural Sequence Generation (NLP Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_nyvm5d,False,dark,0.33,,public,0,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/fD2g9s1Isi4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Detecting Hallucinated Content in Conditional Neural Sequence Generation (NLP Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/fD2g9s1Isi4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/fD2g9s1Isi4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/fD2g9s1Isi4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nyvm5d', 'height': 200}",,False,0,,False,False,,False,,[],{},,False,,1623617362.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nyvm5d,True,,prakhar21,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nyvm5d/detecting_hallucinated_content_in_conditional/,all_ads,False,https://youtu.be/fD2g9s1Isi4,30199,1623588562.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Detecting Hallucinated Content in Conditional Neural Sequence Generation (NLP Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/fD2g9s1Isi4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/fD2g9s1Isi4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/fD2g9s1Isi4,,,,,0
4,,LanguageTechnology,,t2_apv7o2y8,False,,0,False,Text Classification using spaCy v3.0 transformers in Python | Natural Language Processing Tutorial,[],r/LanguageTechnology,False,6,,0,,False,t3_nyv1pp,False,dark,0.5,,public,0,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/NkuqNItEbsc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Text Classification using spaCy v3.0 transformers  in Python | Natural Language Processing Tutorial', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/NkuqNItEbsc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/NkuqNItEbsc/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/NkuqNItEbsc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nyv1pp', 'height': 200}",,False,0,,False,False,,False,,[],{},,False,,1623615362.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nyv1pp,True,,rsree123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nyv1pp/text_classification_using_spacy_v30_transformers/,all_ads,False,https://youtu.be/NkuqNItEbsc,30199,1623586562.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Text Classification using spaCy v3.0 transformers  in Python | Natural Language Processing Tutorial', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/NkuqNItEbsc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/NkuqNItEbsc/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,https://youtu.be/NkuqNItEbsc,,,,,0
5,,LanguageTechnology,"Hello everyone,

Really excited to share with you that our paper ""Towards Emotional Support Dialog Systems"" got accepted to the main conference of the Association for Computational Linguistics this year (ACL2021). We strongly believe that dialogue systems have the potential to become powerful emotional supporters that can help many individuals with their daily struggles. Towards this goal, we created a high-quality dataset of emotional conversations between trained crowdsourcing workers and will be making this dataset publicly available.

Feel free to read our [paper](https://arxiv.org/abs/2106.01144) for more details.

Thank you and wish you a great day!",t2_aoiqtg2t,False,,0,False,Towards Emotional Support Dialog Systems,[],r/LanguageTechnology,False,6,,0,,False,t3_nyndog,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,1623564425.0,,[],{},,True,,1623582401.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;

&lt;p&gt;Really excited to share with you that our paper &amp;quot;Towards Emotional Support Dialog Systems&amp;quot; got accepted to the main conference of the Association for Computational Linguistics this year (ACL2021). We strongly believe that dialogue systems have the potential to become powerful emotional supporters that can help many individuals with their daily struggles. Towards this goal, we created a high-quality dataset of emotional conversations between trained crowdsourcing workers and will be making this dataset publicly available.&lt;/p&gt;

&lt;p&gt;Feel free to read our &lt;a href=""https://arxiv.org/abs/2106.01144""&gt;paper&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;Thank you and wish you a great day!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nyndog,True,,Sahand_sab,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nyndog/towards_emotional_support_dialog_systems/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nyndog/towards_emotional_support_dialog_systems/,30199,1623553601.0,0,,False,,,,,,663
6,,LanguageTechnology,"Hi,
I downloaded gpt neo from theeye.eye on my pc.
It downloaded a various checkpoints.
How do i use them? ... Because in order too load and use model I'd need encoder. Json, pytorch. Bin, etc..",t2_7wron7g8,False,,0,False,Using gpt neo checkpoints,[],r/LanguageTechnology,False,6,,0,,False,t3_nys2gg,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1623602422.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,
I downloaded gpt neo from theeye.eye on my pc.
It downloaded a various checkpoints.
How do i use them? ... Because in order too load and use model I&amp;#39;d need encoder. Json, pytorch. Bin, etc..&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nys2gg,True,,arkhamrising,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nys2gg/using_gpt_neo_checkpoints/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nys2gg/using_gpt_neo_checkpoints/,30199,1623573622.0,0,,False,,,,,,194
7,,LanguageTechnology,"Hey!  
I am involved in a project where I am trying to create a programming language that uses machine learning to compile text as computer code, some info here ([https://github.com/quantleaf/quantleaf-language-documentation](https://github.com/quantleaf/quantleaf-language-documentation)). This is not yet open source as a whole, but I am currently in the process of doing so. The first subproject to be released is a search engine library that enables you to score documents with a value between 0 and 1 (I call it “zero-to-one” score). In short, this is done by evaluating the product of how close we are to a perfect match regarding document length and query length, but also in terms of the amount of tokens, in the document and in the query.

I have created a small recipe search demo for this to showcase it benefits (and potential drawbacks)

[https://quantleaf.github.io/probly-search-demo/](https://quantleaf.github.io/probly-search-demo/)

When searching “Garlic Chicken”, the first two results are:

For the “zero-to-one” scoring
“Garlic Chicken” score 1.
“Garlic-Sherry Chicken” score: 0.7307692307692308

For BM25 (standard parameters)
“Garlic Oven Fried Chicken” score 8.564332563809089
“Garlic Chicken“ score 8.455662889754347

For the BM25 algorithm the perfect match is not the top score. You could circumvent this behaviour by adjusting the parameters of the BM25 algorithm. Or is it in conjunction with a term matching algorithm. But for my programming language project, this was not good enough. I needed a score to be 1, to know when we are 100% matching, 0.5 if we are matching with a 50% relevance, hence I created this.

The search engine is written in Rust, but you could use it in any Node project if you write a little bit of WASM bindgen code (see the demo source code).

Library source code: [https://github.com/quantleaf/probly-search](https://github.com/quantleaf/probly-search)Demo source code: [https://github.com/quantleaf/probly-search-demo](https://github.com/quantleaf/probly-search-demo)

I am curious with what your take is on this scoring function, would you find it useful in comparison to the solution that you currently are using? (Especially for title/label matching)",t2_4db2l3yk,False,,0,False,A Search Engine with a normalized scoring function,[],r/LanguageTechnology,False,6,,0,,False,t3_nybd9c,False,dark,0.75,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,True,,1623604248.0,,[],{},,True,,1623546518.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey!&lt;br/&gt;
I am involved in a project where I am trying to create a programming language that uses machine learning to compile text as computer code, some info here (&lt;a href=""https://github.com/quantleaf/quantleaf-language-documentation""&gt;https://github.com/quantleaf/quantleaf-language-documentation&lt;/a&gt;). This is not yet open source as a whole, but I am currently in the process of doing so. The first subproject to be released is a search engine library that enables you to score documents with a value between 0 and 1 (I call it “zero-to-one” score). In short, this is done by evaluating the product of how close we are to a perfect match regarding document length and query length, but also in terms of the amount of tokens, in the document and in the query.&lt;/p&gt;

&lt;p&gt;I have created a small recipe search demo for this to showcase it benefits (and potential drawbacks)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://quantleaf.github.io/probly-search-demo/""&gt;https://quantleaf.github.io/probly-search-demo/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;When searching “Garlic Chicken”, the first two results are:&lt;/p&gt;

&lt;p&gt;For the “zero-to-one” scoring
“Garlic Chicken” score 1.
“Garlic-Sherry Chicken” score: 0.7307692307692308&lt;/p&gt;

&lt;p&gt;For BM25 (standard parameters)
“Garlic Oven Fried Chicken” score 8.564332563809089
“Garlic Chicken“ score 8.455662889754347&lt;/p&gt;

&lt;p&gt;For the BM25 algorithm the perfect match is not the top score. You could circumvent this behaviour by adjusting the parameters of the BM25 algorithm. Or is it in conjunction with a term matching algorithm. But for my programming language project, this was not good enough. I needed a score to be 1, to know when we are 100% matching, 0.5 if we are matching with a 50% relevance, hence I created this.&lt;/p&gt;

&lt;p&gt;The search engine is written in Rust, but you could use it in any Node project if you write a little bit of WASM bindgen code (see the demo source code).&lt;/p&gt;

&lt;p&gt;Library source code: &lt;a href=""https://github.com/quantleaf/probly-search""&gt;https://github.com/quantleaf/probly-search&lt;/a&gt;Demo source code: &lt;a href=""https://github.com/quantleaf/probly-search-demo""&gt;https://github.com/quantleaf/probly-search-demo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I am curious with what your take is on this scoring function, would you find it useful in comparison to the solution that you currently are using? (Especially for title/label matching)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nybd9c,True,,marcus-pousette,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nybd9c/a_search_engine_with_a_normalized_scoring_function/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nybd9c/a_search_engine_with_a_normalized_scoring_function/,30199,1623517718.0,0,,False,,,,,,2212
8,,LanguageTechnology,"Context: I'm a hobbyist that got into NLP superficially, so I don't expect a definitive answer. I'd be grateful if you could just point me in the right direction, because I was overwhelmed with all the different approaches after googling it.

Here's what I want to do:

I have laws and legislation split into its smallest pieces. Each piece is a specific string. So one or two lines, maybe a paragraph, with a specific rule saying what you can and cannot do, who is responsible for what, how many days you have to do something, what is the punishment for breaking it etc. I also have a corpus of written tests (about 70k total), with questions that are often about said rules. For each small string, I want to see how many times it was mentioned in tests, so I can rank them in regards to which subjects are more likely to be asked about in tests.

How would you go about doing something like that?",t2_ix5xm,False,,0,False,Best approach to find (similar) matches for a string in a corpus of documents?,[],r/LanguageTechnology,False,6,,0,,False,t3_nyctxd,False,dark,0.72,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1623550725.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Context: I&amp;#39;m a hobbyist that got into NLP superficially, so I don&amp;#39;t expect a definitive answer. I&amp;#39;d be grateful if you could just point me in the right direction, because I was overwhelmed with all the different approaches after googling it.&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s what I want to do:&lt;/p&gt;

&lt;p&gt;I have laws and legislation split into its smallest pieces. Each piece is a specific string. So one or two lines, maybe a paragraph, with a specific rule saying what you can and cannot do, who is responsible for what, how many days you have to do something, what is the punishment for breaking it etc. I also have a corpus of written tests (about 70k total), with questions that are often about said rules. For each small string, I want to see how many times it was mentioned in tests, so I can rank them in regards to which subjects are more likely to be asked about in tests.&lt;/p&gt;

&lt;p&gt;How would you go about doing something like that?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nyctxd,True,,LiarsEverywhere,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nyctxd/best_approach_to_find_similar_matches_for_a/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nyctxd/best_approach_to_find_similar_matches_for_a/,30199,1623521925.0,0,,False,,,,,,898
9,,LanguageTechnology,"Hi, I want to extract text from image where text is printed columnwise (like research papers) does anybody know any libraries that will help me with it? Or advice how to approach with such use case? 

Thanks in advance!",t2_4qmstn7u,False,,0,False,Extract text from image which is printed columnwise,[],r/LanguageTechnology,False,6,,0,,False,t3_nyfn5w,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1623558422.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I want to extract text from image where text is printed columnwise (like research papers) does anybody know any libraries that will help me with it? Or advice how to approach with such use case? &lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nyfn5w,True,,karishmaD,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nyfn5w/extract_text_from_image_which_is_printed/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nyfn5w/extract_text_from_image_which_is_printed/,30199,1623529622.0,0,,False,,,,,,219
10,,LanguageTechnology,"Hi All,
I downloaded the model from
https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/

after which i changed model_path in config.json to:
""model_path"" : ""C:\Users\GPT_NEO_2\GPT3_XL""

Whenever i run the following code:
model = GPTNeoForCausalLM.from_pretrained(""C:\Users\GPT_NEO_2\GPT3_XL"")

i get an error:
f""Error no file named {[WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME + '.index', FLAX_WEIGHTS_NAME]} found in ""
OSError: Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index', 'flax_model.msgpack'] found in directory C:\Users\GPT_NEO_2\GPT3_XL or from_tf and from_flax set to False.

and while running :
generator = pipeline('text-generation', model=""C:\Users\GPT_NEO_2\GPT3_XL"")

i get following error:
f""Unrecognized model in {pretrained_model_name_or_path}. ""

I have the latest TF and torch (both cpu).

Thanks",t2_7wron7g8,False,,0,False,Can't run gpt3 xl,[],r/LanguageTechnology,False,6,,0,,False,t3_ny88wd,False,dark,0.4,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1623537663.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All,
I downloaded the model from
&lt;a href=""https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/""&gt;https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;after which i changed model_path in config.json to:
&amp;quot;model_path&amp;quot; : &amp;quot;C:\Users\GPT_NEO_2\GPT3_XL&amp;quot;&lt;/p&gt;

&lt;p&gt;Whenever i run the following code:
model = GPTNeoForCausalLM.from_pretrained(&amp;quot;C:\Users\GPT_NEO_2\GPT3_XL&amp;quot;)&lt;/p&gt;

&lt;p&gt;i get an error:
f&amp;quot;Error no file named {[WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME + &amp;#39;.index&amp;#39;, FLAX_WEIGHTS_NAME]} found in &amp;quot;
OSError: Error no file named [&amp;#39;pytorch_model.bin&amp;#39;, &amp;#39;tf_model.h5&amp;#39;, &amp;#39;model.ckpt.index&amp;#39;, &amp;#39;flax_model.msgpack&amp;#39;] found in directory C:\Users\GPT_NEO_2\GPT3_XL or from_tf and from_flax set to False.&lt;/p&gt;

&lt;p&gt;and while running :
generator = pipeline(&amp;#39;text-generation&amp;#39;, model=&amp;quot;C:\Users\GPT_NEO_2\GPT3_XL&amp;quot;)&lt;/p&gt;

&lt;p&gt;i get following error:
f&amp;quot;Unrecognized model in {pretrained_model_name_or_path}. &amp;quot;&lt;/p&gt;

&lt;p&gt;I have the latest TF and torch (both cpu).&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ny88wd,True,,arkhamrising,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ny88wd/cant_run_gpt3_xl/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ny88wd/cant_run_gpt3_xl/,30199,1623508863.0,0,,False,,,,,,850
11,,LanguageTechnology,"Hi folks. I wanted to find all words that indicated a certain attribute. For example, consider finding all words that are 'hot'. That could include 'fire', 'flame', 'lava', 'heat' or a 'stove'.

The approach I came up with is to use word embeddings and the resulting word graph. If I start with 'hot', I could add all words within a certain distance, and then perform this process for all subsequent nodes to find paths of a certain max total distance. All vertices in this subgraph would be my answer. But I would still have to account for colloquialism.

Are there any existing frameworks or datasets that already do it like this or with any other methods?

Thanks in advance!",t2_1sw9bp4y,False,,0,False,Detecting all words/entities with a certain attribute,[],r/LanguageTechnology,False,6,,0,,False,t3_nxlmi0,False,dark,0.87,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1623461786.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi folks. I wanted to find all words that indicated a certain attribute. For example, consider finding all words that are &amp;#39;hot&amp;#39;. That could include &amp;#39;fire&amp;#39;, &amp;#39;flame&amp;#39;, &amp;#39;lava&amp;#39;, &amp;#39;heat&amp;#39; or a &amp;#39;stove&amp;#39;.&lt;/p&gt;

&lt;p&gt;The approach I came up with is to use word embeddings and the resulting word graph. If I start with &amp;#39;hot&amp;#39;, I could add all words within a certain distance, and then perform this process for all subsequent nodes to find paths of a certain max total distance. All vertices in this subgraph would be my answer. But I would still have to account for colloquialism.&lt;/p&gt;

&lt;p&gt;Are there any existing frameworks or datasets that already do it like this or with any other methods?&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nxlmi0,True,,aklagoo,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nxlmi0/detecting_all_wordsentities_with_a_certain/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nxlmi0/detecting_all_wordsentities_with_a_certain/,30199,1623432986.0,0,,False,,,,,,678
12,,LanguageTechnology,,t2_3lgg4,False,,0,False,At OpenDialog we make NLU more useful by making it work less. Combining context and pro-active conversation management to reduce reliance on NLU understanding state.,[],r/LanguageTechnology,False,6,,0,,False,t3_nxb3ar,False,dark,0.73,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,False,,1623429460.0,text,6,,,text,opendialog.ai,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nxb3ar,True,,istos,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nxb3ar/at_opendialog_we_make_nlu_more_useful_by_making/,all_ads,False,https://opendialog.ai/2021/06/08/how-opendialog-approaches-natural-language-understanding/,30199,1623400660.0,0,,False,https://opendialog.ai/2021/06/08/how-opendialog-approaches-natural-language-understanding/,,,,,0
13,,LanguageTechnology,,t2_apv7o2y8,False,,0,False,Custom Named Entity (Disease) Recognition in clinical text with spaCy v3 Transformers in Python,[],r/LanguageTechnology,False,6,,0,,False,t3_nxie9m,False,dark,0.57,,public,1,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Nv3TqzT2RLI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Custom  Named Entity (Disease) Recognition in clinical text with spaCy v3 Transformers in Python', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Nv3TqzT2RLI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Nv3TqzT2RLI/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Nv3TqzT2RLI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nxie9m', 'height': 200}",,False,1,,False,False,,False,,[],{},,False,,1623453424.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nxie9m,True,,rsree123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nxie9m/custom_named_entity_disease_recognition_in/,all_ads,False,https://youtu.be/Nv3TqzT2RLI,30199,1623424624.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Custom  Named Entity (Disease) Recognition in clinical text with spaCy v3 Transformers in Python', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Nv3TqzT2RLI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Nv3TqzT2RLI/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,https://youtu.be/Nv3TqzT2RLI,,,,,0
14,,LanguageTechnology,"Im able to predict document topics (such as document A is 60% Topic 1, and 40% topic 2). But cant find a way to classify what sentences are Topic 1 and which ones are Topic 2. One way of doing this could be to use sentence boundaries to extract sentences and then use gensim to predict each sentence but im looking for a formal way to do it.",t2_5r4mo7fh,False,,0,False,How to use gensim topic modeling to predict sentences in a document?,[],r/LanguageTechnology,False,6,,0,,False,t3_nxj9dj,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,1623427603.0,,[],{},,True,,1623455695.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Im able to predict document topics (such as document A is 60% Topic 1, and 40% topic 2). But cant find a way to classify what sentences are Topic 1 and which ones are Topic 2. One way of doing this could be to use sentence boundaries to extract sentences and then use gensim to predict each sentence but im looking for a formal way to do it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nxj9dj,True,,Epiphany925,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nxj9dj/how_to_use_gensim_topic_modeling_to_predict/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nxj9dj/how_to_use_gensim_topic_modeling_to_predict/,30199,1623426895.0,0,,False,,,,,,341
15,,LanguageTechnology,"I have read the paper "" Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency "", which proposed a classical model DMV( Dependency Model with Valence ) for unsupervised dependency parsing.

As far as I know, the conception ""valence"" is used to describe the number of  “action element” which is dependenct by a verb.

But I can't find more details about how the DMV used ""valence"" in aforementioned paper excepted the model's name.

Can anyone solve my doubts? Thank you very much!",t2_437sde26,False,,0,False,"How to understand the ""valence"" of DMV in unsupervised dependency parsing?",[],r/LanguageTechnology,False,6,,0,,False,t3_nxh2t8,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1623449962.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have read the paper &amp;quot; Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency &amp;quot;, which proposed a classical model DMV( Dependency Model with Valence ) for unsupervised dependency parsing.&lt;/p&gt;

&lt;p&gt;As far as I know, the conception &amp;quot;valence&amp;quot; is used to describe the number of  “action element” which is dependenct by a verb.&lt;/p&gt;

&lt;p&gt;But I can&amp;#39;t find more details about how the DMV used &amp;quot;valence&amp;quot; in aforementioned paper excepted the model&amp;#39;s name.&lt;/p&gt;

&lt;p&gt;Can anyone solve my doubts? Thank you very much!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nxh2t8,True,,Boda_Lin,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nxh2t8/how_to_understand_the_valence_of_dmv_in/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nxh2t8/how_to_understand_the_valence_of_dmv_in/,30199,1623421162.0,0,,False,,,,,,510
16,,LanguageTechnology,"I've been trying for weeks to do daily topic modeling in this [methodology](https://github.com/Stveshawn/contextual_topic_identification). What I did was divide my dataset into groups by day and loop through the architecture by mini dataset. The first ""problem"" is that I'm getting a warning that I can't loop in an autoencoder and the second is that my metric values don't change much despite the dataset I use (I'm using average coherence at the end, but if I run the code with another dataset with other texts, the average coherence remains similar). Anyone could tell me what I'm doing wrong and how do I model the daily topics correctly, as I'm doing this without any examples (because I couldn't find any).

Code:

    for i, group in enumerate(data_groups.groups):
        #LDA
        
        #BERT
        
        #Concatenation 
        
        #Autoencoder
        AE = Autoencoder()
        AE.fit(ldabert)
        vec = AE.encoder.predict(ldabert)
        
        #Kmeans
    
        #Metrics

Warning:

    WARNING:tensorflow:5 out of the last 5 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x00000161989E3AF0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.

Metric values I'm having: +- 0.55 in coherence, doesn't change much beyond that",t2_6fzlklsz,False,,0,False,Problem while doing daily topic modeling,[],r/LanguageTechnology,False,6,,0,,False,t3_nxgveu,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1623449395.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been trying for weeks to do daily topic modeling in this &lt;a href=""https://github.com/Stveshawn/contextual_topic_identification""&gt;methodology&lt;/a&gt;. What I did was divide my dataset into groups by day and loop through the architecture by mini dataset. The first &amp;quot;problem&amp;quot; is that I&amp;#39;m getting a warning that I can&amp;#39;t loop in an autoencoder and the second is that my metric values don&amp;#39;t change much despite the dataset I use (I&amp;#39;m using average coherence at the end, but if I run the code with another dataset with other texts, the average coherence remains similar). Anyone could tell me what I&amp;#39;m doing wrong and how do I model the daily topics correctly, as I&amp;#39;m doing this without any examples (because I couldn&amp;#39;t find any).&lt;/p&gt;

&lt;p&gt;Code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i, group in enumerate(data_groups.groups):
    #LDA

    #BERT

    #Concatenation 

    #Autoencoder
    AE = Autoencoder()
    AE.fit(ldabert)
    vec = AE.encoder.predict(ldabert)

    #Kmeans

    #Metrics
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Warning:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WARNING:tensorflow:5 out of the last 5 calls to &amp;lt;function Model.make_predict_function.&amp;lt;locals&amp;gt;.predict_function at 0x00000161989E3AF0&amp;gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Metric values I&amp;#39;m having: +- 0.55 in coherence, doesn&amp;#39;t change much beyond that&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nxgveu,True,,OrranaLhaynher,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nxgveu/problem_while_doing_daily_topic_modeling/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nxgveu/problem_while_doing_daily_topic_modeling/,30199,1623420595.0,0,,False,,,,,,1879
17,,LanguageTechnology,"I recently started, kaggle notebooks don't suggest it's that important.


 But I recently got access to Prodigy, and it took me an embarassngly long time to install it in Pycharm. But the concept and ease of use is  convenient but i am not sure how often business/ orgs require this. 

If you're working in a real world project/Enterprise, what is your exp. with NER. 

Also if you happen to know a repo or list of public projects lemmino.",t2_iu54y,False,,0,False,How often is NER (named entity recognition) a process in your model building process?,[],r/LanguageTechnology,False,6,,0,,False,t3_nwwsuc,False,dark,0.94,,public,15,0,{},,False,[],,False,False,,{},,False,15,,False,False,,False,,[],{},,True,,1623383670.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I recently started, kaggle notebooks don&amp;#39;t suggest it&amp;#39;s that important.&lt;/p&gt;

&lt;p&gt;But I recently got access to Prodigy, and it took me an embarassngly long time to install it in Pycharm. But the concept and ease of use is  convenient but i am not sure how often business/ orgs require this. &lt;/p&gt;

&lt;p&gt;If you&amp;#39;re working in a real world project/Enterprise, what is your exp. with NER. &lt;/p&gt;

&lt;p&gt;Also if you happen to know a repo or list of public projects lemmino.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nwwsuc,True,,redditssexiestguy,,16,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nwwsuc/how_often_is_ner_named_entity_recognition_a/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nwwsuc/how_often_is_ner_named_entity_recognition_a/,30199,1623354870.0,0,,False,,,,,,439
18,,LanguageTechnology,"I am writing a text classifier and want to use MI for feature selection but not sure how to compute when the features are words. If I compute scores for features, say using TFIDF, and then apply MI on it, then I get the same top features as with just TFIDF and there is no improvement in results.

Is there any resource I can refer for the same?",t2_a40fenez,False,,0,False,How to apply Mutual Information for feature selection in Text Classification where features are words?,[],r/LanguageTechnology,False,6,,0,,False,t3_nxa329,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1623425357.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am writing a text classifier and want to use MI for feature selection but not sure how to compute when the features are words. If I compute scores for features, say using TFIDF, and then apply MI on it, then I get the same top features as with just TFIDF and there is no improvement in results.&lt;/p&gt;

&lt;p&gt;Is there any resource I can refer for the same?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nxa329,True,,puzzled-cognition,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nxa329/how_to_apply_mutual_information_for_feature/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nxa329/how_to_apply_mutual_information_for_feature/,30199,1623396557.0,0,,False,,,,,,345
19,,LanguageTechnology,"Hey, I've created a tutorial on how to replace missing values in a data frame by the column mean (i.e. mean imputation) using the R programming language: [https://statisticsglobe.com/replace-missing-values-by-column-mean-in-r](https://statisticsglobe.com/replace-missing-values-by-column-mean-in-r)",t2_77cigax1,False,,0,False,Tutorial on how to replace missing values in a data frame by the column mean (i.e. mean imputation) using R,[],r/LanguageTechnology,False,6,,0,,False,t3_nxb4wm,False,dark,0.29,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1623429643.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, I&amp;#39;ve created a tutorial on how to replace missing values in a data frame by the column mean (i.e. mean imputation) using the R programming language: &lt;a href=""https://statisticsglobe.com/replace-missing-values-by-column-mean-in-r""&gt;https://statisticsglobe.com/replace-missing-values-by-column-mean-in-r&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nxb4wm,True,,JoachimSchork,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nxb4wm/tutorial_on_how_to_replace_missing_values_in_a/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nxb4wm/tutorial_on_how_to_replace_missing_values_in_a/,30199,1623400843.0,0,,False,,,,,,298
20,,LanguageTechnology,"Hi everyone,

For my project I have a set of word vectors which I have to classify in an unsupervised manner to identify topics, similar concepts, etc. The requirement is to allow each word to belong to multiple topics (overlapping), and allow the topics to include other subtopics (hierarchical).

The problem is that because I don't have documents it's not straightforward to apply topic modeling ideas here.

I know one can look at the problem from pure clustering perspective and use kmeans/GMMs/HDBSCAN/deep learning based clustering, but the problem is that most of such methods assume non-overlapping or non-hierarchical clusters, and there's little research on hierarchical overlapping clustering.

I've been also thinking to leverage community detection methods on graphs, as it's possible to treat each word as a node, however, such methods could be computationally expensive, and I just want to make sure there's no a more natural choice before pursuing this.

Would appreciate any ideas, thank you!",t2_jzgetov,False,,0,False,Clustering word vectors for topic discovery without the actual documents (overlapping &amp; hierarchical),[],r/LanguageTechnology,False,6,,0,,False,t3_nwk09n,False,dark,1.0,,public,20,0,{},,False,[],,False,False,,{},,False,20,,False,False,,False,,[],{},,True,,1623348719.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;For my project I have a set of word vectors which I have to classify in an unsupervised manner to identify topics, similar concepts, etc. The requirement is to allow each word to belong to multiple topics (overlapping), and allow the topics to include other subtopics (hierarchical).&lt;/p&gt;

&lt;p&gt;The problem is that because I don&amp;#39;t have documents it&amp;#39;s not straightforward to apply topic modeling ideas here.&lt;/p&gt;

&lt;p&gt;I know one can look at the problem from pure clustering perspective and use kmeans/GMMs/HDBSCAN/deep learning based clustering, but the problem is that most of such methods assume non-overlapping or non-hierarchical clusters, and there&amp;#39;s little research on hierarchical overlapping clustering.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been also thinking to leverage community detection methods on graphs, as it&amp;#39;s possible to treat each word as a node, however, such methods could be computationally expensive, and I just want to make sure there&amp;#39;s no a more natural choice before pursuing this.&lt;/p&gt;

&lt;p&gt;Would appreciate any ideas, thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nwk09n,True,,vlfom,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nwk09n/clustering_word_vectors_for_topic_discovery/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nwk09n/clustering_word_vectors_for_topic_discovery/,30199,1623319919.0,0,,False,,,,,,1010
21,,LanguageTechnology,"I'm working on a question answering system based on the Wikipedia API. A problem is that ""wikipedia.search"" yields a couple of articles, the first one not always being the best one. Any ideas how to chose the most appropriate article from the results?",t2_1a5ug84,False,,0,False,Question answering with Wikipedia API,[],r/LanguageTechnology,False,6,,0,,False,t3_nwk5sp,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1623349324.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working on a question answering system based on the Wikipedia API. A problem is that &amp;quot;wikipedia.search&amp;quot; yields a couple of articles, the first one not always being the best one. Any ideas how to chose the most appropriate article from the results?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nwk5sp,True,,johannadambergk,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nwk5sp/question_answering_with_wikipedia_api/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nwk5sp/question_answering_with_wikipedia_api/,30199,1623320524.0,0,,False,,,,,,251
22,,LanguageTechnology,"Hi everyone,

I'm currently learning about text summarization and I'd really like to fine-tune a transformer model for summarizing speeches (i.e. texts of speeches by political leaders or parliament debates, NOT voice recordings).

Does anyone know about an English dataset containing both the speeches and the respective summaries? It would also be great if someone had an idea as to where one could source such a dataset (e.g. websites from parliaments). I already checked the websites of the US senate, UK government etc., but no luck so far.

Thank you!",t2_b2c5gp0a,False,,0,False,Speech summarization datasets,[],r/LanguageTechnology,False,6,,0,,False,t3_nwmezs,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1623356976.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m currently learning about text summarization and I&amp;#39;d really like to fine-tune a transformer model for summarizing speeches (i.e. texts of speeches by political leaders or parliament debates, NOT voice recordings).&lt;/p&gt;

&lt;p&gt;Does anyone know about an English dataset containing both the speeches and the respective summaries? It would also be great if someone had an idea as to where one could source such a dataset (e.g. websites from parliaments). I already checked the websites of the US senate, UK government etc., but no luck so far.&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nwmezs,True,,looking_for_speeches,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nwmezs/speech_summarization_datasets/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nwmezs/speech_summarization_datasets/,30199,1623328176.0,0,,False,,,,,,557
23,,LanguageTechnology,,t2_8l6cewm2,False,,0,False,The creators of GPT-Neo just released a 6B parameter open-source version of GPT-3 called GPT-J-6B,[],r/LanguageTechnology,False,6,,0,,False,t3_nw6yx3,False,dark,0.89,,public,7,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/6w5sgWo68E0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Introducing GPT-J -- An Open Source Version Of GPT-3 (NLP News)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/6w5sgWo68E0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Vennify AI', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/6w5sgWo68E0/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC7-EWrr8YdcQgPPk76OiUVw'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/6w5sgWo68E0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nw6yx3', 'height': 200}",,False,7,,False,False,,False,,[],{},,False,,1623303219.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nw6yx3,True,,VennifyAI,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nw6yx3/the_creators_of_gptneo_just_released_a_6b/,all_ads,False,https://www.youtube.com/watch?v=6w5sgWo68E0,30199,1623274419.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Introducing GPT-J -- An Open Source Version Of GPT-3 (NLP News)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/6w5sgWo68E0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Vennify AI', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/6w5sgWo68E0/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC7-EWrr8YdcQgPPk76OiUVw'}}",False,https://www.youtube.com/watch?v=6w5sgWo68E0,,,,,0
24,,LanguageTechnology,"Google researcher’s new study suggests modifying the conventional transformer architecture to process byte sequences in natural language processing (NLP). The new competitive byte-level models can effectively balance computational cost trade-offs of contemporary large language models.

Tokenization splits the sentences into a sequence of tokens. Most NLP tasks follow a tokenization procedure to preprocess the data. However, tokenization can struggle with typos, irregularities in spelling and capitalization, morphological changes, and out-of-vocabulary tokenization problems.

Summary: [https://www.marktechpost.com/2021/06/08/google-ai-introduces-byt5-pre-trained-byte-to-byte-models-for-nlp-tasks/](https://www.marktechpost.com/2021/06/08/google-ai-introduces-byt5-pre-trained-byte-to-byte-models-for-nlp-tasks/) 

GitHub: https://github.com/google-research/byt5

Paper: https://arxiv.org/abs/2105.13626",t2_2wsvqwhg,False,,0,False,Google AI Introduces ByT5: Pre-Trained Byte-to-Byte Models for NLP Tasks,[],r/LanguageTechnology,False,6,,0,,False,t3_nvot6o,False,dark,1.0,,public,47,0,{},,False,[],,False,False,,{},,False,47,,False,False,,False,,[],{},,True,,1623247927.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Google researcher’s new study suggests modifying the conventional transformer architecture to process byte sequences in natural language processing (NLP). The new competitive byte-level models can effectively balance computational cost trade-offs of contemporary large language models.&lt;/p&gt;

&lt;p&gt;Tokenization splits the sentences into a sequence of tokens. Most NLP tasks follow a tokenization procedure to preprocess the data. However, tokenization can struggle with typos, irregularities in spelling and capitalization, morphological changes, and out-of-vocabulary tokenization problems.&lt;/p&gt;

&lt;p&gt;Summary: &lt;a href=""https://www.marktechpost.com/2021/06/08/google-ai-introduces-byt5-pre-trained-byte-to-byte-models-for-nlp-tasks/""&gt;https://www.marktechpost.com/2021/06/08/google-ai-introduces-byt5-pre-trained-byte-to-byte-models-for-nlp-tasks/&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;GitHub: &lt;a href=""https://github.com/google-research/byt5""&gt;https://github.com/google-research/byt5&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/abs/2105.13626""&gt;https://arxiv.org/abs/2105.13626&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nvot6o,True,,ai-lover,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nvot6o/google_ai_introduces_byt5_pretrained_bytetobyte/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nvot6o/google_ai_introduces_byt5_pretrained_bytetobyte/,30199,1623219127.0,0,,False,,,,,,910
25,,LanguageTechnology,"Hi all,

What is the best word embedding to use for automatic text summarization? I want to make extractive summaries of documents.",t2_1i3ta8mq,False,,0,False,Language Model for Summarization,[],r/LanguageTechnology,False,6,,0,,False,t3_nvxizn,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1623278427.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;What is the best word embedding to use for automatic text summarization? I want to make extractive summaries of documents.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nvxizn,True,,panteahk,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nvxizn/language_model_for_summarization/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nvxizn/language_model_for_summarization/,30199,1623249627.0,0,,False,,,,,,131
26,,LanguageTechnology,"Hey, 

Given that I have a Named Entity extractor trained over a BERT pre-trained model, is it possible to utilize the already computed attention scores for extracting Relations between these entities?  

Obviously, categorizing the active relations is still a challenge, but is it possible to detect if a relation is active only by using the attention scores? Specifically if the BERT model is only trained for NER.",t2_1uz63xco,False,,0,False,Unsupervised Relation Extraction using BERT attention scores,[],r/LanguageTechnology,False,6,,0,,False,t3_nvvvzs,False,dark,0.88,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1623273906.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, &lt;/p&gt;

&lt;p&gt;Given that I have a Named Entity extractor trained over a BERT pre-trained model, is it possible to utilize the already computed attention scores for extracting Relations between these entities?  &lt;/p&gt;

&lt;p&gt;Obviously, categorizing the active relations is still a challenge, but is it possible to detect if a relation is active only by using the attention scores? Specifically if the BERT model is only trained for NER.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nvvvzs,True,,pauloamed,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nvvvzs/unsupervised_relation_extraction_using_bert/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nvvvzs/unsupervised_relation_extraction_using_bert/,30199,1623245106.0,0,,False,,,,,,416
27,,LanguageTechnology,"I would like to extract relationships in plain text between two named entities.  

I first wanted to try with Machine Learning but it's complicated to find an annotated corpus for training ... Then I wanted to create patterns (for example: PERSON live LOCATION) but it is not not very precise because I'm trying to find relationships between each pair of named entities (and honestly it takes a long time to write a good dictionary).

Do you have any suggestions for doing this more efficiently please? Maybe a corpus that exists, an algorithm next to which I pass ? 

Thaaaanks :)",t2_66evjh50,False,,0,False,Relationship extraction,[],r/LanguageTechnology,False,6,,0,,False,t3_nvuwiw,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1623271033.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I would like to extract relationships in plain text between two named entities.  &lt;/p&gt;

&lt;p&gt;I first wanted to try with Machine Learning but it&amp;#39;s complicated to find an annotated corpus for training ... Then I wanted to create patterns (for example: PERSON live LOCATION) but it is not not very precise because I&amp;#39;m trying to find relationships between each pair of named entities (and honestly it takes a long time to write a good dictionary).&lt;/p&gt;

&lt;p&gt;Do you have any suggestions for doing this more efficiently please? Maybe a corpus that exists, an algorithm next to which I pass ? &lt;/p&gt;

&lt;p&gt;Thaaaanks :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nvuwiw,True,,happisland,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nvuwiw/relationship_extraction/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nvuwiw/relationship_extraction/,30199,1623242233.0,0,,False,,,,,,581
28,,LanguageTechnology,"I need to generate german paraphrases and I was already looking at some huggingface models which work really well for english sentences. For example tuner007/pegasus\_paraphrase or Vamsi/T5\_Paraphrase\_Paws.

    tokenizer = PegasusTokenizer.from_pretrained(model_name)
    model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device) 

I can't find any german models for paraphrasing on Huggingface. How do I build a model on my own? Is this the best way to generate german paraphrases with transformers or should I use other methods? Thanks!",t2_6fr49wdr,False,,0,False,How to build a model for german paraphrase generation?,[],r/LanguageTechnology,False,6,,0,,False,t3_nvr3zl,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1623257600.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I need to generate german paraphrases and I was already looking at some huggingface models which work really well for english sentences. For example tuner007/pegasus_paraphrase or Vamsi/T5_Paraphrase_Paws.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I can&amp;#39;t find any german models for paraphrasing on Huggingface. How do I build a model on my own? Is this the best way to generate german paraphrases with transformers or should I use other methods? Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nvr3zl,True,,LargeBrick7,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nvr3zl/how_to_build_a_model_for_german_paraphrase/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nvr3zl/how_to_build_a_model_for_german_paraphrase/,30199,1623228800.0,0,,False,,,,,,568
29,,LanguageTechnology,,t2_6ib7gcgr,False,,0,False,[d] Inspecting Neural Networks with Canonical Correlation Analysis (CKA/SVCCA Video),[],r/LanguageTechnology,False,6,,0,,False,t3_nvr1cf,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,False,,1623257288.0,text,6,,,text,self.MachineLearning,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nvr1cf,True,,jayalammar,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nvr1cf/d_inspecting_neural_networks_with_canonical/,all_ads,False,/r/MachineLearning/comments/nvr0v0/d_inspecting_neural_networks_with_canonical/,30199,1623228488.0,0,,False,/r/MachineLearning/comments/nvr0v0/d_inspecting_neural_networks_with_canonical/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'Canonical Correlation Analysis is one of the methods used to explore deep neural networks. Methods like CKA and SVCCA reveal to us insights into how a neural network processes its inputs. This is often done by using CKA and SVCCA as a similarity measure for different activation matrices. In this video, we look at a number of papers that compare different neural networks together. We also look at papers that compare the representations of the various layers of a neural network. \n\n[https://www.youtube.com/watch?v=u7Dvb\\_a1D-0](https://www.youtube.com/watch?v=u7Dvb_a1D-0)\n\n&amp;#x200B;\n\nPapers covered:\n\n   \nSVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability  \n[https://arxiv.org/pdf/1706.05806.pdf](https://arxiv.org/pdf/1706.05806.pdf)\n\nUnderstanding Learning Dynamics Of Language Models with SVCCA   \n[https://arxiv.org/pdf/1811.00225.pdf](https://arxiv.org/pdf/1811.00225.pdf)  \n\nInsights on representational similarity in neural networks with canonical correlation   \n[https://arxiv.org/pdf/1806.05759.pdf](https://arxiv.org/pdf/1806.05759.pdf)  \n\nBERT is Not an Interlingua and the Bias of Tokenization   \n[https://www.aclweb.org/anthology/D19-6106.pdf](https://www.aclweb.org/anthology/D19-6106.pdf)  \n\nSimilarity of Neural Network Representations Revisited   \n[http://proceedings.mlr.press/v97/kornblith19a/kornblith19a.pdf](http://proceedings.mlr.press/v97/kornblith19a/kornblith19a.pdf)  \n\nSimilarity Analysis of Contextual Word Representation Models   \n[https://arxiv.org/pdf/2005.01172.pdf](https://arxiv.org/pdf/2005.01172.pdf)  \n\nDo Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth   \n[https://arxiv.org/pdf/2010.15327.pdf](https://arxiv.org/pdf/2010.15327.pdf)', 'author_fullname': 't2_6ib7gcgr', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[d] Inspecting Neural Networks with Canonical Correlation Analysis (CKA/SVCCA Video)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_nvr0v0', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1623257236.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Canonical Correlation Analysis is one of the methods used to explore deep neural networks. Methods like CKA and SVCCA reveal to us insights into how a neural network processes its inputs. This is often done by using CKA and SVCCA as a similarity measure for different activation matrices. In this video, we look at a number of papers that compare different neural networks together. We also look at papers that compare the representations of the various layers of a neural network. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.youtube.com/watch?v=u7Dvb_a1D-0""&gt;https://www.youtube.com/watch?v=u7Dvb_a1D-0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Papers covered:&lt;/p&gt;\n\n&lt;p&gt;SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability&lt;br/&gt;\n&lt;a href=""https://arxiv.org/pdf/1706.05806.pdf""&gt;https://arxiv.org/pdf/1706.05806.pdf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Understanding Learning Dynamics Of Language Models with SVCCA&lt;br/&gt;\n&lt;a href=""https://arxiv.org/pdf/1811.00225.pdf""&gt;https://arxiv.org/pdf/1811.00225.pdf&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Insights on representational similarity in neural networks with canonical correlation&lt;br/&gt;\n&lt;a href=""https://arxiv.org/pdf/1806.05759.pdf""&gt;https://arxiv.org/pdf/1806.05759.pdf&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;BERT is Not an Interlingua and the Bias of Tokenization&lt;br/&gt;\n&lt;a href=""https://www.aclweb.org/anthology/D19-6106.pdf""&gt;https://www.aclweb.org/anthology/D19-6106.pdf&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Similarity of Neural Network Representations Revisited&lt;br/&gt;\n&lt;a href=""http://proceedings.mlr.press/v97/kornblith19a/kornblith19a.pdf""&gt;http://proceedings.mlr.press/v97/kornblith19a/kornblith19a.pdf&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Similarity Analysis of Contextual Word Representation Models&lt;br/&gt;\n&lt;a href=""https://arxiv.org/pdf/2005.01172.pdf""&gt;https://arxiv.org/pdf/2005.01172.pdf&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth&lt;br/&gt;\n&lt;a href=""https://arxiv.org/pdf/2010.15327.pdf""&gt;https://arxiv.org/pdf/2010.15327.pdf&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'nvr0v0', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'jayalammar', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/nvr0v0/d_inspecting_neural_networks_with_canonical/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/nvr0v0/d_inspecting_neural_networks_with_canonical/', 'subreddit_subscribers': 1930685, 'created_utc': 1623228436.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_nvr0v0,,,0
30,,LanguageTechnology,,t2_an5dtfk8,False,,0,False,"I am experimenting with using NLP to measure market sentiment with custom classifiers for neutrality, FUD, hype etc. stuff typical for crypto world. I trained my classifiers in natural.js with data I pulled from social media. This is my attempt number one",[],r/LanguageTechnology,False,6,,0,,False,t3_nvycft,False,dark,0.33,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,False,,1623280649.0,text,6,,,text,comint.ai,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nvycft,True,,flexxxatron,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nvycft/i_am_experimenting_with_using_nlp_to_measure/,all_ads,False,https://comint.ai/coins/btc,30199,1623251849.0,0,,False,https://comint.ai/coins/btc,,,,,0
31,,LanguageTechnology,"For example:

exist exists existed existing 

run ran running 

And so forth?",t2_aw7plin,False,,0,False,Is there a text list of words and their variations?,[],r/LanguageTechnology,False,6,,0,,False,t3_nv89vk,False,dark,0.89,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1623198866.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For example:&lt;/p&gt;

&lt;p&gt;exist exists existed existing &lt;/p&gt;

&lt;p&gt;run ran running &lt;/p&gt;

&lt;p&gt;And so forth?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nv89vk,True,,blessedarethegeek,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nv89vk/is_there_a_text_list_of_words_and_their_variations/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nv89vk/is_there_a_text_list_of_words_and_their_variations/,30199,1623170066.0,0,,False,,,,,,77
32,,LanguageTechnology,"Very interesting job to teach at a new [MSc. in Voice technology](https://www.rug.nl/masters/voice-technology/)!

Key points:

* English language program
* 80-100% full-time position (depending on how many classes you want to teach)
* Balance between teaching and research is 60/40 (really!)

In addition to supervising theses within your area of expertise, you will support the teaching and/or curriculum development of courses in speech synthesis, speech recognition, Python, and machine learning for voice tech (all courses already have detailed week-by-week descriptions but lack student-ready syllabi, giving you some creative freedom -- more information about the courses, including learning outcomes, is available upon request):

● Speech Synthesis I and II  
● Speech Recognition I and II  
● Python for Voice Technology (and Intro to Python at the undergraduate level)  
● Machine Learning for Voice Technology

[More details](https://www.rug.nl/about-ug/work-with-us/job-opportunities/?details=00347-02S0008E4P)  (qualifications, application procedure, etc.)

Deadline: 13 June 11:59pm (CEST - European time)",t2_8wbdz21a,False,,0,False,Job opportunity: Assistant Professor in Speech Technology at the University of Groningen (the Netherlands),[],r/LanguageTechnology,False,6,,0,,False,t3_nuzw46,False,dark,0.92,,public,24,0,{},,False,[],,False,False,,{},,False,24,,False,False,,False,,[],{},,True,,1623169612.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Very interesting job to teach at a new &lt;a href=""https://www.rug.nl/masters/voice-technology/""&gt;MSc. in Voice technology&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;Key points:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;English language program&lt;/li&gt;
&lt;li&gt;80-100% full-time position (depending on how many classes you want to teach)&lt;/li&gt;
&lt;li&gt;Balance between teaching and research is 60/40 (really!)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition to supervising theses within your area of expertise, you will support the teaching and/or curriculum development of courses in speech synthesis, speech recognition, Python, and machine learning for voice tech (all courses already have detailed week-by-week descriptions but lack student-ready syllabi, giving you some creative freedom -- more information about the courses, including learning outcomes, is available upon request):&lt;/p&gt;

&lt;p&gt;● Speech Synthesis I and II&lt;br/&gt;
● Speech Recognition I and II&lt;br/&gt;
● Python for Voice Technology (and Intro to Python at the undergraduate level)&lt;br/&gt;
● Machine Learning for Voice Technology&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.rug.nl/about-ug/work-with-us/job-opportunities/?details=00347-02S0008E4P""&gt;More details&lt;/a&gt;  (qualifications, application procedure, etc.)&lt;/p&gt;

&lt;p&gt;Deadline: 13 June 11:59pm (CEST - European time)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nuzw46,True,,VoiceTech,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nuzw46/job_opportunity_assistant_professor_in_speech/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nuzw46/job_opportunity_assistant_professor_in_speech/,30199,1623140812.0,0,,False,,,,,,1118
33,,LanguageTechnology,"I saw this paper and thought it was an interesting application of sentiment analysis. 

[https://arxiv.org/abs/2106.00665](https://arxiv.org/abs/2106.00665)

The paper proposes using GAN-BERT with BioBERT for 3 class sentiment classification in clinical trial abstracts as a way to assess reporting trends in literature. 

They also found that the accuracy of the algorithm was far better than using an expert rater (which is the standard right now in clinical literature for these types of studies).

I'm curious what the r/LanguageTechnology world thinks; This seems like a really cool application but I'm a total novice when it comes to NLP (coming from a signal processing background)",t2_buawjx1q,False,,0,False,GAN-BioBERT: A Methodology For Assessing Reporting Trends In Clinical Trials (Paper Summary),[],r/LanguageTechnology,False,6,,0,,False,t3_nv5yb8,False,dark,0.78,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1623193039.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I saw this paper and thought it was an interesting application of sentiment analysis. &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://arxiv.org/abs/2106.00665""&gt;https://arxiv.org/abs/2106.00665&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The paper proposes using GAN-BERT with BioBERT for 3 class sentiment classification in clinical trial abstracts as a way to assess reporting trends in literature. &lt;/p&gt;

&lt;p&gt;They also found that the accuracy of the algorithm was far better than using an expert rater (which is the standard right now in clinical literature for these types of studies).&lt;/p&gt;

&lt;p&gt;I&amp;#39;m curious what the &lt;a href=""/r/LanguageTechnology""&gt;r/LanguageTechnology&lt;/a&gt; world thinks; This seems like a really cool application but I&amp;#39;m a total novice when it comes to NLP (coming from a signal processing background)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nv5yb8,True,,Jornkatarn,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nv5yb8/ganbiobert_a_methodology_for_assessing_reporting/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nv5yb8/ganbiobert_a_methodology_for_assessing_reporting/,30199,1623164239.0,0,,False,,,,,,688
34,,LanguageTechnology,I want to do a contextual analysis on two topics and learn the in what  context some common keywords has been used in both of the topics. Thank  you.,t2_4rzsj2ib,False,,0,False,"Need direction related a project, like what topics(study material) should I look into.",[],r/LanguageTechnology,False,6,,0,,False,t3_nvaccr,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1623204330.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to do a contextual analysis on two topics and learn the in what  context some common keywords has been used in both of the topics. Thank  you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nvaccr,True,,mrbrownstone07__,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nvaccr/need_direction_related_a_project_like_what/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nvaccr/need_direction_related_a_project_like_what/,30199,1623175530.0,0,,False,,,,,,149
35,,LanguageTechnology,,t2_8hz4mq4a,False,,0,False,Benefits of Using PHP for Web Development,[],r/LanguageTechnology,False,6,,0,,False,t3_nvqkl1,False,dark,0.14,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,False,,1623255313.0,text,6,,,text,coresumo.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nvqkl1,True,,mcscs,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nvqkl1/benefits_of_using_php_for_web_development/,all_ads,False,https://coresumo.com/benefits-of-using-php-for-web-development-2021/,30199,1623226513.0,0,,False,https://coresumo.com/benefits-of-using-php-for-web-development-2021/,,,,,0
36,,LanguageTechnology,"Hello,
I am planning on studying NLP in the fall and have been accepted to two programs that really interest me.
They are the University of Strasbourg's Master in Language Technology and the PluriTAL program in Paris that is organized by Paris 3, Paris Nanterre, and Inalco.
I was wondering if anyone here has studied in one of these programs or knows anyone that has and would have any pros and cons between them.

Thank you for your help!",t2_m9rqx,False,,0,False,Advice - Choosing a Master's in NLP (France),[],r/LanguageTechnology,False,6,,0,,False,t3_nuylqi,False,dark,0.95,,public,19,0,{},,False,[],,False,False,,{},,False,19,,False,False,,False,,[],{},,True,,1623164305.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,
I am planning on studying NLP in the fall and have been accepted to two programs that really interest me.
They are the University of Strasbourg&amp;#39;s Master in Language Technology and the PluriTAL program in Paris that is organized by Paris 3, Paris Nanterre, and Inalco.
I was wondering if anyone here has studied in one of these programs or knows anyone that has and would have any pros and cons between them.&lt;/p&gt;

&lt;p&gt;Thank you for your help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nuylqi,True,,ConnorIsGreat,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nuylqi/advice_choosing_a_masters_in_nlp_france/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nuylqi/advice_choosing_a_masters_in_nlp_france/,30199,1623135505.0,0,,False,,,,,,440
37,,LanguageTechnology," 

I'm doing topic modeling for the first time in my life and I have a problem. My intention is to model daily topics, but my number of daily samples varies a lot, from 5 samples in one day to 100 in another, for example. The desired number of topics is 7, so I have problems from the first day of the dataset.

The methodology I'm following is [this](https://blog.insightdatascience.com/contextual-topic-identification-4291d256a032).

Then the vector resulting from the LDA+BERT concatenation is passed in an Autoencoder and then used in a clustering model. This is where I have the problem at hand. My number of clusters is 7, but the representation vectors are 5.

With this I have the error:

ValueError: n\_samples=5 should be &gt;= n\_clusters=7.

Does anyone know how I could fix this?",t2_6fzlklsz,False,,0,False,Clustering latent representation vectors with a size less than the number of clusters,[],r/LanguageTechnology,False,6,,0,,False,t3_nv1y5k,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1623180908.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m doing topic modeling for the first time in my life and I have a problem. My intention is to model daily topics, but my number of daily samples varies a lot, from 5 samples in one day to 100 in another, for example. The desired number of topics is 7, so I have problems from the first day of the dataset.&lt;/p&gt;

&lt;p&gt;The methodology I&amp;#39;m following is &lt;a href=""https://blog.insightdatascience.com/contextual-topic-identification-4291d256a032""&gt;this&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Then the vector resulting from the LDA+BERT concatenation is passed in an Autoencoder and then used in a clustering model. This is where I have the problem at hand. My number of clusters is 7, but the representation vectors are 5.&lt;/p&gt;

&lt;p&gt;With this I have the error:&lt;/p&gt;

&lt;p&gt;ValueError: n_samples=5 should be &amp;gt;= n_clusters=7.&lt;/p&gt;

&lt;p&gt;Does anyone know how I could fix this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nv1y5k,True,,OrranaLhaynher,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nv1y5k/clustering_latent_representation_vectors_with_a/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nv1y5k/clustering_latent_representation_vectors_with_a/,30199,1623152108.0,0,,False,,,,,,792
38,,LanguageTechnology,,t2_l6ugg,False,,0,False,FREE WEBINAR - Automating Data Annotation with MicroModels - Automating processes within the workflow to improve efficiency &amp; guarantee high quality,[],r/LanguageTechnology,False,6,,0,,False,t3_nv0z0r,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1623174055.0,text,6,,,text,re-work.co,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nv0z0r,True,,nikitaljohnson,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nv0z0r/free_webinar_automating_data_annotation_with/,all_ads,False,https://www.re-work.co/events/webinar:automating-data-annotation-with-micromodels?utm_source=Promo&amp;utm_medium=Promo&amp;utm_campaign=LK_Promo_Sama_Webinar,30199,1623145255.0,0,,False,https://www.re-work.co/events/webinar:automating-data-annotation-with-micromodels?utm_source=Promo&amp;utm_medium=Promo&amp;utm_campaign=LK_Promo_Sama_Webinar,,,,,0
39,,LanguageTechnology,"Hi all. I have a little problem I am facing and I'm not quite sure where I should start looking. I have an application where I want to identify sentences inline as the user types. The goal is to operate on the sentences and provide feedback to the user without them needing to indicate the sentence as completed.

Does anyone have any ideas on how to effectively identify complete sentences as they are being typed? The quick solutions I've thought of seem a little too simple and error prone. 

Please let me know.",t2_3wufr1fg,False,,0,False,How to identify sentences from a stream of characters?,[],r/LanguageTechnology,False,6,,0,,False,t3_nuvgii,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1623152430.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all. I have a little problem I am facing and I&amp;#39;m not quite sure where I should start looking. I have an application where I want to identify sentences inline as the user types. The goal is to operate on the sentences and provide feedback to the user without them needing to indicate the sentence as completed.&lt;/p&gt;

&lt;p&gt;Does anyone have any ideas on how to effectively identify complete sentences as they are being typed? The quick solutions I&amp;#39;ve thought of seem a little too simple and error prone. &lt;/p&gt;

&lt;p&gt;Please let me know.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nuvgii,True,,mr_super_doodle,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nuvgii/how_to_identify_sentences_from_a_stream_of/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nuvgii/how_to_identify_sentences_from_a_stream_of/,30199,1623123630.0,0,,False,,,,,,515
40,,LanguageTechnology,,t2_v7nu2,False,,0,False,"John Snow Labs Spark-NLP 3.1.0: Over 2600+ new models and pipelines in 200+ languages, new DistilBERT, RoBERTa, and XLM-RoBERTa transformers, support for external Transformers, and lots more!",[],r/LanguageTechnology,False,6,,0,,False,t3_nufyy8,False,dark,0.91,,public,26,0,{},,False,[],,False,False,,{},,False,26,,False,False,,False,,[],{},,False,,1623110201.0,text,6,,,text,github.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nufyy8,True,,dark-night-rises,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nufyy8/john_snow_labs_sparknlp_310_over_2600_new_models/,all_ads,False,https://github.com/JohnSnowLabs/spark-nlp/releases/tag/3.1.0,30199,1623081401.0,0,,False,https://github.com/JohnSnowLabs/spark-nlp/releases/tag/3.1.0,,,,,0
41,,LanguageTechnology,"I tried to find some well-summarized answers, but there really isn't any.

I can only guess some combination of:

* Query Expansion
* Query Embedding and Finding Semantically Similar Queries
* Query-Answer scores(from the field data) for raking PAA suggestions
* Text Summarization(for the snippet of answers)
* Natural Language Generation(for the ""general"" form of questions)

Anyone can give a bit more detailed version of what is actually going on behind?

Thanks!",t2_cidssi7w,False,,0,False,"What's the algorithm pipeline for Google's ""People Also Ask""?",[],r/LanguageTechnology,False,6,,0,,False,t3_nuo2xl,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1623130011.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I tried to find some well-summarized answers, but there really isn&amp;#39;t any.&lt;/p&gt;

&lt;p&gt;I can only guess some combination of:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Query Expansion&lt;/li&gt;
&lt;li&gt;Query Embedding and Finding Semantically Similar Queries&lt;/li&gt;
&lt;li&gt;Query-Answer scores(from the field data) for raking PAA suggestions&lt;/li&gt;
&lt;li&gt;Text Summarization(for the snippet of answers)&lt;/li&gt;
&lt;li&gt;Natural Language Generation(for the &amp;quot;general&amp;quot; form of questions)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Anyone can give a bit more detailed version of what is actually going on behind?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nuo2xl,True,,nlp_ttt,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nuo2xl/whats_the_algorithm_pipeline_for_googles_people/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nuo2xl/whats_the_algorithm_pipeline_for_googles_people/,30199,1623101211.0,0,,False,,,,,,467
42,,LanguageTechnology,,t2_qig8p,False,,0,False,"google-research/mozolm - A language model serving library, with middleware functionality including mixing of probabilities from disparate base language model types and tokenizations along with RPC client/server interactions.",[],r/LanguageTechnology,False,6,,0,,False,t3_nunqg8,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1623129117.0,text,6,,,text,github.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nunqg8,True,,acecentre,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nunqg8/googleresearchmozolm_a_language_model_serving/,all_ads,False,https://github.com/google-research/mozolm,30199,1623100317.0,0,,False,https://github.com/google-research/mozolm,,,,,0
43,,LanguageTechnology,,t2_qig8p,False,,0,False,"Building a corpus of AAC users speech/writing: ""Know of any AAC users who want a faster system? This research study aims to build a large open database for researchers and developers to make far better systems. End users can now help this effort directly""",[],r/LanguageTechnology,False,6,,0,,False,t3_nunmmp,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1623128853.0,text,6,,,text,spark.adobe.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nunmmp,True,,acecentre,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nunmmp/building_a_corpus_of_aac_users_speechwriting_know/,all_ads,False,https://spark.adobe.com/video/F8MNMohwepvrG,30199,1623100053.0,0,,False,https://spark.adobe.com/video/F8MNMohwepvrG,"[{'approved_at_utc': None, 'subreddit': 'slp', 'selftext': '', 'author_fullname': 't2_qig8p', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Know of any AAC users who want a faster system? This research study aims to build a large open database for researchers and developers to make far better systems. End users can now help this effort directly', 'link_flair_richtext': [{'e': 'text', 't': '[AAC]'}], 'subreddit_name_prefixed': 'r/slp', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_ntzce4', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': '[AAC]', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1623053343.0, 'link_flair_type': 'richtext', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'spark.adobe.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://spark.adobe.com/video/F8MNMohwepvrG', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '15944b8a-a920-11e2-995a-12313d1841d1', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2sjju', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'ntzce4', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'acecentre', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/slp/comments/ntzce4/know_of_any_aac_users_who_want_a_faster_system/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://spark.adobe.com/video/F8MNMohwepvrG', 'subreddit_subscribers': 26127, 'created_utc': 1623024543.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_ntzce4,,,0
44,,LanguageTechnology,,t2_x7cw3eo,False,,0,False,Preventing ‘Hallucination’ In GPT-3 And Other Complex Language Models,[],r/LanguageTechnology,False,6,,0,,False,t3_nu8qyy,False,dark,0.94,,public,14,0,{},,False,[],,False,False,,{},,False,14,,False,False,,False,,[],{},,False,,1623089094.0,text,6,,,text,unite.ai,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nu8qyy,True,,Symbiot10000,,1,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/nu8qyy/preventing_hallucination_in_gpt3_and_other/,all_ads,False,https://www.unite.ai/preventing-hallucination-in-gpt-3-and-other-complex-language-models/,30199,1623060294.0,0,,False,https://www.unite.ai/preventing-hallucination-in-gpt-3-and-other-complex-language-models/,,,,,0
45,,LanguageTechnology,"Most of the pre-trained language models operate on sequences of tokens corresponding to word or subword units. 

This paper proposes Token-free models that instead operate directly on raw bytes. 

https://link.medium.com/qKwrXYXqTgb

P.S. Most of this blog was written from my mobile device. Please excuse brevity and typos.


Actual Paper: https://arxiv.org/pdf/2105.13626v1.pdf",t2_hkv9s,False,,0,False,ByT5: Towards a token-free future with pre-trained byte-to-byte models (Research Paper Summary),[],r/LanguageTechnology,False,6,,0,,False,t3_nu9ty9,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1623093228.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Most of the pre-trained language models operate on sequences of tokens corresponding to word or subword units. &lt;/p&gt;

&lt;p&gt;This paper proposes Token-free models that instead operate directly on raw bytes. &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://link.medium.com/qKwrXYXqTgb""&gt;https://link.medium.com/qKwrXYXqTgb&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;P.S. Most of this blog was written from my mobile device. Please excuse brevity and typos.&lt;/p&gt;

&lt;p&gt;Actual Paper: &lt;a href=""https://arxiv.org/pdf/2105.13626v1.pdf""&gt;https://arxiv.org/pdf/2105.13626v1.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nu9ty9,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nu9ty9/byt5_towards_a_tokenfree_future_with_pretrained/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nu9ty9/byt5_towards_a_tokenfree_future_with_pretrained/,30199,1623064428.0,0,,False,,,,,,379
46,,LanguageTechnology,"I am new to Hugging Face and masked language modelling (MLM), and I was wondering how to include emojis when doing such a task.

I have a dataset with tweets, with each tweet containing an emoji at the end - here is a sample of my data:

| ID |        Tweet |
| -------- | -------------- |
| 1    | Looking good today 😎         |
| 2   | Weather is so hot, lol ☀️          |
| 3  | I hate you!!! 🤬        |

At the moment, I have fully trained my masked language model using my dataset, but when I predict something, it does **NOT** output or predict the emojis. It just predicts words.

This is my desired input from using my dataset for MLM:

```
""You look great [MASK]""
```

This is my desired output from using my dataset for MLM:

```
[{'score': 0.26041436195373535,
  'sequence': 'You look great 😎""',
  'token': 72,
  'token_str': '.""'},
 {'score': 0.1813151091337204,
  'sequence': 'you look great 💯""',
  'token': 2901,
  'token_str': '!""'},
 {'score': 0.14516998827457428,
  'sequence': 'you look great 👌',
  'token': 328,
  'token_str': '!'},]
```

However, this is what I am actually getting from my output:

```
[{'score': 0.26041436195373535,
  'sequence': 'You look great?""',
  'token': 72,
  'token_str': '.""'},
 {'score': 0.1813151091337204,
  'sequence': 'You look great.""',
  'token': 2901,
  'token_str': '!""'},
 {'score': 0.14516998827457428,
  'sequence': 'You look great!',
  'token': 328,
  'token_str': '!'},]
```

I know it is possible to do this, but how do I do it? I am close, but not very.

Likewise, I have my model fully trained on my dataset, but it just does not seem to output emojis, even though I have included them in the training. 

Does something need to be included to accept emoji? If so, what?

Thanks - I would really appreciate the help!",t2_13ussz,False,,0,False,Hugging Face: Unable to use emojis for masked language modelling?,[],r/LanguageTechnology,False,6,,0,,False,t3_nu90dm,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1623090148.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am new to Hugging Face and masked language modelling (MLM), and I was wondering how to include emojis when doing such a task.&lt;/p&gt;

&lt;p&gt;I have a dataset with tweets, with each tweet containing an emoji at the end - here is a sample of my data:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;ID&lt;/th&gt;
&lt;th&gt;Tweet&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Looking good today 😎&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;Weather is so hot, lol ☀️&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;I hate you!!! 🤬&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;At the moment, I have fully trained my masked language model using my dataset, but when I predict something, it does &lt;strong&gt;NOT&lt;/strong&gt; output or predict the emojis. It just predicts words.&lt;/p&gt;

&lt;p&gt;This is my desired input from using my dataset for MLM:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
&amp;quot;You look great [MASK]&amp;quot;
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This is my desired output from using my dataset for MLM:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
[{&amp;#39;score&amp;#39;: 0.26041436195373535,
  &amp;#39;sequence&amp;#39;: &amp;#39;You look great 😎&amp;quot;&amp;#39;,
  &amp;#39;token&amp;#39;: 72,
  &amp;#39;token_str&amp;#39;: &amp;#39;.&amp;quot;&amp;#39;},
 {&amp;#39;score&amp;#39;: 0.1813151091337204,
  &amp;#39;sequence&amp;#39;: &amp;#39;you look great 💯&amp;quot;&amp;#39;,
  &amp;#39;token&amp;#39;: 2901,
  &amp;#39;token_str&amp;#39;: &amp;#39;!&amp;quot;&amp;#39;},
 {&amp;#39;score&amp;#39;: 0.14516998827457428,
  &amp;#39;sequence&amp;#39;: &amp;#39;you look great 👌&amp;#39;,
  &amp;#39;token&amp;#39;: 328,
  &amp;#39;token_str&amp;#39;: &amp;#39;!&amp;#39;},]
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;However, this is what I am actually getting from my output:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
[{&amp;#39;score&amp;#39;: 0.26041436195373535,
  &amp;#39;sequence&amp;#39;: &amp;#39;You look great?&amp;quot;&amp;#39;,
  &amp;#39;token&amp;#39;: 72,
  &amp;#39;token_str&amp;#39;: &amp;#39;.&amp;quot;&amp;#39;},
 {&amp;#39;score&amp;#39;: 0.1813151091337204,
  &amp;#39;sequence&amp;#39;: &amp;#39;You look great.&amp;quot;&amp;#39;,
  &amp;#39;token&amp;#39;: 2901,
  &amp;#39;token_str&amp;#39;: &amp;#39;!&amp;quot;&amp;#39;},
 {&amp;#39;score&amp;#39;: 0.14516998827457428,
  &amp;#39;sequence&amp;#39;: &amp;#39;You look great!&amp;#39;,
  &amp;#39;token&amp;#39;: 328,
  &amp;#39;token_str&amp;#39;: &amp;#39;!&amp;#39;},]
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I know it is possible to do this, but how do I do it? I am close, but not very.&lt;/p&gt;

&lt;p&gt;Likewise, I have my model fully trained on my dataset, but it just does not seem to output emojis, even though I have included them in the training. &lt;/p&gt;

&lt;p&gt;Does something need to be included to accept emoji? If so, what?&lt;/p&gt;

&lt;p&gt;Thanks - I would really appreciate the help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nu90dm,True,,saucyhambon,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nu90dm/hugging_face_unable_to_use_emojis_for_masked/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nu90dm/hugging_face_unable_to_use_emojis_for_masked/,30199,1623061348.0,0,,False,,,,,,1780
47,,LanguageTechnology,"Hello everyone!

I'm an Italian uni student, and I'm going to graduate in foreign languages and literature in October (English and German).

I'm interested in studying CL after my Bachelor (Stuttgart seems to be a good choice, and spending some time in a German-speaking country would enable me to improve my German), but I don't know whether I stand any chance of getting accepted into this programme.

I took a couple of linguistics exams, but since in Italy Bachelors of linguistics don't exist at all, all I could do was enrol in the languages programme. Stuttgart university accepts Bachelors related to linguistics, but I'm still afraid that my degree won't be enough to satisfy their admission requirements.

I know the basics of programming (Python), but I actually don't know what I could do so as to stand a better chance of getting accepted. 

Is there anything else I could do?",t2_5abdti3m,False,,0,False,Master's degree in Computational Linguistics (Stuttgart),[],r/LanguageTechnology,False,6,,0,,False,t3_ntw7yu,False,dark,0.86,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1623043955.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m an Italian uni student, and I&amp;#39;m going to graduate in foreign languages and literature in October (English and German).&lt;/p&gt;

&lt;p&gt;I&amp;#39;m interested in studying CL after my Bachelor (Stuttgart seems to be a good choice, and spending some time in a German-speaking country would enable me to improve my German), but I don&amp;#39;t know whether I stand any chance of getting accepted into this programme.&lt;/p&gt;

&lt;p&gt;I took a couple of linguistics exams, but since in Italy Bachelors of linguistics don&amp;#39;t exist at all, all I could do was enrol in the languages programme. Stuttgart university accepts Bachelors related to linguistics, but I&amp;#39;m still afraid that my degree won&amp;#39;t be enough to satisfy their admission requirements.&lt;/p&gt;

&lt;p&gt;I know the basics of programming (Python), but I actually don&amp;#39;t know what I could do so as to stand a better chance of getting accepted. &lt;/p&gt;

&lt;p&gt;Is there anything else I could do?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ntw7yu,True,,sardina9,,14,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ntw7yu/masters_degree_in_computational_linguistics/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ntw7yu/masters_degree_in_computational_linguistics/,30199,1623015155.0,0,,False,,,,,,889
48,,LanguageTechnology,"Three days ago I made a video explaining how my NLP transformers course would be entirely free as part of a limited-time promo. I shared that video here and in a couple of other subreddits too, r/learnmachinglearning and r/Python being two.

Three days and 10823 downloads later, here we are! I thought we'd be lucky to hit 1K!

Incredible response, and very happy to be able to have been able to give so many of you an opportunity to access the course where some of you may not have been able to otherwise. I'm looking forward to working with all the students and helping you guys out, just please don't all ask me questions at once! 😬

Thanks all, truly humbled by the response - it's really *really* cool, it has blown my mind.

For any of you that are still interested, I will leave a final discount link [here](https://www.udemy.com/course/nlp-with-transformers/?couponCode=MEDIUM), thanks all!",t2_oupz3m9,False,,0,False,More than 10K of you downloaded the free NLP transformers course... Wow!,[],r/LanguageTechnology,False,6,,0,,False,t3_nti8vm,False,dark,0.94,,public,49,1,{},,False,[],,False,False,,{},,False,49,,False,True,,False,,[],{},,True,,1623001846.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Three days ago I made a video explaining how my NLP transformers course would be entirely free as part of a limited-time promo. I shared that video here and in a couple of other subreddits too, &lt;a href=""/r/learnmachinglearning""&gt;r/learnmachinglearning&lt;/a&gt; and &lt;a href=""/r/Python""&gt;r/Python&lt;/a&gt; being two.&lt;/p&gt;

&lt;p&gt;Three days and 10823 downloads later, here we are! I thought we&amp;#39;d be lucky to hit 1K!&lt;/p&gt;

&lt;p&gt;Incredible response, and very happy to be able to have been able to give so many of you an opportunity to access the course where some of you may not have been able to otherwise. I&amp;#39;m looking forward to working with all the students and helping you guys out, just please don&amp;#39;t all ask me questions at once! 😬&lt;/p&gt;

&lt;p&gt;Thanks all, truly humbled by the response - it&amp;#39;s really &lt;em&gt;really&lt;/em&gt; cool, it has blown my mind.&lt;/p&gt;

&lt;p&gt;For any of you that are still interested, I will leave a final discount link &lt;a href=""https://www.udemy.com/course/nlp-with-transformers/?couponCode=MEDIUM""&gt;here&lt;/a&gt;, thanks all!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nti8vm,True,,jamescalam,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nti8vm/more_than_10k_of_you_downloaded_the_free_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nti8vm/more_than_10k_of_you_downloaded_the_free_nlp/,30199,1622973046.0,0,,False,,,,,,899
49,,LanguageTechnology,,t2_l7idt,False,,0,False,Debiasing large pretrained language models using distributional control,[],r/LanguageTechnology,False,6,,0,,False,t3_ntqm8h,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,False,,1623028654.0,text,6,,,text,europe.naverlabs.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ntqm8h,True,,pigdogsheep,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ntqm8h/debiasing_large_pretrained_language_models_using/,all_ads,False,https://europe.naverlabs.com/blog/debiasing-large-pretrained-language-models-using-distributional-control/,30199,1622999854.0,0,,False,https://europe.naverlabs.com/blog/debiasing-large-pretrained-language-models-using-distributional-control/,,,,,0
50,,LanguageTechnology,,t2_m8kccne,False,,0,False,Does anyone know of coreference resolution tools where you can specify the entity?,[],r/LanguageTechnology,False,6,,0,,False,t3_ntb3m6,False,dark,1.0,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,False,,1622973933.0,text,6,,,text,self.MachineLearning,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ntb3m6,True,,Seankala,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ntb3m6/does_anyone_know_of_coreference_resolution_tools/,all_ads,False,/r/MachineLearning/comments/nta7gu/d_does_anyone_know_of_coreference_resolution/,30199,1622945133.0,0,,False,/r/MachineLearning/comments/nta7gu/d_does_anyone_know_of_coreference_resolution/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': ""Hi. Let me elaborate on the title. I'm currently working on paragraph-level data and want to perform coreference resolution. I've tried working with spaCy's [NeuralCoref](https://github.com/huggingface/neuralcoref), and although it works great it receives a string as input and returns all entities and mentions it deems appropriate. Rather than that I'm looking for something where you can specify the entity and the model will return all such instances for that particular entity.\n\nDoes anyone know if something like that exists? Thanks."", 'author_fullname': 't2_m8kccne', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] Does anyone know of coreference resolution tools where you can specify the entity?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_nta7gu', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': 'c5cf3b2a-6abd-11ea-a37b-0ebd427f43f1', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1622970877.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi. Let me elaborate on the title. I&amp;#39;m currently working on paragraph-level data and want to perform coreference resolution. I&amp;#39;ve tried working with spaCy&amp;#39;s &lt;a href=""https://github.com/huggingface/neuralcoref""&gt;NeuralCoref&lt;/a&gt;, and although it works great it receives a string as input and returns all entities and mentions it deems appropriate. Rather than that I&amp;#39;m looking for something where you can specify the entity and the model will return all such instances for that particular entity.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know if something like that exists? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Student', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'nta7gu', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Seankala', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/MachineLearning/comments/nta7gu/d_does_anyone_know_of_coreference_resolution/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/nta7gu/d_does_anyone_know_of_coreference_resolution/', 'subreddit_subscribers': 1930695, 'created_utc': 1622942077.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_nta7gu,,,0
51,,LanguageTechnology,"Hello! I am a PhD researcher. I am looking urgently for someone having used ConvoKit and knowing how to extract coded variables through it. Please reply here or email me at:

[ax23@kent.ac.uk](mailto:ax23@kent.ac.uk)",t2_4yilft17,False,,0,False,ConvoKit corpus research,[],r/LanguageTechnology,False,6,,0,,False,t3_ntia70,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1623002003.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! I am a PhD researcher. I am looking urgently for someone having used ConvoKit and knowing how to extract coded variables through it. Please reply here or email me at:&lt;/p&gt;

&lt;p&gt;[&lt;a href=""mailto:ax23@kent.ac.uk""&gt;ax23@kent.ac.uk&lt;/a&gt;](mailto:&lt;a href=""mailto:ax23@kent.ac.uk""&gt;ax23@kent.ac.uk&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ntia70,True,,annaksig,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ntia70/convokit_corpus_research/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ntia70/convokit_corpus_research/,30199,1622973203.0,0,,False,,,,,,216
52,,LanguageTechnology,"I have followed this tutorial for masked language modelling from Hugging Face using BERT, but I am unsure how to actually deploy the model.

Tutorial: [https://github.com/huggingface/notebooks/blob/master/examples/language\_modeling.ipynb](https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb)

I have trained the model using my own dataset, which has worked fine, but I don't know how to actually use the model, as the notebook does not include an example on how to do this, sadly.

&amp;#x200B;

On the Hugging Face website, this is the code used in the example; hence, I want to do this exact thing but with my model:

    &gt;&gt;&gt; from transformers import pipeline
    &gt;&gt;&gt; unmasker = pipeline('fill-mask', model='bert-base-uncased')
    &gt;&gt;&gt; unmasker(""Hello I'm a [MASK] model."")
    
    [{'sequence': ""[CLS] hello i'm a fashion model. [SEP]"",
      'score': 0.1073106899857521,
      'token': 4827,
      'token_str': 'fashion'},
     {'sequence': ""[CLS] hello i'm a role model. [SEP]"",
      'score': 0.08774490654468536,
      'token': 2535,
      'token_str': 'role'},
     {'sequence': ""[CLS] hello i'm a new model. [SEP]"",
      'score': 0.05338378623127937,
      'token': 2047,
      'token_str': 'new'},
     {'sequence': ""[CLS] hello i'm a super model. [SEP]"",
      'score': 0.04667217284440994,
      'token': 3565,
      'token_str': 'super'},
     {'sequence': ""[CLS] hello i'm a fine model. [SEP]"",
      'score': 0.027095865458250046,
      'token': 2986,
      'token_str': 'fine'}

Any help on how to do this would be great.",t2_13ussz,False,,0,False,Hugging Face: How to test masked language model after training it?,[],r/LanguageTechnology,False,6,,0,,False,t3_nt04tj,False,dark,0.95,,public,15,0,{},,False,[],,False,False,,{},,False,15,,False,False,,False,,[],{},,True,,1622941039.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have followed this tutorial for masked language modelling from Hugging Face using BERT, but I am unsure how to actually deploy the model.&lt;/p&gt;

&lt;p&gt;Tutorial: &lt;a href=""https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb""&gt;https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have trained the model using my own dataset, which has worked fine, but I don&amp;#39;t know how to actually use the model, as the notebook does not include an example on how to do this, sadly.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;On the Hugging Face website, this is the code used in the example; hence, I want to do this exact thing but with my model:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from transformers import pipeline
&amp;gt;&amp;gt;&amp;gt; unmasker = pipeline(&amp;#39;fill-mask&amp;#39;, model=&amp;#39;bert-base-uncased&amp;#39;)
&amp;gt;&amp;gt;&amp;gt; unmasker(&amp;quot;Hello I&amp;#39;m a [MASK] model.&amp;quot;)

[{&amp;#39;sequence&amp;#39;: &amp;quot;[CLS] hello i&amp;#39;m a fashion model. [SEP]&amp;quot;,
  &amp;#39;score&amp;#39;: 0.1073106899857521,
  &amp;#39;token&amp;#39;: 4827,
  &amp;#39;token_str&amp;#39;: &amp;#39;fashion&amp;#39;},
 {&amp;#39;sequence&amp;#39;: &amp;quot;[CLS] hello i&amp;#39;m a role model. [SEP]&amp;quot;,
  &amp;#39;score&amp;#39;: 0.08774490654468536,
  &amp;#39;token&amp;#39;: 2535,
  &amp;#39;token_str&amp;#39;: &amp;#39;role&amp;#39;},
 {&amp;#39;sequence&amp;#39;: &amp;quot;[CLS] hello i&amp;#39;m a new model. [SEP]&amp;quot;,
  &amp;#39;score&amp;#39;: 0.05338378623127937,
  &amp;#39;token&amp;#39;: 2047,
  &amp;#39;token_str&amp;#39;: &amp;#39;new&amp;#39;},
 {&amp;#39;sequence&amp;#39;: &amp;quot;[CLS] hello i&amp;#39;m a super model. [SEP]&amp;quot;,
  &amp;#39;score&amp;#39;: 0.04667217284440994,
  &amp;#39;token&amp;#39;: 3565,
  &amp;#39;token_str&amp;#39;: &amp;#39;super&amp;#39;},
 {&amp;#39;sequence&amp;#39;: &amp;quot;[CLS] hello i&amp;#39;m a fine model. [SEP]&amp;quot;,
  &amp;#39;score&amp;#39;: 0.027095865458250046,
  &amp;#39;token&amp;#39;: 2986,
  &amp;#39;token_str&amp;#39;: &amp;#39;fine&amp;#39;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Any help on how to do this would be great.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nt04tj,True,,saucyhambon,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nt04tj/hugging_face_how_to_test_masked_language_model/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nt04tj/hugging_face_how_to_test_masked_language_model/,30199,1622912239.0,0,,False,,,,,,1601
53,,LanguageTechnology,"I have a task where i want to use multilingual embeddings for 2 different languages(one of them being english). 
I first checked fasttext but its aligned vectors does have one pf my language. So i check a basic vector aligning algo and it was using common words between two languages to align them. But one of my language does not have english characters so cant use that algo.

Then i read about BERT embeddings and found multilingual model on their git repo. But i dont know how to get word embeddings using using that model.
So, does anybody know how to use the embeddings from bert multilingual model.",t2_5pmdgm1l,False,,0,False,How to use BERT multilingual embedding,[],r/LanguageTechnology,False,6,,0,,False,t3_nt6wsx,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1622960253.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a task where i want to use multilingual embeddings for 2 different languages(one of them being english). 
I first checked fasttext but its aligned vectors does have one pf my language. So i check a basic vector aligning algo and it was using common words between two languages to align them. But one of my language does not have english characters so cant use that algo.&lt;/p&gt;

&lt;p&gt;Then i read about BERT embeddings and found multilingual model on their git repo. But i dont know how to get word embeddings using using that model.
So, does anybody know how to use the embeddings from bert multilingual model.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nt6wsx,True,,inopico3,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nt6wsx/how_to_use_bert_multilingual_embedding/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nt6wsx/how_to_use_bert_multilingual_embedding/,30199,1622931453.0,0,,False,,,,,,605
54,,LanguageTechnology,"Hi, I'd like to write a program that does the following: you put in a word (or words) and as an output you get a similar word (words) to it. So if I give the words 'investment banker' as an input, it should give back the words 'investment analyst' or 'private equity manager'.

The language wouldn't be English, but French, German and Dutch. Which frameworks, libraries and techniques should I look at to implement this and how do I make it work as accurate as possible? Does anyone have any experience with this that you'd like or good guides on it? Is it hard to implement this program? Thanks in advance.",t2_6eu9a1fu,False,,0,False,"How do I find similar words to certain words (NLP, other techniques)?",[],r/LanguageTechnology,False,6,,0,,False,t3_nt3kgx,False,dark,0.87,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1622950683.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;d like to write a program that does the following: you put in a word (or words) and as an output you get a similar word (words) to it. So if I give the words &amp;#39;investment banker&amp;#39; as an input, it should give back the words &amp;#39;investment analyst&amp;#39; or &amp;#39;private equity manager&amp;#39;.&lt;/p&gt;

&lt;p&gt;The language wouldn&amp;#39;t be English, but French, German and Dutch. Which frameworks, libraries and techniques should I look at to implement this and how do I make it work as accurate as possible? Does anyone have any experience with this that you&amp;#39;d like or good guides on it? Is it hard to implement this program? Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nt3kgx,True,,throwaway228526,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nt3kgx/how_do_i_find_similar_words_to_certain_words_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nt3kgx/how_do_i_find_similar_words_to_certain_words_nlp/,30199,1622921883.0,0,,False,,,,,,607
55,,LanguageTechnology,"I’m interested in parsing manual test cases into classified/formatted data so I can extract the intent of the tester and then generate automated test cases (e.g. Selenium). I’ve been playing around with Python’s spaCy library and noticed that when I process the following string, I get incorrect tags:

“User enters Password and press tab key”

“press” is incorrectly tagged as a noun when it should really be a verb. I realize that the word should have been “pressED” and may be the reason why tags are coming out wrong, but is there anything that can be done to work around typo issues like these?",t2_cqf7g,False,,0,False,Incorrect tags when parsing test case,[],r/LanguageTechnology,False,6,,0,,False,t3_nt6aqr,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1622958469.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m interested in parsing manual test cases into classified/formatted data so I can extract the intent of the tester and then generate automated test cases (e.g. Selenium). I’ve been playing around with Python’s spaCy library and noticed that when I process the following string, I get incorrect tags:&lt;/p&gt;

&lt;p&gt;“User enters Password and press tab key”&lt;/p&gt;

&lt;p&gt;“press” is incorrectly tagged as a noun when it should really be a verb. I realize that the word should have been “pressED” and may be the reason why tags are coming out wrong, but is there anything that can be done to work around typo issues like these?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nt6aqr,True,,coolio777,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nt6aqr/incorrect_tags_when_parsing_test_case/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nt6aqr/incorrect_tags_when_parsing_test_case/,30199,1622929669.0,0,,False,,,,,,599
56,,LanguageTechnology,Hi everyone I search for good resource to implement ALBERT for sarcasm detection on Reddit dataset...can u help me?,t2_66lguvzc,False,,0,False,How to implement ALBERT for sarcasm detection,[],r/LanguageTechnology,False,6,,0,,False,t3_nsr07m,False,dark,0.92,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,True,,1622910099.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone I search for good resource to implement ALBERT for sarcasm detection on Reddit dataset...can u help me?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nsr07m,True,,mahdir74,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nsr07m/how_to_implement_albert_for_sarcasm_detection/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nsr07m/how_to_implement_albert_for_sarcasm_detection/,30199,1622881299.0,0,,False,,,,,,115
57,,LanguageTechnology,,t2_21qaqh1p,False,,0,False,Building Grammar Correction API in Python with Gramformer and FastApI,[],r/LanguageTechnology,False,6,,0,,False,t3_nsdki0,False,dark,0.89,,public,15,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/yH36NQGp4NQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Build Grammarly API Opensource Grammar Correction Alternative with Gramformer &amp; FastAPI in Python', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/yH36NQGp4NQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/yH36NQGp4NQ/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/yH36NQGp4NQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nsdki0', 'height': 200}",,False,15,,False,False,,False,,[],{},,False,,1622864402.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nsdki0,True,,dulldata,,0,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/nsdki0/building_grammar_correction_api_in_python_with/,all_ads,False,https://youtu.be/yH36NQGp4NQ,30199,1622835602.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Build Grammarly API Opensource Grammar Correction Alternative with Gramformer &amp; FastAPI in Python', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/yH36NQGp4NQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/yH36NQGp4NQ/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,https://youtu.be/yH36NQGp4NQ,,,,,0
58,,LanguageTechnology,"Hey,

I'm working with rule-based language models and I'm wondering if there is anything that could be called a ""comprehensive set of rules""  for the English grammar. My gut feeling tells me that it is close to impossible to catch all possible grammatical variations of the English language, but I'd love to be proven wrong!

I'm know of Lambeks Pregroup grammar and Montague grammar but I'm not sure if any of those formalism captures the English language without major flaws.

I appreciate any references!",t2_43ky8wby,False,,0,False,Does a truly comprehensive rule-based grammar for the English language exist? Or is there any (recent) study that discusses their limitations?,[],r/LanguageTechnology,False,6,,0,,False,t3_ns27v5,False,dark,0.96,,public,18,0,{},,False,[],,False,False,,{},,False,18,,False,False,,False,,[],{},,True,,1622832539.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working with rule-based language models and I&amp;#39;m wondering if there is anything that could be called a &amp;quot;comprehensive set of rules&amp;quot;  for the English grammar. My gut feeling tells me that it is close to impossible to catch all possible grammatical variations of the English language, but I&amp;#39;d love to be proven wrong!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m know of Lambeks Pregroup grammar and Montague grammar but I&amp;#39;m not sure if any of those formalism captures the English language without major flaws.&lt;/p&gt;

&lt;p&gt;I appreciate any references!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ns27v5,True,,chappy_tha_janitor,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ns27v5/does_a_truly_comprehensive_rulebased_grammar_for/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ns27v5/does_a_truly_comprehensive_rulebased_grammar_for/,30199,1622803739.0,0,,False,,,,,,507
59,,LanguageTechnology,"I've been using gImageReader for a few years and I've always used it to export the OCR'ed text as an invisible text layer over the existing pdf scan, resulting in a searchable pdf file, which is useful for research, working with the text etc.

I was doing this on Windows 10, and recently installed a dual boot with Linux Mint and reinstalled Windows due to slow-down. Now I reinstalled gImageReader and I can no longer find the option to export to PDF with text layer. In fact I'm having trouble even finding much mention of said function online. I don't know if I originally installed some fork or beta or what, but the way the (probably) official version of the program looks is not what I'd been using so far.

I was always able to set certain post-production parameters, such as size of the invisible text etc.

Any ideas where I could find that version again? I'd prefer a Linux Mint compatible version, but Windows would also be fine, if that's the only one.  
Thanks!",t2_5lrs9ing,False,,0,False,Trouble with gImageReader (Tesseract) - Exporting to PDF (Linux Mint),[],r/LanguageTechnology,False,6,,0,,False,t3_nsd4bj,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1622863252.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been using gImageReader for a few years and I&amp;#39;ve always used it to export the OCR&amp;#39;ed text as an invisible text layer over the existing pdf scan, resulting in a searchable pdf file, which is useful for research, working with the text etc.&lt;/p&gt;

&lt;p&gt;I was doing this on Windows 10, and recently installed a dual boot with Linux Mint and reinstalled Windows due to slow-down. Now I reinstalled gImageReader and I can no longer find the option to export to PDF with text layer. In fact I&amp;#39;m having trouble even finding much mention of said function online. I don&amp;#39;t know if I originally installed some fork or beta or what, but the way the (probably) official version of the program looks is not what I&amp;#39;d been using so far.&lt;/p&gt;

&lt;p&gt;I was always able to set certain post-production parameters, such as size of the invisible text etc.&lt;/p&gt;

&lt;p&gt;Any ideas where I could find that version again? I&amp;#39;d prefer a Linux Mint compatible version, but Windows would also be fine, if that&amp;#39;s the only one.&lt;br/&gt;
Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nsd4bj,True,,Party-Permission,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nsd4bj/trouble_with_gimagereader_tesseract_exporting_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nsd4bj/trouble_with_gimagereader_tesseract_exporting_to/,30199,1622834452.0,0,,False,,,,,,975
60,,LanguageTechnology,I would like to subscribe to some weekly news about NLP in order to be up-to-date with latest research in NLP. Could somebody please recommend me one?,t2_5pyrrh3o,False,,0,False,Best weekly digest to keep up with NLP research?,[],r/LanguageTechnology,False,6,,0,,False,t3_ns2x5i,False,dark,0.81,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1622835103.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I would like to subscribe to some weekly news about NLP in order to be up-to-date with latest research in NLP. Could somebody please recommend me one?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ns2x5i,True,,luisda2994,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ns2x5i/best_weekly_digest_to_keep_up_with_nlp_research/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ns2x5i/best_weekly_digest_to_keep_up_with_nlp_research/,30199,1622806303.0,0,,False,,,,,,150
61,,LanguageTechnology,I am playing to write a paper in the NLP field on fake news classifier. Any suggestions about what I could try out and write on? ,t2_85ssy9vj,False,,0,False,Advice for fake news classifier research paper,[],r/LanguageTechnology,False,6,,0,,False,t3_nscnd2,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1622862033.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am playing to write a paper in the NLP field on fake news classifier. Any suggestions about what I could try out and write on? &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nscnd2,True,,The-Bored-Guy10,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nscnd2/advice_for_fake_news_classifier_research_paper/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nscnd2/advice_for_fake_news_classifier_research_paper/,30199,1622833233.0,0,,False,,,,,,129
62,,LanguageTechnology,I understand that models like GPT 3 are auto regressive models which uses word statistics but what I am struggling to understand is why the discernable semantic information that you can get from a word2vec model is not symbolic representation in at least in some way?,t2_ci2ras9r,False,,0,False,Can you guys help me understand how word embeddings is not a symbolic representation of language?,[],r/LanguageTechnology,False,6,,0,,False,t3_ns23bt,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1622832034.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I understand that models like GPT 3 are auto regressive models which uses word statistics but what I am struggling to understand is why the discernable semantic information that you can get from a word2vec model is not symbolic representation in at least in some way?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ns23bt,True,,FairVector,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ns23bt/can_you_guys_help_me_understand_how_word/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ns23bt/can_you_guys_help_me_understand_how_word/,30199,1622803234.0,0,,False,,,,,,267
63,,LanguageTechnology,,t2_hkv9s,False,,0,False,Training T5 model in just 3 lines of code with ONNX Inference,[],r/LanguageTechnology,False,6,,0,,False,t3_ns3hip,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,False,,1622837048.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ns3hip,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ns3hip/training_t5_model_in_just_3_lines_of_code_with/,all_ads,False,https://link.medium.com/KYA6WERvOgb,30199,1622808248.0,0,,False,https://link.medium.com/KYA6WERvOgb,,,,,0
64,,LanguageTechnology,"Hi, I am relatively new to NLP. I am trying to write a code that allows me to grade elementary student's  Science questions. For keyword-based answers, I am able to easily match student's answer to the model answer using lemmatization and such.

**Question: Why did you classify Animal A as a mammal?**

**Answer: fur/ produces milk/ give birth/ warm-blooded**

matching the keywords in the answer is relatively straightforward.

&amp;#x200B;

However, the issue comes when we need to match answers that are more complex.

**Question: what is the relationship between the amount of water given and the height of the seedling?**

**Answer: As the amount of water given increases, the height of the seedling increases.**

&amp;#x200B;

A student may phrase the above answer differently and get a correct answer. (e.g. more water given, taller the plant grows). Take note that students are not graded on language skills ( i.e. grammar and spelling to a reasonable extent.)

I have tried using a pre-trained corpus to calculate the cosine similarity and use that to determine whether an answer matches the model answer or not. However, the resulting cosine values are rather indistinguishable. As a result, I am not able to pick a suitable threshold to ""differentiate"" right from wrong.

I have considered breaking down the answers into clauses and perform a similar analysis on them individually before adding up the scores for each clause. I could possibly use the dependency matcher of spacy to fix the correct subject-verb-object relation in an attempt to extract semantic meaning using algorithms. I guess the next step forward would be to apply machine learning. Thank you for taking the time to read this. I would greatly appreciate any inputs from anyone here. Thanks again!",t2_3rs6f9dy,False,,0,False,Answer matching using sentence similarity,[],r/LanguageTechnology,False,6,,0,,False,t3_ns4nw3,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1622840718.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I am relatively new to NLP. I am trying to write a code that allows me to grade elementary student&amp;#39;s  Science questions. For keyword-based answers, I am able to easily match student&amp;#39;s answer to the model answer using lemmatization and such.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Question: Why did you classify Animal A as a mammal?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Answer: fur/ produces milk/ give birth/ warm-blooded&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;matching the keywords in the answer is relatively straightforward.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;However, the issue comes when we need to match answers that are more complex.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Question: what is the relationship between the amount of water given and the height of the seedling?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Answer: As the amount of water given increases, the height of the seedling increases.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;A student may phrase the above answer differently and get a correct answer. (e.g. more water given, taller the plant grows). Take note that students are not graded on language skills ( i.e. grammar and spelling to a reasonable extent.)&lt;/p&gt;

&lt;p&gt;I have tried using a pre-trained corpus to calculate the cosine similarity and use that to determine whether an answer matches the model answer or not. However, the resulting cosine values are rather indistinguishable. As a result, I am not able to pick a suitable threshold to &amp;quot;differentiate&amp;quot; right from wrong.&lt;/p&gt;

&lt;p&gt;I have considered breaking down the answers into clauses and perform a similar analysis on them individually before adding up the scores for each clause. I could possibly use the dependency matcher of spacy to fix the correct subject-verb-object relation in an attempt to extract semantic meaning using algorithms. I guess the next step forward would be to apply machine learning. Thank you for taking the time to read this. I would greatly appreciate any inputs from anyone here. Thanks again!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ns4nw3,True,,tyx8099,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ns4nw3/answer_matching_using_sentence_similarity/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ns4nw3/answer_matching_using_sentence_similarity/,30199,1622811918.0,0,,False,,,,,,1778
65,,LanguageTechnology,"Hi there, I am building a text classification model to match the name and description of a customer's item (e.g. name: ""suction press nip"", category: ""paper machine parts"") to a list of 10k basic items (name: ""steel, unalloyed"", category: ""metals""). I have some initial matched data to test and I will get more and more, hopefully.

I've build a sentiment analysis program in the past, this is a good example of what I used: [https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/](https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/) (Spacy, Scikitlearn).

This current problem is more complex though, it's 1 to 10k+ match and not binary (or max 5, 6 values), the string for the item is short and absolutely at the discretion of the source (client item log).

Which reads/tutorials/examples would you suggest to take a look at? (in Python please)",t2_10p8w7,False,,0,False,"Text classification for item matching, best setup?",[],r/LanguageTechnology,False,6,,0,,False,t3_ns1dt0,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1622829326.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there, I am building a text classification model to match the name and description of a customer&amp;#39;s item (e.g. name: &amp;quot;suction press nip&amp;quot;, category: &amp;quot;paper machine parts&amp;quot;) to a list of 10k basic items (name: &amp;quot;steel, unalloyed&amp;quot;, category: &amp;quot;metals&amp;quot;). I have some initial matched data to test and I will get more and more, hopefully.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve build a sentiment analysis program in the past, this is a good example of what I used: &lt;a href=""https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/""&gt;https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/&lt;/a&gt; (Spacy, Scikitlearn).&lt;/p&gt;

&lt;p&gt;This current problem is more complex though, it&amp;#39;s 1 to 10k+ match and not binary (or max 5, 6 values), the string for the item is short and absolutely at the discretion of the source (client item log).&lt;/p&gt;

&lt;p&gt;Which reads/tutorials/examples would you suggest to take a look at? (in Python please)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ns1dt0,True,,lele-canfora,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ns1dt0/text_classification_for_item_matching_best_setup/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ns1dt0/text_classification_for_item_matching_best_setup/,30199,1622800526.0,0,,False,,,,,,904
66,,LanguageTechnology," Given a table, I am able to convert natural language questions into appropriate SQL query with transformers.

The architecture of a chatbot should be:

1. Natural language question to SQL query translation using transformers (this part is completed)
2. Fed SQL query to SQL engine and collect response (this part is completed)
3. Convert SQL engine response to Natural Language Response

How can I accomplish the last part? What kind of architecture or model should I use?",t2_8tvf8evl,False,,0,False,[Discussion] What should be the data driven chatbot architecture using NLP2SQL?,[],r/LanguageTechnology,False,6,,0,,False,t3_nrxxpr,False,dark,0.87,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1622815079.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Given a table, I am able to convert natural language questions into appropriate SQL query with transformers.&lt;/p&gt;

&lt;p&gt;The architecture of a chatbot should be:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Natural language question to SQL query translation using transformers (this part is completed)&lt;/li&gt;
&lt;li&gt;Fed SQL query to SQL engine and collect response (this part is completed)&lt;/li&gt;
&lt;li&gt;Convert SQL engine response to Natural Language Response&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;How can I accomplish the last part? What kind of architecture or model should I use?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nrxxpr,True,,Current_Dark6603,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nrxxpr/discussion_what_should_be_the_data_driven_chatbot/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nrxxpr/discussion_what_should_be_the_data_driven_chatbot/,30199,1622786279.0,0,,False,,,,,,473
67,,LanguageTechnology,Any method to negate a Verb or Noun word in the sentence using NLP,t2_cgu1aqt8,False,,0,False,To Negate a word,[],r/LanguageTechnology,False,6,,0,,False,t3_nrwarv,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1622808862.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Any method to negate a Verb or Noun word in the sentence using NLP&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nrwarv,True,,Effective-Piglet-334,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nrwarv/to_negate_a_word/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nrwarv/to_negate_a_word/,30199,1622780062.0,0,,False,,,,,,66
68,,LanguageTechnology,"Hi,

It might be a novice question, but could be a deeper one as well.

Anyone out there please guide me with what is actually happening when you say,

**Play me Butter**

to your Alexa/Google/Siri?

What I know is that, the ML/NLP engine under the hood will

1. Classify the intent -&gt; Music Play
2. Recognize what ""Butter"" means -&gt; It is a song by BTS
3. Tell ""here's Butter by BTS"" to the user
4. And actually play it.

But this is just the high-level sketch. For example, the word ""Butter"" was meaning the food ingredient before BTS launched the single.

**How will the engine behind update their Knowledge Base according to new releases? In a timely manner?** 

I can't imagine it's being done manually. And,

What will be the pipeline of classifiers for finally recognizing Butter as a BTS song? I doubt it'll be done with a single classifier.

Thanks a ton in advance!",t2_cidssi7w,False,,0,False,"What is happening when you say ""play me butter"" to your Alexa/Google/Siri?",[],r/LanguageTechnology,False,6,,0,,False,t3_nrmv0a,False,dark,0.86,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1622780194.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;It might be a novice question, but could be a deeper one as well.&lt;/p&gt;

&lt;p&gt;Anyone out there please guide me with what is actually happening when you say,&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Play me Butter&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;to your Alexa/Google/Siri?&lt;/p&gt;

&lt;p&gt;What I know is that, the ML/NLP engine under the hood will&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Classify the intent -&amp;gt; Music Play&lt;/li&gt;
&lt;li&gt;Recognize what &amp;quot;Butter&amp;quot; means -&amp;gt; It is a song by BTS&lt;/li&gt;
&lt;li&gt;Tell &amp;quot;here&amp;#39;s Butter by BTS&amp;quot; to the user&lt;/li&gt;
&lt;li&gt;And actually play it.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;But this is just the high-level sketch. For example, the word &amp;quot;Butter&amp;quot; was meaning the food ingredient before BTS launched the single.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How will the engine behind update their Knowledge Base according to new releases? In a timely manner?&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;I can&amp;#39;t imagine it&amp;#39;s being done manually. And,&lt;/p&gt;

&lt;p&gt;What will be the pipeline of classifiers for finally recognizing Butter as a BTS song? I doubt it&amp;#39;ll be done with a single classifier.&lt;/p&gt;

&lt;p&gt;Thanks a ton in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nrmv0a,True,,nlp_ttt,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nrmv0a/what_is_happening_when_you_say_play_me_butter_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nrmv0a/what_is_happening_when_you_say_play_me_butter_to/,30199,1622751394.0,0,,False,,,,,,880
69,,LanguageTechnology,"I have a deadline tonight and I wont make it because the code takes several days to finish making the corpus and then passing them it through LDA.

&amp;#x200B;

Edit: I use gensim in python",t2_5r4mo7fh,False,,0,False,Is there a cloud service where I can run my LDA code at a good speed?,[],r/LanguageTechnology,False,6,,0,,False,t3_ns09fc,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,1622799902.0,,[],{},,True,,1622824825.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a deadline tonight and I wont make it because the code takes several days to finish making the corpus and then passing them it through LDA.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Edit: I use gensim in python&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ns09fc,True,,Epiphany925,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ns09fc/is_there_a_cloud_service_where_i_can_run_my_lda/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ns09fc/is_there_a_cloud_service_where_i_can_run_my_lda/,30199,1622796025.0,0,,False,,,,,,190
70,,LanguageTechnology,"&amp;#x200B;

https://reddit.com/link/nrp6bw/video/dzi6hndkh4371/player

This application builds a local txtai index using book data from [openlibrary.org](https://openlibrary.org). It supports natural language queries to find the best matching books.

Applications that support natural language queries open up exciting possibilities. Take conversational AI as an example, you wouldn't expect users to speak in an abrupt way that is typical with traditional token-based search systems.

GitHub: [https://github.com/neuml/txtai](https://github.com/neuml/txtai)  
Application: [https://github.com/neuml/txtai/blob/master/examples/books.py](https://github.com/neuml/txtai/blob/master/examples/books.py)",t2_536lg1nv,False,,0,False,Build and query a book similarity index,[],r/LanguageTechnology,False,6,,0,,False,t3_nrp6bw,False,dark,0.88,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1622786334.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://reddit.com/link/nrp6bw/video/dzi6hndkh4371/player""&gt;https://reddit.com/link/nrp6bw/video/dzi6hndkh4371/player&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This application builds a local txtai index using book data from &lt;a href=""https://openlibrary.org""&gt;openlibrary.org&lt;/a&gt;. It supports natural language queries to find the best matching books.&lt;/p&gt;

&lt;p&gt;Applications that support natural language queries open up exciting possibilities. Take conversational AI as an example, you wouldn&amp;#39;t expect users to speak in an abrupt way that is typical with traditional token-based search systems.&lt;/p&gt;

&lt;p&gt;GitHub: &lt;a href=""https://github.com/neuml/txtai""&gt;https://github.com/neuml/txtai&lt;/a&gt;&lt;br/&gt;
Application: &lt;a href=""https://github.com/neuml/txtai/blob/master/examples/books.py""&gt;https://github.com/neuml/txtai/blob/master/examples/books.py&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nrp6bw,True,,davidmezzetti,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nrp6bw/build_and_query_a_book_similarity_index/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nrp6bw/build_and_query_a_book_similarity_index/,30199,1622757534.0,0,,False,,,,"{'dzi6hndkh4371': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/nrp6bw/asset/dzi6hndkh4371/DASHPlaylist.mpd?a=1626417135%2CYzUzMDI3YjY2N2IwZDU5NWZiMzEwOTg5ZThiZDUxMzVlZjMyNDc4YjNlYjg4ZmY4MjFkMWU3M2RlOWU1MmI3OA%3D%3D&amp;v=1&amp;f=sd', 'x': 655, 'y': 480, 'hlsUrl': 'https://v.redd.it/link/nrp6bw/asset/dzi6hndkh4371/HLSPlaylist.m3u8?a=1626417135%2CZGIxOWY0YjgwNGI4ZWY5NjhhYTExYTQzNDM0OTBjZGI3MWZjMTg3NzMyMzhmYjQ0MmNjM2UxYzllNDUxOWFkOQ%3D%3D&amp;v=1&amp;f=sd', 'id': 'dzi6hndkh4371', 'isGif': False}}",,700
71,,LanguageTechnology,"In the recent update on his upcoming book ""Deep Learning with Python. 2nd edition"" F. Chollet refers to research done in 2017: He and his team did a systematic analysis of text classification using different data sets. He claims that they discovered a simple rule of thumb: If the (number of samples / mean sample length)  &gt; 1500 one should use a sequence model, if  it is &lt; 1500, then one should use a bag-of-bigrams. 

It seems they didn't publish this finding in a research paper but only in a [Google guide to text classification](https://developers.google.com/machine-learning/guides/text-classification) (without any names except 'Google'). I am gathering this from the fact that Chollet only refers to the guide. The guide gives a little bit more information: they ran 450k experiments "" across problems of different types (especially sentiment analysis and topic classification problems), using 12 datasets, alternating for each dataset between different data preprocessing techniques and different model architectures"" ([source](https://developers.google.com/machine-learning/guides/text-classification/step-2-5)). 

As this was done 2017, it would be very interesting to see whether this rule is still valid with the context-sensitive language models like Bert. Does anyone know about research checking this claim in recent years?",t2_w801bv,False,,0,False,Text classification: When to use sequence models over bag-of-words model?,[],r/LanguageTechnology,False,6,,0,,False,t3_nrcaqj,False,dark,0.96,,public,26,1,{},,False,[],,False,False,,{},,False,26,,False,False,,False,,[],{},,True,,1622752118.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In the recent update on his upcoming book &amp;quot;Deep Learning with Python. 2nd edition&amp;quot; F. Chollet refers to research done in 2017: He and his team did a systematic analysis of text classification using different data sets. He claims that they discovered a simple rule of thumb: If the (number of samples / mean sample length)  &amp;gt; 1500 one should use a sequence model, if  it is &amp;lt; 1500, then one should use a bag-of-bigrams. &lt;/p&gt;

&lt;p&gt;It seems they didn&amp;#39;t publish this finding in a research paper but only in a &lt;a href=""https://developers.google.com/machine-learning/guides/text-classification""&gt;Google guide to text classification&lt;/a&gt; (without any names except &amp;#39;Google&amp;#39;). I am gathering this from the fact that Chollet only refers to the guide. The guide gives a little bit more information: they ran 450k experiments &amp;quot; across problems of different types (especially sentiment analysis and topic classification problems), using 12 datasets, alternating for each dataset between different data preprocessing techniques and different model architectures&amp;quot; (&lt;a href=""https://developers.google.com/machine-learning/guides/text-classification/step-2-5""&gt;source&lt;/a&gt;). &lt;/p&gt;

&lt;p&gt;As this was done 2017, it would be very interesting to see whether this rule is still valid with the context-sensitive language models like Bert. Does anyone know about research checking this claim in recent years?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nrcaqj,True,,rickschott,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nrcaqj/text_classification_when_to_use_sequence_models/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nrcaqj/text_classification_when_to_use_sequence_models/,30199,1622723318.0,0,,False,,,,,,1346
72,,LanguageTechnology,"I want to develop software that helps streamline connecting people with roomates/apartments. Right now I'm using a few facebook groups where people post about either looking for someone to rent a room in their apartment or they post about needing a room. I'd like to write something that automatically parses the data out of these that people commonly need. Like: Is this apartment pet friendly? How much is the room? What neighborhood is it in? How many bedrooms? etc.

My background is in computer vision for robotics with CNNs, so this is a totally different domain I'm not familiar with. From the research I've done so far, it sounds like I should look into entity recognition and relationship extraction. But I'm not sure what models are good for that, how much labeled data I need to get started.

I'd be willing to put a few thousand dollars into data annotation I think if that could get me something I could use for my own apartment search.

What models should I look into? What data labeling services/tools? How should I approach this?",t2_56mozcie,False,,0,False,Advice for how to approach classifying apartment posts on facebook?,[],r/LanguageTechnology,False,6,,0,,False,t3_nrv8xt,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1622805212.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to develop software that helps streamline connecting people with roomates/apartments. Right now I&amp;#39;m using a few facebook groups where people post about either looking for someone to rent a room in their apartment or they post about needing a room. I&amp;#39;d like to write something that automatically parses the data out of these that people commonly need. Like: Is this apartment pet friendly? How much is the room? What neighborhood is it in? How many bedrooms? etc.&lt;/p&gt;

&lt;p&gt;My background is in computer vision for robotics with CNNs, so this is a totally different domain I&amp;#39;m not familiar with. From the research I&amp;#39;ve done so far, it sounds like I should look into entity recognition and relationship extraction. But I&amp;#39;m not sure what models are good for that, how much labeled data I need to get started.&lt;/p&gt;

&lt;p&gt;I&amp;#39;d be willing to put a few thousand dollars into data annotation I think if that could get me something I could use for my own apartment search.&lt;/p&gt;

&lt;p&gt;What models should I look into? What data labeling services/tools? How should I approach this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nrv8xt,True,,robotic-rambling,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nrv8xt/advice_for_how_to_approach_classifying_apartment/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nrv8xt/advice_for_how_to_approach_classifying_apartment/,30199,1622776412.0,0,,False,,,,,,1045
73,,LanguageTechnology,,t2_7wbpamj7,False,,0,False,What are state-of-the-art methods for abstractive text summarization ?,[],r/LanguageTechnology,False,6,,0,,False,t3_nr6xx7,False,dark,0.93,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,True,,1622731569.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nr6xx7,True,,Melodic_Stomach_2704,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nr6xx7/what_are_stateoftheart_methods_for_abstractive/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nr6xx7/what_are_stateoftheart_methods_for_abstractive/,30199,1622702769.0,0,,False,,,,,,0
74,,LanguageTechnology,"Hi, All. I am new to NLP but have a problem I'd like to solve. I host a podcast of sorts at work and I am always on the lookout for great questions. Since I also enjoy python and data science I decided to see if I could uncover anything using those tools.

Did a bit of research and found a corpus of transcribed npr interviews. That seemed like a good place to start so I wrote the code to tokenize all the sentences and find those ending in '?'. So, that's all the questions.

In 3M+ utterances, there are a bunch of questions. Many are...not good questions.

'Beth, are you out there?'

'What do you say, Bob?'

'Is that right?'

Fewer are actually good or more meaningful questions.

'Clinton and Obama are pretty similar on policy issues, so why would a Democrat switch loyalties like that?'

""The issue in the Hollywood writer's strike is, do writers get paid for work that winds up online?""

In general, these questions tend to be longer and have larger words in them but, I am curious if there are any established methods for determining the meaning or value of a question? I am not really certain even what the right word is to use (meaning, value, etc).

Anyways, I always open to learn something new. If anyone can point me in the right direction, i'd appreciate it. Thanks!

&amp;#x200B;

Code thus far for anyone interested:

`import nltk`

`from nltk.corpus import stopwords`

`from nltk import word_tokenize`

`from nltk import sent_tokenize`

`import numpy as np`

`import pandas as pd`

[`nltk.download`](https://nltk.download)`('nps_chat')`

&amp;#x200B;

`npr = pd.read_csv('C:\Users\...\Desktop\Python Scripts\Data\utterances.csv')`

`npr = npr[npr['utterance'].notna()]`

&amp;#x200B;

`tokens = npr['utterance'].apply(lambda x: sent_tokenize(x))`

`#put tokens into the DF`

`npr['utterance_tokenized'] = tokens`

`#build lists of question sentences`

`is_question = npr['utterance_tokenized'].apply(lambda x:`

`[q for q in x if '?' in q])`

`#put questions into the DF`

`npr['questions'] = is_question`

`#identify non-empty lists of questions`

`has_question = npr['questions'].apply(lambda x: True if (len(x) &gt; 0) else False)`

`#put boolean results into data frame`

`npr['has_questions'] = has_question`",t2_13s54e,False,,0,False,Assessing the “Value” of a question.,[],r/LanguageTechnology,False,6,,0,,False,t3_nqzvav,False,dark,1.0,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,1622685259.0,,[],{},,True,,1622706688.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, All. I am new to NLP but have a problem I&amp;#39;d like to solve. I host a podcast of sorts at work and I am always on the lookout for great questions. Since I also enjoy python and data science I decided to see if I could uncover anything using those tools.&lt;/p&gt;

&lt;p&gt;Did a bit of research and found a corpus of transcribed npr interviews. That seemed like a good place to start so I wrote the code to tokenize all the sentences and find those ending in &amp;#39;?&amp;#39;. So, that&amp;#39;s all the questions.&lt;/p&gt;

&lt;p&gt;In 3M+ utterances, there are a bunch of questions. Many are...not good questions.&lt;/p&gt;

&lt;p&gt;&amp;#39;Beth, are you out there?&amp;#39;&lt;/p&gt;

&lt;p&gt;&amp;#39;What do you say, Bob?&amp;#39;&lt;/p&gt;

&lt;p&gt;&amp;#39;Is that right?&amp;#39;&lt;/p&gt;

&lt;p&gt;Fewer are actually good or more meaningful questions.&lt;/p&gt;

&lt;p&gt;&amp;#39;Clinton and Obama are pretty similar on policy issues, so why would a Democrat switch loyalties like that?&amp;#39;&lt;/p&gt;

&lt;p&gt;&amp;quot;The issue in the Hollywood writer&amp;#39;s strike is, do writers get paid for work that winds up online?&amp;quot;&lt;/p&gt;

&lt;p&gt;In general, these questions tend to be longer and have larger words in them but, I am curious if there are any established methods for determining the meaning or value of a question? I am not really certain even what the right word is to use (meaning, value, etc).&lt;/p&gt;

&lt;p&gt;Anyways, I always open to learn something new. If anyone can point me in the right direction, i&amp;#39;d appreciate it. Thanks!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Code thus far for anyone interested:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;import nltk&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;from nltk.corpus import stopwords&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;from nltk import word_tokenize&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;from nltk import sent_tokenize&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;import numpy as np&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;import pandas as pd&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://nltk.download""&gt;&lt;code&gt;nltk.download&lt;/code&gt;&lt;/a&gt;&lt;code&gt;(&amp;#39;nps_chat&amp;#39;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;npr = pd.read_csv(&amp;#39;C:\Users\...\Desktop\Python Scripts\Data\utterances.csv&amp;#39;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;npr = npr[npr[&amp;#39;utterance&amp;#39;].notna()]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tokens = npr[&amp;#39;utterance&amp;#39;].apply(lambda x: sent_tokenize(x))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#put tokens into the DF&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;npr[&amp;#39;utterance_tokenized&amp;#39;] = tokens&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#build lists of question sentences&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;is_question = npr[&amp;#39;utterance_tokenized&amp;#39;].apply(lambda x:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;[q for q in x if &amp;#39;?&amp;#39; in q])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#put questions into the DF&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;npr[&amp;#39;questions&amp;#39;] = is_question&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#identify non-empty lists of questions&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;has_question = npr[&amp;#39;questions&amp;#39;].apply(lambda x: True if (len(x) &amp;gt; 0) else False)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#put boolean results into data frame&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;npr[&amp;#39;has_questions&amp;#39;] = has_question&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nqzvav,True,,mtnbiker98,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nqzvav/assessing_the_value_of_a_question/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nqzvav/assessing_the_value_of_a_question/,30199,1622677888.0,0,,False,,,,,,2235
75,,LanguageTechnology,"I work in the field of NLP and realized I wasn't reading enough papers on what's going around in the field. What is the latest paper you read and that leaves you mindblown (or at least excited)?

It can be any fields of NLP, I'm just curious.",t2_npixkyh,False,,0,False,What have you read recently?,[],r/LanguageTechnology,False,6,,0,,False,t3_nqvvka,False,dark,1.0,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1622695503.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I work in the field of NLP and realized I wasn&amp;#39;t reading enough papers on what&amp;#39;s going around in the field. What is the latest paper you read and that leaves you mindblown (or at least excited)?&lt;/p&gt;

&lt;p&gt;It can be any fields of NLP, I&amp;#39;m just curious.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nqvvka,True,,Arthurion9,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nqvvka/what_have_you_read_recently/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nqvvka/what_have_you_read_recently/,30199,1622666703.0,0,,False,,,,,,242
76,,LanguageTechnology,"Hi, I'm interested to know whether the few-shot learning can be done on BERT for sentence similarity.  


Update: Found the pipeline in hugging face for zero-shot learning.  
[https://huggingface.co/facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)  
But still, I'm not sure about the few-shot though.  
",t2_7yhonl6y,False,,0,False,Few Shot Learning in BERT,[],r/LanguageTechnology,False,6,,0,,False,t3_nqh6jw,False,dark,1.0,,public,17,0,{},,False,[],,False,False,,{},,False,17,,False,False,,1622641154.0,,[],{},,True,,1622652264.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m interested to know whether the few-shot learning can be done on BERT for sentence similarity.  &lt;/p&gt;

&lt;p&gt;Update: Found the pipeline in hugging face for zero-shot learning.&lt;br/&gt;
&lt;a href=""https://huggingface.co/facebook/bart-large-mnli""&gt;https://huggingface.co/facebook/bart-large-mnli&lt;/a&gt;&lt;br/&gt;
But still, I&amp;#39;m not sure about the few-shot though.  &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nqh6jw,True,,pingu_henry,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nqh6jw/few_shot_learning_in_bert/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nqh6jw/few_shot_learning_in_bert/,30199,1622623464.0,0,,False,,,,,,329
77,,LanguageTechnology,"Hi! I am a digital humanities student and a newbie in NLP and for my dissertation I tried sentiment analysis with flair library on around 517 poems. My problem is: I used a pre-trained model and it labeled my poems as Negative/Positive, but I do not know how to evaluate how accurate this was. My data was unlabeled before applying the pre-trained model.  

If someone could please help me, that would be great. Thank you!",t2_725zn8ev,False,,0,False,Sentiment Analysis with flair on poetry - model evaluation,[],r/LanguageTechnology,False,6,,0,,False,t3_nqhf6g,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1622653296.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi! I am a digital humanities student and a newbie in NLP and for my dissertation I tried sentiment analysis with flair library on around 517 poems. My problem is: I used a pre-trained model and it labeled my poems as Negative/Positive, but I do not know how to evaluate how accurate this was. My data was unlabeled before applying the pre-trained model.  &lt;/p&gt;

&lt;p&gt;If someone could please help me, that would be great. Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nqhf6g,True,,yuraeh,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nqhf6g/sentiment_analysis_with_flair_on_poetry_model/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nqhf6g/sentiment_analysis_with_flair_on_poetry_model/,30199,1622624496.0,0,,False,,,,,,422
78,,LanguageTechnology,"[https://towardsdatascience.com/how-to-annotate-pdfs-and-scanned-images-for-nlp-applications-f7b7b1db5c4a](https://towardsdatascience.com/how-to-annotate-pdfs-and-scanned-images-for-nlp-applications-f7b7b1db5c4a)

Disclaimer: The OCR feature mentioned in the article is only available for paid subscriptions, if you are interested by a demo send us an email at [admin@ubiai.tools](mailto:admin@ubiai.tools).",t2_32tnavmg,False,,0,False,"If you are looking to automatically extract information from PDFs or scanned images, check out this article on how to leverage OCR to create training data.",[],r/LanguageTechnology,False,6,,0,,False,t3_nq8ahy,False,dark,0.57,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,1622656672.0,,[],{},,True,,1622620738.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://towardsdatascience.com/how-to-annotate-pdfs-and-scanned-images-for-nlp-applications-f7b7b1db5c4a""&gt;https://towardsdatascience.com/how-to-annotate-pdfs-and-scanned-images-for-nlp-applications-f7b7b1db5c4a&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Disclaimer: The OCR feature mentioned in the article is only available for paid subscriptions, if you are interested by a demo send us an email at [&lt;a href=""mailto:admin@ubiai.tools""&gt;admin@ubiai.tools&lt;/a&gt;](mailto:&lt;a href=""mailto:admin@ubiai.tools""&gt;admin@ubiai.tools&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nq8ahy,True,,UBIAI,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nq8ahy/if_you_are_looking_to_automatically_extract/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nq8ahy/if_you_are_looking_to_automatically_extract/,30199,1622591938.0,0,,False,,,,,,407
79,,LanguageTechnology,"Hey there,
I am waiting currently waiting my master's thesis and I had an idea for an analysis. 
I have a list of domain names and I was wondering whether I could automatically categorize them. I already tried the word embedding methods GloVe methods as presented by Stanford in 2013, googleNews negative 300 and Facebooks fasttext. My best results were with fasttext but they are still somewhat unreliable and when I look at the vectorization of words I find the their neighbours but I don't really get ""super-categories"" of these words. 
I would love to have something like:
f(""cars-for-sale.com"")=  ""car sales website"".
Am I overlooking some super obvious method for this? I am new to the field of NLP so please forgive me my ignorance.",t2_76x7914w,False,,0,False,Assigning a category to a word,[],r/LanguageTechnology,False,6,,0,,False,t3_nq0nl5,False,dark,0.87,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1622600285.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey there,
I am waiting currently waiting my master&amp;#39;s thesis and I had an idea for an analysis. 
I have a list of domain names and I was wondering whether I could automatically categorize them. I already tried the word embedding methods GloVe methods as presented by Stanford in 2013, googleNews negative 300 and Facebooks fasttext. My best results were with fasttext but they are still somewhat unreliable and when I look at the vectorization of words I find the their neighbours but I don&amp;#39;t really get &amp;quot;super-categories&amp;quot; of these words. 
I would love to have something like:
f(&amp;quot;cars-for-sale.com&amp;quot;)=  &amp;quot;car sales website&amp;quot;.
Am I overlooking some super obvious method for this? I am new to the field of NLP so please forgive me my ignorance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nq0nl5,True,,bwdmn,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nq0nl5/assigning_a_category_to_a_word/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nq0nl5/assigning_a_category_to_a_word/,30199,1622571485.0,0,,False,,,,,,739
80,,LanguageTechnology," Many of the papers and tutorials I'm reading on sentiment analysis seem to treat the problem as classification problem where the goal is to classify some text as ""positive"", ""negative"", or ""neutral"". However, sentiment is discrete concept, so why would you treat is as such? Wouldn't you need to model sentiment as a float value between say -1 and 1, and then preform a regression? Is there are problem with the latter solution that the former method fixes?",t2_6ybo2,False,,0,False,What advantage is there to treating sentiment analysis as a classification problem vs a regression problem?,[],r/LanguageTechnology,False,6,,0,,False,t3_nq5zz8,False,dark,0.75,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1622614034.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Many of the papers and tutorials I&amp;#39;m reading on sentiment analysis seem to treat the problem as classification problem where the goal is to classify some text as &amp;quot;positive&amp;quot;, &amp;quot;negative&amp;quot;, or &amp;quot;neutral&amp;quot;. However, sentiment is discrete concept, so why would you treat is as such? Wouldn&amp;#39;t you need to model sentiment as a float value between say -1 and 1, and then preform a regression? Is there are problem with the latter solution that the former method fixes?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nq5zz8,True,,Revlong57,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nq5zz8/what_advantage_is_there_to_treating_sentiment/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nq5zz8/what_advantage_is_there_to_treating_sentiment/,30199,1622585234.0,0,,False,,,,,,458
81,,LanguageTechnology,"Hi, 

I am designing a study where I will collect daily diary entries and try to predict emotions/sentiment as well as levels of mental health issues (the ground truth scores will be acquired through daily questionnaires). Since I am new to NLP, what would be a good minimum text length of these diary entries for using machine learning/NLP methods and why? I would be grateful for any relevant sources as well.

Thanks!",t2_dvgke,False,,0,False,Minimum text length in diary entries for detecting emotions/sentiment and classifying levels of mental health issues,[],r/LanguageTechnology,False,6,,0,,False,t3_npw3c1,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1622588692.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, &lt;/p&gt;

&lt;p&gt;I am designing a study where I will collect daily diary entries and try to predict emotions/sentiment as well as levels of mental health issues (the ground truth scores will be acquired through daily questionnaires). Since I am new to NLP, what would be a good minimum text length of these diary entries for using machine learning/NLP methods and why? I would be grateful for any relevant sources as well.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,npw3c1,True,,tiensss,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/npw3c1/minimum_text_length_in_diary_entries_for/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/npw3c1/minimum_text_length_in_diary_entries_for/,30199,1622559892.0,0,,False,,,,,,420
82,,LanguageTechnology,"Autocompletion has become a handy and widely used tool in contemporary messaging and other writing tasks. It is also an essential feature of an integrated development environment (IDE) for computer programming. Recently, research has shown that autocompletion can be powered by deep learning, thus allowing software language models to achieve significant accuracy improvements by training on real-world datasets collected from programmers’ IDE activity. However, a common issue with less popular programming languages is that the available IDE datasets may be insufficient for training.

In a [paper](https://arxiv.org/pdf/2105.05991.pdf), a research team from Facebook demonstrates how transfer learning can enable pre-training on non-IDE, non-autocompletion, and different-language example code sequences before fine-tuning on the autocompletion prediction task. The proposed method improves model accuracy by more than 50 percent on small fine-tuning datasets and over 10 percent on 50k labeled examples.

Summary: [https://www.marktechpost.com/2021/05/31/facebook-demonstrates-how-the-power-of-transfer-learning-can-boost-code-autocompletion-accuracy-by-over-50/](https://www.marktechpost.com/2021/05/31/facebook-demonstrates-how-the-power-of-transfer-learning-can-boost-code-autocompletion-accuracy-by-over-50/?_ga=2.133350797.2041055336.1622397040-488125022.1618729090)

Paper: https://arxiv.org/pdf/2105.05991.pdf",t2_4wudjgid,False,,0,False,Facebook AI Demonstrates How The Power Of Transfer Learning Can Boost Code Autocompletion Accuracy By Over 50%,[],r/LanguageTechnology,False,6,,0,,False,t3_nplea7,False,dark,0.95,,public,18,0,{},,False,[],,False,False,,{},,False,18,,False,False,,False,,[],{},,True,,1622549507.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Autocompletion has become a handy and widely used tool in contemporary messaging and other writing tasks. It is also an essential feature of an integrated development environment (IDE) for computer programming. Recently, research has shown that autocompletion can be powered by deep learning, thus allowing software language models to achieve significant accuracy improvements by training on real-world datasets collected from programmers’ IDE activity. However, a common issue with less popular programming languages is that the available IDE datasets may be insufficient for training.&lt;/p&gt;

&lt;p&gt;In a &lt;a href=""https://arxiv.org/pdf/2105.05991.pdf""&gt;paper&lt;/a&gt;, a research team from Facebook demonstrates how transfer learning can enable pre-training on non-IDE, non-autocompletion, and different-language example code sequences before fine-tuning on the autocompletion prediction task. The proposed method improves model accuracy by more than 50 percent on small fine-tuning datasets and over 10 percent on 50k labeled examples.&lt;/p&gt;

&lt;p&gt;Summary: &lt;a href=""https://www.marktechpost.com/2021/05/31/facebook-demonstrates-how-the-power-of-transfer-learning-can-boost-code-autocompletion-accuracy-by-over-50/?_ga=2.133350797.2041055336.1622397040-488125022.1618729090""&gt;https://www.marktechpost.com/2021/05/31/facebook-demonstrates-how-the-power-of-transfer-learning-can-boost-code-autocompletion-accuracy-by-over-50/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/pdf/2105.05991.pdf""&gt;https://arxiv.org/pdf/2105.05991.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nplea7,True,,techsucker,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nplea7/facebook_ai_demonstrates_how_the_power_of/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nplea7/facebook_ai_demonstrates_how_the_power_of/,30199,1622520707.0,0,,False,,,,,,1420
83,,LanguageTechnology,,t2_6ib7gcgr,False,,0,False,[D] Probing Classifiers: A Gentle Intro (Explainable AI for Deep Learning) [Video],[],r/LanguageTechnology,False,6,,0,,False,t3_nprdjx,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,False,,1622574248.0,text,6,,,text,self.MachineLearning,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nprdjx,True,,jayalammar,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nprdjx/d_probing_classifiers_a_gentle_intro_explainable/,all_ads,False,/r/MachineLearning/comments/npraw7/d_probing_classifiers_a_gentle_intro_explainable/,30199,1622545448.0,0,,False,/r/MachineLearning/comments/npraw7/d_probing_classifiers_a_gentle_intro_explainable/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': '[removed]', 'author_fullname': 't2_6ib7gcgr', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] Probing Classifiers: A Gentle Intro (Explainable AI for Deep Learning) [Video]', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_npraw7', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Rule 6 - Beginner tutorial or project', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1622574007.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': 'moderator', 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '53594546-128c-11eb-a1a3-0e29e49f7ff7', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'npraw7', 'is_robot_indexable': False, 'report_reasons': None, 'author': 'jayalammar', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/npraw7/d_probing_classifiers_a_gentle_intro_explainable/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/npraw7/d_probing_classifiers_a_gentle_intro_explainable/', 'subreddit_subscribers': 1930696, 'created_utc': 1622545207.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_npraw7,,,0
84,,LanguageTechnology,"Hi. I'm working in a company that has recently started accepting projects on conversational and vocal interfaces but I am personaly at a point in my path where I often doubt that these kind of applications with the current level of performance are actually useful outside very specific cases (like for disabled people).

So, in the spirit of the ""are chatbots just a fad?"" I would like to know your opinions on the following subjects:

# Are the current tecnologies for conversational and vocal interfaces really useful? Are we just offering on the market something that is ""cool"" but useless? What are examples of conversational and vocal interfaces the general public is using in their lifes right now? Where might we use these technologies in the very near future that we still don't?",t2_4i3g89wl,False,,0,False,Your opinions: Where are conversational interfaces actually useful?,[],r/LanguageTechnology,False,6,,0,,False,t3_np4mqr,False,dark,1.0,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1622500018.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi. I&amp;#39;m working in a company that has recently started accepting projects on conversational and vocal interfaces but I am personaly at a point in my path where I often doubt that these kind of applications with the current level of performance are actually useful outside very specific cases (like for disabled people).&lt;/p&gt;

&lt;p&gt;So, in the spirit of the &amp;quot;are chatbots just a fad?&amp;quot; I would like to know your opinions on the following subjects:&lt;/p&gt;

&lt;h1&gt;Are the current tecnologies for conversational and vocal interfaces really useful? Are we just offering on the market something that is &amp;quot;cool&amp;quot; but useless? What are examples of conversational and vocal interfaces the general public is using in their lifes right now? Where might we use these technologies in the very near future that we still don&amp;#39;t?&lt;/h1&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,np4mqr,True,,francesco_on_the_job,,17,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/np4mqr/your_opinions_where_are_conversational_interfaces/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/np4mqr/your_opinions_where_are_conversational_interfaces/,30199,1622471218.0,0,,False,,,,,,787
85,,LanguageTechnology,,t2_5smymm7p,False,,0,False,Into NLP - Part-of-speech tagging,[],r/LanguageTechnology,False,6,,0,,False,t3_np15gv,False,dark,0.85,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,False,,1622489061.0,text,6,,,text,qualicen.de,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,np15gv,True,,QualicenDS,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/np15gv/into_nlp_partofspeech_tagging/,all_ads,False,https://www.qualicen.de/into-nlp-5-numerous-language-parts-pos-tagging/,30199,1622460261.0,0,,False,https://www.qualicen.de/into-nlp-5-numerous-language-parts-pos-tagging/,,,,,0
86,,LanguageTechnology,,t2_5cvo57qv,False,,0,False,[2105.13626] ByT5: Towards a token-free future with pre-trained byte-to-byte models,[],r/LanguageTechnology,False,6,,0,,False,t3_np39fr,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,False,,1622496023.0,text,6,,,text,arxiv.org,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,np39fr,True,,argosopentech,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/np39fr/210513626_byt5_towards_a_tokenfree_future_with/,all_ads,False,https://arxiv.org/abs/2105.13626,30199,1622467223.0,0,,False,https://arxiv.org/abs/2105.13626,,,,,0
87,,LanguageTechnology,,t2_hkv9s,False,,0,False,Entity-level Factual Consistency of Abstractive Text Summarization (Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_noyvf4,False,dark,0.9,,public,7,1,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/P9wr8IBfDQs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Entity-level Factual Consistency of Abstractive Text Summarization (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/P9wr8IBfDQs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/P9wr8IBfDQs/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/P9wr8IBfDQs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/noyvf4', 'height': 200}",,False,7,,False,False,,False,,[],{},,False,,1622480294.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,noyvf4,True,,prakhar21,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/noyvf4/entitylevel_factual_consistency_of_abstractive/,all_ads,False,https://youtu.be/P9wr8IBfDQs,30199,1622451494.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Entity-level Factual Consistency of Abstractive Text Summarization (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/P9wr8IBfDQs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/P9wr8IBfDQs/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/P9wr8IBfDQs,,,,,0
88,,LanguageTechnology,Generate code*,t2_bdd86e85,False,,0,False,Noob question - What are the better tools/programs that general code using instructions in natural language/plain English?,[],r/LanguageTechnology,False,6,,0,,False,t3_noxvs0,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,1622551494.0,,[],{},,True,,1622476154.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Generate code*&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,noxvs0,True,,IHateSelectingNames,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/noxvs0/noob_question_what_are_the_better_toolsprograms/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/noxvs0/noob_question_what_are_the_better_toolsprograms/,30199,1622447354.0,0,,False,,,,,,14
89,,LanguageTechnology,"Everyone who is interested in NLP or even DL and ML for that matter, has definitely heard about the BERT family of models. BERT, RoBERTa, DistilBERT and many many more. This paper ""BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"" first introduced this and it has now completely changed the way AI practitioners are solving and looking at NLP problems these days.

As a part of my Paper Notes series, I have gone through the paper and created an informative summary of the paper. This time it goes a bit longer than the previous paper summaries, but it had to be done. The paper contained many tiny interesting nuggets that I had to include. Check out the links below and happy reading!

Paper Summary -  [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/)

Annotated Paper -  [https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/BERT.pdf](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/BERT.pdf)",t2_5xzd9om,False,,0,False,BERT - Annotated Paper + Paper Summary,[],r/LanguageTechnology,False,6,,0,,False,t3_np03n3,False,dark,0.71,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1622485150.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Everyone who is interested in NLP or even DL and ML for that matter, has definitely heard about the BERT family of models. BERT, RoBERTa, DistilBERT and many many more. This paper &amp;quot;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&amp;quot; first introduced this and it has now completely changed the way AI practitioners are solving and looking at NLP problems these days.&lt;/p&gt;

&lt;p&gt;As a part of my Paper Notes series, I have gone through the paper and created an informative summary of the paper. This time it goes a bit longer than the previous paper summaries, but it had to be done. The paper contained many tiny interesting nuggets that I had to include. Check out the links below and happy reading!&lt;/p&gt;

&lt;p&gt;Paper Summary -  &lt;a href=""https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/""&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Annotated Paper -  &lt;a href=""https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/BERT.pdf""&gt;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/BERT.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,np03n3,True,,shreyansh26,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/np03n3/bert_annotated_paper_paper_summary/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/np03n3/bert_annotated_paper_paper_summary/,30199,1622456350.0,0,,False,,,,,,1079
90,,LanguageTechnology,,t2_4wudjgid,False,,0,False,Google’s Multitask Unified Model (MUM) Transforms How Google AI Understands Complex Queries,[],r/LanguageTechnology,False,6,,0,,False,t3_np2pfo,False,dark,0.4,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,False,,1622494329.0,text,6,,,text,marktechpost.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,np2pfo,True,,techsucker,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/np2pfo/googles_multitask_unified_model_mum_transforms/,all_ads,False,https://www.marktechpost.com/2021/05/31/googles-multitask-unified-model-mum-transforms-how-google-ai-understands-complex-queries/,30199,1622465529.0,0,,False,https://www.marktechpost.com/2021/05/31/googles-multitask-unified-model-mum-transforms-how-google-ai-understands-complex-queries/,,,,,0
91,,LanguageTechnology,"Last year, Facebook AI released [Dynabench](https://dynabench.org/?fbclid=IwAR1ScAjtZoAb_PwA0-rPlCQYxWS-9-iGJpcYKQ5eyFUvLnQVVbSZtV8j3as), a platform that radically rethinks benchmarking in AI, starting with natural language processing (NLP) models. Going forward, they have now announced a new evaluation-as-a-service platform for comprehensive, standardized evaluations of NLP models called [Dynaboard](https://dynabench.org/dynaboard.pdf?fbclid=IwAR3rTJa8jRQtaJp8FFp5wf-eyWZ2QmXHTapxHjmpqw-j1t0Sw9n1xD31ASs). Dynaboard can perform apples-to-apples comparisons dynamically without common issues from bugs in evaluation code, inconsistencies in filtering test data, backward compatibility, accessibility, and several other reproducibility issues.

Dynaboard enables AI researchers to customize a new Dynascore metric based on multiple axes of evaluation, including compute, accuracy, robustness, memory, and fairness.

Full Summary: [https://www.marktechpost.com/2021/05/30/facebook-ai-releases-dynaboard-a-new-evaluation-platform-for-nlp-models/](https://www.marktechpost.com/2021/05/30/facebook-ai-releases-dynaboard-a-new-evaluation-platform-for-nlp-models/?_ga=2.190349480.2041055336.1622397040-488125022.1618729090)

Github: https://github.com/facebookresearch/dynalab

Paper: https://dynabench.org/dynaboard.pdf",t2_4wudjgid,False,,0,False,Facebook AI releases Dynaboard: A New Evaluation platform for NLP Models,[],r/LanguageTechnology,False,6,,0,,False,t3_nogmtg,False,dark,0.88,,public,27,0,{},,False,[],,False,False,,{},,False,27,,False,False,,False,,[],{},,True,,1622426400.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Last year, Facebook AI released &lt;a href=""https://dynabench.org/?fbclid=IwAR1ScAjtZoAb_PwA0-rPlCQYxWS-9-iGJpcYKQ5eyFUvLnQVVbSZtV8j3as""&gt;Dynabench&lt;/a&gt;, a platform that radically rethinks benchmarking in AI, starting with natural language processing (NLP) models. Going forward, they have now announced a new evaluation-as-a-service platform for comprehensive, standardized evaluations of NLP models called &lt;a href=""https://dynabench.org/dynaboard.pdf?fbclid=IwAR3rTJa8jRQtaJp8FFp5wf-eyWZ2QmXHTapxHjmpqw-j1t0Sw9n1xD31ASs""&gt;Dynaboard&lt;/a&gt;. Dynaboard can perform apples-to-apples comparisons dynamically without common issues from bugs in evaluation code, inconsistencies in filtering test data, backward compatibility, accessibility, and several other reproducibility issues.&lt;/p&gt;

&lt;p&gt;Dynaboard enables AI researchers to customize a new Dynascore metric based on multiple axes of evaluation, including compute, accuracy, robustness, memory, and fairness.&lt;/p&gt;

&lt;p&gt;Full Summary: &lt;a href=""https://www.marktechpost.com/2021/05/30/facebook-ai-releases-dynaboard-a-new-evaluation-platform-for-nlp-models/?_ga=2.190349480.2041055336.1622397040-488125022.1618729090""&gt;https://www.marktechpost.com/2021/05/30/facebook-ai-releases-dynaboard-a-new-evaluation-platform-for-nlp-models/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Github: &lt;a href=""https://github.com/facebookresearch/dynalab""&gt;https://github.com/facebookresearch/dynalab&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://dynabench.org/dynaboard.pdf""&gt;https://dynabench.org/dynaboard.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nogmtg,True,,techsucker,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nogmtg/facebook_ai_releases_dynaboard_a_new_evaluation/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nogmtg/facebook_ai_releases_dynaboard_a_new_evaluation/,30199,1622397600.0,0,,False,,,,,,1317
92,,LanguageTechnology,"Hi reddit, NLP newbie here

I am trying to understand if there is any value in creating a table out of free text, versus predictive analysis on the free text itself. 

For context, I am working with 2 million clinical notes from the MIMIC-III dataset, and I would like to tabluate all this unstructured data. Would this yield much value, considering I could design a bespoke predicitve model directly on to the free text? Would there be much difference in results from the free text compared to its structured counterpart?",t2_1477nx,False,,0,False,"Structuring free text, then performing analysis vs. Performing analysis on unstructured free text",[],r/LanguageTechnology,False,6,,0,,False,t3_not78h,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1622462985.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi reddit, NLP newbie here&lt;/p&gt;

&lt;p&gt;I am trying to understand if there is any value in creating a table out of free text, versus predictive analysis on the free text itself. &lt;/p&gt;

&lt;p&gt;For context, I am working with 2 million clinical notes from the MIMIC-III dataset, and I would like to tabluate all this unstructured data. Would this yield much value, considering I could design a bespoke predicitve model directly on to the free text? Would there be much difference in results from the free text compared to its structured counterpart?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,not78h,True,,dannyhoes,,10,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/not78h/structuring_free_text_then_performing_analysis_vs/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/not78h/structuring_free_text_then_performing_analysis_vs/,30199,1622434185.0,0,,False,,,,,,522
93,,LanguageTechnology,,t2_5cvo57qv,False,,0,False,Python bindings for LibreTranslate,[],r/LanguageTechnology,False,6,,0,,False,t3_nooqsl,False,dark,0.76,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1622451137.0,text,6,,,text,github.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nooqsl,True,,argosopentech,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nooqsl/python_bindings_for_libretranslate/,all_ads,False,https://github.com/argosopentech/LibreTranslate-py,30199,1622422337.0,0,,False,https://github.com/argosopentech/LibreTranslate-py,,,,,0
94,,LanguageTechnology,"So basically what i'm attempting is to benchmark Cosine and WMD for a very generic use-case of textual similarity (This is probably quite the noob question).

My dataset is a parallel corpus made of pairs of paraphrased sentences. My issue is mainly in how I can compare the two metrics since they are of different order of magnitude:

* Cosine is \[-1, 1\] (or \[1, 0\]).
* WMD is not bound by an upper limit and goes from 0 to \~3.83 for my small dataset, and I can't think of a way to normalise it without introducing a bias.

My use case does not involve some form of classification, in which case I could just compare normally by how good the classification is. As for setting a threshold, (for example if WMD(S1, S2) &lt; 0.5 then consider the pair to be similar) and I really don't know how I can set a threshold based on this benchmarking alone because all pairs are actually paraphrases, so the expected output is always a distance of 0, while my results are quite sparse.

What would you guys suggest (a metric to compare with or a way to determine a threshold for the two of them, or anything else) to pick which of the 2 is performing better?",t2_4ba7h526,False,,0,False,Comparing between Cosine Similarity and Word Mover's Distance,[],r/LanguageTechnology,False,6,,0,,False,t3_noobre,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1622449605.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So basically what i&amp;#39;m attempting is to benchmark Cosine and WMD for a very generic use-case of textual similarity (This is probably quite the noob question).&lt;/p&gt;

&lt;p&gt;My dataset is a parallel corpus made of pairs of paraphrased sentences. My issue is mainly in how I can compare the two metrics since they are of different order of magnitude:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Cosine is [-1, 1] (or [1, 0]).&lt;/li&gt;
&lt;li&gt;WMD is not bound by an upper limit and goes from 0 to ~3.83 for my small dataset, and I can&amp;#39;t think of a way to normalise it without introducing a bias.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My use case does not involve some form of classification, in which case I could just compare normally by how good the classification is. As for setting a threshold, (for example if WMD(S1, S2) &amp;lt; 0.5 then consider the pair to be similar) and I really don&amp;#39;t know how I can set a threshold based on this benchmarking alone because all pairs are actually paraphrases, so the expected output is always a distance of 0, while my results are quite sparse.&lt;/p&gt;

&lt;p&gt;What would you guys suggest (a metric to compare with or a way to determine a threshold for the two of them, or anything else) to pick which of the 2 is performing better?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,noobre,True,,Meaveready,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/noobre/comparing_between_cosine_similarity_and_word/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/noobre/comparing_between_cosine_similarity_and_word/,30199,1622420805.0,0,,False,,,,,,1154
95,,LanguageTechnology,"Hi everyone, 

I'm a language learning/teaching researcher who has been trying to develop little tools to help other researchers recently. As you might guess, low-proficiency learner texts are quite a pain in the neck in this context due to frequent grammatical errors. 

So I'm actually looking for suggestions regarding a particular POS tagging problem as seen below:

""I think very good idea."" 

There, the learner obviously means ""I think -it is a- very good idea"" but since the function words are not there, POS taggers cannot accurately identify the parts of speech in the sentence. 

How could one deal with such sentences from a matching/POS tagging perspective? 

I have been considering ""next word generation"" but so far I haven't tested it properly. So any suggestion is welcome. 

Thanks in advance big time.",t2_qrcj0xf,False,,0,False,"Spacy Matcher, POS Tagging and Grammatical Errors",[],r/LanguageTechnology,False,6,,0,,False,t3_no7wrt,False,dark,1.0,,public,14,0,{},,False,[],,False,False,,{},,False,14,,False,False,,False,,[],{},,True,,1622397292.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;

&lt;p&gt;I&amp;#39;m a language learning/teaching researcher who has been trying to develop little tools to help other researchers recently. As you might guess, low-proficiency learner texts are quite a pain in the neck in this context due to frequent grammatical errors. &lt;/p&gt;

&lt;p&gt;So I&amp;#39;m actually looking for suggestions regarding a particular POS tagging problem as seen below:&lt;/p&gt;

&lt;p&gt;&amp;quot;I think very good idea.&amp;quot; &lt;/p&gt;

&lt;p&gt;There, the learner obviously means &amp;quot;I think -it is a- very good idea&amp;quot; but since the function words are not there, POS taggers cannot accurately identify the parts of speech in the sentence. &lt;/p&gt;

&lt;p&gt;How could one deal with such sentences from a matching/POS tagging perspective? &lt;/p&gt;

&lt;p&gt;I have been considering &amp;quot;next word generation&amp;quot; but so far I haven&amp;#39;t tested it properly. So any suggestion is welcome. &lt;/p&gt;

&lt;p&gt;Thanks in advance big time.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,no7wrt,True,,applingu,,10,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/no7wrt/spacy_matcher_pos_tagging_and_grammatical_errors/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/no7wrt/spacy_matcher_pos_tagging_and_grammatical_errors/,30199,1622368492.0,0,,False,,,,,,820
96,,LanguageTechnology,"Whenever i try to run :
model = MBartForConditionalGeneration.from_pretrained("" [local path]/mbart-large-50-one-to-many-mmt"")
My computer ether freezes or it takes 15-20 minutes to load the model.

I am using it for translation
Code: https://huggingface.co/facebook/mbart-large-50-one-to-many-mmt

Any solution fo this?

-Thanks",t2_7wron7g8,False,,0,False,Loading mbart-large-50-one-to-many-mmt is very slow,[],r/LanguageTechnology,False,6,,0,,False,t3_no6idv,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1622391064.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Whenever i try to run :
model = MBartForConditionalGeneration.from_pretrained(&amp;quot; [local path]/mbart-large-50-one-to-many-mmt&amp;quot;)
My computer ether freezes or it takes 15-20 minutes to load the model.&lt;/p&gt;

&lt;p&gt;I am using it for translation
Code: &lt;a href=""https://huggingface.co/facebook/mbart-large-50-one-to-many-mmt""&gt;https://huggingface.co/facebook/mbart-large-50-one-to-many-mmt&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Any solution fo this?&lt;/p&gt;

&lt;p&gt;-Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,no6idv,True,,arkhamrising,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/no6idv/loading_mbartlarge50onetomanymmt_is_very_slow/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/no6idv/loading_mbartlarge50onetomanymmt_is_very_slow/,30199,1622362264.0,0,,False,,,,,,328
97,,LanguageTechnology,,t2_9pdfk3qa,False,,0,False,The competition that involves linguistics and logic,[],r/LanguageTechnology,False,6,,0,,False,t3_nniznw,False,dark,0.95,,public,15,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/9tyG71ogKlE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'International Linguistics Olympiad --- Basque Numbers', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/9tyG71ogKlE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'LogicaLing', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/9tyG71ogKlE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCw97O4KNq__XiyyS7IWqARA'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/9tyG71ogKlE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nniznw', 'height': 200}",,False,15,,False,False,,False,,[],{},,False,,1622305872.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nniznw,True,,Proof_Ad2445,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nniznw/the_competition_that_involves_linguistics_and/,all_ads,False,https://youtu.be/9tyG71ogKlE,30199,1622277072.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'International Linguistics Olympiad --- Basque Numbers', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/9tyG71ogKlE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'LogicaLing', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/9tyG71ogKlE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCw97O4KNq__XiyyS7IWqARA'}}",False,https://youtu.be/9tyG71ogKlE,,,,,0
98,,LanguageTechnology,,t2_1sev,False,,0,False,How to dramatically improve the reasoning ability of GPT-3,[],r/LanguageTechnology,False,6,,0,,False,t3_nn75r9,False,dark,1.0,,public,19,0,{},,False,[],,False,False,,{},,False,19,,False,False,,False,,[],{},,False,,1622263427.0,text,6,,,text,blog.andrewcantino.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nn75r9,True,,tectonic,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nn75r9/how_to_dramatically_improve_the_reasoning_ability/,all_ads,False,https://blog.andrewcantino.com/blog/2021/05/28/how-to-dramatically-improve-the-reasoning-ability-of-GPT-3/,30199,1622234627.0,0,,False,https://blog.andrewcantino.com/blog/2021/05/28/how-to-dramatically-improve-the-reasoning-ability-of-GPT-3/,,,,,0
99,,LanguageTechnology,"I am looking for a web based tool for an NLP annotation task (similar to docanno for example). 

The catch is that I have to create a project and make it easily accessed with a link without the need to create an account per each annotator.

Any tool that comes close to this criteria?",t2_60btb74b,False,,0,False,Any free open-access NLP annotation tools?,[],r/LanguageTechnology,False,6,,0,,False,t3_nn0lyf,False,dark,0.96,,public,19,0,{},,False,[],,False,False,,{},,False,19,,False,False,,False,,[],{},,True,,1622245310.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am looking for a web based tool for an NLP annotation task (similar to docanno for example). &lt;/p&gt;

&lt;p&gt;The catch is that I have to create a project and make it easily accessed with a link without the need to create an account per each annotator.&lt;/p&gt;

&lt;p&gt;Any tool that comes close to this criteria?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nn0lyf,True,,shoowmewhatyougoot,,11,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nn0lyf/any_free_openaccess_nlp_annotation_tools/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nn0lyf/any_free_openaccess_nlp_annotation_tools/,30199,1622216510.0,0,,False,,,,,,284
100,,LanguageTechnology,"Aside from self-hosting, where can I try non-fine-tuned T5 11B? 

If that’s not possible, are there any publicly available examples of its output?",t2_8xraz5pp,False,,0,False,Trying T5 11B,[],r/LanguageTechnology,False,6,,0,,False,t3_nnggrj,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1622295130.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Aside from self-hosting, where can I try non-fine-tuned T5 11B? &lt;/p&gt;

&lt;p&gt;If that’s not possible, are there any publicly available examples of its output?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nnggrj,True,,Effective_Sea_9367,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nnggrj/trying_t5_11b/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nnggrj/trying_t5_11b/,30199,1622266330.0,0,,False,,,,,,146
101,,LanguageTechnology,"Just as the title says, I'm looking for a way to benchmark the sentiment analysis component of two or more NLP tools like retextjs and Stanford Core NLP. Are there any tools or techniques out for doing this?",t2_5xfvbvw9,False,,0,False,Benchmarking An NLP Tool,[],r/LanguageTechnology,False,6,,0,,False,t3_nnas13,False,dark,0.75,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1622274228.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Just as the title says, I&amp;#39;m looking for a way to benchmark the sentiment analysis component of two or more NLP tools like retextjs and Stanford Core NLP. Are there any tools or techniques out for doing this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nnas13,True,,ProperWait1,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nnas13/benchmarking_an_nlp_tool/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nnas13/benchmarking_an_nlp_tool/,30199,1622245428.0,0,,False,,,,,,207
102,,LanguageTechnology,Which are some coveted NLP Certifications available in the market having good industry recognition?,t2_b3gv09ny,False,,0,False,NLP Certifications,[],r/LanguageTechnology,False,6,,0,,False,t3_nmxhny,False,dark,0.83,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1622236167.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Which are some coveted NLP Certifications available in the market having good industry recognition?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nmxhny,True,,Key-Feature-1228,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nmxhny/nlp_certifications/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nmxhny/nlp_certifications/,30199,1622207367.0,0,,False,,,,,,99
103,,LanguageTechnology,"I've wondered how good neural nets can get at predicting semantic similarity at a more abstract level between different topics. For example, finding context between math terminology like 'vector' and 'matrix' and programming terminology like 'list/array' and '2d array'. Also, finding context between common data types, functions, concepts between different languages and frameworks/libraries. Ideally, I'd want a binary output that takes two strings as input to compare.

Question is what would be the approach to a problem like this, would it need carefully labelled data ('translating' the terminology between the two topics) or is a self-supervised method at all possible? I've only recently got into data so I could be way off here on what is possible and what is not.",t2_4av6pmbp,False,,0,False,Semantic similarity between programming languages and math terminology,[],r/LanguageTechnology,False,6,,0,,False,t3_nmuy6b,False,dark,0.83,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1622227070.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve wondered how good neural nets can get at predicting semantic similarity at a more abstract level between different topics. For example, finding context between math terminology like &amp;#39;vector&amp;#39; and &amp;#39;matrix&amp;#39; and programming terminology like &amp;#39;list/array&amp;#39; and &amp;#39;2d array&amp;#39;. Also, finding context between common data types, functions, concepts between different languages and frameworks/libraries. Ideally, I&amp;#39;d want a binary output that takes two strings as input to compare.&lt;/p&gt;

&lt;p&gt;Question is what would be the approach to a problem like this, would it need carefully labelled data (&amp;#39;translating&amp;#39; the terminology between the two topics) or is a self-supervised method at all possible? I&amp;#39;ve only recently got into data so I could be way off here on what is possible and what is not.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nmuy6b,True,,01jonathanf,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nmuy6b/semantic_similarity_between_programming_languages/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nmuy6b/semantic_similarity_between_programming_languages/,30199,1622198270.0,0,,False,,,,,,773
104,,LanguageTechnology,"Hi all, at the moment im working on a TTS system and i want to evaluate it. Unfortunately i can not do the usual Mean opinion score, so i'm looking at more numeric evaluations to compare my generated speech to gt.

While looking for some i found the fastspeech2 paper where they use Dynamic Time Warping (DTW) to evaluate the pitch in the generated speech. Here they explain it as: ""average DTW distances (DTW) of pitch in ground-truth and synthesized audio"".

Now, when i try to calculate the DTW with [this](https://dynamictimewarping.github.io/python/) python package i seem to get strange results. In the fast speech 2 paper their DTW distances are between 24-26. Mine are all between 9000-10000 when i try to do this.

the way i try to do this is as followed(in a short pseudo code):

    for all gt and prediction:
        distance = DTW(gt,pred)
        total += distance
    result = total/number_of_samples

I've tried several distances from the [scipy page](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html) but none of them seem to get close to the range of 24-26 (when i use the seuclidean distance i get a total of 96, but this still seems too high)

the only way i can come close to the 24-26 range is by also averaging the dtw (with normal euclidean distance) by the time series length as followed:

    for all gt and prediction:
        distance = DTW(gt,pred)/length_of_prediction
        total += distance
    result = total/number_of_samples

can someone tell me if this is correct or how i should calculate the average DTW distance instead? Maybe any links (i tried to read the refrence from the paper to Meinard Muller's chapter on this, but i couldnt figure out which distance to use from there).

Any other metrics to evaluate speech is also always welcome :)

Thank you and kind regards!

(i'm tagging u/rayeren since he posted the FS1/FS2 papers on r/MachineLearning and i think he's one of the authors. if so, can you comment on this please? :) )",t2_4k7iew3x,False,,0,False,"FastSpeech2 DTW computing, and other TTS evaluation methods",[],r/LanguageTechnology,False,6,,0,,False,t3_nn272c,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,1622221538.0,,[],{},,True,,1622249691.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, at the moment im working on a TTS system and i want to evaluate it. Unfortunately i can not do the usual Mean opinion score, so i&amp;#39;m looking at more numeric evaluations to compare my generated speech to gt.&lt;/p&gt;

&lt;p&gt;While looking for some i found the fastspeech2 paper where they use Dynamic Time Warping (DTW) to evaluate the pitch in the generated speech. Here they explain it as: &amp;quot;average DTW distances (DTW) of pitch in ground-truth and synthesized audio&amp;quot;.&lt;/p&gt;

&lt;p&gt;Now, when i try to calculate the DTW with &lt;a href=""https://dynamictimewarping.github.io/python/""&gt;this&lt;/a&gt; python package i seem to get strange results. In the fast speech 2 paper their DTW distances are between 24-26. Mine are all between 9000-10000 when i try to do this.&lt;/p&gt;

&lt;p&gt;the way i try to do this is as followed(in a short pseudo code):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for all gt and prediction:
    distance = DTW(gt,pred)
    total += distance
result = total/number_of_samples
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&amp;#39;ve tried several distances from the &lt;a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html""&gt;scipy page&lt;/a&gt; but none of them seem to get close to the range of 24-26 (when i use the seuclidean distance i get a total of 96, but this still seems too high)&lt;/p&gt;

&lt;p&gt;the only way i can come close to the 24-26 range is by also averaging the dtw (with normal euclidean distance) by the time series length as followed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for all gt and prediction:
    distance = DTW(gt,pred)/length_of_prediction
    total += distance
result = total/number_of_samples
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;can someone tell me if this is correct or how i should calculate the average DTW distance instead? Maybe any links (i tried to read the refrence from the paper to Meinard Muller&amp;#39;s chapter on this, but i couldnt figure out which distance to use from there).&lt;/p&gt;

&lt;p&gt;Any other metrics to evaluate speech is also always welcome :)&lt;/p&gt;

&lt;p&gt;Thank you and kind regards!&lt;/p&gt;

&lt;p&gt;(i&amp;#39;m tagging &lt;a href=""/u/rayeren""&gt;u/rayeren&lt;/a&gt; since he posted the FS1/FS2 papers on &lt;a href=""/r/MachineLearning""&gt;r/MachineLearning&lt;/a&gt; and i think he&amp;#39;s one of the authors. if so, can you comment on this please? :) )&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nn272c,True,,PlatoTheSloth,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nn272c/fastspeech2_dtw_computing_and_other_tts/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nn272c/fastspeech2_dtw_computing_and_other_tts/,30199,1622220891.0,3,,False,,,,,,2011
105,,LanguageTechnology,,t2_50ul4xke,False,,0,False,[P] Do Context-Aware Translation Models Pay the Right Attention?,[],r/LanguageTechnology,False,6,,0,,False,t3_nmuhni,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1622225214.0,text,6,,,text,self.DeepLearningPapers,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nmuhni,True,,DL_updates,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nmuhni/p_do_contextaware_translation_models_pay_the/,all_ads,False,/r/DeepLearningPapers/comments/nmhyfr/p_do_contextaware_translation_models_pay_the/,30199,1622196414.0,0,,False,/r/DeepLearningPapers/comments/nmhyfr/p_do_contextaware_translation_models_pay_the/,"[{'approved_at_utc': None, 'subreddit': 'DeepLearningPapers', 'selftext': '🔗 Paper: [https://arxiv.org/abs/2105.06977v2](https://arxiv.org/abs/2105.06977v2)\n\n👫 Authors: Kayo Yin, Patrick Fernandes, Danish Pruthi, Aditi Chaudhary, André F. T. Martins, Graham Neubig\n\nIt is interesting to see how the attention mechanism has been hyper-investigated in ACL 2021. This study on human-machine behavior seems interesting. What do you think?\n\n60sec highlights: [https://www.youtube.com/watch?v=9e3thC4U\\_sU](https://www.youtube.com/watch?v=9e3thC4U_sU)\n\nJoin Telegram Channel: [https://t.me/deeplearning\\_updates](https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbThsNFpCczRnX0Q4bHNna1ZEZUo0Y3JMUXp1QXxBQ3Jtc0trem9rSWJLYWxRRmFTamtDSHlCVm5FMG5RT2FYYkl3LW9BV1VDMW51RHBSSkdaNnloOEVXbnJhVW5ndTJ6RmQ3Rnd5WFdqUVVHNU9UNFZJcThJekZ4M3ZIUkw0ZEVZMmJOMEhSTzRwNkdyLWhGSmVsYw&amp;q=https%3A%2F%2Ft.me%2Fdeeplearning_updates)', 'author_fullname': 't2_50ul4xke', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[P] Do Context-Aware Translation Models Pay the Right Attention?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/DeepLearningPapers', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_nmhyfr', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1622179263.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.DeepLearningPapers', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;🔗 Paper: &lt;a href=""https://arxiv.org/abs/2105.06977v2""&gt;https://arxiv.org/abs/2105.06977v2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;👫 Authors: Kayo Yin, Patrick Fernandes, Danish Pruthi, Aditi Chaudhary, André F. T. Martins, Graham Neubig&lt;/p&gt;\n\n&lt;p&gt;It is interesting to see how the attention mechanism has been hyper-investigated in ACL 2021. This study on human-machine behavior seems interesting. What do you think?&lt;/p&gt;\n\n&lt;p&gt;60sec highlights: &lt;a href=""https://www.youtube.com/watch?v=9e3thC4U_sU""&gt;https://www.youtube.com/watch?v=9e3thC4U_sU&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Join Telegram Channel: &lt;a href=""https://www.youtube.com/redirect?event=video_description&amp;amp;redir_token=QUFFLUhqbThsNFpCczRnX0Q4bHNna1ZEZUo0Y3JMUXp1QXxBQ3Jtc0trem9rSWJLYWxRRmFTamtDSHlCVm5FMG5RT2FYYkl3LW9BV1VDMW51RHBSSkdaNnloOEVXbnJhVW5ndTJ6RmQ3Rnd5WFdqUVVHNU9UNFZJcThJekZ4M3ZIUkw0ZEVZMmJOMEhSTzRwNkdyLWhGSmVsYw&amp;amp;q=https%3A%2F%2Ft.me%2Fdeeplearning_updates""&gt;https://t.me/deeplearning_updates&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_38ri8', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'nmhyfr', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'DL_updates', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/DeepLearningPapers/comments/nmhyfr/p_do_contextaware_translation_models_pay_the/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/DeepLearningPapers/comments/nmhyfr/p_do_contextaware_translation_models_pay_the/', 'subreddit_subscribers': 15753, 'created_utc': 1622150463.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_nmhyfr,,,0
106,,LanguageTechnology,"I using the BERT transformer to solve the classification problem.

I am confused with the size of the learning rate of the BERT

The author suggests of using one of the following parameters

    learning rates: 3e-4, 1e-4, 5e-5, 3e-5
    
     

I know that a small learning rate makes our model learn very slow, however it also helps prevent overfitting, in contrast to big learning which learns faster but it can lead to overfitting.

&amp;#x200B;

When  I use a learning rate of 1e-5 I get the following result

    {""train"": {""eval_examples_count"": 8548, ""metrics"": {""f1_weighted"": 0.721, ""f1_macro"": 0.7201, ""accuracy"": 0.7255, ""roc_auc"": 0.8883}, ""time_spent"": ""0:02:06""}}
    {""valid"": {""eval_examples_count"": 2849, ""metrics"": {""f1_weighted"": 0.6766, ""f1_macro"": 0.6784, ""accuracy"": 0.6816, ""roc_auc"": 0.8545}, ""time_spent"": ""0:00:42""}}
    {""test"": {""eval_examples_count"": 2850, ""metrics"": {""f1_weighted"": 0.7003, ""f1_macro"": 0.7008, ""accuracy"": 0.7046, ""roc_auc"": 0.8685}, ""time_spent"": ""0:00:42""}}

However when I use 5e-5:

    {""train"": {""eval_examples_count"": 8548, ""metrics"": {""f1_weighted"": 0.1617, ""f1_macro"": 0.1647, ""accuracy"": 0.3269, ""roc_auc"": 0.5159}, ""time_spent"": ""0:02:07""}}
    {""valid"": {""eval_examples_count"": 2849, ""metrics"": {""f1_weighted"": 0.1743, ""f1_macro"": 0.1704, ""accuracy"": 0.3412, ""roc_auc"": 0.5321}, ""time_spent"": ""0:00:42""}}
    {""test"": {""eval_examples_count"": 2850, ""metrics"": {""f1_weighted"": 0.1758, ""f1_macro"": 0.1705, ""accuracy"": 0.3435, ""roc_auc"": 0.5208}, ""time_spent"": ""0:00:42""}}

IT DOES NOT LEARN ANYTHING

&amp;#x200B;

So I thought 1e-5 is small and 5e-5 is the big learning rate, am I right? 

What is the ascending order of these learning rates?

Which one considered as big and which is considered as small?",t2_6c0lef9b,False,,0,False,What is considered as a small learning rate?,[],r/LanguageTechnology,False,6,,0,,False,t3_nmljji,False,dark,1.0,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1622190286.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I using the BERT transformer to solve the classification problem.&lt;/p&gt;

&lt;p&gt;I am confused with the size of the learning rate of the BERT&lt;/p&gt;

&lt;p&gt;The author suggests of using one of the following parameters&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;learning rates: 3e-4, 1e-4, 5e-5, 3e-5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I know that a small learning rate makes our model learn very slow, however it also helps prevent overfitting, in contrast to big learning which learns faster but it can lead to overfitting.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;When  I use a learning rate of 1e-5 I get the following result&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;train&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 8548, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.721, &amp;quot;f1_macro&amp;quot;: 0.7201, &amp;quot;accuracy&amp;quot;: 0.7255, &amp;quot;roc_auc&amp;quot;: 0.8883}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:02:06&amp;quot;}}
{&amp;quot;valid&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 2849, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.6766, &amp;quot;f1_macro&amp;quot;: 0.6784, &amp;quot;accuracy&amp;quot;: 0.6816, &amp;quot;roc_auc&amp;quot;: 0.8545}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:00:42&amp;quot;}}
{&amp;quot;test&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 2850, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.7003, &amp;quot;f1_macro&amp;quot;: 0.7008, &amp;quot;accuracy&amp;quot;: 0.7046, &amp;quot;roc_auc&amp;quot;: 0.8685}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:00:42&amp;quot;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However when I use 5e-5:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;train&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 8548, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.1617, &amp;quot;f1_macro&amp;quot;: 0.1647, &amp;quot;accuracy&amp;quot;: 0.3269, &amp;quot;roc_auc&amp;quot;: 0.5159}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:02:07&amp;quot;}}
{&amp;quot;valid&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 2849, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.1743, &amp;quot;f1_macro&amp;quot;: 0.1704, &amp;quot;accuracy&amp;quot;: 0.3412, &amp;quot;roc_auc&amp;quot;: 0.5321}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:00:42&amp;quot;}}
{&amp;quot;test&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 2850, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.1758, &amp;quot;f1_macro&amp;quot;: 0.1705, &amp;quot;accuracy&amp;quot;: 0.3435, &amp;quot;roc_auc&amp;quot;: 0.5208}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:00:42&amp;quot;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;IT DOES NOT LEARN ANYTHING&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So I thought 1e-5 is small and 5e-5 is the big learning rate, am I right? &lt;/p&gt;

&lt;p&gt;What is the ascending order of these learning rates?&lt;/p&gt;

&lt;p&gt;Which one considered as big and which is considered as small?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nmljji,True,,strangeguy111,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nmljji/what_is_considered_as_a_small_learning_rate/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nmljji/what_is_considered_as_a_small_learning_rate/,30199,1622161486.0,0,,False,,,,,,1763
107,,LanguageTechnology,"Although BERT became really popular after its release, it did have some limitations. And there were certain limitations associated with autoregressive methods like ELMo and GPT as well. XLNet was introduced to get the best of both worlds while at the same time not include their weaknesses.

In continuation of my Paper Notes series, I have written an informative summary of the paper. Personally, reading the XLNet paper was a very fun experience. I was amazed at every step, how they were including stuff to make the whole model work so well. The paper contained many interesting concepts that I had to give time to understand. So don't worry if you don't get it on the first go. Check out the links below and happy reading!

Paper Summary - [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/)

Annotated Paper - [https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/XLNet.pdf](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/XLNet.pdf)",t2_5xzd9om,False,,0,False,XLNet - Annotated Paper + Paper Summary,[],r/LanguageTechnology,False,6,,0,,False,t3_nmr2qi,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1622210163.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Although BERT became really popular after its release, it did have some limitations. And there were certain limitations associated with autoregressive methods like ELMo and GPT as well. XLNet was introduced to get the best of both worlds while at the same time not include their weaknesses.&lt;/p&gt;

&lt;p&gt;In continuation of my Paper Notes series, I have written an informative summary of the paper. Personally, reading the XLNet paper was a very fun experience. I was amazed at every step, how they were including stuff to make the whole model work so well. The paper contained many interesting concepts that I had to give time to understand. So don&amp;#39;t worry if you don&amp;#39;t get it on the first go. Check out the links below and happy reading!&lt;/p&gt;

&lt;p&gt;Paper Summary - &lt;a href=""https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/""&gt;XLNet: Generalized Autoregressive Pretraining for Language Understanding&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Annotated Paper - &lt;a href=""https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/XLNet.pdf""&gt;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/XLNet.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nmr2qi,True,,shreyansh26,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nmr2qi/xlnet_annotated_paper_paper_summary/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nmr2qi/xlnet_annotated_paper_paper_summary/,30199,1622181363.0,0,,False,,,,,,1075
108,,LanguageTechnology,"I need a good way to generate paraphrases from given input sentences. The input sentences already have the same meaning/are paraphrases. But I want more paraphrases (for example with different syntactic structures, with synonyms etc.). Ideally, I want to combine lexical and syntactic paraphrasing. What could be the state of the art solution for this problem? I read in some paper about phrase based translation methods but I don't think this suits my task. I am open for paper suggestions!",t2_6fr49wdr,False,,0,False,Paraphrase Generation given Input Sentences,[],r/LanguageTechnology,False,6,,0,,False,t3_nmcowu,False,dark,1.0,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1622165383.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I need a good way to generate paraphrases from given input sentences. The input sentences already have the same meaning/are paraphrases. But I want more paraphrases (for example with different syntactic structures, with synonyms etc.). Ideally, I want to combine lexical and syntactic paraphrasing. What could be the state of the art solution for this problem? I read in some paper about phrase based translation methods but I don&amp;#39;t think this suits my task. I am open for paper suggestions!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nmcowu,True,,LargeBrick7,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nmcowu/paraphrase_generation_given_input_sentences/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nmcowu/paraphrase_generation_given_input_sentences/,30199,1622136583.0,0,,False,,,,,,491
109,,LanguageTechnology,,t2_21qaqh1p,False,,0,False,English Paraphrasing in Python with Parrot,[],r/LanguageTechnology,False,6,,0,,False,t3_nmizd0,False,dark,0.83,,public,4,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/FFjpQiVFPZo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'English Paraphrasing in Python for NLU Text Augmentation - Chatbots Training | Applied NLP in Python', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/FFjpQiVFPZo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/FFjpQiVFPZo/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/FFjpQiVFPZo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nmizd0', 'height': 200}",,False,4,,False,False,,False,,[],{},,False,,1622182123.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nmizd0,True,,dulldata,,0,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/nmizd0/english_paraphrasing_in_python_with_parrot/,all_ads,False,https://youtu.be/FFjpQiVFPZo,30199,1622153323.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'English Paraphrasing in Python for NLU Text Augmentation - Chatbots Training | Applied NLP in Python', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/FFjpQiVFPZo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/FFjpQiVFPZo/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,https://youtu.be/FFjpQiVFPZo,,,,,0
110,,LanguageTechnology,,t2_hkv9s,False,,0,False,Summarizing NLP Research Papers,[],r/LanguageTechnology,False,6,,0,,False,t3_nmacyt,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,False,,1622159175.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nmacyt,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nmacyt/summarizing_nlp_research_papers/,all_ads,False,https://link.medium.com/82UC06vuBgb,30199,1622130375.0,0,,False,https://link.medium.com/82UC06vuBgb,,,,,0
111,,LanguageTechnology,"The GPT-2 model was a major breakthrough in the path of creating a general multitask NLP system that was totally unsupervised. It demonstrated that given a large training corpus and a large model size, the language model was capable of learning the knowledge required for solving these tasks. It was not perfect, however, and performed poorly on some tasks as well.

I went through the paper and have written an informative summary of the paper. The paper was quite easy to follow and the experimentation section had interesting observations. Check out the links below and happy reading!

Paper Summary -  [Language Models are Unsupervised Multitask Learners](https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/)

Annotated Paper -  [https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT2.pdf](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT2.pdf)",t2_5xzd9om,False,,0,False,GPT-2 - Annotated Paper + Paper Summary,[],r/LanguageTechnology,False,6,,0,,False,t3_nm39gt,False,dark,0.93,,public,21,0,{},,False,[],,False,False,,{},,False,21,,False,False,,False,,[],{},,True,,1622134886.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The GPT-2 model was a major breakthrough in the path of creating a general multitask NLP system that was totally unsupervised. It demonstrated that given a large training corpus and a large model size, the language model was capable of learning the knowledge required for solving these tasks. It was not perfect, however, and performed poorly on some tasks as well.&lt;/p&gt;

&lt;p&gt;I went through the paper and have written an informative summary of the paper. The paper was quite easy to follow and the experimentation section had interesting observations. Check out the links below and happy reading!&lt;/p&gt;

&lt;p&gt;Paper Summary -  &lt;a href=""https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/""&gt;Language Models are Unsupervised Multitask Learners&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Annotated Paper -  &lt;a href=""https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT2.pdf""&gt;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT2.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nm39gt,True,,shreyansh26,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nm39gt/gpt2_annotated_paper_paper_summary/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nm39gt/gpt2_annotated_paper_paper_summary/,30199,1622106086.0,0,,False,,,,,,923
112,,LanguageTechnology,How long was it and what was it about?,t2_be2v1j11,False,,0,False,For GPT3 users - What's the longest output you've had it write?,[],r/LanguageTechnology,False,6,,0,,False,t3_nm53h3,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1622142844.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How long was it and what was it about?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nm53h3,True,,ricksElar,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nm53h3/for_gpt3_users_whats_the_longest_output_youve_had/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nm53h3/for_gpt3_users_whats_the_longest_output_youve_had/,30199,1622114044.0,0,,False,,,,,,38
113,,LanguageTechnology,"My first youtube video discussing a research paper -

**Transformer-XL: Attentive Language Models Beyond a Fixed Length Context** [https://youtu.be/2cmVnQNWQt8](https://youtu.be/2cmVnQNWQt8) 

Check it out!",t2_ccnld7oq,False,,0,False,Transformer-XL: Attentive Language Models Beyond a Fixed Length Context,[],r/LanguageTechnology,False,6,,0,,False,t3_nlqr66,False,dark,0.92,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1622089911.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My first youtube video discussing a research paper -&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Transformer-XL: Attentive Language Models Beyond a Fixed Length Context&lt;/strong&gt; &lt;a href=""https://youtu.be/2cmVnQNWQt8""&gt;https://youtu.be/2cmVnQNWQt8&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Check it out!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nlqr66,True,,abhinandy,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nlqr66/transformerxl_attentive_language_models_beyond_a/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nlqr66/transformerxl_attentive_language_models_beyond_a/,30199,1622061111.0,0,,False,,,,,,206
114,,LanguageTechnology,"Hi there,

I recently wrote a blog post about zero-shot knowledge distillation from big language models with a method called DINO (Datasets from Instructions 🦕) that does not require any (labeled or unlabeled) data or access to model internals. The key idea is to prompt the LM to generate an entire dataset from scratch on which much smaller models can then be trained.

 I'd be very happy to hear your thoughts (both on the blog post and on the method) 😊

📝 Link: http://timoschick.com/research/2021/05/19/dino.html",t2_383etasr,False,,0,False,Zero-Shot Knowledge Distillation From GPT-3,[],r/LanguageTechnology,False,6,,0,,False,t3_nlcx98,False,dark,0.99,,public,27,0,{},,False,[],,False,False,,{},,False,27,,False,False,,False,,[],{},,True,,1622050788.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there,&lt;/p&gt;

&lt;p&gt;I recently wrote a blog post about zero-shot knowledge distillation from big language models with a method called DINO (Datasets from Instructions 🦕) that does not require any (labeled or unlabeled) data or access to model internals. The key idea is to prompt the LM to generate an entire dataset from scratch on which much smaller models can then be trained.&lt;/p&gt;

&lt;p&gt;I&amp;#39;d be very happy to hear your thoughts (both on the blog post and on the method) 😊&lt;/p&gt;

&lt;p&gt;📝 Link: &lt;a href=""http://timoschick.com/research/2021/05/19/dino.html""&gt;http://timoschick.com/research/2021/05/19/dino.html&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nlcx98,True,,timoschick,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nlcx98/zeroshot_knowledge_distillation_from_gpt3/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nlcx98/zeroshot_knowledge_distillation_from_gpt3/,30199,1622021988.0,0,,False,,,,,,517
115,,LanguageTechnology,"Hi there!

I'm working on a project in which I need to extract the topics from a collection of 250K facebook posts. 

Can someone recommend (in high level) what's the best approach to do something like that? Ways that will generate the best results.

Thanks!",t2_4v9mxawe,False,,0,False,Extracting topics from 250k facebook posts,[],r/LanguageTechnology,False,6,,0,,False,t3_nlfysm,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1622061427.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working on a project in which I need to extract the topics from a collection of 250K facebook posts. &lt;/p&gt;

&lt;p&gt;Can someone recommend (in high level) what&amp;#39;s the best approach to do something like that? Ways that will generate the best results.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nlfysm,True,,itreallyreallydoesnt,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nlfysm/extracting_topics_from_250k_facebook_posts/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nlfysm/extracting_topics_from_250k_facebook_posts/,30199,1622032627.0,0,,False,,,,,,258
116,,LanguageTechnology,,t2_hkv9s,False,,0,False,Efficient System for Grammar Error Correction on Mobile Devices (Research Paper Summary),[],r/LanguageTechnology,False,6,,0,,False,t3_nlau0l,False,dark,1.0,,public,13,0,{},,False,[],,False,False,,{},,False,13,,False,False,,False,,[],{},,False,,1622042197.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nlau0l,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nlau0l/efficient_system_for_grammar_error_correction_on/,all_ads,False,https://link.medium.com/tVSykpafzgb,30199,1622013397.0,0,,False,https://link.medium.com/tVSykpafzgb,,,,,0
117,,LanguageTechnology,,t2_icvu8,False,,0,False,Can you classify texts without any labeled examples? Putting zero-shot classification to the test.,[],r/LanguageTechnology,False,6,,0,,False,t3_nkr5gr,False,dark,0.99,,public,33,0,{},,False,[],,False,False,,{},,False,33,,False,False,,False,,[],{},,False,,1621981972.0,text,6,,,text,nlp.town,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nkr5gr,True,,yvespeirsman,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nkr5gr/can_you_classify_texts_without_any_labeled/,all_ads,False,https://nlp.town/blog/zero-shot-classification/,30199,1621953172.0,0,,False,https://nlp.town/blog/zero-shot-classification/,,,,,0
118,,LanguageTechnology,"In today’s era, all top benchmarks in natural language processing are dominated by Transformer-based models. In a machine learning model, the most critical elements of the training process are the model code, training data, and available computing resources.

With the Transformer family of models, researchers have now finally come up with a way to increase the performance of a model infinitely by increasing the amount of training data and compute power.

OpenAI did this with GPT-2 and with GPT-3. They used a private corpus of 500 billion tokens for training the model and spent $50 million in computing costs.

Full Article: [https://www.marktechpost.com/2021/05/24/eleutherai-develops-gpt-3s-free-alternative-gpt-neo/](https://www.marktechpost.com/2021/05/24/eleutherai-develops-gpt-3s-free-alternative-gpt-neo/?_ga=2.62220524.1924646600.1621739878-488125022.1618729090)

Github: [https://github.com/EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo)",t2_4wudjgid,False,,0,False,EleutherAI Develops GPT-3’s Free Alternative: GPT-Neo,[],r/LanguageTechnology,False,6,,0,,False,t3_njzmmr,False,dark,0.93,,public,32,0,{},,False,[],,False,False,,{},,False,32,,False,False,,False,,[],{},,True,,1621896370.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In today’s era, all top benchmarks in natural language processing are dominated by Transformer-based models. In a machine learning model, the most critical elements of the training process are the model code, training data, and available computing resources.&lt;/p&gt;

&lt;p&gt;With the Transformer family of models, researchers have now finally come up with a way to increase the performance of a model infinitely by increasing the amount of training data and compute power.&lt;/p&gt;

&lt;p&gt;OpenAI did this with GPT-2 and with GPT-3. They used a private corpus of 500 billion tokens for training the model and spent $50 million in computing costs.&lt;/p&gt;

&lt;p&gt;Full Article: &lt;a href=""https://www.marktechpost.com/2021/05/24/eleutherai-develops-gpt-3s-free-alternative-gpt-neo/?_ga=2.62220524.1924646600.1621739878-488125022.1618729090""&gt;https://www.marktechpost.com/2021/05/24/eleutherai-develops-gpt-3s-free-alternative-gpt-neo/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Github: &lt;a href=""https://github.com/EleutherAI/gpt-neo""&gt;https://github.com/EleutherAI/gpt-neo&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,njzmmr,True,,techsucker,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/njzmmr/eleutherai_develops_gpt3s_free_alternative_gptneo/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/njzmmr/eleutherai_develops_gpt3s_free_alternative_gptneo/,30199,1621867570.0,0,,False,,,,,,965
119,,LanguageTechnology,"Using Python.

Having trouble putting this question into words.

Is there a way to score the structure of a sentence against a corpus of other sentences?

So if a sentence if badly written in terms of grammar/word order, the score would be low.

I guess one would need to know the most common sentence structure within the corpus and work back from there?

Any ideas much appreciated and apologies for the vagueness of the question.",t2_4s6pt7r6,False,,0,False,Is there a way to score the structure of a sentence against a corpus of other sentences?,[],r/LanguageTechnology,False,6,,0,,False,t3_njvtsw,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1621884763.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Using Python.&lt;/p&gt;

&lt;p&gt;Having trouble putting this question into words.&lt;/p&gt;

&lt;p&gt;Is there a way to score the structure of a sentence against a corpus of other sentences?&lt;/p&gt;

&lt;p&gt;So if a sentence if badly written in terms of grammar/word order, the score would be low.&lt;/p&gt;

&lt;p&gt;I guess one would need to know the most common sentence structure within the corpus and work back from there?&lt;/p&gt;

&lt;p&gt;Any ideas much appreciated and apologies for the vagueness of the question.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,njvtsw,True,,breadncheesetheking1,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/njvtsw/is_there_a_way_to_score_the_structure_of_a/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/njvtsw/is_there_a_way_to_score_the_structure_of_a/,30199,1621855963.0,0,,False,,,,,,432
120,,LanguageTechnology,"Hi all, so I'm doing some constituency parsing with SpaCy and benepar, and i'd ideally like to be able to tag the words in a sentence with their corresponding constituents. 

Currently however, Spacy allows you to print the parse string, but I'd like to be able to attach the actual syntactic structure to each word, as a POS tagger would do. So for the following code; 

    import spacy
    import benepar
    
    nlp = spacy.load('en_core_web_md')
    
    if spacy.__version__.startswith('2'):
      nlp.add_pipe('benepar', config={'model': 'benepar_en3'})
    else:
      nlp.add_pipe(""benepar"", config={""model"": ""benepar_en3""})
    
    doc = nlp(""Last Tuesday, I thought to myself that I saw a cat."")
    sent = list(doc.sents)[0]
    
    constituents = sent._.parse_string
    print(constituents)
    
    &gt;&gt;&gt; (S (NP (JJ Last) (NNP Tuesday)) (, ,) (NP (PRP I)) (VP (VBD thought) (PP (IN to) (NP (PRP myself))) (SBAR (IN that) (S (NP (PRP I)) (VP (VBD saw) (NP (DT a) (NN cat)))))) (. .))
    

I'd like for each word to be tagged with it's corresponding grammatical structure, but currently this is just one string. Any ideas on how to achieve this?",t2_48eyl33r,False,,0,False,Tokenising SpaCy constituency parse output,[],r/LanguageTechnology,False,6,,0,,False,t3_njudn4,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1621879400.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, so I&amp;#39;m doing some constituency parsing with SpaCy and benepar, and i&amp;#39;d ideally like to be able to tag the words in a sentence with their corresponding constituents. &lt;/p&gt;

&lt;p&gt;Currently however, Spacy allows you to print the parse string, but I&amp;#39;d like to be able to attach the actual syntactic structure to each word, as a POS tagger would do. So for the following code; &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import spacy
import benepar

nlp = spacy.load(&amp;#39;en_core_web_md&amp;#39;)

if spacy.__version__.startswith(&amp;#39;2&amp;#39;):
  nlp.add_pipe(&amp;#39;benepar&amp;#39;, config={&amp;#39;model&amp;#39;: &amp;#39;benepar_en3&amp;#39;})
else:
  nlp.add_pipe(&amp;quot;benepar&amp;quot;, config={&amp;quot;model&amp;quot;: &amp;quot;benepar_en3&amp;quot;})

doc = nlp(&amp;quot;Last Tuesday, I thought to myself that I saw a cat.&amp;quot;)
sent = list(doc.sents)[0]

constituents = sent._.parse_string
print(constituents)

&amp;gt;&amp;gt;&amp;gt; (S (NP (JJ Last) (NNP Tuesday)) (, ,) (NP (PRP I)) (VP (VBD thought) (PP (IN to) (NP (PRP myself))) (SBAR (IN that) (S (NP (PRP I)) (VP (VBD saw) (NP (DT a) (NN cat)))))) (. .))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&amp;#39;d like for each word to be tagged with it&amp;#39;s corresponding grammatical structure, but currently this is just one string. Any ideas on how to achieve this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,njudn4,True,,crowpup783,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/njudn4/tokenising_spacy_constituency_parse_output/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/njudn4/tokenising_spacy_constituency_parse_output/,30199,1621850600.0,0,,False,,,,,,1168
121,,LanguageTechnology,"EDIT - solved 

I have a list of strings, which I have turned into SpaCy documents in order to be able to POS tag each string as such; 

    example_sents = [nlp(sent) for sent in example_sents[0:2]]
    print(example_sents)
    
    &gt;&gt;&gt; [well as long as you accept that it is  testing uhyou know  alleviate, she cant accept that we want to be the care givers]

However, when I try and POS tag multiple strings, I get the error 'AttributeError: 'spacy.tokens.doc.Doc' object has no attribute 'pos\_''

Does anyone know why this is happening when I try to iterate over each spacy document as such; 

    for token in example_sents[1:2]:
      print(token.text, token.pos_)

Thanks!",t2_48eyl33r,False,,0,False,Get POS tags for multiple spacy documents,[],r/LanguageTechnology,False,6,,0,,False,t3_njzz7v,False,dark,0.67,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,1621878316.0,,[],{},,True,,1621897312.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;EDIT - solved &lt;/p&gt;

&lt;p&gt;I have a list of strings, which I have turned into SpaCy documents in order to be able to POS tag each string as such; &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;example_sents = [nlp(sent) for sent in example_sents[0:2]]
print(example_sents)

&amp;gt;&amp;gt;&amp;gt; [well as long as you accept that it is  testing uhyou know  alleviate, she cant accept that we want to be the care givers]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, when I try and POS tag multiple strings, I get the error &amp;#39;AttributeError: &amp;#39;spacy.tokens.doc.Doc&amp;#39; object has no attribute &amp;#39;pos_&amp;#39;&amp;#39;&lt;/p&gt;

&lt;p&gt;Does anyone know why this is happening when I try to iterate over each spacy document as such; &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for token in example_sents[1:2]:
  print(token.text, token.pos_)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,njzz7v,True,,crowpup783,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/njzz7v/get_pos_tags_for_multiple_spacy_documents/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/njzz7v/get_pos_tags_for_multiple_spacy_documents/,30199,1621868512.0,0,,False,,,,,,689
122,,LanguageTechnology,,t2_3fhicx1x,False,,0,False,Analyzing the vast coronavirus literature with CoronaCentral,[],r/LanguageTechnology,False,6,,0,,False,t3_nju4az,False,dark,0.67,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1621878360.0,text,6,,,text,pnas.org,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nju4az,True,,jakelikestextmining,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nju4az/analyzing_the_vast_coronavirus_literature_with/,all_ads,False,https://www.pnas.org/content/118/23/e2100766118,30199,1621849560.0,0,,False,https://www.pnas.org/content/118/23/e2100766118,,,,,0
123,,LanguageTechnology,,t2_21qaqh1p,False,,0,False,Controllable Text Summarization in Python based on CtrlSum,[],r/LanguageTechnology,False,6,,0,,False,t3_nk1w7q,False,dark,0.6,,public,1,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/P8CqGqR1Zr4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Controllable Text Summarization in Python based on CtrlSum | Applied NLP in Python', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/P8CqGqR1Zr4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/P8CqGqR1Zr4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/P8CqGqR1Zr4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nk1w7q', 'height': 200}",,False,1,,False,False,,False,,[],{},,False,,1621902346.0,text,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nk1w7q,True,,dulldata,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nk1w7q/controllable_text_summarization_in_python_based/,all_ads,False,https://www.youtube.com/watch?v=P8CqGqR1Zr4,30199,1621873546.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Controllable Text Summarization in Python based on CtrlSum | Applied NLP in Python', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/P8CqGqR1Zr4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/P8CqGqR1Zr4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,https://www.youtube.com/watch?v=P8CqGqR1Zr4,,,,,0
124,,LanguageTechnology,"Here is the list of all NAACL-2021 (Annual Conference of the North American Chapter of the Association for Computational Linguistics) papers, and a one sentence highlight for each of them. The proceeding was released today.

[https://www.paperdigest.org/2021/05/naacl-2021-highlights/](https://www.paperdigest.org/2021/05/naacl-2021-highlights/)",t2_2niqx8mz,False,,0,False,One sentence highlight for every NAACL-2021 Paper,[],r/LanguageTechnology,False,6,,0,,False,t3_njkuau,False,dark,0.99,,public,14,1,{},,False,[],,False,False,,{},,False,14,,False,False,,False,,[],{'gid_1': 1},,True,,1621843442.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Here is the list of all NAACL-2021 (Annual Conference of the North American Chapter of the Association for Computational Linguistics) papers, and a one sentence highlight for each of them. The proceeding was released today.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.paperdigest.org/2021/05/naacl-2021-highlights/""&gt;https://www.paperdigest.org/2021/05/naacl-2021-highlights/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,njkuau,True,,biandangou,,2,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/njkuau/one_sentence_highlight_for_every_naacl2021_paper/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/njkuau/one_sentence_highlight_for_every_naacl2021_paper/,30199,1621814642.0,0,,False,,,,,,345
125,,LanguageTechnology,"I am in my last year of undergraduate studies and want to use my electives on math. Other than basic math like Probability theory, Statistics, Linear Algebra, or Multivariate Calculus, are there any other important math classes that would be beneficial for a career or research in NLP in 2021-2022?

I assume ODEs can help with Neural Nets and higher statistics are always beneficial but there must be more.",t2_7kpfukab,False,,0,False,Advanced Math for NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_njibcy,False,dark,1.0,,public,15,0,{},,False,[],,False,False,,{},,False,15,,False,False,,False,,[],{},,True,,1621835693.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am in my last year of undergraduate studies and want to use my electives on math. Other than basic math like Probability theory, Statistics, Linear Algebra, or Multivariate Calculus, are there any other important math classes that would be beneficial for a career or research in NLP in 2021-2022?&lt;/p&gt;

&lt;p&gt;I assume ODEs can help with Neural Nets and higher statistics are always beneficial but there must be more.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,njibcy,True,,throwaway1287odc,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/njibcy/advanced_math_for_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/njibcy/advanced_math_for_nlp/,30199,1621806893.0,0,,False,,,,,,407
126,,LanguageTechnology,"What would be the way to go for a search engine to search on similar words. When I check txtai or similar implementations they use pooling (averaging) on words. What I am looking for is that if i search on a query of 3 words for example that it searches for the 100 closest words (cosine similarity) for each word and creates some rank. But this rank should not be compromised by matching words of only one semantic meaning. 
Example if I search for:  “great iphone tutorials” this ranking should not be compromised by documents with a lot of matching similarities for “great” with no mentioning for iphone and tutorials.",t2_7tb2j,False,,0,False,Search on vector embeddings,[],r/LanguageTechnology,False,6,,0,,False,t3_njc3wk,False,dark,1.0,,public,14,0,{},,False,[],,False,False,,{},,False,14,,False,False,,False,,[],{},,True,,1621818601.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What would be the way to go for a search engine to search on similar words. When I check txtai or similar implementations they use pooling (averaging) on words. What I am looking for is that if i search on a query of 3 words for example that it searches for the 100 closest words (cosine similarity) for each word and creates some rank. But this rank should not be compromised by matching words of only one semantic meaning. 
Example if I search for:  “great iphone tutorials” this ranking should not be compromised by documents with a lot of matching similarities for “great” with no mentioning for iphone and tutorials.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,njc3wk,True,,gevezex,,13,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/njc3wk/search_on_vector_embeddings/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/njc3wk/search_on_vector_embeddings/,30199,1621789801.0,0,,False,,,,,,621
127,,LanguageTechnology,"I am new to machine learning.  I am trying to build a text classifier.

I have the following code:

    title_tfidf = TfidfVectorizer(use_idf=True, max_df=0.12, stop_words='english', preprocessor=custom_preprocessor) 
    text_tfidf = TfidfVectorizer(use_idf=True, max_df=0.12, stop_words='english', preprocessor=custom_preprocessor)  
    
    preprocess = ColumnTransformer([('title_tfidf', title_tfidf, 'Title'), ('text_tfidf', text_tfidf, 'Text')])  
    
    model = make_pipeline(preprocess, LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000))  
    
    model.fit(x_train, y_train)

I access the feature-names list and coefficients in the following way:

    regressor = model.named_steps['logisticregression'] 
    print (""Coefficients: "", regressor.coef_) 
    feature_names = preprocess.get_feature_names()

how can one retrieve the feature vector?

Would be grateful for any inputs!",t2_a40fenez,False,,0,False,NLP : Access feature vector from Pipeline object,[],r/LanguageTechnology,False,6,,0,,False,t3_njs41v,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,1621853146.0,,[],{},,True,,1621869803.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am new to machine learning.  I am trying to build a text classifier.&lt;/p&gt;

&lt;p&gt;I have the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;title_tfidf = TfidfVectorizer(use_idf=True, max_df=0.12, stop_words=&amp;#39;english&amp;#39;, preprocessor=custom_preprocessor) 
text_tfidf = TfidfVectorizer(use_idf=True, max_df=0.12, stop_words=&amp;#39;english&amp;#39;, preprocessor=custom_preprocessor)  

preprocess = ColumnTransformer([(&amp;#39;title_tfidf&amp;#39;, title_tfidf, &amp;#39;Title&amp;#39;), (&amp;#39;text_tfidf&amp;#39;, text_tfidf, &amp;#39;Text&amp;#39;)])  

model = make_pipeline(preprocess, LogisticRegression(verbose=1, solver=&amp;#39;liblinear&amp;#39;,random_state=0, C=5, penalty=&amp;#39;l2&amp;#39;,max_iter=1000))  

model.fit(x_train, y_train)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I access the feature-names list and coefficients in the following way:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;regressor = model.named_steps[&amp;#39;logisticregression&amp;#39;] 
print (&amp;quot;Coefficients: &amp;quot;, regressor.coef_) 
feature_names = preprocess.get_feature_names()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;how can one retrieve the feature vector?&lt;/p&gt;

&lt;p&gt;Would be grateful for any inputs!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,njs41v,True,,puzzled-cognition,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/njs41v/nlp_access_feature_vector_from_pipeline_object/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/njs41v/nlp_access_feature_vector_from_pipeline_object/,30199,1621841003.0,0,,False,,,,,,940
128,,LanguageTechnology,"I have to attend a quota of seminars as part of my degree, but the offerings at my university are a bit limited. Is there an equivalent to [http://researchseminars.org/](http://researchseminars.org/) for language technology / computation linguistics / that sort of space?

Or, if you don't know of a large universal one, does your research group have a website where you publish your upcoming seminars?",t2_bz3u6,False,,0,False,Interesting seminars and talks?,[],r/LanguageTechnology,False,6,,0,,False,t3_njoaqt,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1621854999.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have to attend a quota of seminars as part of my degree, but the offerings at my university are a bit limited. Is there an equivalent to &lt;a href=""http://researchseminars.org/""&gt;http://researchseminars.org/&lt;/a&gt; for language technology / computation linguistics / that sort of space?&lt;/p&gt;

&lt;p&gt;Or, if you don&amp;#39;t know of a large universal one, does your research group have a website where you publish your upcoming seminars?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,njoaqt,True,,solresol,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/njoaqt/interesting_seminars_and_talks/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/njoaqt/interesting_seminars_and_talks/,30199,1621826199.0,0,,False,,,,,,402
129,,LanguageTechnology,,t2_hkv9s,False,,0,False,A Graph-based Text Similarity Method with Named Entity Information in NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_nj4xc6,False,dark,0.9,,public,14,0,{},,False,[],,False,False,,{},,False,14,,False,False,,False,,[],{},,False,,1621793927.0,text,6,,,text,medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nj4xc6,True,,prakhar21,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nj4xc6/a_graphbased_text_similarity_method_with_named/,all_ads,False,https://medium.com/p/abc7f1201d96?source=linkShare-bcb8dddfcc90-1621765097,30199,1621765127.0,0,,False,https://medium.com/p/abc7f1201d96?source=linkShare-bcb8dddfcc90-1621765097,,,,,0
130,,LanguageTechnology,"Hey guys am creating a keyword extraction model to get keywords from text, for Spanish language. Any and all resources or ideas are welcome. Thanks 😊",t2_9v2jnqq5,False,,0,False,Spanish keyword extraction [D],[],r/LanguageTechnology,False,6,,0,,False,t3_njgmwd,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1621831091.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys am creating a keyword extraction model to get keywords from text, for Spanish language. Any and all resources or ideas are welcome. Thanks 😊&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,njgmwd,True,,Sensitive-Loss1522,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/njgmwd/spanish_keyword_extraction_d/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/njgmwd/spanish_keyword_extraction_d/,30199,1621802291.0,0,,False,,,,,,149
131,,LanguageTechnology,"Hello,

What is the best laptops for deep learning under 1000$?

Thank you",t2_9e6mmc22,False,,0,False,Laptop for deep learning,[],r/LanguageTechnology,False,6,,0,,False,t3_nj4xvz,False,dark,0.9,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1621793993.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;What is the best laptops for deep learning under 1000$?&lt;/p&gt;

&lt;p&gt;Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nj4xvz,True,,Ok_Inspection_5208,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nj4xvz/laptop_for_deep_learning/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nj4xvz/laptop_for_deep_learning/,30199,1621765193.0,0,,False,,,,,,74
132,,LanguageTechnology,"Hello everybody, I am a student working on his bachelor thesis.. I have a some ML background but I am quite new to NLP.

Shortly, the goal of my thesis is to extract text from CSV bank transactions files and EBICS exports and categorize them into about 20 categories.. So far I have tried with a basic Naive-Bayes classificator getting +- 65% accuracy but nothing more.. do you have any tips on some specific models or papers that could help me achieve a higher precision?

To add some more details, the bank statements that I have so far are not that big in terms of transactions, I have about 400 rows of data right now.. this is probably impacting the accuracy but it's kinda a long process to gather those kind of files.

Additionally, the data from the CVS and EBIC files are quite short sentences.. around 60/80 characters per line.

Thanks in advance for any suggestion! have a nice day :)",t2_11j08j,False,,0,False,Classification of bank statement transactions,[],r/LanguageTechnology,False,6,,0,,False,t3_nje9nc,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1621824557.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everybody, I am a student working on his bachelor thesis.. I have a some ML background but I am quite new to NLP.&lt;/p&gt;

&lt;p&gt;Shortly, the goal of my thesis is to extract text from CSV bank transactions files and EBICS exports and categorize them into about 20 categories.. So far I have tried with a basic Naive-Bayes classificator getting +- 65% accuracy but nothing more.. do you have any tips on some specific models or papers that could help me achieve a higher precision?&lt;/p&gt;

&lt;p&gt;To add some more details, the bank statements that I have so far are not that big in terms of transactions, I have about 400 rows of data right now.. this is probably impacting the accuracy but it&amp;#39;s kinda a long process to gather those kind of files.&lt;/p&gt;

&lt;p&gt;Additionally, the data from the CVS and EBIC files are quite short sentences.. around 60/80 characters per line.&lt;/p&gt;

&lt;p&gt;Thanks in advance for any suggestion! have a nice day :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nje9nc,True,,thisunamewasfree,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nje9nc/classification_of_bank_statement_transactions/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nje9nc/classification_of_bank_statement_transactions/,30199,1621795757.0,0,,False,,,,,,896
133,,LanguageTechnology,"To clarify, is it difficult for a non-programmer to write a code or mini program that would collect articles based on several simple criteria or maybe there are some other solutions for this purpose?

Example: I want to collect and gather all articles published on BBC website between January 2021 and April 2021 that contain the word ""doggo"" for the purposes of corpus linguistics. 

I am aware of BootCat but I never managed to get it work properly, though",t2_6g45m2tc,False,,0,False,How to write a code/mini program that collects articles?,[],r/LanguageTechnology,False,6,,0,,False,t3_nimrip,False,dark,0.85,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1621730971.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;To clarify, is it difficult for a non-programmer to write a code or mini program that would collect articles based on several simple criteria or maybe there are some other solutions for this purpose?&lt;/p&gt;

&lt;p&gt;Example: I want to collect and gather all articles published on BBC website between January 2021 and April 2021 that contain the word &amp;quot;doggo&amp;quot; for the purposes of corpus linguistics. &lt;/p&gt;

&lt;p&gt;I am aware of BootCat but I never managed to get it work properly, though&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nimrip,True,,Sedulas,,13,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nimrip/how_to_write_a_codemini_program_that_collects/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nimrip/how_to_write_a_codemini_program_that_collects/,30199,1621702171.0,0,,False,,,,,,458
134,,LanguageTechnology,,t2_hkv9s,False,,0,False,An Efficient System for Grammatical Error Correction on Mobile Devices (Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_nidynd,False,dark,0.95,,public,20,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3rVn14m8zaM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'An Efficient System for Grammatical Error Correction on Mobile Devices (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3rVn14m8zaM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3rVn14m8zaM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3rVn14m8zaM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nidynd', 'height': 200}",,False,20,,False,False,,False,,[],{},,False,,1621700579.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nidynd,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nidynd/an_efficient_system_for_grammatical_error/,all_ads,False,https://youtu.be/3rVn14m8zaM,30199,1621671779.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'An Efficient System for Grammatical Error Correction on Mobile Devices (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3rVn14m8zaM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3rVn14m8zaM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/3rVn14m8zaM,,,,,0
135,,LanguageTechnology,"I'm working with some older texts, were the letters are sometimes a little smudgy.

The letters and words get recognized near-perfectly, and when I look at the hOCR or html file, the text looks perfect.

But when I **export to PDF** with an invisible text layer the spaces between words frequently go missing, for paragraphs at a time. This is annoying when trying to highlight parts of the text and then copy-paste those excerpts.

Any advice?

Other than these old texts, gImageReader is absolutely amazing and does exactly what I want.",t2_5lrs9ing,False,,0,False,gImageReader/Tesseract: Having trouble with old texts and spaces between words,[],r/LanguageTechnology,False,6,,0,,False,t3_nidwks,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1621700317.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working with some older texts, were the letters are sometimes a little smudgy.&lt;/p&gt;

&lt;p&gt;The letters and words get recognized near-perfectly, and when I look at the hOCR or html file, the text looks perfect.&lt;/p&gt;

&lt;p&gt;But when I &lt;strong&gt;export to PDF&lt;/strong&gt; with an invisible text layer the spaces between words frequently go missing, for paragraphs at a time. This is annoying when trying to highlight parts of the text and then copy-paste those excerpts.&lt;/p&gt;

&lt;p&gt;Any advice?&lt;/p&gt;

&lt;p&gt;Other than these old texts, gImageReader is absolutely amazing and does exactly what I want.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nidwks,True,,Party-Permission,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nidwks/gimagereadertesseract_having_trouble_with_old/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nidwks/gimagereadertesseract_having_trouble_with_old/,30199,1621671517.0,0,,False,,,,,,538
136,,LanguageTechnology,"I'm trying to automate the process of web scraping. I have a dataset of real-estate data with fields like Title, Price, Street, Postcode, etc. I want to make a model that takes the content of a webpage and returns words/phrases that are similar to/match those in the dataset. This way I won't have to write a custom web scraper for every website. I think this is similar to keyword extraction except that I want the model to learn a dataset of keywords and then extract those keywords from a webpage.

Can anyone guide me on how to approach this problem?",t2_c7r1mlx,False,,0,False,Automating a Web Scraper,[],r/LanguageTechnology,False,6,,0,,False,t3_nie7t0,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1621701729.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to automate the process of web scraping. I have a dataset of real-estate data with fields like Title, Price, Street, Postcode, etc. I want to make a model that takes the content of a webpage and returns words/phrases that are similar to/match those in the dataset. This way I won&amp;#39;t have to write a custom web scraper for every website. I think this is similar to keyword extraction except that I want the model to learn a dataset of keywords and then extract those keywords from a webpage.&lt;/p&gt;

&lt;p&gt;Can anyone guide me on how to approach this problem?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nie7t0,True,,Jack7heRapper,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nie7t0/automating_a_web_scraper/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nie7t0/automating_a_web_scraper/,30199,1621672929.0,0,,False,,,,,,554
137,,LanguageTechnology,"Can anyone recommend any NLU services, libraries, or methods that can perform NLU tasks using both language tokens and simple document formatting such as indentation level. For example, in lecture notes, indentations could help refine coreference resolution by understanding that there’s a good chance indented text is highly related to less indented text on an earlier document line or paragraph.",t2_3ardsqrn,False,,0,False,Using document formatting for NLU,[],r/LanguageTechnology,False,6,,0,,False,t3_ni48rc,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1621663872.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can anyone recommend any NLU services, libraries, or methods that can perform NLU tasks using both language tokens and simple document formatting such as indentation level. For example, in lecture notes, indentations could help refine coreference resolution by understanding that there’s a good chance indented text is highly related to less indented text on an earlier document line or paragraph.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ni48rc,True,,GreenOnGray,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ni48rc/using_document_formatting_for_nlu/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ni48rc/using_document_formatting_for_nlu/,30199,1621635072.0,0,,False,,,,,,397
138,,LanguageTechnology,"I recently published a blog post about how the original WSD task formulation is not suitable for modern domain-specific and enterprise disambiguation settings and how Target Sense Verification can improve this situation. Here is the link: [https://annabreit.medium.com/is-word-sense-disambiguation-outdated-ef05a139576](https://annabreit.medium.com/is-word-sense-disambiguation-outdated-ef05a139576)  


Would be glad for any feedback and am happy to discuss!",t2_c3u2hpml,False,,0,False,Is Word Sense Disambiguation outdated?,[],r/LanguageTechnology,False,6,,0,,False,t3_nhohkx,False,dark,0.96,,public,17,1,{},,False,[],,False,False,,{},,False,17,,False,False,,False,,[],{},,True,,1621618542.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I recently published a blog post about how the original WSD task formulation is not suitable for modern domain-specific and enterprise disambiguation settings and how Target Sense Verification can improve this situation. Here is the link: &lt;a href=""https://annabreit.medium.com/is-word-sense-disambiguation-outdated-ef05a139576""&gt;https://annabreit.medium.com/is-word-sense-disambiguation-outdated-ef05a139576&lt;/a&gt;  &lt;/p&gt;

&lt;p&gt;Would be glad for any feedback and am happy to discuss!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nhohkx,True,,anna_with_b,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nhohkx/is_word_sense_disambiguation_outdated/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nhohkx/is_word_sense_disambiguation_outdated/,30199,1621589742.0,0,,False,,,,,,459
139,,LanguageTechnology,New to NLP. Can anyone suggest me how I can use NLP to extract important details from large documents? Thanks in advance.,t2_b3gv09ny,False,,0,False,Text Extraction,[],r/LanguageTechnology,False,6,,0,,False,t3_nhnpyv,False,dark,0.81,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1621615340.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;New to NLP. Can anyone suggest me how I can use NLP to extract important details from large documents? Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nhnpyv,True,,Key-Feature-1228,,11,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nhnpyv/text_extraction/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nhnpyv/text_extraction/,30199,1621586540.0,0,,False,,,,,,121
140,,LanguageTechnology,,t2_hkv9s,False,,0,False,Automatic Extraction of Hypernym Relations from Text,[],r/LanguageTechnology,False,6,,0,,False,t3_nhlyzg,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,False,,1621607930.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nhlyzg,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nhlyzg/automatic_extraction_of_hypernym_relations_from/,all_ads,False,https://link.medium.com/r22yL4WTqgb,30199,1621579130.0,0,,False,https://link.medium.com/r22yL4WTqgb,,,,,0
141,,LanguageTechnology,"# NLU 3.0.1 Release Notes
We are very excited to announce NLU 3.0.1 has been released!
This is one of the most visually appealing releases, with the integration of the [Spark-NLP-Display](https://nlp.johnsnowlabs.com/docs/en/display) library and visualizations for `dependency trees`, `entity resolution`, `entity assertion`, `relationship between entities` and `named
entity recognition`. In addition to this, the schema of how columns are named by NLU has been reworked and all 140+ tutorial notebooks have been updated to reflect the latest changes in NLU 3.0.0+
Finally, new multilingual models for `Afrikaans`, `Welsh`, `Maltese`, `Tamil`, and`Vietnamese` are now available.




# New Features and Enhancements
- 1 line to visualization for `NER`, `Dependency`, `Resolution`, `Assertion` and `Relation` via [Spark-NLP-Display](https://nlp.johnsnowlabs.com/docs/en/display) integration
- Improved column naming schema
- [Over 140 + NLU tutorial Notebooks updated](https://github.com/JohnSnowLabs/nlu/tree/master/examples) and improved to reflect latest changes in NLU 3.0.0 +
- New multilingual models for `Afrikaans`, `Welsh`, `Maltese`, `Tamil`, and`Vietnamese`
- Enhanced offline loading


## NLU visualization
The latest NLU release integrated the beautiful Spark-NLP-Display package visualizations. You do not need to worry about installing it, when you try to visualize something, NLU will check if
Spark-NLP-Display is installed, if it is missing it will be dynamically installed into your python executable environment, so you don't need to worry about anything!

See the [visualization tutorial notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/visualization/NLU_visualizations_tutorial.ipynb)  and [visualization docs](https://nlu.johnsnowlabs.com/docs/en/viz_examples) for more info.

![Cheat Sheet visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/cheat_sheet.png)

## NER visualization
Applicable to any of the [100+ NER models! See here for an overview](https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition)
```python
nlu.load('ner').viz(""Donald Trump from America and Angela Merkel from Germany don't share many oppinions."")
```
![NER visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/NER.png)

## Dependency tree visualization
Visualizes the structure of the labeled dependency tree and part of speech tags
```python
nlu.load('dep.typed').viz(""Billy went to the mall"")
```

![Dependency Tree visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/DEP.png)

```python
#Bigger Example
nlu.load('dep.typed').viz(""Donald Trump from America and Angela Merkel from Germany don't share many oppinions but they both love John Snow Labs software"")
```
![Dependency Tree visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/DEP_big.png)

## Assertion status visualization
Visualizes asserted statuses and entities.        
Applicable to any of the [10 + Assertion models! See here for an overview](https://nlp.johnsnowlabs.com/models?task=Assertion+Status)
```python
nlu.load('med_ner.clinical assert').viz(""The MRI scan showed no signs of cancer in the left lung"")
```


![Assert visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/assertion.png)

```python
#bigger example
data ='This is the case of a very pleasant 46-year-old Caucasian female, seen in clinic on 12/11/07 during which time MRI of the left shoulder showed no evidence of rotator cuff tear. She did have a previous MRI of the cervical spine that did show an osteophyte on the left C6-C7 level. Based on this, negative MRI of the shoulder, the patient was recommended to have anterior cervical discectomy with anterior interbody fusion at C6-C7 level. Operation, expected outcome, risks, and benefits were discussed with her. Risks include, but not exclusive of bleeding and infection, bleeding could be soft tissue bleeding, which may compromise airway and may result in return to the operating room emergently for evacuation of said hematoma. There is also the possibility of bleeding into the epidural space, which can compress the spinal cord and result in weakness and numbness of all four extremities as well as impairment of bowel and bladder function. However, the patient may develop deeper-seated infection, which may require return to the operating room. Should the infection be in the area of the spinal instrumentation, this will cause a dilemma since there might be a need to remove the spinal instrumentation and/or allograft. There is also the possibility of potential injury to the esophageus, the trachea, and the carotid artery. There is also the risks of stroke on the right cerebral circulation should an undiagnosed plaque be propelled from the right carotid. She understood all of these risks and agreed to have the procedure performed.'
nlu.load('med_ner.clinical assert').viz(data)
```
![Assert visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/assertion_big.png)


## Relationship between entities visualization
Visualizes the extracted entities between relationship.    
Applicable to any of the [20 + Relation Extractor models See here for an overview](https://nlp.johnsnowlabs.com/models?task=Relation+Extraction)
```python
nlu.load('med_ner.jsl.wip.clinical relation.temporal_events').viz('The patient developed cancer after a mercury poisoning in 1999 ')
```
![Entity Relation visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/relation.png)

```python
# bigger example
data = 'This is the case of a very pleasant 46-year-old Caucasian female, seen in clinic on 12/11/07 during which time MRI of the left shoulder showed no evidence of rotator cuff tear. She did have a previous MRI of the cervical spine that did show an osteophyte on the left C6-C7 level. Based on this, negative MRI of the shoulder, the patient was recommended to have anterior cervical discectomy with anterior interbody fusion at C6-C7 level. Operation, expected outcome, risks, and benefits were discussed with her. Risks include, but not exclusive of bleeding and infection, bleeding could be soft tissue bleeding, which may compromise airway and may result in return to the operating room emergently for evacuation of said hematoma. There is also the possibility of bleeding into the epidural space, which can compress the spinal cord and result in weakness and numbness of all four extremities as well as impairment of bowel and bladder function. However, the patient may develop deeper-seated infection, which may require return to the operating room. Should the infection be in the area of the spinal instrumentation, this will cause a dilemma since there might be a need to remove the spinal instrumentation and/or allograft. There is also the possibility of potential injury to the esophageus, the trachea, and the carotid artery. There is also the risks of stroke on the right cerebral circulation should an undiagnosed plaque be propelled from the right carotid. She understood all of these risks and agreed to have the procedure performed'
pipe = nlu.load('med_ner.jsl.wip.clinical relation.clinical').viz(data)
```
![Entity Relation visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/relation_big.png)


## Entity Resolution visualization for chunks
Visualizes resolutions of entities
Applicable to any of the [100+ Resolver models See here for an overview](https://nlp.johnsnowlabs.com/models?task=Entity+Resolution)
```python
nlu.load('med_ner.jsl.wip.clinical resolve_chunk.rxnorm.in').viz(""He took Prevacid 30 mg  daily"")
```
![Chunk Resolution visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_chunk.png)

```python
# bigger example
data = ""This is an 82 - year-old male with a history of prior tobacco use , hypertension , chronic renal insufficiency , COPD , gastritis , and TIA who initially presented to Braintree with a non-ST elevation MI and Guaiac positive stools , transferred to St . Margaret\'s Center for Women &amp; Infants for cardiac catheterization with PTCA to mid LAD lesion complicated by hypotension and bradycardia requiring Atropine , IV fluids and transient dopamine possibly secondary to vagal reaction , subsequently transferred to CCU for close monitoring , hemodynamically stable at the time of admission to the CCU .""
nlu.load('med_ner.jsl.wip.clinical resolve_chunk.rxnorm.in').viz(data)
```

![Chunk Resolution visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_chunk_big.png)


## Entity Resolution visualization for sentences
Visualizes resolutions of entities in sentences
Applicable to any of the [100+ Resolver models See here for an overview](https://nlp.johnsnowlabs.com/models?task=Entity+Resolution)
```python
nlu.load('med_ner.jsl.wip.clinical resolve.icd10cm').viz('She was diagnosed with a respiratory congestion')
```
![Sentence Resolution visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_sentence.png)

```python
# bigger example
data = 'The patient is a 5-month-old infant who presented initially on Monday with a cold, cough, and runny nose for 2 days. Mom states she had no fever. Her appetite was good but she was spitting up a lot. She had no difficulty breathing and her cough was described as dry and hacky. At that time, physical exam showed a right TM, which was red. Left TM was okay. She was fairly congested but looked happy and playful. She was started on Amoxil and Aldex and we told to recheck in 2 weeks to recheck her ear. Mom returned to clinic again today because she got much worse overnight. She was having difficulty breathing. She was much more congested and her appetite had decreased significantly today. She also spiked a temperature yesterday of 102.6 and always having trouble sleeping secondary to congestion'
nlu.load('med_ner.jsl.wip.clinical resolve.icd10cm').viz(data)
```
![Sentence Resolution visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_sentence_big.png)

## Configure visualizations
### Define custom colors for labels
Some entity and relation labels will be highlighted with a pre-defined color, which you [can find here](https://github.com/JohnSnowLabs/spark-nlp-display/tree/main/sparknlp_display/label_colors).    
For labels that have no color defined, a random color will be generated.     
You can define colors for labels manually, by specifying via the `viz_colors` parameter
and defining `hex color codes` in a dictionary that maps `labels` to `colors` .
```python
data = 'Dr. John Snow suggested that Fritz takes 5mg penicilin for his cough'
# Define custom colors for labels
viz_colors={'STRENGTH':'#800080', 'DRUG_BRANDNAME':'#77b5fe', 'GENDER':'#77ffe'}
nlu.load('med_ner.jsl.wip.clinical').viz(data,viz_colors =viz_colors)
```
![define colors labels](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/define_colors.png)


### Filter entities that get highlighted
By default every entity class will be visualized.    
The `labels_to_viz` can be used to define a set of labels to highlight.       
Applicable for ner, resolution and assert.
```python
data = 'Dr. John Snow suggested that Fritz takes 5mg penicilin for his cough'
# Filter wich NER label to viz
labels_to_viz=['SYMPTOM']
nlu.load('med_ner.jsl.wip.clinical').viz(data,labels_to_viz=labels_to_viz)
```
![filter labels](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/filter_labels.png)


## New models
New multilingual models for `Afrikaans`, `Welsh`, `Maltese`, `Tamil`, and`Vietnamese`

| nlu.load() Refrence                                          | Spark NLP Refrence                                           |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [vi.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_vi.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_vi.html) |
| [mt.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_mt.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_mt.html) |
| [ta.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_ta.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_ta.html) |
| [af.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_af.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_af.html) |
| [af.pos](https://nlp.johnsnowlabs.com/2021/04/06/pos_afribooms_af.html) | [pos_afribooms](https://nlp.johnsnowlabs.com/2021/04/06/pos_afribooms_af.html) |
| [cy.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_cy.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_cy.html) |

## Reworked and updated NLU tutorial notebooks

All of the [140+ NLU tutorial Notebooks](https://github.com/JohnSnowLabs/nlu/tree/master/examples) have been updated and reworked to reflect the latest changes in NLU 3.0.0+


## Improved Column Name generation
- NLU categorized each internal component now with boolean labels for `name_deductable` and `always_name_deductable` .
- Before generating column names, NLU checks wether each component is of unique in the pipeline or not. If a component is not unique in the
  pipe and there are multiple components of same type, i.e. multiple `NER` models, NLU will deduct a base name for the final output columns from the
  NLU reference each NER model is pointing to.
- If on the other hand, there is only one `NER` model in the pipeline, only the default `ner` column prefixed will be generated.
- For some components, like `embeddings` and `classifiers` are now defined as `always_name_deductable`, for those NLU will always try to infer a meaningful base name for the output columns.
- Newly trained component output columns will now be prefixed with `trained_&lt;type&gt;` , for types `pos` , `ner`, `cLassifier`, `sentiment` and `multi_classifier`

## Enhanced offline mode
- You can still load a model from a path as usual with `nlu.load(path=model_path)` and output columns will be suffixed with `from_disk`
- You can now optionally also specify `request` parameter during  load a model from HDD, it will be used to deduct more meaningful column name suffixes, instead of `from_disk`, i.e. by calling `nlu.load(request ='en.embed_sentence.biobert.pubmed_pmc_base_cased', path=model_path)`


### Bugfixes
- Fixed a bug that caused  resolution algorithms output level to be inferred incorrectly
- Fixed a bug that caused stranger cols got dropped
- Fixed a bug that caused endings to miss when  .predict(position=True) was specified
- Fixed a bug that caused pd.Series to be converted incorrectly internally
- Fixed a bug that caused output level transformations to crash
- Fixed a bug that caused verbose mode not to turn of properly after turning it on.
- fixed a bug that caused some models to crash when loaded for HDD

# Additional NLU resources
* [140+ updates tutorials](https://github.com/JohnSnowLabs/nlu/tree/master/examples)
* [Updated visualization docs](https://nlu.johnsnowlabs.com/docs/en/viz_examples)
* [Models Hub](https://nlp.johnsnowlabs.com/models) with new models
* [Spark NLP publications](https://medium.com/spark-nlp)
* [NLU in Action](https://nlp.johnsnowlabs.com/demo)
* [NLU documentation](https://nlu.johnsnowlabs.com/docs/en/install)
* [Discussions](https://github.com/JohnSnowLabs/spark-nlp/discussions) Engage with other community members, share ideas, and show off how you use Spark NLP and NLU!

# 1 line Install NLU on Google Colab
```!wget https://setup.johnsnowlabs.com/nlu/colab.sh  -O - | bash```
# 1 line Install NLU on Kaggle
```!wget https://setup.johnsnowlabs.com/nlu/kaggle.sh  -O - | bash```
# Install via PIP
```! pip install nlu pyspark==3.0.1```",t2_53n73cus,False,,0,False,"1 line to visualizations for dependency trees, entity relationships, resolution, assertion, NER and new models for Afrikaans, Welsh, Maltese, Tamil, and Vietnamese - John Snow Labs NLU 3.0.1 for Python",[],r/LanguageTechnology,False,6,,0,,False,t3_nh4qzh,False,dark,1.0,,public,16,0,{},,False,[],,False,False,,{},,False,16,,False,False,,False,,[],{},,True,,1621557047.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;h1&gt;NLU 3.0.1 Release Notes&lt;/h1&gt;

&lt;p&gt;We are very excited to announce NLU 3.0.1 has been released!
This is one of the most visually appealing releases, with the integration of the &lt;a href=""https://nlp.johnsnowlabs.com/docs/en/display""&gt;Spark-NLP-Display&lt;/a&gt; library and visualizations for &lt;code&gt;dependency trees&lt;/code&gt;, &lt;code&gt;entity resolution&lt;/code&gt;, &lt;code&gt;entity assertion&lt;/code&gt;, &lt;code&gt;relationship between entities&lt;/code&gt; and &lt;code&gt;named
entity recognition&lt;/code&gt;. In addition to this, the schema of how columns are named by NLU has been reworked and all 140+ tutorial notebooks have been updated to reflect the latest changes in NLU 3.0.0+
Finally, new multilingual models for &lt;code&gt;Afrikaans&lt;/code&gt;, &lt;code&gt;Welsh&lt;/code&gt;, &lt;code&gt;Maltese&lt;/code&gt;, &lt;code&gt;Tamil&lt;/code&gt;, and&lt;code&gt;Vietnamese&lt;/code&gt; are now available.&lt;/p&gt;

&lt;h1&gt;New Features and Enhancements&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;1 line to visualization for &lt;code&gt;NER&lt;/code&gt;, &lt;code&gt;Dependency&lt;/code&gt;, &lt;code&gt;Resolution&lt;/code&gt;, &lt;code&gt;Assertion&lt;/code&gt; and &lt;code&gt;Relation&lt;/code&gt; via &lt;a href=""https://nlp.johnsnowlabs.com/docs/en/display""&gt;Spark-NLP-Display&lt;/a&gt; integration&lt;/li&gt;
&lt;li&gt;Improved column naming schema&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/tree/master/examples""&gt;Over 140 + NLU tutorial Notebooks updated&lt;/a&gt; and improved to reflect latest changes in NLU 3.0.0 +&lt;/li&gt;
&lt;li&gt;New multilingual models for &lt;code&gt;Afrikaans&lt;/code&gt;, &lt;code&gt;Welsh&lt;/code&gt;, &lt;code&gt;Maltese&lt;/code&gt;, &lt;code&gt;Tamil&lt;/code&gt;, and&lt;code&gt;Vietnamese&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Enhanced offline loading&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;NLU visualization&lt;/h2&gt;

&lt;p&gt;The latest NLU release integrated the beautiful Spark-NLP-Display package visualizations. You do not need to worry about installing it, when you try to visualize something, NLU will check if
Spark-NLP-Display is installed, if it is missing it will be dynamically installed into your python executable environment, so you don&amp;#39;t need to worry about anything!&lt;/p&gt;

&lt;p&gt;See the &lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/visualization/NLU_visualizations_tutorial.ipynb""&gt;visualization tutorial notebook&lt;/a&gt;  and &lt;a href=""https://nlu.johnsnowlabs.com/docs/en/viz_examples""&gt;visualization docs&lt;/a&gt; for more info.&lt;/p&gt;

&lt;p&gt;![Cheat Sheet visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/cheat_sheet.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/cheat_sheet.png&lt;/a&gt;)&lt;/p&gt;

&lt;h2&gt;NER visualization&lt;/h2&gt;

&lt;p&gt;Applicable to any of the &lt;a href=""https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition""&gt;100+ NER models! See here for an overview&lt;/a&gt;
&lt;code&gt;python
nlu.load(&amp;#39;ner&amp;#39;).viz(&amp;quot;Donald Trump from America and Angela Merkel from Germany don&amp;#39;t share many oppinions.&amp;quot;)
&lt;/code&gt;
![NER visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/NER.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/NER.png&lt;/a&gt;)&lt;/p&gt;

&lt;h2&gt;Dependency tree visualization&lt;/h2&gt;

&lt;p&gt;Visualizes the structure of the labeled dependency tree and part of speech tags
&lt;code&gt;python
nlu.load(&amp;#39;dep.typed&amp;#39;).viz(&amp;quot;Billy went to the mall&amp;quot;)
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;![Dependency Tree visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/DEP.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/DEP.png&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;Bigger Example&lt;/h1&gt;

&lt;p&gt;nlu.load(&amp;#39;dep.typed&amp;#39;).viz(&amp;quot;Donald Trump from America and Angela Merkel from Germany don&amp;#39;t share many oppinions but they both love John Snow Labs software&amp;quot;)
```
![Dependency Tree visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/DEP_big.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/DEP_big.png&lt;/a&gt;)&lt;/p&gt;

&lt;h2&gt;Assertion status visualization&lt;/h2&gt;

&lt;p&gt;Visualizes asserted statuses and entities.&lt;br/&gt;
Applicable to any of the &lt;a href=""https://nlp.johnsnowlabs.com/models?task=Assertion+Status""&gt;10 + Assertion models! See here for an overview&lt;/a&gt;
&lt;code&gt;python
nlu.load(&amp;#39;med_ner.clinical assert&amp;#39;).viz(&amp;quot;The MRI scan showed no signs of cancer in the left lung&amp;quot;)
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;![Assert visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/assertion.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/assertion.png&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;bigger example&lt;/h1&gt;

&lt;p&gt;data =&amp;#39;This is the case of a very pleasant 46-year-old Caucasian female, seen in clinic on 12/11/07 during which time MRI of the left shoulder showed no evidence of rotator cuff tear. She did have a previous MRI of the cervical spine that did show an osteophyte on the left C6-C7 level. Based on this, negative MRI of the shoulder, the patient was recommended to have anterior cervical discectomy with anterior interbody fusion at C6-C7 level. Operation, expected outcome, risks, and benefits were discussed with her. Risks include, but not exclusive of bleeding and infection, bleeding could be soft tissue bleeding, which may compromise airway and may result in return to the operating room emergently for evacuation of said hematoma. There is also the possibility of bleeding into the epidural space, which can compress the spinal cord and result in weakness and numbness of all four extremities as well as impairment of bowel and bladder function. However, the patient may develop deeper-seated infection, which may require return to the operating room. Should the infection be in the area of the spinal instrumentation, this will cause a dilemma since there might be a need to remove the spinal instrumentation and/or allograft. There is also the possibility of potential injury to the esophageus, the trachea, and the carotid artery. There is also the risks of stroke on the right cerebral circulation should an undiagnosed plaque be propelled from the right carotid. She understood all of these risks and agreed to have the procedure performed.&amp;#39;
nlu.load(&amp;#39;med_ner.clinical assert&amp;#39;).viz(data)
```
![Assert visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/assertion_big.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/assertion_big.png&lt;/a&gt;)&lt;/p&gt;

&lt;h2&gt;Relationship between entities visualization&lt;/h2&gt;

&lt;p&gt;Visualizes the extracted entities between relationship.&lt;br/&gt;
Applicable to any of the &lt;a href=""https://nlp.johnsnowlabs.com/models?task=Relation+Extraction""&gt;20 + Relation Extractor models See here for an overview&lt;/a&gt;
&lt;code&gt;python
nlu.load(&amp;#39;med_ner.jsl.wip.clinical relation.temporal_events&amp;#39;).viz(&amp;#39;The patient developed cancer after a mercury poisoning in 1999 &amp;#39;)
&lt;/code&gt;
![Entity Relation visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/relation.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/relation.png&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;bigger example&lt;/h1&gt;

&lt;p&gt;data = &amp;#39;This is the case of a very pleasant 46-year-old Caucasian female, seen in clinic on 12/11/07 during which time MRI of the left shoulder showed no evidence of rotator cuff tear. She did have a previous MRI of the cervical spine that did show an osteophyte on the left C6-C7 level. Based on this, negative MRI of the shoulder, the patient was recommended to have anterior cervical discectomy with anterior interbody fusion at C6-C7 level. Operation, expected outcome, risks, and benefits were discussed with her. Risks include, but not exclusive of bleeding and infection, bleeding could be soft tissue bleeding, which may compromise airway and may result in return to the operating room emergently for evacuation of said hematoma. There is also the possibility of bleeding into the epidural space, which can compress the spinal cord and result in weakness and numbness of all four extremities as well as impairment of bowel and bladder function. However, the patient may develop deeper-seated infection, which may require return to the operating room. Should the infection be in the area of the spinal instrumentation, this will cause a dilemma since there might be a need to remove the spinal instrumentation and/or allograft. There is also the possibility of potential injury to the esophageus, the trachea, and the carotid artery. There is also the risks of stroke on the right cerebral circulation should an undiagnosed plaque be propelled from the right carotid. She understood all of these risks and agreed to have the procedure performed&amp;#39;
pipe = nlu.load(&amp;#39;med_ner.jsl.wip.clinical relation.clinical&amp;#39;).viz(data)
```
![Entity Relation visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/relation_big.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/relation_big.png&lt;/a&gt;)&lt;/p&gt;

&lt;h2&gt;Entity Resolution visualization for chunks&lt;/h2&gt;

&lt;p&gt;Visualizes resolutions of entities
Applicable to any of the &lt;a href=""https://nlp.johnsnowlabs.com/models?task=Entity+Resolution""&gt;100+ Resolver models See here for an overview&lt;/a&gt;
&lt;code&gt;python
nlu.load(&amp;#39;med_ner.jsl.wip.clinical resolve_chunk.rxnorm.in&amp;#39;).viz(&amp;quot;He took Prevacid 30 mg  daily&amp;quot;)
&lt;/code&gt;
![Chunk Resolution visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_chunk.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_chunk.png&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;bigger example&lt;/h1&gt;

&lt;p&gt;data = &amp;quot;This is an 82 - year-old male with a history of prior tobacco use , hypertension , chronic renal insufficiency , COPD , gastritis , and TIA who initially presented to Braintree with a non-ST elevation MI and Guaiac positive stools , transferred to St . Margaret\&amp;#39;s Center for Women &amp;amp; Infants for cardiac catheterization with PTCA to mid LAD lesion complicated by hypotension and bradycardia requiring Atropine , IV fluids and transient dopamine possibly secondary to vagal reaction , subsequently transferred to CCU for close monitoring , hemodynamically stable at the time of admission to the CCU .&amp;quot;
nlu.load(&amp;#39;med_ner.jsl.wip.clinical resolve_chunk.rxnorm.in&amp;#39;).viz(data)
```&lt;/p&gt;

&lt;p&gt;![Chunk Resolution visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_chunk_big.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_chunk_big.png&lt;/a&gt;)&lt;/p&gt;

&lt;h2&gt;Entity Resolution visualization for sentences&lt;/h2&gt;

&lt;p&gt;Visualizes resolutions of entities in sentences
Applicable to any of the &lt;a href=""https://nlp.johnsnowlabs.com/models?task=Entity+Resolution""&gt;100+ Resolver models See here for an overview&lt;/a&gt;
&lt;code&gt;python
nlu.load(&amp;#39;med_ner.jsl.wip.clinical resolve.icd10cm&amp;#39;).viz(&amp;#39;She was diagnosed with a respiratory congestion&amp;#39;)
&lt;/code&gt;
![Sentence Resolution visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_sentence.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_sentence.png&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;bigger example&lt;/h1&gt;

&lt;p&gt;data = &amp;#39;The patient is a 5-month-old infant who presented initially on Monday with a cold, cough, and runny nose for 2 days. Mom states she had no fever. Her appetite was good but she was spitting up a lot. She had no difficulty breathing and her cough was described as dry and hacky. At that time, physical exam showed a right TM, which was red. Left TM was okay. She was fairly congested but looked happy and playful. She was started on Amoxil and Aldex and we told to recheck in 2 weeks to recheck her ear. Mom returned to clinic again today because she got much worse overnight. She was having difficulty breathing. She was much more congested and her appetite had decreased significantly today. She also spiked a temperature yesterday of 102.6 and always having trouble sleeping secondary to congestion&amp;#39;
nlu.load(&amp;#39;med_ner.jsl.wip.clinical resolve.icd10cm&amp;#39;).viz(data)
```
![Sentence Resolution visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_sentence_big.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_sentence_big.png&lt;/a&gt;)&lt;/p&gt;

&lt;h2&gt;Configure visualizations&lt;/h2&gt;

&lt;h3&gt;Define custom colors for labels&lt;/h3&gt;

&lt;p&gt;Some entity and relation labels will be highlighted with a pre-defined color, which you &lt;a href=""https://github.com/JohnSnowLabs/spark-nlp-display/tree/main/sparknlp_display/label_colors""&gt;can find here&lt;/a&gt;.&lt;br/&gt;
For labels that have no color defined, a random color will be generated.&lt;br/&gt;
You can define colors for labels manually, by specifying via the &lt;code&gt;viz_colors&lt;/code&gt; parameter
and defining &lt;code&gt;hex color codes&lt;/code&gt; in a dictionary that maps &lt;code&gt;labels&lt;/code&gt; to &lt;code&gt;colors&lt;/code&gt; .
```python
data = &amp;#39;Dr. John Snow suggested that Fritz takes 5mg penicilin for his cough&amp;#39;&lt;/p&gt;

&lt;h1&gt;Define custom colors for labels&lt;/h1&gt;

&lt;p&gt;viz_colors={&amp;#39;STRENGTH&amp;#39;:&amp;#39;#800080&amp;#39;, &amp;#39;DRUG_BRANDNAME&amp;#39;:&amp;#39;#77b5fe&amp;#39;, &amp;#39;GENDER&amp;#39;:&amp;#39;#77ffe&amp;#39;}
nlu.load(&amp;#39;med_ner.jsl.wip.clinical&amp;#39;).viz(data,viz_colors =viz_colors)
```
![define colors labels](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/define_colors.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/define_colors.png&lt;/a&gt;)&lt;/p&gt;

&lt;h3&gt;Filter entities that get highlighted&lt;/h3&gt;

&lt;p&gt;By default every entity class will be visualized.&lt;br/&gt;
The &lt;code&gt;labels_to_viz&lt;/code&gt; can be used to define a set of labels to highlight.&lt;br/&gt;
Applicable for ner, resolution and assert.
```python
data = &amp;#39;Dr. John Snow suggested that Fritz takes 5mg penicilin for his cough&amp;#39;&lt;/p&gt;

&lt;h1&gt;Filter wich NER label to viz&lt;/h1&gt;

&lt;p&gt;labels_to_viz=[&amp;#39;SYMPTOM&amp;#39;]
nlu.load(&amp;#39;med_ner.jsl.wip.clinical&amp;#39;).viz(data,labels_to_viz=labels_to_viz)
```
![filter labels](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/filter_labels.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/filter_labels.png&lt;/a&gt;)&lt;/p&gt;

&lt;h2&gt;New models&lt;/h2&gt;

&lt;p&gt;New multilingual models for &lt;code&gt;Afrikaans&lt;/code&gt;, &lt;code&gt;Welsh&lt;/code&gt;, &lt;code&gt;Maltese&lt;/code&gt;, &lt;code&gt;Tamil&lt;/code&gt;, and&lt;code&gt;Vietnamese&lt;/code&gt;&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;nlu.load() Refrence&lt;/th&gt;
&lt;th&gt;Spark NLP Refrence&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/lemma_vi.html""&gt;vi.lemma&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/lemma_vi.html""&gt;lemma&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/lemma_mt.html""&gt;mt.lemma&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/lemma_mt.html""&gt;lemma&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/lemma_ta.html""&gt;ta.lemma&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/lemma_ta.html""&gt;lemma&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/lemma_af.html""&gt;af.lemma&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/lemma_af.html""&gt;lemma&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/06/pos_afribooms_af.html""&gt;af.pos&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/06/pos_afribooms_af.html""&gt;pos_afribooms&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/lemma_cy.html""&gt;cy.lemma&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/lemma_cy.html""&gt;lemma&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;Reworked and updated NLU tutorial notebooks&lt;/h2&gt;

&lt;p&gt;All of the &lt;a href=""https://github.com/JohnSnowLabs/nlu/tree/master/examples""&gt;140+ NLU tutorial Notebooks&lt;/a&gt; have been updated and reworked to reflect the latest changes in NLU 3.0.0+&lt;/p&gt;

&lt;h2&gt;Improved Column Name generation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;NLU categorized each internal component now with boolean labels for &lt;code&gt;name_deductable&lt;/code&gt; and &lt;code&gt;always_name_deductable&lt;/code&gt; .&lt;/li&gt;
&lt;li&gt;Before generating column names, NLU checks wether each component is of unique in the pipeline or not. If a component is not unique in the
pipe and there are multiple components of same type, i.e. multiple &lt;code&gt;NER&lt;/code&gt; models, NLU will deduct a base name for the final output columns from the
NLU reference each NER model is pointing to.&lt;/li&gt;
&lt;li&gt;If on the other hand, there is only one &lt;code&gt;NER&lt;/code&gt; model in the pipeline, only the default &lt;code&gt;ner&lt;/code&gt; column prefixed will be generated.&lt;/li&gt;
&lt;li&gt;For some components, like &lt;code&gt;embeddings&lt;/code&gt; and &lt;code&gt;classifiers&lt;/code&gt; are now defined as &lt;code&gt;always_name_deductable&lt;/code&gt;, for those NLU will always try to infer a meaningful base name for the output columns.&lt;/li&gt;
&lt;li&gt;Newly trained component output columns will now be prefixed with &lt;code&gt;trained_&amp;lt;type&amp;gt;&lt;/code&gt; , for types &lt;code&gt;pos&lt;/code&gt; , &lt;code&gt;ner&lt;/code&gt;, &lt;code&gt;cLassifier&lt;/code&gt;, &lt;code&gt;sentiment&lt;/code&gt; and &lt;code&gt;multi_classifier&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Enhanced offline mode&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;You can still load a model from a path as usual with &lt;code&gt;nlu.load(path=model_path)&lt;/code&gt; and output columns will be suffixed with &lt;code&gt;from_disk&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;You can now optionally also specify &lt;code&gt;request&lt;/code&gt; parameter during  load a model from HDD, it will be used to deduct more meaningful column name suffixes, instead of &lt;code&gt;from_disk&lt;/code&gt;, i.e. by calling &lt;code&gt;nlu.load(request =&amp;#39;en.embed_sentence.biobert.pubmed_pmc_base_cased&amp;#39;, path=model_path)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Bugfixes&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Fixed a bug that caused  resolution algorithms output level to be inferred incorrectly&lt;/li&gt;
&lt;li&gt;Fixed a bug that caused stranger cols got dropped&lt;/li&gt;
&lt;li&gt;Fixed a bug that caused endings to miss when  .predict(position=True) was specified&lt;/li&gt;
&lt;li&gt;Fixed a bug that caused pd.Series to be converted incorrectly internally&lt;/li&gt;
&lt;li&gt;Fixed a bug that caused output level transformations to crash&lt;/li&gt;
&lt;li&gt;Fixed a bug that caused verbose mode not to turn of properly after turning it on.&lt;/li&gt;
&lt;li&gt;fixed a bug that caused some models to crash when loaded for HDD&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;Additional NLU resources&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/tree/master/examples""&gt;140+ updates tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/docs/en/viz_examples""&gt;Updated visualization docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlp.johnsnowlabs.com/models""&gt;Models Hub&lt;/a&gt; with new models&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://medium.com/spark-nlp""&gt;Spark NLP publications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlp.johnsnowlabs.com/demo""&gt;NLU in Action&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/docs/en/install""&gt;NLU documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/spark-nlp/discussions""&gt;Discussions&lt;/a&gt; Engage with other community members, share ideas, and show off how you use Spark NLP and NLU!&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;1 line Install NLU on Google Colab&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;!wget https://setup.johnsnowlabs.com/nlu/colab.sh  -O - | bash&lt;/code&gt;&lt;/p&gt;

&lt;h1&gt;1 line Install NLU on Kaggle&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;!wget https://setup.johnsnowlabs.com/nlu/kaggle.sh  -O - | bash&lt;/code&gt;&lt;/p&gt;

&lt;h1&gt;Install via PIP&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;! pip install nlu pyspark==3.0.1&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nh4qzh,True,,CKL-IT,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nh4qzh/1_line_to_visualizations_for_dependency_trees/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nh4qzh/1_line_to_visualizations_for_dependency_trees/,30199,1621528247.0,0,,False,,,,,,16375
142,,LanguageTechnology," I have to study the structure of questions comparing electronic products, e.g., Computers, Laptops, Tablets, etc.

Example questions:

1. What are the disadvantages of a Chromebook as opposed to a regular laptop computer?
2. what are the pros and cons of buying a dell laptop?
3. what can a $1000 apple laptop do which $500 Lenovo laptop not? is an expensive one worth it?
4. are hp laptops good for students?

I would be grateful If anyone can provide a lead to such a dataset. Currently, I am looking at questions on Quora, Reddit, and other forums manually.",t2_7nrnwrud,False,,0,False,Dataset to study structure of comparison questions,[],r/LanguageTechnology,False,6,,0,,False,t3_nh2flv,False,dark,1.0,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1621551522.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have to study the structure of questions comparing electronic products, e.g., Computers, Laptops, Tablets, etc.&lt;/p&gt;

&lt;p&gt;Example questions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;What are the disadvantages of a Chromebook as opposed to a regular laptop computer?&lt;/li&gt;
&lt;li&gt;what are the pros and cons of buying a dell laptop?&lt;/li&gt;
&lt;li&gt;what can a $1000 apple laptop do which $500 Lenovo laptop not? is an expensive one worth it?&lt;/li&gt;
&lt;li&gt;are hp laptops good for students?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I would be grateful If anyone can provide a lead to such a dataset. Currently, I am looking at questions on Quora, Reddit, and other forums manually.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nh2flv,True,,linuxjain,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nh2flv/dataset_to_study_structure_of_comparison_questions/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nh2flv/dataset_to_study_structure_of_comparison_questions/,30199,1621522722.0,0,,False,,,,,,561
143,,LanguageTechnology,"If you have a specific use case that can benefit from knowledge graphs, please share below, we would love to learn about the different applications of NLP. If you have any question, please reply below or send an email at [admin@ubiai.tools](mailto:admin@ubiai.tools)

&amp;#x200B;

[https://walidamamou.medium.com/building-a-knowledge-graph-for-job-search-using-bert-transformer-8677c8b3a2e7](https://walidamamou.medium.com/building-a-knowledge-graph-for-job-search-using-bert-transformer-8677c8b3a2e7)",t2_32tnavmg,False,,0,False,Looking to extract insights from your unstructured text? Check out this article on how to combine NLP and knowledge graphs to uncover new information.,[],r/LanguageTechnology,False,6,,0,,False,t3_nhhp3y,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1621592178.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If you have a specific use case that can benefit from knowledge graphs, please share below, we would love to learn about the different applications of NLP. If you have any question, please reply below or send an email at [&lt;a href=""mailto:admin@ubiai.tools""&gt;admin@ubiai.tools&lt;/a&gt;](mailto:&lt;a href=""mailto:admin@ubiai.tools""&gt;admin@ubiai.tools&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://walidamamou.medium.com/building-a-knowledge-graph-for-job-search-using-bert-transformer-8677c8b3a2e7""&gt;https://walidamamou.medium.com/building-a-knowledge-graph-for-job-search-using-bert-transformer-8677c8b3a2e7&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nhhp3y,True,,UBIAI,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nhhp3y/looking_to_extract_insights_from_your/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nhhp3y/looking_to_extract_insights_from_your/,30199,1621563378.0,0,,False,,,,,,502
144,,LanguageTechnology,"I'm talking about the little translate button at the top left (can't put images).

&amp;#x200B;

As I understood it takes the html and translates the text in it. What I can't comprehend is it fits the correct words perfectly, for example in the &lt;strong&gt; tag.

For example &lt;p&gt;there is &lt;strong&gt;some text here&lt;/strong&gt; google will translate it&lt;/p&gt;

After the translation google finds the exact match to put inside the &lt;strong&gt; tag. They can't translate it separately it would distort the meaning. Even sentence is complex and inside the tag does not make a sentence on its own they handle it somehow.

I tried to make it as clear as possible. I've been searching but I couldn't come up with a solution. Do you have any ideas? Please share.

&amp;#x200B;

Edit (Clarification)

&amp;#x200B;

We have a sentence: ""It’s recommended that you do that in a [virtual environment using virtualenv](https://pythonbasics.org/virtualenv/).""

&amp;#x200B;

as you can see ""virtual environment using virtualenv"" is let's say tagged. After translating this sentence google is able to tag the same piece in the target language. For example

&amp;#x200B;

Es wird empfohlen, dies in einer [virtuellen Umgebung mit virtualenv zu tun](https://pythonbasics.org/virtualenv/) .

&amp;#x200B;

How they can match the meaning and ""tag"" the correct positions? Obviously translation of the tagged part won't be same if it's done independently from the original sentence. So how they match it?",t2_3q5i07qa,False,,0,False,How does Google Translate's add-on translates a web page so perfectly?,[],r/LanguageTechnology,False,6,,0,,False,t3_nhbztd,False,dark,0.4,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,1621589300.0,,[],{},,True,,1621574794.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m talking about the little translate button at the top left (can&amp;#39;t put images).&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;As I understood it takes the html and translates the text in it. What I can&amp;#39;t comprehend is it fits the correct words perfectly, for example in the &amp;lt;strong&amp;gt; tag.&lt;/p&gt;

&lt;p&gt;For example &amp;lt;p&amp;gt;there is &amp;lt;strong&amp;gt;some text here&amp;lt;/strong&amp;gt; google will translate it&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;After the translation google finds the exact match to put inside the &amp;lt;strong&amp;gt; tag. They can&amp;#39;t translate it separately it would distort the meaning. Even sentence is complex and inside the tag does not make a sentence on its own they handle it somehow.&lt;/p&gt;

&lt;p&gt;I tried to make it as clear as possible. I&amp;#39;ve been searching but I couldn&amp;#39;t come up with a solution. Do you have any ideas? Please share.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Edit (Clarification)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;We have a sentence: &amp;quot;It’s recommended that you do that in a &lt;a href=""https://pythonbasics.org/virtualenv/""&gt;virtual environment using virtualenv&lt;/a&gt;.&amp;quot;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;as you can see &amp;quot;virtual environment using virtualenv&amp;quot; is let&amp;#39;s say tagged. After translating this sentence google is able to tag the same piece in the target language. For example&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Es wird empfohlen, dies in einer &lt;a href=""https://pythonbasics.org/virtualenv/""&gt;virtuellen Umgebung mit virtualenv zu tun&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;How they can match the meaning and &amp;quot;tag&amp;quot; the correct positions? Obviously translation of the tagged part won&amp;#39;t be same if it&amp;#39;s done independently from the original sentence. So how they match it?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nhbztd,True,,DoIHAVeaNIdenTItY,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nhbztd/how_does_google_translates_addon_translates_a_web/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nhbztd/how_does_google_translates_addon_translates_a_web/,30199,1621545994.0,0,,False,,,,,,1500
145,,LanguageTechnology,"Hi, hi!

Basically, my case is the opposite as the one archived here:

https://www.reddit.com/r/LanguageTechnology/comments/8w06zn/msc_computational_linguistics_in_germany/

I come from a theoretical/computational physics background and would like to properly educate myself in computational linguistics. Linguistics has been much of a hobby for me, while math and programming were part of my curriculum. 

Now I am finishing my master's on physics of complex systems and had one introduction to speech and text analysis, as well as a deep learning course.

This last information is also relevant, since I wouldn't like paying the tution fees for a second master (Zweitstudiengebühren) which would be applied in BaWü (Heidelberg, Tübingen, Stuttgart)

Any recommendations here?",t2_62v3m7yv,False,,0,False,M.Sc. Computer linguistics in Germany,[],r/LanguageTechnology,False,6,,0,,False,t3_ngyzvo,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1621542828.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, hi!&lt;/p&gt;

&lt;p&gt;Basically, my case is the opposite as the one archived here:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.reddit.com/r/LanguageTechnology/comments/8w06zn/msc_computational_linguistics_in_germany/""&gt;https://www.reddit.com/r/LanguageTechnology/comments/8w06zn/msc_computational_linguistics_in_germany/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I come from a theoretical/computational physics background and would like to properly educate myself in computational linguistics. Linguistics has been much of a hobby for me, while math and programming were part of my curriculum. &lt;/p&gt;

&lt;p&gt;Now I am finishing my master&amp;#39;s on physics of complex systems and had one introduction to speech and text analysis, as well as a deep learning course.&lt;/p&gt;

&lt;p&gt;This last information is also relevant, since I wouldn&amp;#39;t like paying the tution fees for a second master (Zweitstudiengebühren) which would be applied in BaWü (Heidelberg, Tübingen, Stuttgart)&lt;/p&gt;

&lt;p&gt;Any recommendations here?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ngyzvo,True,,GokuPotenciaExtrema,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ngyzvo/msc_computer_linguistics_in_germany/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ngyzvo/msc_computer_linguistics_in_germany/,30199,1621514028.0,0,,False,,,,,,777
146,,LanguageTechnology,,t2_2wsvqwhg,False,,0,False,A New Google Research Introduces FNet By Replacing Self-Attention Sublayers With Simple Linear Transformations Achieving 92% Accuracy and Runs Up To Seven Times Faster On GPUs And Twice As Fast On TPUs (Paper link in comments),[],r/LanguageTechnology,False,6,,0,,False,t3_ngsmcs,False,dark,0.67,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1621520210.0,text,6,,,text,marktechpost.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ngsmcs,True,,ai-lover,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ngsmcs/a_new_google_research_introduces_fnet_by/,all_ads,False,https://www.marktechpost.com/2021/05/19/a-new-google-research-introduces-fnet-by-replacing-self-attention-sublayers-with-simple-linear-transformations-achieving-92-accuracy-and-runs-up-to-seven-times-faster-on-gpus-and-twice-as-fast-on-tpus/,30199,1621491410.0,0,,False,https://www.marktechpost.com/2021/05/19/a-new-google-research-introduces-fnet-by-replacing-self-attention-sublayers-with-simple-linear-transformations-achieving-92-accuracy-and-runs-up-to-seven-times-faster-on-gpus-and-twice-as-fast-on-tpus/,,,,,0
147,,LanguageTechnology,"Hello! Been playing around with summary generators, and I'm pretty sure I came up with a novel approach, which should produce a summary that has the most resemblance to an author's typical writing style. Basically I'm just trying to find out if there's a word for this algo yet or not. Project is called Bite, and can be found [here](https://github.com/cyberrumor/bite). 

&amp;#x200B;

Say we have the following corpus:  


""Mary had a little lamb.""  


Bite will tokenize the sentence like so:  


* mary had
* mary had a 
* mary had a little
* mary had a little lamb
* had a
* had a little
* had a little lamb
* a little
* a little lamb
* a lamb

Next, it creates a frequency map to count all occurrences of each token, assigning scores to sentences by adding up the sum of token scores. 

&amp;#x200B;

Is there a word for this type of categorization?",t2_kulz0,False,,0,False,Is this approach to summary production novel?,[],r/LanguageTechnology,False,6,,0,,False,t3_nggaag,False,dark,0.75,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1621485211.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! Been playing around with summary generators, and I&amp;#39;m pretty sure I came up with a novel approach, which should produce a summary that has the most resemblance to an author&amp;#39;s typical writing style. Basically I&amp;#39;m just trying to find out if there&amp;#39;s a word for this algo yet or not. Project is called Bite, and can be found &lt;a href=""https://github.com/cyberrumor/bite""&gt;here&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Say we have the following corpus:  &lt;/p&gt;

&lt;p&gt;&amp;quot;Mary had a little lamb.&amp;quot;  &lt;/p&gt;

&lt;p&gt;Bite will tokenize the sentence like so:  &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;mary had&lt;/li&gt;
&lt;li&gt;mary had a &lt;/li&gt;
&lt;li&gt;mary had a little&lt;/li&gt;
&lt;li&gt;mary had a little lamb&lt;/li&gt;
&lt;li&gt;had a&lt;/li&gt;
&lt;li&gt;had a little&lt;/li&gt;
&lt;li&gt;had a little lamb&lt;/li&gt;
&lt;li&gt;a little&lt;/li&gt;
&lt;li&gt;a little lamb&lt;/li&gt;
&lt;li&gt;a lamb&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Next, it creates a frequency map to count all occurrences of each token, assigning scores to sentences by adding up the sum of token scores. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Is there a word for this type of categorization?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nggaag,True,,cyberrumor,,4,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/nggaag/is_this_approach_to_summary_production_novel/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nggaag/is_this_approach_to_summary_production_novel/,30199,1621456411.0,0,,False,,,,,,855
148,,LanguageTechnology,,t2_hkv9s,False,,0,False,Data Augmentation Techniques in NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_nfzwl5,False,dark,0.9,,public,16,0,{},,False,[],,False,False,,{},,False,16,,False,False,,False,,[],{},,False,,1621440834.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nfzwl5,True,,prakhar21,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nfzwl5/data_augmentation_techniques_in_nlp/,all_ads,False,https://youtube.com/playlist?list=PLsAqq9lZFOtUg63g_95OuV-R2GhV1UiIZ,30199,1621412034.0,0,,False,https://youtube.com/playlist?list=PLsAqq9lZFOtUg63g_95OuV-R2GhV1UiIZ,,,,,0
149,,LanguageTechnology,,t2_oro2l,False,,0,False,What are some resources to learn about incorporating images into word vectorization model?,[],r/LanguageTechnology,False,6,,0,,False,t3_ngcis0,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1621475888.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ngcis0,True,,krews2,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ngcis0/what_are_some_resources_to_learn_about/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ngcis0/what_are_some_resources_to_learn_about/,30199,1621447088.0,0,,False,,,,,,0
150,,LanguageTechnology,,t2_79ir5cir,False,,0,False,Neural nets for word sense disambiguation?,[],r/LanguageTechnology,False,6,,0,,False,t3_ng6e8v,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,False,,1621461033.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ng6e8v,True,,c_metaphorique,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ng6e8v/neural_nets_for_word_sense_disambiguation/,all_ads,False,/r/learnmachinelearning/comments/ng6dfp/neural_nets_for_word_sense_disambiguation/,30199,1621432233.0,0,,False,/r/learnmachinelearning/comments/ng6dfp/neural_nets_for_word_sense_disambiguation/,"[{'approved_at_utc': None, 'subreddit': 'learnmachinelearning', 'selftext': ""Hello. \n\nDoes anyone know of any recent work in trying to use artificial neural networks to disambiguate different senses of function words? E.g., the *of* in `Mother of the bride` is different from the *of* in `Eliza is of Dutch heritage` is different from the *of* in `The Brooklyn Borough of New York City` is different from the *of* in `The New York City borough of Brooklyn`. \n\nI've found some papers that deal with disambiguating content words (e.g., *bank*, the financial institution, vs *bank*, a feature of geography), but none that deal with function words. \n\nThanks in advance."", 'author_fullname': 't2_79ir5cir', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Neural nets for word sense disambiguation?', 'link_flair_richtext': [{'e': 'text', 't': 'Question'}], 'subreddit_name_prefixed': 'r/learnmachinelearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_ng6dfp', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Question', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1621460980.0, 'link_flair_type': 'richtext', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.learnmachinelearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello. &lt;/p&gt;\n\n&lt;p&gt;Does anyone know of any recent work in trying to use artificial neural networks to disambiguate different senses of function words? E.g., the &lt;em&gt;of&lt;/em&gt; in &lt;code&gt;Mother of the bride&lt;/code&gt; is different from the &lt;em&gt;of&lt;/em&gt; in &lt;code&gt;Eliza is of Dutch heritage&lt;/code&gt; is different from the &lt;em&gt;of&lt;/em&gt; in &lt;code&gt;The Brooklyn Borough of New York City&lt;/code&gt; is different from the &lt;em&gt;of&lt;/em&gt; in &lt;code&gt;The New York City borough of Brooklyn&lt;/code&gt;. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve found some papers that deal with disambiguating content words (e.g., &lt;em&gt;bank&lt;/em&gt;, the financial institution, vs &lt;em&gt;bank&lt;/em&gt;, a feature of geography), but none that deal with function words. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'ec81b8ee-accf-11e9-b8f8-0ebea2df7d78', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_3cqa1', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ffb000', 'id': 'ng6dfp', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'c_metaphorique', 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/learnmachinelearning/comments/ng6dfp/neural_nets_for_word_sense_disambiguation/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/learnmachinelearning/comments/ng6dfp/neural_nets_for_word_sense_disambiguation/', 'subreddit_subscribers': 232284, 'created_utc': 1621432180.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_ng6dfp,,,0
151,,LanguageTechnology,"I'm curious as to the best way to tag the syntactic structures of some text. So not just POS tags, but things like complement and relative clauses? Are there taggers specifically for this?",t2_48eyl33r,False,,0,False,Automatically label syntactic structures in text,[],r/LanguageTechnology,False,6,,0,,False,t3_ng3bic,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1621452665.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m curious as to the best way to tag the syntactic structures of some text. So not just POS tags, but things like complement and relative clauses? Are there taggers specifically for this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ng3bic,True,,crowpup783,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ng3bic/automatically_label_syntactic_structures_in_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ng3bic/automatically_label_syntactic_structures_in_text/,30199,1621423865.0,0,,False,,,,,,188
152,,LanguageTechnology,,t2_cwydf,False,,0,False,MUM: a new AI milestone for understanding information,[],r/LanguageTechnology,False,6,,0,,False,t3_nfjtj1,False,dark,0.94,,public,31,0,{},,False,[],,False,False,,{},,False,31,,False,False,,False,,[],{},,False,,1621394992.0,text,6,,,text,blog.google,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nfjtj1,True,,hedekar,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nfjtj1/mum_a_new_ai_milestone_for_understanding/,all_ads,False,https://blog.google/products/search/introducing-mum/,30199,1621366192.0,0,,False,https://blog.google/products/search/introducing-mum/,,,,,0
153,,LanguageTechnology,,t2_97mkm0fp,False,,0,False,Māori are trying to save their language from Big Tech,[],r/LanguageTechnology,False,6,,0,,False,t3_nfkiao,False,dark,0.8,,public,16,0,{},,False,[],,False,False,,{},,False,16,,False,True,,False,,[],{},,False,,1621396622.0,text,6,,,text,wired.co.uk,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nfkiao,True,,Madame_President_,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nfkiao/māori_are_trying_to_save_their_language_from_big/,all_ads,False,https://www.wired.co.uk/article/maori-language-tech,30199,1621367822.0,0,,False,https://www.wired.co.uk/article/maori-language-tech,,,,,0
154,,LanguageTechnology,"Hi all,

I have trained a simple multi-input NN. I have 4 inputs ( one text field &amp; other 3 categorical variables : cat1, cat2 , cat3).

It is a classification model.

For the text field, I use Glove embeddings in the Embedding layer, followed by LSTM layer.

For the other 3 categorical fields, I just encode them in a dense layer. Finally, I concatenate these 2 layers followed by softmax for classification. Below is the main block of code :

&amp;#x200B;

    embeddings_dictionary = dict()
    glove_file = open('glove.6B.100d.txt', encoding=""utf8"")
    for line in glove_file:
        records = line.split()
        word = records[0]
        vector_dimensions = asarray(records[1:], dtype='float32')
        embeddings_dictionary[word] = vector_dimensions
    glove_file.close()
    embedding_matrix = zeros((vocab_size, 100))
    for word, index in tokenizer.word_index.items():
        embedding_vector = embeddings_dictionary.get(word)
        if embedding_vector is not None:
            embedding_matrix[index] = embedding_vector
    
    # Text input handling
    text_input_1 = Input(shape=(maxlen,))
    embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(text_input_1)
    LSTM_Layer_1 = LSTM(128)(embedding_layer)
    
    
    # categorical variables handling
    layout_input_2 = Input(shape=(3,))
    dense_layer_1 = Dense(10, activation='relu')(layout_input_2)
    dense_layer_2 = Dense(10, activation='relu')(dense_layer_1)
    
    # Concatenating the above 2
    concat_layer = Concatenate()([LSTM_Layer_1, dense_layer_2])
    dense_layer_3 = Dense(10, activation='relu')(concat_layer)
    output = Dense(3, activation='softmax')(dense_layer_3)
    model = Model(inputs=[text_input_1, layout_input_2], outputs=output)
    
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
    print(model.summary())

The problem is , when I infer/predict using the above model weights, the respose is like all the weightage is being given to the categorical variables only , irrespective of the text input.

e.g. predict (text1, cat1, cat2, cat3)  &amp;  predict (text2, cat1, cat2, cat3)  is exactly same.

Even if I provide blank text as input, the output remains same if I don't change the categorical variable values.

Can anybody help troubleshooting this ?",t2_3ckj43a7,False,,0,False,Debugging a NN model,[],r/LanguageTechnology,False,6,,0,,False,t3_nfxmqa,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1621432783.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I have trained a simple multi-input NN. I have 4 inputs ( one text field &amp;amp; other 3 categorical variables : cat1, cat2 , cat3).&lt;/p&gt;

&lt;p&gt;It is a classification model.&lt;/p&gt;

&lt;p&gt;For the text field, I use Glove embeddings in the Embedding layer, followed by LSTM layer.&lt;/p&gt;

&lt;p&gt;For the other 3 categorical fields, I just encode them in a dense layer. Finally, I concatenate these 2 layers followed by softmax for classification. Below is the main block of code :&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;embeddings_dictionary = dict()
glove_file = open(&amp;#39;glove.6B.100d.txt&amp;#39;, encoding=&amp;quot;utf8&amp;quot;)
for line in glove_file:
    records = line.split()
    word = records[0]
    vector_dimensions = asarray(records[1:], dtype=&amp;#39;float32&amp;#39;)
    embeddings_dictionary[word] = vector_dimensions
glove_file.close()
embedding_matrix = zeros((vocab_size, 100))
for word, index in tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

# Text input handling
text_input_1 = Input(shape=(maxlen,))
embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(text_input_1)
LSTM_Layer_1 = LSTM(128)(embedding_layer)


# categorical variables handling
layout_input_2 = Input(shape=(3,))
dense_layer_1 = Dense(10, activation=&amp;#39;relu&amp;#39;)(layout_input_2)
dense_layer_2 = Dense(10, activation=&amp;#39;relu&amp;#39;)(dense_layer_1)

# Concatenating the above 2
concat_layer = Concatenate()([LSTM_Layer_1, dense_layer_2])
dense_layer_3 = Dense(10, activation=&amp;#39;relu&amp;#39;)(concat_layer)
output = Dense(3, activation=&amp;#39;softmax&amp;#39;)(dense_layer_3)
model = Model(inputs=[text_input_1, layout_input_2], outputs=output)

model.compile(loss=&amp;#39;categorical_crossentropy&amp;#39;, optimizer=&amp;#39;adam&amp;#39;, metrics=[&amp;#39;acc&amp;#39;])
print(model.summary())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The problem is , when I infer/predict using the above model weights, the respose is like all the weightage is being given to the categorical variables only , irrespective of the text input.&lt;/p&gt;

&lt;p&gt;e.g. predict (text1, cat1, cat2, cat3)  &amp;amp;  predict (text2, cat1, cat2, cat3)  is exactly same.&lt;/p&gt;

&lt;p&gt;Even if I provide blank text as input, the output remains same if I don&amp;#39;t change the categorical variable values.&lt;/p&gt;

&lt;p&gt;Can anybody help troubleshooting this ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nfxmqa,True,,lonewolf_9,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nfxmqa/debugging_a_nn_model/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nfxmqa/debugging_a_nn_model/,30199,1621403983.0,0,,False,,,,,,2347
155,,LanguageTechnology,"Hello all,

Recently I wrote an article about deploying spaCy with FastAPI for NER. As many people told me it was helpful, I did a new article about deploying transformer-based models with FastAPI for text classification (using Facebook's Bart Large MNLI model).

FastAPI  is a great is great framework for API development in Python in my opinion. It helped me save a lot of troubles when developing the [NLPCloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=d13c16ae-b7d8-11eb-8529-0242ac130003) API.

Here's the article:

[https://nlpcloud.io/nlp-machine-learning-classification-api-production-fastapi-transformers-nlpcloud.html](https://nlpcloud.io/nlp-machine-learning-classification-api-production-fastapi-transformers-nlpcloud.html?utm_source=reddit&amp;utm_campaign=d13c16ae-b7d8-11eb-8529-0242ac130003)

I'd love to have your feedback on this! Have you ever deployed transformer-based models to production? If so, which tools did you use?

Thanks!",t2_4z4m2qcs,False,,0,False,Deploying a transformer-based text classification NLP model with FastAPI,[],r/LanguageTechnology,False,6,,0,,False,t3_nfa5ek,False,dark,0.9,,public,15,0,{},,False,[],,False,False,,{},,False,15,,False,False,,1621344253.0,,[],{},,True,,1621371728.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all,&lt;/p&gt;

&lt;p&gt;Recently I wrote an article about deploying spaCy with FastAPI for NER. As many people told me it was helpful, I did a new article about deploying transformer-based models with FastAPI for text classification (using Facebook&amp;#39;s Bart Large MNLI model).&lt;/p&gt;

&lt;p&gt;FastAPI  is a great is great framework for API development in Python in my opinion. It helped me save a lot of troubles when developing the &lt;a href=""https://nlpcloud.io/?utm_source=reddit&amp;amp;utm_campaign=d13c16ae-b7d8-11eb-8529-0242ac130003""&gt;NLPCloud.io&lt;/a&gt; API.&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s the article:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://nlpcloud.io/nlp-machine-learning-classification-api-production-fastapi-transformers-nlpcloud.html?utm_source=reddit&amp;amp;utm_campaign=d13c16ae-b7d8-11eb-8529-0242ac130003""&gt;https://nlpcloud.io/nlp-machine-learning-classification-api-production-fastapi-transformers-nlpcloud.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;d love to have your feedback on this! Have you ever deployed transformer-based models to production? If so, which tools did you use?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nfa5ek,True,,juliensalinas,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nfa5ek/deploying_a_transformerbased_text_classification/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nfa5ek/deploying_a_transformerbased_text_classification/,30199,1621342928.0,0,,False,,,,,,970
156,,LanguageTechnology,,t2_hkv9s,False,,0,False,Explanability for Transformers with Transformers-Interpret — A Model Explainability Tool,[],r/LanguageTechnology,False,6,,0,,False,t3_nf5o8o,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,False,,1621357102.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nf5o8o,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nf5o8o/explanability_for_transformers_with/,all_ads,False,https://link.medium.com/cNDQwZ84lgb,30199,1621328302.0,0,,False,https://link.medium.com/cNDQwZ84lgb,,,,,0
157,,LanguageTechnology,"So I'm looking to create a subtitle file from a film that is only in Portuguese. 

I can do it by hand but that would take sooo long. 

I figure that I can probably extract the audio and use Google's transcription API to do a majority of the work for me, but I'm not sure how to go about doing that with a long length file.

Any tips?",t2_4un59,False,,0,False,Long Form Automatic Transcription from Audio (Transcribe Movie Dialogue to Text) - How Do I Do It?,[],r/LanguageTechnology,False,6,,0,,False,t3_nf3vmz,False,dark,1.0,,public,8,1,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{'gid_1': 1},,True,,1621350363.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I&amp;#39;m looking to create a subtitle file from a film that is only in Portuguese. &lt;/p&gt;

&lt;p&gt;I can do it by hand but that would take sooo long. &lt;/p&gt;

&lt;p&gt;I figure that I can probably extract the audio and use Google&amp;#39;s transcription API to do a majority of the work for me, but I&amp;#39;m not sure how to go about doing that with a long length file.&lt;/p&gt;

&lt;p&gt;Any tips?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nf3vmz,True,,MattyXarope,,14,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nf3vmz/long_form_automatic_transcription_from_audio/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nf3vmz/long_form_automatic_transcription_from_audio/,30199,1621321563.0,0,,False,,,,,,334
158,,LanguageTechnology,,t2_hkv9s,False,,0,False,Text Summarisation Techniques in NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_neo5lz,False,dark,0.91,,public,19,0,{},,False,[],,False,False,,{},,False,19,,False,False,,False,,[],{},,False,,1621305562.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,neo5lz,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/neo5lz/text_summarisation_techniques_in_nlp/,all_ads,False,https://youtube.com/playlist?list=PLsAqq9lZFOtV8jYq3JlkqPQUN5QxcWq0f,30199,1621276762.0,0,,False,https://youtube.com/playlist?list=PLsAqq9lZFOtV8jYq3JlkqPQUN5QxcWq0f,,,,,0
159,,LanguageTechnology,,t2_y7ny0,False,,0,False,The importance of language technologies for the future of the public health system (in Spanish),[],r/LanguageTechnology,False,6,,0,,False,t3_nf73gw,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,False,,1621362322.0,text,6,,,text,theconversation.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nf73gw,True,,luisgasco,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nf73gw/the_importance_of_language_technologies_for_the/,all_ads,False,https://theconversation.com/el-desarrollo-de-las-tecnologias-del-lenguaje-para-el-futuro-de-la-sanidad-159013,30199,1621333522.0,0,,False,https://theconversation.com/el-desarrollo-de-las-tecnologias-del-lenguaje-para-el-futuro-de-la-sanidad-159013,,,,,0
160,,LanguageTechnology,"If anyone has any experience with this I would love to pick your brain / ask you a little assistance. 

Been struggling to correctly initialize my model.",t2_44md8gjv,False,,0,False,Spacy 3.0 multi class Textcat,[],r/LanguageTechnology,False,6,,0,,False,t3_nerxde,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1621314460.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If anyone has any experience with this I would love to pick your brain / ask you a little assistance. &lt;/p&gt;

&lt;p&gt;Been struggling to correctly initialize my model.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nerxde,True,,washknoxnash2255,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nerxde/spacy_30_multi_class_textcat/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nerxde/spacy_30_multi_class_textcat/,30199,1621285660.0,0,,False,,,,,,153
161,,LanguageTechnology,"Hello all! Currently have a Spacy NER model that looks for 6 custom entities, while only two (Person and Organization) are truly important.

Would it be beneficial to cut out the other four entities in the model? My boss added them because he thought they would help with accuracy essentially through lessening the chance of false positives for person and organization. 

I personally believe the model would perform fine with only Person and organization, but am new to data science so just wanted to verify my hunch.",t2_44md8gjv,False,,0,False,Spacy NER Model 2 Entities versus 6,[],r/LanguageTechnology,False,6,,0,,False,t3_nerh99,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1621313385.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all! Currently have a Spacy NER model that looks for 6 custom entities, while only two (Person and Organization) are truly important.&lt;/p&gt;

&lt;p&gt;Would it be beneficial to cut out the other four entities in the model? My boss added them because he thought they would help with accuracy essentially through lessening the chance of false positives for person and organization. &lt;/p&gt;

&lt;p&gt;I personally believe the model would perform fine with only Person and organization, but am new to data science so just wanted to verify my hunch.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nerh99,True,,washknoxnash2255,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nerh99/spacy_ner_model_2_entities_versus_6/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nerh99/spacy_ner_model_2_entities_versus_6/,30199,1621284585.0,0,,False,,,,,,518
162,,LanguageTechnology,"My final year project is on Multi-class text classification and simply explores existing techniques (TF-IDF and Word2Vec) on a new and different dataset. Pipeline is standard: Data Pre-processing and Cleaning, Vectorization and Dimensionality Reduction, Model splitting and Training, Hyperparameter tuning, model re-training and finally model evaluation.

Is it worthy of sending it as a paper to conferences even though it is nothing novel technique-wise but is on a different dataset?

Any guidance would be appreciated!",t2_581q2gxy,False,,0,False,Is it conference paper worthy?,[],r/LanguageTechnology,False,6,,0,,False,t3_nehdv5,False,dark,0.88,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1621289684.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My final year project is on Multi-class text classification and simply explores existing techniques (TF-IDF and Word2Vec) on a new and different dataset. Pipeline is standard: Data Pre-processing and Cleaning, Vectorization and Dimensionality Reduction, Model splitting and Training, Hyperparameter tuning, model re-training and finally model evaluation.&lt;/p&gt;

&lt;p&gt;Is it worthy of sending it as a paper to conferences even though it is nothing novel technique-wise but is on a different dataset?&lt;/p&gt;

&lt;p&gt;Any guidance would be appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nehdv5,True,,jgj0707,,10,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nehdv5/is_it_conference_paper_worthy/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nehdv5/is_it_conference_paper_worthy/,30199,1621260884.0,0,,False,,,,,,522
163,,LanguageTechnology,"I am doing my individual research on automatic question generation from the paragraphs which will be input in to the system. I have divided this idea into three steps,

1. sentence selection
2. complex sentence simplification
3. sentence classification based on POS and NE tagged information

I would be grateful to hear your opinions based on the methodology which i am going to use. Please be kind enough to mention if i need to upgrade the methodology or any other changes that i should consider.",t2_8ve5eb9h,False,,0,False,Automatic question generation from input paragraph,[],r/LanguageTechnology,False,6,,0,,False,t3_neav37,False,dark,1.0,,public,14,0,{},,False,[],,False,False,,{},,False,14,,False,False,,False,,[],{},,True,,1621269815.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am doing my individual research on automatic question generation from the paragraphs which will be input in to the system. I have divided this idea into three steps,&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;sentence selection&lt;/li&gt;
&lt;li&gt;complex sentence simplification&lt;/li&gt;
&lt;li&gt;sentence classification based on POS and NE tagged information&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I would be grateful to hear your opinions based on the methodology which i am going to use. Please be kind enough to mention if i need to upgrade the methodology or any other changes that i should consider.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,neav37,True,,Visual-Milk8009,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/neav37/automatic_question_generation_from_input_paragraph/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/neav37/automatic_question_generation_from_input_paragraph/,30199,1621241015.0,0,,False,,,,,,499
164,,LanguageTechnology,,t2_atcn4bta,False,,0,False,IBM's Project Codenet will teach AI to code in dozens of programming languages. Extremely vast dataset,[],r/LanguageTechnology,False,6,,0,,False,t3_nep2tj,False,dark,0.75,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1621307731.0,text,6,,,text,artificialintelligence-news.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nep2tj,True,,abbumm,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nep2tj/ibms_project_codenet_will_teach_ai_to_code_in/,all_ads,False,https://artificialintelligence-news.com/2021/05/11/ibm-project-codenet-wants-teach-ai-how-code/,30199,1621278931.0,0,,False,https://artificialintelligence-news.com/2021/05/11/ibm-project-codenet-wants-teach-ai-how-code/,,,,,0
165,,LanguageTechnology,"Are there any tools that determine a word's difficulty? GPT3 has this thing where it can simplify things so that a 2nd grader would understand, how would it distinguish a word's level of complexity?",t2_be2v1j11,False,,0,False,Are there tools that tell a word's difficulty?,[],r/LanguageTechnology,False,6,,0,,False,t3_nehs9k,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1621290646.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Are there any tools that determine a word&amp;#39;s difficulty? GPT3 has this thing where it can simplify things so that a 2nd grader would understand, how would it distinguish a word&amp;#39;s level of complexity?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nehs9k,True,,ricksElar,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nehs9k/are_there_tools_that_tell_a_words_difficulty/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nehs9k/are_there_tools_that_tell_a_words_difficulty/,30199,1621261846.0,0,,False,,,,,,198
166,,LanguageTechnology,"Hello. I recently decided to start exploring the use of NLP for solving language and machine translation problems.

I'm familiar with Python but don't really know my way around what NLP libraries are out there as of 2021. **Does anyone have a list of must-know Python-based NLP libraries (I assume Python is the best tool for NLP) and NLP libraries specific to machine translation?**

Thank you in advance.",t2_hwnig,False,,0,False,New to NLP. Looking for library recommendations.,[],r/LanguageTechnology,False,6,,0,,False,t3_ndz50q,False,dark,0.94,,public,15,0,{},,False,[],,False,False,,{},,False,15,,False,False,,False,,[],{},,True,,1621231271.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello. I recently decided to start exploring the use of NLP for solving language and machine translation problems.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m familiar with Python but don&amp;#39;t really know my way around what NLP libraries are out there as of 2021. &lt;strong&gt;Does anyone have a list of must-know Python-based NLP libraries (I assume Python is the best tool for NLP) and NLP libraries specific to machine translation?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Thank you in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ndz50q,True,,PeleMaradona,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ndz50q/new_to_nlp_looking_for_library_recommendations/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ndz50q/new_to_nlp_looking_for_library_recommendations/,30199,1621202471.0,0,,False,,,,,,406
167,,LanguageTechnology,"I am a sophomore and have studied ML and DL for last 6 months. Recently I started working on NLP. I was wondering how these models (BERT, GPT) are created. Following are some questions

1. Is it possible to create my own model? I was thinking of making a hybrid of encoders and decoders of transformers. 

2. Is it even feasible to create a model at my stage? 

3. How much time would it take to create it?",t2_89k44e4d,False,,0,False,How to create a model like BERT or GPT?,[],r/LanguageTechnology,False,6,,0,,False,t3_ne5mkc,False,dark,0.83,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1621251150.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am a sophomore and have studied ML and DL for last 6 months. Recently I started working on NLP. I was wondering how these models (BERT, GPT) are created. Following are some questions&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Is it possible to create my own model? I was thinking of making a hybrid of encoders and decoders of transformers. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is it even feasible to create a model at my stage? &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How much time would it take to create it?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ne5mkc,True,,maisterdetemplar,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ne5mkc/how_to_create_a_model_like_bert_or_gpt/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ne5mkc/how_to_create_a_model_like_bert_or_gpt/,30199,1621222350.0,0,,False,,,,,,406
168,,LanguageTechnology,"Hi, I'm currently using a dataset, that has 6 different target labels. The number of instance of target 1-6 is 20k, 16k, 3k, 2k, 1.5k, 1.2k.

I'm making use of fastText to classify. The model gets over-fitted and for most of times gives the targets that have 20k (target 1) or 16k (target 2) instances.

The accuracy given by fastText is 90% but that is because of over fitting.
In another try, I provided (almost) equal amount of all instances ( &lt;= 2k ) for each target label and the accuracy had become 78%.

Is there any better way of handling this problem without neglecting thousands of instances?

Any suggestions would be helpful!",t2_8n5d56qx,False,,0,False,Techniques to handle over-fitting with text classifier,[],r/LanguageTechnology,False,6,,0,,False,t3_ndw6ys,False,dark,0.81,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1621223024.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m currently using a dataset, that has 6 different target labels. The number of instance of target 1-6 is 20k, 16k, 3k, 2k, 1.5k, 1.2k.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m making use of fastText to classify. The model gets over-fitted and for most of times gives the targets that have 20k (target 1) or 16k (target 2) instances.&lt;/p&gt;

&lt;p&gt;The accuracy given by fastText is 90% but that is because of over fitting.
In another try, I provided (almost) equal amount of all instances ( &amp;lt;= 2k ) for each target label and the accuracy had become 78%.&lt;/p&gt;

&lt;p&gt;Is there any better way of handling this problem without neglecting thousands of instances?&lt;/p&gt;

&lt;p&gt;Any suggestions would be helpful!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ndw6ys,True,,ChandlerBingggggggg,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ndw6ys/techniques_to_handle_overfitting_with_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ndw6ys/techniques_to_handle_overfitting_with_text/,30199,1621194224.0,0,,False,,,,,,640
169,,LanguageTechnology,"This question might be inappropriate here, but IDK where else to ask it. I have trouble getting a hugging face model to work, but I do have sufficient data and not enough time. I want to 'predict' a sentence from a previous one, or a sentence fragment. What is the simplest thing that I can do with NLTK and counters that is still useful?",t2_7bzamk8n,False,,0,False,Is a primitive method using NLTK and counters viable?,[],r/LanguageTechnology,False,6,,0,,False,t3_ndq9fq,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1621206340.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This question might be inappropriate here, but IDK where else to ask it. I have trouble getting a hugging face model to work, but I do have sufficient data and not enough time. I want to &amp;#39;predict&amp;#39; a sentence from a previous one, or a sentence fragment. What is the simplest thing that I can do with NLTK and counters that is still useful?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ndq9fq,True,,reniairtanitram,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ndq9fq/is_a_primitive_method_using_nltk_and_counters/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ndq9fq/is_a_primitive_method_using_nltk_and_counters/,30199,1621177540.0,0,,False,,,,,,338
170,,LanguageTechnology,"This blog lists out (All?) popular Unsupervised Keyword Extraction Algorithms in NLP. 

Here, I summarize almost 10 papers w.r.t all these techniques. Enjoy the read! 🎉 


https://link.medium.com/4ah0jdgXhgb",t2_hkv9s,False,,0,False,10 popular Keyword Extraction Techniques in NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_nd8q5a,False,dark,0.81,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1621142252.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This blog lists out (All?) popular Unsupervised Keyword Extraction Algorithms in NLP. &lt;/p&gt;

&lt;p&gt;Here, I summarize almost 10 papers w.r.t all these techniques. Enjoy the read! 🎉 &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://link.medium.com/4ah0jdgXhgb""&gt;https://link.medium.com/4ah0jdgXhgb&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nd8q5a,True,,prakhar21,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nd8q5a/10_popular_keyword_extraction_techniques_in_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nd8q5a/10_popular_keyword_extraction_techniques_in_nlp/,30199,1621113452.0,0,,False,,,,,,207
171,,LanguageTechnology,"Has anyone here gotten LexiDB to work? I recently found LexiDB as a potenialy corpus querying alternative to CWB's CQP.

These tools allow you to query large corpora (my current largest corpus has 800 million tokens in it) for patterns. For example, you could search for ""the &lt;adjective&gt; *"" and the tool will quickly return a list of matches.

I have CQP working already but LexiDB sounds a little better for my needs. Unfortunately so far I can't get it to work.",t2_2o4k3icq,False,,0,False,CQP vs LexiDB,[],r/LanguageTechnology,False,6,,0,,False,t3_nd8ra2,False,dark,0.91,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1621142343.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Has anyone here gotten LexiDB to work? I recently found LexiDB as a potenialy corpus querying alternative to CWB&amp;#39;s CQP.&lt;/p&gt;

&lt;p&gt;These tools allow you to query large corpora (my current largest corpus has 800 million tokens in it) for patterns. For example, you could search for &amp;quot;the &amp;lt;adjective&amp;gt; *&amp;quot; and the tool will quickly return a list of matches.&lt;/p&gt;

&lt;p&gt;I have CQP working already but LexiDB sounds a little better for my needs. Unfortunately so far I can&amp;#39;t get it to work.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nd8ra2,True,,anlinguist,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nd8ra2/cqp_vs_lexidb/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nd8ra2/cqp_vs_lexidb/,30199,1621113543.0,0,,False,,,,,,469
172,,LanguageTechnology,"In short: I listen to lots of interesting podcasts, but with my limited human memory, I tend to forget some of the juicier details that I would like to remember.. My solution to this would be to summarize podcast episodes that I find memorable so that a quick glimpse at the highlights would bring the topics back to memory. However, instead of spending time doing manual summations, I would like to practice some ML/NLP applications by making my own podcast summarizer.

My thought is to divide the problem into two main steps:

1. Audio to transcript
2. Transcript summation/extraction of main topics

What ideas and thoughts do you all having regarding which models/libraries to use for the two steps? Or any other inputs on how to approach it?

Super excited to hear your thoughts!",t2_uvm35oi,False,,0,False,Ides for making a podcast summarizer?,[],r/LanguageTechnology,False,6,,0,,False,t3_nd3pff,False,dark,1.0,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1621127744.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In short: I listen to lots of interesting podcasts, but with my limited human memory, I tend to forget some of the juicier details that I would like to remember.. My solution to this would be to summarize podcast episodes that I find memorable so that a quick glimpse at the highlights would bring the topics back to memory. However, instead of spending time doing manual summations, I would like to practice some ML/NLP applications by making my own podcast summarizer.&lt;/p&gt;

&lt;p&gt;My thought is to divide the problem into two main steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Audio to transcript&lt;/li&gt;
&lt;li&gt;Transcript summation/extraction of main topics&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;What ideas and thoughts do you all having regarding which models/libraries to use for the two steps? Or any other inputs on how to approach it?&lt;/p&gt;

&lt;p&gt;Super excited to hear your thoughts!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nd3pff,True,,ErnieBernie2017,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nd3pff/ides_for_making_a_podcast_summarizer/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nd3pff/ides_for_making_a_podcast_summarizer/,30199,1621098944.0,0,,False,,,,,,785
173,,LanguageTechnology,"Hello,

I am a Computer science Master graduate from a top EU university.  During my Masters I focused mainly on NLP and I did 2 internships as an NLP engineer. I am currently looking(not actively looking) for opportunities in NLP in Europe(mainly London, Switzerland, and maybe Germany). 

But I am having trouble finding opportunities that match my profile and that I like. The main issue that I have is that a lot of big companies(not start ups) say that they do NLP but in reality most of the time you end up working on software engineering tasks. The other problem is that all the posts that I like require a PhD.  I also applied in FAANG companies but they only accepted me to interview for a Software engineering roles. 

1. Do any of you work at a FAANG company as a Software engineer and work on NLP projects ? What exactly is your job, do you come up with architecture of the model or do you just build the infrastructure around the model ... ? 
2. For those who work in NLP but don't have a PhD can you give me an example of NLP projects that you do at work and if possible tell me for which company you work. 

Any recommendations of companies that do NLP and hire people without a PhD are welcomed.",t2_gz4ps3x,False,,0,False,NLP job opportunities for Master graduates(no PhD),[],r/LanguageTechnology,False,6,,0,,False,t3_ncue0g,False,dark,1.0,,public,24,0,{},,False,[],,False,False,,{},,False,24,,False,False,,False,,[],{},,True,,1621097488.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am a Computer science Master graduate from a top EU university.  During my Masters I focused mainly on NLP and I did 2 internships as an NLP engineer. I am currently looking(not actively looking) for opportunities in NLP in Europe(mainly London, Switzerland, and maybe Germany). &lt;/p&gt;

&lt;p&gt;But I am having trouble finding opportunities that match my profile and that I like. The main issue that I have is that a lot of big companies(not start ups) say that they do NLP but in reality most of the time you end up working on software engineering tasks. The other problem is that all the posts that I like require a PhD.  I also applied in FAANG companies but they only accepted me to interview for a Software engineering roles. &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Do any of you work at a FAANG company as a Software engineer and work on NLP projects ? What exactly is your job, do you come up with architecture of the model or do you just build the infrastructure around the model ... ? &lt;/li&gt;
&lt;li&gt;For those who work in NLP but don&amp;#39;t have a PhD can you give me an example of NLP projects that you do at work and if possible tell me for which company you work. &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Any recommendations of companies that do NLP and hire people without a PhD are welcomed.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ncue0g,True,,angular-calendar,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ncue0g/nlp_job_opportunities_for_master_graduatesno_phd/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ncue0g/nlp_job_opportunities_for_master_graduatesno_phd/,30199,1621068688.0,0,,False,,,,,,1211
174,,LanguageTechnology,"When I train mobileBERT on GTX 1070, I get 3.8 it/sec. However, when I train DistilBERT on the same GPU, I get 15 it/sec. Am I missing something? The paper on MobileBERT states that MobileBERT should be faster.",t2_2w70t3st,False,,0,False,Is it just me or is mobileBERT is much slower than DistilBERT on Huggingface,[],r/LanguageTechnology,False,6,,0,,False,t3_nczb5s,False,dark,0.91,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1621115374.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;When I train mobileBERT on GTX 1070, I get 3.8 it/sec. However, when I train DistilBERT on the same GPU, I get 15 it/sec. Am I missing something? The paper on MobileBERT states that MobileBERT should be faster.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nczb5s,True,,SiegeMemeLord,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nczb5s/is_it_just_me_or_is_mobilebert_is_much_slower/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nczb5s/is_it_just_me_or_is_mobilebert_is_much_slower/,30199,1621086574.0,0,,False,,,,,,210
175,,LanguageTechnology,"Title, basically sums up my question.",t2_dwsy8,False,,0,False,"Is there a network available for download that has a very long attention span? Something that could summarize a book, for instance.",[],r/LanguageTechnology,False,6,,0,,False,t3_ncb54r,False,dark,0.94,,public,14,0,{},,False,[],,False,False,,{},,False,14,,False,True,,False,,[],{},,True,,1621034508.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Title, basically sums up my question.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ncb54r,True,,urammar,,13,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ncb54r/is_there_a_network_available_for_download_that/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ncb54r/is_there_a_network_available_for_download_that/,30199,1621005708.0,0,,False,,,,,,37
176,,LanguageTechnology,"Hi everyone! I'm wrapping my head around regular expressions and I managed to write a simple line of code that detects age. I tried my best, unfortunately that doesn't work every time because there are several ways people can express their age.

1) ""I'm 25""
2) ""I'm 25yo""
3) ""I'm 25 years old""
4) ""I'm 25M""

My regex is very simple re.search(r'\d+\w+', text) and it works fine with 2 and 4 since there's no space between them. Do you have any idea to improve this regex so that it works with all of them?

Thanks for your time!",t2_b31us5mn,False,,0,False,Regex to detect age in a sting literal on Python,[],r/LanguageTechnology,False,6,,0,,False,t3_ncczd0,False,dark,0.8,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1621039259.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone! I&amp;#39;m wrapping my head around regular expressions and I managed to write a simple line of code that detects age. I tried my best, unfortunately that doesn&amp;#39;t work every time because there are several ways people can express their age.&lt;/p&gt;

&lt;p&gt;1) &amp;quot;I&amp;#39;m 25&amp;quot;
2) &amp;quot;I&amp;#39;m 25yo&amp;quot;
3) &amp;quot;I&amp;#39;m 25 years old&amp;quot;
4) &amp;quot;I&amp;#39;m 25M&amp;quot;&lt;/p&gt;

&lt;p&gt;My regex is very simple re.search(r&amp;#39;\d+\w+&amp;#39;, text) and it works fine with 2 and 4 since there&amp;#39;s no space between them. Do you have any idea to improve this regex so that it works with all of them?&lt;/p&gt;

&lt;p&gt;Thanks for your time!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ncczd0,True,,Dr_Funkmachine,,13,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ncczd0/regex_to_detect_age_in_a_sting_literal_on_python/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ncczd0/regex_to_detect_age_in_a_sting_literal_on_python/,30199,1621010459.0,0,,False,,,,,,527
177,,LanguageTechnology,,t2_hkv9s,False,,0,False,EmbedRank: Simple Unsupervised Keyphrase Extraction using Sentence Embeddings,[],r/LanguageTechnology,False,6,,0,,False,t3_ncbjs8,False,dark,0.67,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1621035562.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ncbjs8,True,,prakhar21,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ncbjs8/embedrank_simple_unsupervised_keyphrase/,all_ads,False,https://link.medium.com/zaWf1MfUfgb,30199,1621006762.0,0,,False,https://link.medium.com/zaWf1MfUfgb,,,,,0
178,,LanguageTechnology,"I'm looking for a large (more than 100K records) corpus dataset for a summarization task in Portuguese. Something like the CNN/DailyMail dataset ([https://huggingface.co/datasets/cnn\_dailymail](https://huggingface.co/datasets/cnn_dailymail)) but in Portuguese.

Does anyone know of such a dataset?",t2_7gloh8o4,False,,0,False,Large summarization dataset in Portuguese,[],r/LanguageTechnology,False,6,,0,,False,t3_nc6mku,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1621021452.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m looking for a large (more than 100K records) corpus dataset for a summarization task in Portuguese. Something like the CNN/DailyMail dataset (&lt;a href=""https://huggingface.co/datasets/cnn_dailymail""&gt;https://huggingface.co/datasets/cnn_dailymail&lt;/a&gt;) but in Portuguese.&lt;/p&gt;

&lt;p&gt;Does anyone know of such a dataset?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nc6mku,True,,GlitteringPitch7989,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nc6mku/large_summarization_dataset_in_portuguese/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nc6mku/large_summarization_dataset_in_portuguese/,30199,1620992652.0,0,,False,,,,,,298
179,,LanguageTechnology,,t2_auwgbh53,False,,0,False,Evolution of NLP search methods and the latest method - Neural Search. What is it and how to get started with it,[],r/LanguageTechnology,False,6,,0,,False,t3_nbzarl,False,dark,0.63,,public,4,1,{},,False,[],,False,False,,{},,False,4,,False,True,,False,,[],{'gid_1': 1},,False,,1620991793.0,text,6,,,text,medium.com,False,,,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbzarl,True,,opensourcecolumbus,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbzarl/evolution_of_nlp_search_methods_and_the_latest/,all_ads,False,https://medium.com/nerd-for-tech/what-is-neural-search-537853f3d628,30199,1620962993.0,2,,False,https://medium.com/nerd-for-tech/what-is-neural-search-537853f3d628,,,,,0
180,,LanguageTechnology,"I've developed a conversational architecture for my robots that works fairly well in an unsupervised setting.  The robots recognize the folks they talk to (if they've been introduced) and save the interactions and the full text of the dialog between them by date.  

I am looking for something that can summarize the primary subjects of the conversations after the fact, so that I can sort of ""categorize"" the subjects that interest various folks that the robots talk with.  

I've found things like this:  https://convokit.cornell.edu/documentation/tutorial.html

These seem to offer some useful direction - just wondering if anyone else has any suggestions to summarize a given interaction?  Thanks.",t2_16rqfm,False,,0,False,After-the-fact conversational topic analysis?,[],r/LanguageTechnology,False,6,,0,,False,t3_nbsuph,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1620972728.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve developed a conversational architecture for my robots that works fairly well in an unsupervised setting.  The robots recognize the folks they talk to (if they&amp;#39;ve been introduced) and save the interactions and the full text of the dialog between them by date.  &lt;/p&gt;

&lt;p&gt;I am looking for something that can summarize the primary subjects of the conversations after the fact, so that I can sort of &amp;quot;categorize&amp;quot; the subjects that interest various folks that the robots talk with.  &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve found things like this:  &lt;a href=""https://convokit.cornell.edu/documentation/tutorial.html""&gt;https://convokit.cornell.edu/documentation/tutorial.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;These seem to offer some useful direction - just wondering if anyone else has any suggestions to summarize a given interaction?  Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbsuph,True,,DelosBoard2052,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbsuph/afterthefact_conversational_topic_analysis/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nbsuph/afterthefact_conversational_topic_analysis/,30199,1620943928.0,0,,False,,,,,,701
181,,LanguageTechnology,"I mean is there already a database that I can use. If not, how do I get their website address without looking each one up on google and how to automate the process of collecting information. I use python if you need to know. Any suggestion helps. Thankyou",t2_5r4mo7fh,False,,0,False,I have a list of names of publicly listed companies (around 1000) and want to do textual analysis of information on their websites. Where do I start?,[],r/LanguageTechnology,False,6,,0,,False,t3_nbyu2x,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620990178.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I mean is there already a database that I can use. If not, how do I get their website address without looking each one up on google and how to automate the process of collecting information. I use python if you need to know. Any suggestion helps. Thankyou&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbyu2x,True,,Epiphany925,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbyu2x/i_have_a_list_of_names_of_publicly_listed/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nbyu2x/i_have_a_list_of_names_of_publicly_listed/,30199,1620961378.0,0,,False,,,,,,255
182,,LanguageTechnology,Anyone have experience using Spacy 3.0 Text classifier for multi class classification? I have 6 classes but after running the model get all 0s for the accuracy score. Code is at work but I can provide it tomorrow if anyone has had a similar use case! I have a feeling I’m making a mistake regarding the multiple classes,t2_44md8gjv,False,,0,False,SpaCy 3.0 Text Classifier,[],r/LanguageTechnology,False,6,,0,,False,t3_nbvv0h,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1620980904.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Anyone have experience using Spacy 3.0 Text classifier for multi class classification? I have 6 classes but after running the model get all 0s for the accuracy score. Code is at work but I can provide it tomorrow if anyone has had a similar use case! I have a feeling I’m making a mistake regarding the multiple classes&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbvv0h,True,,washknoxnash2255,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbvv0h/spacy_30_text_classifier/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nbvv0h/spacy_30_text_classifier/,30199,1620952104.0,0,,False,,,,,,319
183,,LanguageTechnology,"Hi everyone!

What would you recommend for an efficient preprocessing pipeline these days? I was checking spaCy but they don't offer bi-gramization out of the box right?

So to include bi-grams I was thinking on gensim.

But there things get hairy. Gensim needs tokens per sentence per doc to learn the phrase model, then I need to tokenize/segment first.

I don't like this since I wanted to use spaCy for the tokenization and lemmatization as a final and single step.

Also, I guess spaCy's lemmatization is much better than NLTK's

All seems like a mess and perhaps can shade some light on this.

Finally, will bi-grams affect lemmatization? Which is the correct order?

Thanks for your insights!!



Edit 1: And how about frequency filtering and min/max length filtering?

Edit 2: Is it better to TF-IDF or just stop words removal?

Edit 3: This preprocessing if for topic modelling",t2_5m0mxfui,False,,0,False,Best approach for preprocessing in 2021,[],r/LanguageTechnology,False,6,,0,,False,t3_nbn0rw,False,dark,1.0,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,1620939743.0,,[],{},,True,,1620957844.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;

&lt;p&gt;What would you recommend for an efficient preprocessing pipeline these days? I was checking spaCy but they don&amp;#39;t offer bi-gramization out of the box right?&lt;/p&gt;

&lt;p&gt;So to include bi-grams I was thinking on gensim.&lt;/p&gt;

&lt;p&gt;But there things get hairy. Gensim needs tokens per sentence per doc to learn the phrase model, then I need to tokenize/segment first.&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t like this since I wanted to use spaCy for the tokenization and lemmatization as a final and single step.&lt;/p&gt;

&lt;p&gt;Also, I guess spaCy&amp;#39;s lemmatization is much better than NLTK&amp;#39;s&lt;/p&gt;

&lt;p&gt;All seems like a mess and perhaps can shade some light on this.&lt;/p&gt;

&lt;p&gt;Finally, will bi-grams affect lemmatization? Which is the correct order?&lt;/p&gt;

&lt;p&gt;Thanks for your insights!!&lt;/p&gt;

&lt;p&gt;Edit 1: And how about frequency filtering and min/max length filtering?&lt;/p&gt;

&lt;p&gt;Edit 2: Is it better to TF-IDF or just stop words removal?&lt;/p&gt;

&lt;p&gt;Edit 3: This preprocessing if for topic modelling&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbn0rw,True,,iblysa,,15,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbn0rw/best_approach_for_preprocessing_in_2021/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nbn0rw/best_approach_for_preprocessing_in_2021/,30199,1620929044.0,0,,False,,,,,,886
184,,LanguageTechnology,"This research talks about using Random Walk inspired Anonymous Walks as graph units to derive feature-based and data-driven Graph Embeddings in an unsupervised fashion. 🔥 

https://youtu.be/VVml3nDiM3E",t2_hkv9s,False,,0,False,Anonymous Walk Embeddings (Graph ML Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_nbk2ja,False,dark,1.0,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1620950566.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This research talks about using Random Walk inspired Anonymous Walks as graph units to derive feature-based and data-driven Graph Embeddings in an unsupervised fashion. 🔥 &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://youtu.be/VVml3nDiM3E""&gt;https://youtu.be/VVml3nDiM3E&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbk2ja,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbk2ja/anonymous_walk_embeddings_graph_ml_research_paper/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nbk2ja/anonymous_walk_embeddings_graph_ml_research_paper/,30199,1620921766.0,0,,False,,,,,,201
185,,LanguageTechnology,"Hi All, I’m looking for a framework that allows for the extraction of knowledge claims/factual statements from a given text. For example…

Input: “Joe Biden won the election, making him the 46th President of the United States”

Output: “Joe Biden won the election”, “Joe Biden is the 46th President of the United States”

I know this has some overlap with sentence entailment, but that’s not exactly what I’m looking for.

Does anyone know of anything like this? Thanks! :)",t2_21v8yh89,False,,0,False,Looking for a Claim Extraction/Sentence Segmentation Framework,[],r/LanguageTechnology,False,6,,0,,False,t3_nbp2ec,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620962814.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All, I’m looking for a framework that allows for the extraction of knowledge claims/factual statements from a given text. For example…&lt;/p&gt;

&lt;p&gt;Input: “Joe Biden won the election, making him the 46th President of the United States”&lt;/p&gt;

&lt;p&gt;Output: “Joe Biden won the election”, “Joe Biden is the 46th President of the United States”&lt;/p&gt;

&lt;p&gt;I know this has some overlap with sentence entailment, but that’s not exactly what I’m looking for.&lt;/p&gt;

&lt;p&gt;Does anyone know of anything like this? Thanks! :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbp2ec,True,,flakessss,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbp2ec/looking_for_a_claim_extractionsentence/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nbp2ec/looking_for_a_claim_extractionsentence/,30199,1620934014.0,0,,False,,,,,,473
186,,LanguageTechnology,,t2_hkv9s,False,,0,False,Beyond Accuracy: Behavioral Testing of NLP Models with CheckList (BEST PAPER ACL),[],r/LanguageTechnology,False,6,,0,,False,t3_nbbbiy,False,dark,1.0,,public,14,0,{},,False,[],,False,False,,{},,False,14,,False,False,,False,,[],{},,False,,1620921142.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbbbiy,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbbbiy/beyond_accuracy_behavioral_testing_of_nlp_models/,all_ads,False,https://link.medium.com/XUG13JXHdgb,30199,1620892342.0,0,,False,https://link.medium.com/XUG13JXHdgb,,,,,0
187,,LanguageTechnology,,t2_apv7o2y8,False,,0,False,Parrot: Paraphrase based utterance augmentation framework | Python #NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_nbhb5l,False,dark,1.0,,public,4,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/7rgvS1qMePo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Parrot: Paraphrase based utterance augmentation framework | Python #NLP', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/7rgvS1qMePo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/7rgvS1qMePo/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/7rgvS1qMePo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nbhb5l', 'height': 200}",,False,4,,False,False,,False,,[],{},,False,,1620943237.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbhb5l,True,,rsree123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbhb5l/parrot_paraphrase_based_utterance_augmentation/,all_ads,False,https://youtu.be/7rgvS1qMePo,30199,1620914437.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Parrot: Paraphrase based utterance augmentation framework | Python #NLP', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/7rgvS1qMePo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/7rgvS1qMePo/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,https://youtu.be/7rgvS1qMePo,,,,,0
188,,LanguageTechnology,,t2_2dwso7l3,False,,0,False,Sentiment Analysis Recommendations on Review data,[],r/LanguageTechnology,False,6,,0,,False,t3_nbj7u6,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,False,,1620948349.0,text,6,,,text,self.datascience,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbj7u6,True,,Jaypal17,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbj7u6/sentiment_analysis_recommendations_on_review_data/,all_ads,False,/r/datascience/comments/nbj29s/sentiment_analysis_recommendations_on_review_data/,30199,1620919549.0,0,,False,/r/datascience/comments/nbj29s/sentiment_analysis_recommendations_on_review_data/,"[{'approved_at_utc': None, 'subreddit': 'datascience', 'selftext': ""I'm looking for recommendations on my project, currently, we have a couple of hundred rows of health care review data. My project manager wants me to find a sentiment analysis tool that gives a compound score that correlates accurately to the stars given for the review. My first attempt I used vaderSentiment and it was around 55% accurate at the score to start rating, my second attempt I used texBlob and that was less accurate (35%). I want to know if there is any off the shelf models or other libraries I can use with python, especially if it understand Healthcare lingo. We hope that we can find something that is about 60-70% accurate from compound score to star rating. Eventually, we will build our own model once we have more data and time. For now, we just want to demo the data we have. Also if you think I'm going about this all wrong please let me know. I am relatively new to data science and this is a part-time project for my job."", 'author_fullname': 't2_2dwso7l3', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Sentiment Analysis Recommendations on Review data', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/datascience', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'discussion', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_nbj29s', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.78, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1620947960.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.datascience', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m looking for recommendations on my project, currently, we have a couple of hundred rows of health care review data. My project manager wants me to find a sentiment analysis tool that gives a compound score that correlates accurately to the stars given for the review. My first attempt I used vaderSentiment and it was around 55% accurate at the score to start rating, my second attempt I used texBlob and that was less accurate (35%). I want to know if there is any off the shelf models or other libraries I can use with python, especially if it understand Healthcare lingo. We hope that we can find something that is about 60-70% accurate from compound score to star rating. Eventually, we will build our own model once we have more data and time. For now, we just want to demo the data we have. Also if you think I&amp;#39;m going about this all wrong please let me know. I am relatively new to data science and this is a part-time project for my job.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': 'confidence', 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '4fad7108-d77d-11e7-b0c6-0ee69f155af2', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2sptq', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'nbj29s', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Jaypal17', 'discussion_type': None, 'num_comments': 10, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/datascience/comments/nbj29s/sentiment_analysis_recommendations_on_review_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/datascience/comments/nbj29s/sentiment_analysis_recommendations_on_review_data/', 'subreddit_subscribers': 512451, 'created_utc': 1620919160.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_nbj29s,,,0
189,,LanguageTechnology,"I am an undergraduate in my third year, I've been working in NLP research for over a year now. My university however, is not very research inclined and there is very little good guidance on good poster submission/publication venues. While I've researched and found places to submit complete research papers, there's very little information on poster submission venues. Any information would be helpful!",t2_8tcci4yd,False,,0,False,What are some good Poster Submission venues for NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_nbpox4,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1620964314.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am an undergraduate in my third year, I&amp;#39;ve been working in NLP research for over a year now. My university however, is not very research inclined and there is very little good guidance on good poster submission/publication venues. While I&amp;#39;ve researched and found places to submit complete research papers, there&amp;#39;s very little information on poster submission venues. Any information would be helpful!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbpox4,True,,Ninja_Similar,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbpox4/what_are_some_good_poster_submission_venues_for/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nbpox4/what_are_some_good_poster_submission_venues_for/,30199,1620935514.0,0,,False,,,,,,402
190,,LanguageTechnology,"I'm faced with a task: Given the URL of webpage, that links to the homepage of a company, can you describe to me, in 1 to 10 sentences, what the company does?

Are there any pre-made solutions for this? I already have a massive amounts of supervised training data for the problem.",t2_ow7xp,False,,0,False,"Webpage summarization: Given the URL of a company webpage, what does the company do?",[],r/LanguageTechnology,False,6,,0,,False,t3_nbcwma,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1620927988.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m faced with a task: Given the URL of webpage, that links to the homepage of a company, can you describe to me, in 1 to 10 sentences, what the company does?&lt;/p&gt;

&lt;p&gt;Are there any pre-made solutions for this? I already have a massive amounts of supervised training data for the problem.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbcwma,True,,shackleshot,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbcwma/webpage_summarization_given_the_url_of_a_company/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nbcwma/webpage_summarization_given_the_url_of_a_company/,30199,1620899188.0,0,,False,,,,,,280
191,,LanguageTechnology,,t2_86ukz5po,False,,0,False,"Graph-Based Framework for Structured Prediction Tasks in Sanskrit by Dr. Pawan Goyal. This is a search-based structured prediction framework, which expects a graph as input, where relevant linguistic info is encoded in the nodes, &amp; the edges are then used to indicate the association b/w these nodes.",[],r/LanguageTechnology,False,6,,0,,False,t3_nb4hty,False,dark,0.92,,public,10,1,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,False,,1620896404.0,text,6,,,text,asiainnovationsummit.com,False,,,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nb4hty,True,,LeagueRude6428,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nb4hty/graphbased_framework_for_structured_prediction/,all_ads,False,https://www.asiainnovationsummit.com/pawan-goyal,30199,1620867604.0,0,,False,https://www.asiainnovationsummit.com/pawan-goyal,,,,,0
192,,LanguageTechnology,"Hello! I am currently a second-year (third) linguistics major student, and I am looking for an academic advice. I am looking forward intro declaring a minor in computer science or math. The problem is that I do not know what to commit to (and I can only choose one). I am aiming to apply to grad school in data science / natural language processing / computational linguistics. I have already developed some Data Science projects and I have developed pretty good coding skills by myself (Python, C++, Swift), and I am pretty much equally good at math. However, I am not entirely sure which minor would help me more career-wise. Can you guys help me out?",t2_9zny9grc,False,,0,False,Linguistics student looking for advice to advance in NLP/Computational Linguistics,[],r/LanguageTechnology,False,6,,0,,False,t3_nbcb2p,False,dark,0.75,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620925451.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! I am currently a second-year (third) linguistics major student, and I am looking for an academic advice. I am looking forward intro declaring a minor in computer science or math. The problem is that I do not know what to commit to (and I can only choose one). I am aiming to apply to grad school in data science / natural language processing / computational linguistics. I have already developed some Data Science projects and I have developed pretty good coding skills by myself (Python, C++, Swift), and I am pretty much equally good at math. However, I am not entirely sure which minor would help me more career-wise. Can you guys help me out?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbcb2p,True,,kaisar_dauletbek,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbcb2p/linguistics_student_looking_for_advice_to_advance/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nbcb2p/linguistics_student_looking_for_advice_to_advance/,30199,1620896651.0,0,,False,,,,,,653
193,,LanguageTechnology,,t2_a0fp5ht8,False,,0,False,Cognitive Scientist Terrence Deacon Says Current AI Lacks Symbolic Representations &amp; True Language Comprehension Abilities,[],r/LanguageTechnology,False,6,,0,,False,t3_nas06j,False,dark,0.87,,public,32,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/gbWX--cpbYU?start=10&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'GC Clips: Can AI Comprehend Language Like Humans?', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/gbWX--cpbYU?start=10&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Nick Jikomes', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/gbWX--cpbYU/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC_dJ5ThfEhj39zkEPQbNdPg'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/gbWX--cpbYU?start=10&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nas06j', 'height': 200}",,False,32,,False,False,,False,,[],{},,False,,1620863590.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nas06j,True,,laitnesba,,16,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nas06j/cognitive_scientist_terrence_deacon_says_current/,all_ads,False,https://www.youtube.com/watch?v=gbWX--cpbYU&amp;t=10s,30199,1620834790.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'GC Clips: Can AI Comprehend Language Like Humans?', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/gbWX--cpbYU?start=10&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Nick Jikomes', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/gbWX--cpbYU/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC_dJ5ThfEhj39zkEPQbNdPg'}}",False,https://www.youtube.com/watch?v=gbWX--cpbYU&amp;t=10s,,,,,0
194,,LanguageTechnology,,t2_3rtgvxbs,False,,0,False,Flashcard Generator - using a multi-transformer pipeline to self-correct,[],r/LanguageTechnology,False,6,,0,,False,t3_nb6nem,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,False,,1620902925.0,text,6,,,text,revision.ai,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nb6nem,True,,dancingnightly,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nb6nem/flashcard_generator_using_a_multitransformer/,all_ads,False,http://www.revision.ai/quiz?cdf,30199,1620874125.0,0,,False,http://www.revision.ai/quiz?cdf,,,,,0
195,,LanguageTechnology,"I'm reading a book 'Speech and Language Processing'. Here it is mentioned that without an end-symbol, the sentence probabilities for all sentences of a given length would sum to one.
I searched google to understand this statement but couldn't get satisfactory answers. Could anyone help me understand this.",t2_1xoxalgq,False,,0,False,Importance of end symbol in N gram,[],r/LanguageTechnology,False,6,,0,,False,t3_nav6mc,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620871491.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m reading a book &amp;#39;Speech and Language Processing&amp;#39;. Here it is mentioned that without an end-symbol, the sentence probabilities for all sentences of a given length would sum to one.
I searched google to understand this statement but couldn&amp;#39;t get satisfactory answers. Could anyone help me understand this.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nav6mc,True,,niteshbisht26,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nav6mc/importance_of_end_symbol_in_n_gram/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nav6mc/importance_of_end_symbol_in_n_gram/,30199,1620842691.0,0,,False,,,,,,306
196,,LanguageTechnology,,t2_hkv9s,False,,0,False,Leveraging BERT for Extractive Text Summarization on Lectures,[],r/LanguageTechnology,False,6,,0,,False,t3_nalfmp,False,dark,0.91,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,False,,1620843981.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nalfmp,True,,prakhar21,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nalfmp/leveraging_bert_for_extractive_text_summarization/,all_ads,False,https://link.medium.com/hvJSmr6dcgb,30199,1620815181.0,0,,False,https://link.medium.com/hvJSmr6dcgb,,,,,0
197,,LanguageTechnology,"Does anyone know where to find a dataset of adjectives used to describe humans in a text? I know named entity recognition exists, but I don't know how I would extend this to only extract human entities and then select adjectives which describe them.",t2_4c4ybmje,False,,0,False,Descriptions of Named Humans,[],r/LanguageTechnology,False,6,,0,,False,t3_naz8vi,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1620881935.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Does anyone know where to find a dataset of adjectives used to describe humans in a text? I know named entity recognition exists, but I don&amp;#39;t know how I would extend this to only extract human entities and then select adjectives which describe them.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,naz8vi,True,,Adolphins,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/naz8vi/descriptions_of_named_humans/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/naz8vi/descriptions_of_named_humans/,30199,1620853135.0,0,,False,,,,,,249
198,,LanguageTechnology,The common techniques to do topic modelling (eg. SVD) depend on multiple documents. What's a good method for extracting topics for a single document?,t2_8py9vlu0,False,,0,False,Topic modelling for single document?,[],r/LanguageTechnology,False,6,,0,,False,t3_nat8fr,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1620866620.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The common techniques to do topic modelling (eg. SVD) depend on multiple documents. What&amp;#39;s a good method for extracting topics for a single document?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nat8fr,True,,nuvicc,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nat8fr/topic_modelling_for_single_document/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nat8fr/topic_modelling_for_single_document/,30199,1620837820.0,0,,False,,,,,,149
199,,LanguageTechnology,,t2_3vcebtqk,False,,0,False,FactSumm: Factual Consistency Scorer for Abstractive Summarization,[],r/LanguageTechnology,False,6,,0,,False,t3_nakqet,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,False,,1620841169.0,text,6,,,text,github.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nakqet,True,,huffonism,,0,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/nakqet/factsumm_factual_consistency_scorer_for/,all_ads,False,https://github.com/Huffon/factsumm,30199,1620812369.0,0,,False,https://github.com/Huffon/factsumm,,,,,0
200,,LanguageTechnology,"&amp;#x200B;

https://reddit.com/link/nacy0z/video/enf2j74sbly61/player

What if we want to extract and summarize text from documents, also handle translation, combine the  outputs and load it into an Embeddings index. Enter workflows! The demo above takes a list of GitHub project pages, extracts text from HTML, summarizes the text and builds a similarity search index. This same concept could be applied towards a list of company pages, wikipedia  pages and more. This is just one example of what txtai workflows can do.

txtai workflows are a simple yet powerful construct that takes a callable and returns elements. Workflows are streaming by nature and work on data in batches, allowing large volumes of data to be processed efficiently. The amount of functionality provided by machine learning models continues to grow rapidly. txtai provides an easy way to interface with these models. The following is a non-comprehensive list.

\- Questions - Extractive question-answering using a text context  
\- Labels - Apply labels to text using a zero-shot classification model  
\- Summary - Abstractive text summarization  
\- Text Extraction - Extract text from documents  
\- Transcription - Transcribe audio to text  
\- Translation - Machine translation

Workflows allows joining these models together to create powerful data transformations. Workflows can also be constructed in JavaScript, Go, Rust and Java via the API.

See the following links for more information.

[GitHub](https://github.com/neuml/txtai) | [Workflow builder](https://github.com/neuml/txtai/blob/master/examples/workflows.py) | [Documentation](https://neuml.github.io/txtai) | [Article](https://towardsdatascience.com/run-machine-learning-workflows-to-transform-data-and-build-ai-powered-text-indices-with-txtai-43d769b566a7)",t2_536lg1nv,False,,0,False,"Machine learning workflows to summarize, translate, transcribe and more",[],r/LanguageTechnology,False,6,,0,,False,t3_nacy0z,False,dark,1.0,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,True,,1620810991.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://reddit.com/link/nacy0z/video/enf2j74sbly61/player""&gt;https://reddit.com/link/nacy0z/video/enf2j74sbly61/player&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;What if we want to extract and summarize text from documents, also handle translation, combine the  outputs and load it into an Embeddings index. Enter workflows! The demo above takes a list of GitHub project pages, extracts text from HTML, summarizes the text and builds a similarity search index. This same concept could be applied towards a list of company pages, wikipedia  pages and more. This is just one example of what txtai workflows can do.&lt;/p&gt;

&lt;p&gt;txtai workflows are a simple yet powerful construct that takes a callable and returns elements. Workflows are streaming by nature and work on data in batches, allowing large volumes of data to be processed efficiently. The amount of functionality provided by machine learning models continues to grow rapidly. txtai provides an easy way to interface with these models. The following is a non-comprehensive list.&lt;/p&gt;

&lt;p&gt;- Questions - Extractive question-answering using a text context&lt;br/&gt;
- Labels - Apply labels to text using a zero-shot classification model&lt;br/&gt;
- Summary - Abstractive text summarization&lt;br/&gt;
- Text Extraction - Extract text from documents&lt;br/&gt;
- Transcription - Transcribe audio to text&lt;br/&gt;
- Translation - Machine translation&lt;/p&gt;

&lt;p&gt;Workflows allows joining these models together to create powerful data transformations. Workflows can also be constructed in JavaScript, Go, Rust and Java via the API.&lt;/p&gt;

&lt;p&gt;See the following links for more information.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/neuml/txtai""&gt;GitHub&lt;/a&gt; | &lt;a href=""https://github.com/neuml/txtai/blob/master/examples/workflows.py""&gt;Workflow builder&lt;/a&gt; | &lt;a href=""https://neuml.github.io/txtai""&gt;Documentation&lt;/a&gt; | &lt;a href=""https://towardsdatascience.com/run-machine-learning-workflows-to-transform-data-and-build-ai-powered-text-indices-with-txtai-43d769b566a7""&gt;Article&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nacy0z,True,,davidmezzetti,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nacy0z/machine_learning_workflows_to_summarize_translate/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nacy0z/machine_learning_workflows_to_summarize_translate/,30199,1620782191.0,0,,False,,,,"{'enf2j74sbly61': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/nacy0z/asset/enf2j74sbly61/DASHPlaylist.mpd?a=1626417215%2CYWE3ZTM3ZmI1OTc3NTE2YzdkNzY1YmQ5M2MwOGZlODMxMmIzMjc1OTBjYzdhMTZhMDU3NzU4NDU4ZmYzOTBkZg%3D%3D&amp;v=1&amp;f=sd', 'x': 1127, 'y': 720, 'hlsUrl': 'https://v.redd.it/link/nacy0z/asset/enf2j74sbly61/HLSPlaylist.m3u8?a=1626417215%2CNjc5NTg4ZDBjMTkzOTY5NjRhM2I3NjA4MjM5ZmI3NGI2ZTFiMWQ4MDMyY2E1YjY5MGRjMGYzMWQ2YmFlYTFlMg%3D%3D&amp;v=1&amp;f=sd', 'id': 'enf2j74sbly61', 'isGif': False}}",,1804
201,,LanguageTechnology,Are [IAAI](https://aaai.org/Conferences/IAAI/iaai.php) INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE and AAAI Conference on Artificial Intelligence considered as tier I conferences in AI? Both of these seem to be quite popular but I am not sure if these could be called tier I. How can I find out if a particular conference is a tier I conference?,t2_9ptho1m,False,,0,False,Are AAAI and IAAI tier I conferences?,[],r/LanguageTechnology,False,6,,0,,False,t3_napn21,False,dark,0.6,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1620857578.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Are &lt;a href=""https://aaai.org/Conferences/IAAI/iaai.php""&gt;IAAI&lt;/a&gt; INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE and AAAI Conference on Artificial Intelligence considered as tier I conferences in AI? Both of these seem to be quite popular but I am not sure if these could be called tier I. How can I find out if a particular conference is a tier I conference?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,napn21,True,,freaky_eater,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/napn21/are_aaai_and_iaai_tier_i_conferences/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/napn21/are_aaai_and_iaai_tier_i_conferences/,30199,1620828778.0,0,,False,,,,,,363
202,,LanguageTechnology,,t2_hkv9s,False,,0,False,BERT-QE: Contextualized Query Expansion for Document Re-ranking,[],r/LanguageTechnology,False,6,,0,,False,t3_n9t3ii,False,dark,0.93,,public,21,0,{},,False,[],,False,False,,{},,False,21,,False,False,,False,,[],{},,False,,1620754336.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n9t3ii,True,,prakhar21,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n9t3ii/bertqe_contextualized_query_expansion_for/,all_ads,False,https://link.medium.com/LxXy8cjvagb,30199,1620725536.0,0,,False,https://link.medium.com/LxXy8cjvagb,,,,,0
203,,LanguageTechnology,"Hello! I'd like to know your opinions about these two programs. I am applying to both, but I'm not sure which one is better. I have a background in Applied linguistics and I would like to move towards Computational linguistics.
I have many argumentd for each program, in fact, so far I have a tie. But what I haven't been able to consider are arguments about the quality and prestige of the programs, so I turn to you. Please, I want to read your opinions!

Thank you in advance!",t2_bbal3uqg,False,,0,False,¿Master in Digital text Analysis (UAntwerp-Belgium) vs Master in Computational linguistics (UStuttgart-Germany)?,[],r/LanguageTechnology,False,6,,0,,False,t3_na5jav,False,dark,0.83,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1620790480.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! I&amp;#39;d like to know your opinions about these two programs. I am applying to both, but I&amp;#39;m not sure which one is better. I have a background in Applied linguistics and I would like to move towards Computational linguistics.
I have many argumentd for each program, in fact, so far I have a tie. But what I haven&amp;#39;t been able to consider are arguments about the quality and prestige of the programs, so I turn to you. Please, I want to read your opinions!&lt;/p&gt;

&lt;p&gt;Thank you in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,na5jav,True,,Ravest_vale,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/na5jav/master_in_digital_text_analysis_uantwerpbelgium/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/na5jav/master_in_digital_text_analysis_uantwerpbelgium/,30199,1620761680.0,0,,False,,,,,,479
204,,LanguageTechnology,,t2_21qaqh1p,False,,0,False,NLP Keywords to Sentences Text Generation with {keytotext},[],r/LanguageTechnology,False,6,,0,,False,t3_na71dg,False,dark,0.75,,public,2,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/I0iBzP-SxFY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'NLP Keywords to Sentences Text Generation with {keytotext} | Python NLP Library - Applied NLP', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/I0iBzP-SxFY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/I0iBzP-SxFY/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/I0iBzP-SxFY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/na71dg', 'height': 200}",,False,2,,False,False,,False,,[],{},,False,,1620794196.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,na71dg,True,,dulldata,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/na71dg/nlp_keywords_to_sentences_text_generation_with/,all_ads,False,https://youtu.be/I0iBzP-SxFY,30199,1620765396.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'NLP Keywords to Sentences Text Generation with {keytotext} | Python NLP Library - Applied NLP', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/I0iBzP-SxFY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/I0iBzP-SxFY/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,https://youtu.be/I0iBzP-SxFY,,,,,0
205,,LanguageTechnology,"“keytotext” introduces the idea of building a model that would translate keywords into sentences using amazingly powerful T5 model. 

For example- 
Input: India, Capital, New Delhi
Output: The capital of India is New Delhi.

Interesting? Then read this walkthrough, 
Blog: https://lnkd.in/dS9_AH7
GitHub: https://github.com/gagan3012/keytotext",t2_hkv9s,False,,0,False,Generating Sentences from Keywords using Transformers,[],r/LanguageTechnology,False,6,,0,,False,t3_n92sfi,False,dark,0.94,,public,27,1,{},,False,[],,False,False,,{},,False,27,,False,False,,False,,[],{},,True,,1620676961.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;“keytotext” introduces the idea of building a model that would translate keywords into sentences using amazingly powerful T5 model. &lt;/p&gt;

&lt;p&gt;For example- 
Input: India, Capital, New Delhi
Output: The capital of India is New Delhi.&lt;/p&gt;

&lt;p&gt;Interesting? Then read this walkthrough, 
Blog: &lt;a href=""https://lnkd.in/dS9_AH7""&gt;https://lnkd.in/dS9_AH7&lt;/a&gt;
GitHub: &lt;a href=""https://github.com/gagan3012/keytotext""&gt;https://github.com/gagan3012/keytotext&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n92sfi,True,,prakhar21,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n92sfi/generating_sentences_from_keywords_using/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n92sfi/generating_sentences_from_keywords_using/,30199,1620648161.0,0,,False,,,,,,343
206,,LanguageTechnology,"I have two text columns. 

The objective is to vectorize each column separately. And then pass them into a MLP, so that the model can also understand which column a word is coming from.

But I am confused as to how to actually implement this.

Any help or resources would be appreciated.",t2_3xirs0ww,False,,0,False,How to pass in two count vectorizers as separate features into MLP?,[],r/LanguageTechnology,False,6,,0,,False,t3_n9cnxk,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620700987.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have two text columns. &lt;/p&gt;

&lt;p&gt;The objective is to vectorize each column separately. And then pass them into a MLP, so that the model can also understand which column a word is coming from.&lt;/p&gt;

&lt;p&gt;But I am confused as to how to actually implement this.&lt;/p&gt;

&lt;p&gt;Any help or resources would be appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n9cnxk,True,,GetStuffTogether,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n9cnxk/how_to_pass_in_two_count_vectorizers_as_separate/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n9cnxk/how_to_pass_in_two_count_vectorizers_as_separate/,30199,1620672187.0,0,,False,,,,,,287
207,,LanguageTechnology,"So far I could only find this one

https://www.aclweb.org/anthology/K18-1020/",t2_1bqxd32j,False,,0,False,Anyone know of any papers that look at latent NER? Entities which are not exactly mentioned in the text.,[],r/LanguageTechnology,False,6,,0,,False,t3_n9asbs,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620696456.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So far I could only find this one&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.aclweb.org/anthology/K18-1020/""&gt;https://www.aclweb.org/anthology/K18-1020/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n9asbs,True,,AdditionalWay,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n9asbs/anyone_know_of_any_papers_that_look_at_latent_ner/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n9asbs/anyone_know_of_any_papers_that_look_at_latent_ner/,30199,1620667656.0,0,,False,,,,,,77
208,,LanguageTechnology,"I have been searching around online weekly for any news or interesting developments in the cryptosphere/LSP world.

Keywords like decentralization, verification, remote, payment, trustless, exchange, control ... to me sound like there is much room for overlap, but there seems to be very little out there besides conjectures of NMT going to the ""cloud"" then the cryptosphere, which doesn't mean anything... 

Crtain coins/platforms like to market themselves as being a translation-crypto coin/platform but it is merely replacing QC and payment with crypto-incentives (that are not very motivating for a career translator).

I would to hear any thoughts on this from fellow LSPs and interested parties who are into augmenting their capabilities with what technology has to offer.

I have cross-posted this across some relevant subreddits.
Thanks in advance!",t2_785fmg9x,False,,0,False,Any known applications/developments between decentralised ledger (crypto) systems and MT/CAT?,[],r/LanguageTechnology,False,6,,0,,False,t3_n90h0s,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620668223.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have been searching around online weekly for any news or interesting developments in the cryptosphere/LSP world.&lt;/p&gt;

&lt;p&gt;Keywords like decentralization, verification, remote, payment, trustless, exchange, control ... to me sound like there is much room for overlap, but there seems to be very little out there besides conjectures of NMT going to the &amp;quot;cloud&amp;quot; then the cryptosphere, which doesn&amp;#39;t mean anything... &lt;/p&gt;

&lt;p&gt;Crtain coins/platforms like to market themselves as being a translation-crypto coin/platform but it is merely replacing QC and payment with crypto-incentives (that are not very motivating for a career translator).&lt;/p&gt;

&lt;p&gt;I would to hear any thoughts on this from fellow LSPs and interested parties who are into augmenting their capabilities with what technology has to offer.&lt;/p&gt;

&lt;p&gt;I have cross-posted this across some relevant subreddits.
Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n90h0s,True,,neon_metaphors,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n90h0s/any_known_applicationsdevelopments_between/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n90h0s/any_known_applicationsdevelopments_between/,30199,1620639423.0,2,,False,,,,,,856
209,,LanguageTechnology,"Hello,

I need to do resume parsing with French resumes. I will use Prodigy ([prodi.gy](https://prodi.gy)) to annotate my dataset and make my model.

My question is : is it better to annotate with the full resume or annotate line by line  ,

for me, the position of the text in the full document is an information but maybe spacy don't care.

Thanks !",t2_3llvpcr6,False,,0,False,NER for Resume parsing,[],r/LanguageTechnology,False,6,,0,,False,t3_n8yidr,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1620660165.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I need to do resume parsing with French resumes. I will use Prodigy (&lt;a href=""https://prodi.gy""&gt;prodi.gy&lt;/a&gt;) to annotate my dataset and make my model.&lt;/p&gt;

&lt;p&gt;My question is : is it better to annotate with the full resume or annotate line by line  ,&lt;/p&gt;

&lt;p&gt;for me, the position of the text in the full document is an information but maybe spacy don&amp;#39;t care.&lt;/p&gt;

&lt;p&gt;Thanks !&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n8yidr,True,,Ekkolo,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n8yidr/ner_for_resume_parsing/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n8yidr/ner_for_resume_parsing/,30199,1620631365.0,0,,False,,,,,,351
210,,LanguageTechnology,"I've recently started working as a .NET web developer building services, but as a side project I'm working on a text summarization API.

I would like to serve [this model](https://huggingface.co/facebook/bart-large-cnn/tree/main) through the API. How should I go about it? I haven't found much after searching, is there any tutorials out there that I'm missing? Do I need to create the API in Python instead of C#?

I would really appreciate any help. Thank you very much and sorry about my English.",t2_6s740nk,False,,0,False,Best way of integrating a NLP model in Python with a .NET Core API?,[],r/LanguageTechnology,False,6,,0,,False,t3_n8nokg,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1620623434.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve recently started working as a .NET web developer building services, but as a side project I&amp;#39;m working on a text summarization API.&lt;/p&gt;

&lt;p&gt;I would like to serve &lt;a href=""https://huggingface.co/facebook/bart-large-cnn/tree/main""&gt;this model&lt;/a&gt; through the API. How should I go about it? I haven&amp;#39;t found much after searching, is there any tutorials out there that I&amp;#39;m missing? Do I need to create the API in Python instead of C#?&lt;/p&gt;

&lt;p&gt;I would really appreciate any help. Thank you very much and sorry about my English.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n8nokg,True,,Wufi,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n8nokg/best_way_of_integrating_a_nlp_model_in_python/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n8nokg/best_way_of_integrating_a_nlp_model_in_python/,30199,1620594634.0,0,,False,,,,,,499
211,,LanguageTechnology,,t2_5fsp2x6v,False,,0,False,Pooled Contextualised Embeddings for NER | Research Papers Summary 017,[],r/LanguageTechnology,False,6,,0,,False,t3_n8jz4r,False,dark,0.92,,public,9,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/HJtapl3zWC0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Pooled Contextualised Embeddings for NER | Research Papers Summary 017', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/HJtapl3zWC0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/HJtapl3zWC0/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/HJtapl3zWC0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/n8jz4r', 'height': 200}",,False,9,,False,False,,False,,[],{},,False,,1620612979.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n8jz4r,True,,RyanAI100,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n8jz4r/pooled_contextualised_embeddings_for_ner_research/,all_ads,False,https://youtu.be/HJtapl3zWC0,30199,1620584179.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Pooled Contextualised Embeddings for NER | Research Papers Summary 017', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/HJtapl3zWC0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/HJtapl3zWC0/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,https://youtu.be/HJtapl3zWC0,,,,,0
212,,LanguageTechnology,,t2_iv8ylbg,False,,0,False,Is there a python library or API that is able to check the grammar of a sentence?,[],r/LanguageTechnology,False,6,,0,,False,t3_n8qpvb,False,dark,0.63,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620632645.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n8qpvb,True,,rodgerdodger17,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n8qpvb/is_there_a_python_library_or_api_that_is_able_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n8qpvb/is_there_a_python_library_or_api_that_is_able_to/,30199,1620603845.0,0,,False,,,,,,0
213,,LanguageTechnology,,t2_hkv9s,False,,0,False,Text Generation using GPT-Neo,[],r/LanguageTechnology,False,6,,0,,False,t3_n8ao75,False,dark,0.77,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,False,,1620581073.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n8ao75,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n8ao75/text_generation_using_gptneo/,all_ads,False,https://link.medium.com/84WG7KXa7fb,30199,1620552273.0,0,,False,https://link.medium.com/84WG7KXa7fb,,,,,0
214,,LanguageTechnology,"I am trying to solve a multi-class emotion classification problem using BERT (NLP). It is my first ever BERT model and my model ended up overfitting.

    {""train"": {""eval_examples_count"": 10481, ""metrics"": {""f1_weighted"": 0.8869, ""f1_macro"": 0.8401, ""accuracy"": 0.8884, ""roc_auc"": 0.9686}, ""time_spent"": ""1:27:10""}} 
    {""valid"": {""eval_examples_count"": 3493, ""metrics"": {""f1_weighted"": 0.6112, ""f1_macro"": 0.5257, ""accuracy"": 0.6184, ""roc_auc"": 0.8177}, ""time_spent"": ""0:28:37""}} 
    {""test"": {""eval_examples_count"": 3494, ""metrics"": {""f1_weighted"": 0.6191, ""f1_macro"": 0.5282, ""accuracy"": 0.6259, ""roc_auc"": 0.8271}, ""time_spent"": ""0:28:26”}} 

Classes were pretty unbalanced, that's probably why it is overfitting i thought, and then I used the undersampling technique to come over this issue.

    Train:
    sad          756
    angry        756
    surprised    756
    smile        756
    kind         756
    
    Test:
    sad          235
    surprised    235
    angry        235
    kind         235
    smile        235
    
    Val:
    sad          235
    surprised    235
    angry        235
    kind         235
    smile        235

I know the dataset became too small, but it is just for training my model faster and quickly experiment with different approaches. In fact, using my entire dataset and this testing dataset both have been giving pretty similar results.

After balancing my dataset result looks like this:

    {""train"": {""eval_examples_count"": 3780, ""metrics"": {""f1_weighted"": 0.8719, ""f1_macro"": 0.8719, ""accuracy"": 0.8725, ""roc_auc"": 0.9744}, ""time_spent"": ""0:38:33""}} 
    {""valid"": {""eval_examples_count"": 1215, ""metrics"": {""f1_weighted"": 0.5165, ""f1_macro"": 0.5165, ""accuracy"": 0.5185, ""roc_auc"": 0.7993}, ""time_spent"": ""0:12:16""}} 
    {""test"": {""eval_examples_count"": 1215, ""metrics"": {""f1_weighted"": 0.5264, ""f1_macro"": 0.5264, ""accuracy"": 0.5276, ""roc_auc"": 0.8051}, ""time_spent"": ""0:12:28""}} 

So, dropping instances and balancing the dataset DIT NOT really solve the overfitting problem.

Then I decided to use the Drop Out technique and made it equal to 0.5

            ""keep_prob"": 0.5,
            ""learning_rate"": 1e-05,
            ""learning_rate_drop_patience"": 2,
            ""learning_rate_drop_div"": 2.0,
            ""load_before_drop"": True, 
            ""attention_probs_keep_prob"": 0.5,
            ""hidden_keep_prob"": 0.5,

that really solved the overfitting issue, however, badly affected on the performance of the training/model:

    {""train"": {""eval_examples_count"": 32, ""metrics"": {""f1_weighted"": 0.4099, ""f1_macro"": 0.3828, ""accuracy"": 0.4688, ""roc_auc"": 0.6969}, ""time_spent"": ""3:57:36"", ""epochs_done"": 3, ""batches_seen"": 357, ""train_examples_seen"": 11340, ""learning_rate"": 1e-05, ""loss"": 1.4732106072562081} 
    {""valid"": {""eval_examples_count"": 1215, ""metrics"": {""f1_weighted"": 0.3798, ""f1_macro"": 0.3798, ""accuracy"": 0.4272, ""roc_auc"": 0.7127}, ""time_spent"": ""4:06:33"", ""epochs_done"": 3, ""batches_seen"": 357, ""train_examples_seen"": 11340, ""impatience"": 0, ""patience_limit"": 2}} 

Now, I think I solved the overfitting problem, however, How can I improve the overall performance/accuracy? Maybe increasing the learning rate? I can really try all possible approaches but it is really taking too much time of mine, each training is roughly about 4-5 hours (On the small dataset) on the big one it goes to 15-17 hours. It is really hit or miss right now. I am really new in this field but i need to solve this problem. Any idea how to increase the performance would be appreciated. Thank you",t2_6c0lef9b,False,,0,False,How to increase the performance?,[],r/LanguageTechnology,False,6,,0,,False,t3_n85h06,False,dark,0.92,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1620559984.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to solve a multi-class emotion classification problem using BERT (NLP). It is my first ever BERT model and my model ended up overfitting.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;train&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 10481, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.8869, &amp;quot;f1_macro&amp;quot;: 0.8401, &amp;quot;accuracy&amp;quot;: 0.8884, &amp;quot;roc_auc&amp;quot;: 0.9686}, &amp;quot;time_spent&amp;quot;: &amp;quot;1:27:10&amp;quot;}} 
{&amp;quot;valid&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 3493, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.6112, &amp;quot;f1_macro&amp;quot;: 0.5257, &amp;quot;accuracy&amp;quot;: 0.6184, &amp;quot;roc_auc&amp;quot;: 0.8177}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:28:37&amp;quot;}} 
{&amp;quot;test&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 3494, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.6191, &amp;quot;f1_macro&amp;quot;: 0.5282, &amp;quot;accuracy&amp;quot;: 0.6259, &amp;quot;roc_auc&amp;quot;: 0.8271}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:28:26”}} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Classes were pretty unbalanced, that&amp;#39;s probably why it is overfitting i thought, and then I used the undersampling technique to come over this issue.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Train:
sad          756
angry        756
surprised    756
smile        756
kind         756

Test:
sad          235
surprised    235
angry        235
kind         235
smile        235

Val:
sad          235
surprised    235
angry        235
kind         235
smile        235
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I know the dataset became too small, but it is just for training my model faster and quickly experiment with different approaches. In fact, using my entire dataset and this testing dataset both have been giving pretty similar results.&lt;/p&gt;

&lt;p&gt;After balancing my dataset result looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;train&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 3780, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.8719, &amp;quot;f1_macro&amp;quot;: 0.8719, &amp;quot;accuracy&amp;quot;: 0.8725, &amp;quot;roc_auc&amp;quot;: 0.9744}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:38:33&amp;quot;}} 
{&amp;quot;valid&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 1215, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.5165, &amp;quot;f1_macro&amp;quot;: 0.5165, &amp;quot;accuracy&amp;quot;: 0.5185, &amp;quot;roc_auc&amp;quot;: 0.7993}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:12:16&amp;quot;}} 
{&amp;quot;test&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 1215, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.5264, &amp;quot;f1_macro&amp;quot;: 0.5264, &amp;quot;accuracy&amp;quot;: 0.5276, &amp;quot;roc_auc&amp;quot;: 0.8051}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:12:28&amp;quot;}} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, dropping instances and balancing the dataset DIT NOT really solve the overfitting problem.&lt;/p&gt;

&lt;p&gt;Then I decided to use the Drop Out technique and made it equal to 0.5&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        &amp;quot;keep_prob&amp;quot;: 0.5,
        &amp;quot;learning_rate&amp;quot;: 1e-05,
        &amp;quot;learning_rate_drop_patience&amp;quot;: 2,
        &amp;quot;learning_rate_drop_div&amp;quot;: 2.0,
        &amp;quot;load_before_drop&amp;quot;: True, 
        &amp;quot;attention_probs_keep_prob&amp;quot;: 0.5,
        &amp;quot;hidden_keep_prob&amp;quot;: 0.5,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;that really solved the overfitting issue, however, badly affected on the performance of the training/model:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;train&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 32, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.4099, &amp;quot;f1_macro&amp;quot;: 0.3828, &amp;quot;accuracy&amp;quot;: 0.4688, &amp;quot;roc_auc&amp;quot;: 0.6969}, &amp;quot;time_spent&amp;quot;: &amp;quot;3:57:36&amp;quot;, &amp;quot;epochs_done&amp;quot;: 3, &amp;quot;batches_seen&amp;quot;: 357, &amp;quot;train_examples_seen&amp;quot;: 11340, &amp;quot;learning_rate&amp;quot;: 1e-05, &amp;quot;loss&amp;quot;: 1.4732106072562081} 
{&amp;quot;valid&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 1215, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.3798, &amp;quot;f1_macro&amp;quot;: 0.3798, &amp;quot;accuracy&amp;quot;: 0.4272, &amp;quot;roc_auc&amp;quot;: 0.7127}, &amp;quot;time_spent&amp;quot;: &amp;quot;4:06:33&amp;quot;, &amp;quot;epochs_done&amp;quot;: 3, &amp;quot;batches_seen&amp;quot;: 357, &amp;quot;train_examples_seen&amp;quot;: 11340, &amp;quot;impatience&amp;quot;: 0, &amp;quot;patience_limit&amp;quot;: 2}} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, I think I solved the overfitting problem, however, How can I improve the overall performance/accuracy? Maybe increasing the learning rate? I can really try all possible approaches but it is really taking too much time of mine, each training is roughly about 4-5 hours (On the small dataset) on the big one it goes to 15-17 hours. It is really hit or miss right now. I am really new in this field but i need to solve this problem. Any idea how to increase the performance would be appreciated. Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n85h06,True,,strangeguy111,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n85h06/how_to_increase_the_performance/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n85h06/how_to_increase_the_performance/,30199,1620531184.0,0,,False,,,,,,3562
215,,LanguageTechnology,"After doing data cleaning and preprocesses for sentences like converting words to their lemma or lowercase, some sentences are equal to 0 and not greater than the minimal length of sentences of the model.

So I think the best way to deal with these sentences is to delete those sentences whose length is 0. 

But I am wondering if there is a more suitable way in this situation?

Thanks in advance.",t2_9juu3c3s,False,,0,False,How to deal with the processed sentences whose length are equal to 0 and not greater than the minimal length of sentences of model,[],r/LanguageTechnology,False,6,,0,,False,t3_n8bimb,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1620585021.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;After doing data cleaning and preprocesses for sentences like converting words to their lemma or lowercase, some sentences are equal to 0 and not greater than the minimal length of sentences of the model.&lt;/p&gt;

&lt;p&gt;So I think the best way to deal with these sentences is to delete those sentences whose length is 0. &lt;/p&gt;

&lt;p&gt;But I am wondering if there is a more suitable way in this situation?&lt;/p&gt;

&lt;p&gt;Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n8bimb,True,,FreAkReddit0303,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n8bimb/how_to_deal_with_the_processed_sentences_whose/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n8bimb/how_to_deal_with_the_processed_sentences_whose/,30199,1620556221.0,0,,False,,,,,,398
216,,LanguageTechnology,,t2_128h0c,False,,0,False,"[Q] Why is interpretability important in natural language processing? This is easier to answer for models that make high stakes decisions (e.g., surgical risk assessment; self-driving car slamming brakes; etc.,), but I would like to understand why we care about interpretability in NLP.",[],r/LanguageTechnology,False,6,,0,,False,t3_n7udhn,False,dark,0.83,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1620524536.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n7udhn,True,,synysterbates,,17,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n7udhn/q_why_is_interpretability_important_in_natural/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n7udhn/q_why_is_interpretability_important_in_natural/,30199,1620495736.0,0,,False,,,,,,0
217,,LanguageTechnology,,t2_hkv9s,False,,0,False,Explanability for Transformers with Transformers-Interpret — A Model Explainability Tool,[],r/LanguageTechnology,False,6,,0,,False,t3_n7o3jb,False,dark,0.93,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,False,,False,,[],{},,False,,1620505303.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n7o3jb,True,,prakhar21,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n7o3jb/explanability_for_transformers_with/,all_ads,False,https://link.medium.com/Xgi4JLHI5fb,30199,1620476503.0,0,,False,https://link.medium.com/Xgi4JLHI5fb,,,,,0
218,,LanguageTechnology,It seems like a low hanging fruit that the architecture that usually have the top results be trained by the pre-training regimen that usually have the top results.,t2_10efjmjx,False,,0,False,How come we haven't seen the Albert architecture trained by the Electra pretraining method?,[],r/LanguageTechnology,False,6,,0,,False,t3_n7lmg8,False,dark,0.85,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,1620505616.0,,[],{},,True,,1620495320.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;It seems like a low hanging fruit that the architecture that usually have the top results be trained by the pre-training regimen that usually have the top results.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n7lmg8,True,,BatmantoshReturns,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n7lmg8/how_come_we_havent_seen_the_albert_architecture/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n7lmg8/how_come_we_havent_seen_the_albert_architecture/,30199,1620466520.0,0,,False,,,,,,163
219,,LanguageTechnology,"Hi,

I'm currently a rising senior majoring in CS at a top 10 university in the US. I'm debating between graduating just with bachelor's or pursuing a master's in NLP/Human Language Technology (would only take me extra two semesters). I'm mainly interested in NLP, Text Mining and recommendation systems (haven't taken speech processing yet). My school is huge (top 3-5) in the NLP realm, and this is important not because of the rankings but because of the support and opportunities I'll have access to during my masters. If I graduate just with a bachelor's, then I'm considering an SWE role at big techs. If I end up doing a master's, then an applied ML position (MLE, NLP engineer, etc) at big techs.

I have taken some courses related to NLP/DL; I enjoyed them, but at the moment I'm not sure if I liked it enough to do a master's in it and potentially commit my career path to it. Job prospects and competitiveness of getting such positions at big techs would factor a lot in my decision.

I'm wondering how competitive it is to get an MLE/NLP engineer positions at big tech firms like FAANg, Linkedin, etc. What would the expectations/requirements be for MLE/NLP positions (ML- and NLP-related knowledge, research/internship experience, personal projects, publication, Leetcode, etc)? Also, what would an engineer at such positions work on on a regular day? In your opinion, what are the pros and cons of each role (MLE/NLP engineer vs. general SWE)? What would be the kinds of advantages an MLE/NLP engineer would have over general SWEs?

Thank you!",t2_1684oril,False,,0,False,MLE/NLP Engineer Positions at Big Tech,[],r/LanguageTechnology,False,6,,0,,False,t3_n7s7j8,False,dark,0.38,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1620518353.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m currently a rising senior majoring in CS at a top 10 university in the US. I&amp;#39;m debating between graduating just with bachelor&amp;#39;s or pursuing a master&amp;#39;s in NLP/Human Language Technology (would only take me extra two semesters). I&amp;#39;m mainly interested in NLP, Text Mining and recommendation systems (haven&amp;#39;t taken speech processing yet). My school is huge (top 3-5) in the NLP realm, and this is important not because of the rankings but because of the support and opportunities I&amp;#39;ll have access to during my masters. If I graduate just with a bachelor&amp;#39;s, then I&amp;#39;m considering an SWE role at big techs. If I end up doing a master&amp;#39;s, then an applied ML position (MLE, NLP engineer, etc) at big techs.&lt;/p&gt;

&lt;p&gt;I have taken some courses related to NLP/DL; I enjoyed them, but at the moment I&amp;#39;m not sure if I liked it enough to do a master&amp;#39;s in it and potentially commit my career path to it. Job prospects and competitiveness of getting such positions at big techs would factor a lot in my decision.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m wondering how competitive it is to get an MLE/NLP engineer positions at big tech firms like FAANg, Linkedin, etc. What would the expectations/requirements be for MLE/NLP positions (ML- and NLP-related knowledge, research/internship experience, personal projects, publication, Leetcode, etc)? Also, what would an engineer at such positions work on on a regular day? In your opinion, what are the pros and cons of each role (MLE/NLP engineer vs. general SWE)? What would be the kinds of advantages an MLE/NLP engineer would have over general SWEs?&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n7s7j8,True,,pjwoo3,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n7s7j8/mlenlp_engineer_positions_at_big_tech/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n7s7j8/mlenlp_engineer_positions_at_big_tech/,30199,1620489553.0,0,,False,,,,,,1557
220,,LanguageTechnology,"This tutorial covers how to implement 5 different question-answering models with Hugging Face, along with the theory behind each model and the different datasets used to pre-train them. We'll also look at the varying baselines for each of the models in terms of F1 and EM scores.  

Topics covered include:

* The Transformer Architecture
* Popular Datasets and Evaluation Metrics
* BERT (Bidirectional Encoder Representations from Transformers)
* ALBERT: A Lite BERT
* ELECTRA
* BART
* Issues with Long Document Question-Answering Using Standard Models
* LONGFORMER: the Long-Document Transformer

Tutorial link: [https://blog.paperspace.com/question-answering-models-a-comparison/](https://blog.paperspace.com/question-answering-models-a-comparison/)

Run the full code on a free GPU: [https://ml-showcase.paperspace.com/projects/question-answering-models](https://ml-showcase.paperspace.com/projects/question-answering-models)

Questions and comments encouraged!",t2_15en0l,False,,0,False,[Tutorial] Implementing different question-answering models with Hugging Face,[],r/LanguageTechnology,False,6,,0,,False,t3_n72vuk,False,dark,0.89,,public,15,2,{},,False,[],,False,False,,{},,False,15,,False,False,,False,,[],{},,True,,1620434571.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This tutorial covers how to implement 5 different question-answering models with Hugging Face, along with the theory behind each model and the different datasets used to pre-train them. We&amp;#39;ll also look at the varying baselines for each of the models in terms of F1 and EM scores.  &lt;/p&gt;

&lt;p&gt;Topics covered include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Transformer Architecture&lt;/li&gt;
&lt;li&gt;Popular Datasets and Evaluation Metrics&lt;/li&gt;
&lt;li&gt;BERT (Bidirectional Encoder Representations from Transformers)&lt;/li&gt;
&lt;li&gt;ALBERT: A Lite BERT&lt;/li&gt;
&lt;li&gt;ELECTRA&lt;/li&gt;
&lt;li&gt;BART&lt;/li&gt;
&lt;li&gt;Issues with Long Document Question-Answering Using Standard Models&lt;/li&gt;
&lt;li&gt;LONGFORMER: the Long-Document Transformer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tutorial link: &lt;a href=""https://blog.paperspace.com/question-answering-models-a-comparison/""&gt;https://blog.paperspace.com/question-answering-models-a-comparison/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Run the full code on a free GPU: &lt;a href=""https://ml-showcase.paperspace.com/projects/question-answering-models""&gt;https://ml-showcase.paperspace.com/projects/question-answering-models&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Questions and comments encouraged!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 1000, 'id': 'award_35c78e6e-507b-4f1d-b3d8-ed43840909a8', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 800, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_128.png', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'The treasure at the end of the rainbow. Gives the author 800 Coins to do with as they please.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': ""Pot o' Coins"", 'resized_static_icons': [{'url': 'https://external-preview.redd.it?width=16&amp;height=16&amp;auto=webp&amp;s=d88c9a453f8ac38850b7a8241cfe5804b7b4905d', 'width': 16, 'height': 16}, {'url': 'https://external-preview.redd.it?width=32&amp;height=32&amp;auto=webp&amp;s=96a25019eb75878bdec4f6c012540f3baffbb1b2', 'width': 32, 'height': 32}, {'url': 'https://external-preview.redd.it?width=48&amp;height=48&amp;auto=webp&amp;s=1a51d27d75afde3fbde8bba84f9338f511211461', 'width': 48, 'height': 48}, {'url': 'https://external-preview.redd.it?width=64&amp;height=64&amp;auto=webp&amp;s=96af5ec460b05669ed60224cb0619bb8884abe27', 'width': 64, 'height': 64}, {'url': 'https://external-preview.redd.it?width=128&amp;height=128&amp;auto=webp&amp;s=2d3e648ed2302e6258673051ca5291f57beb29d4', 'width': 128, 'height': 128}], 'icon_format': 'APNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n72vuk,True,,hellopaperspace,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n72vuk/tutorial_implementing_different_questionanswering/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n72vuk/tutorial_implementing_different_questionanswering/,30199,1620405771.0,0,,False,,,,,,965
221,,LanguageTechnology,,t2_45nhbqwu,False,,0,False,Computer-Aided Design as Language,[],r/LanguageTechnology,False,6,,0,,False,t3_n7ct8t,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,False,,1620461391.0,text,6,,,text,arxiv.org,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n7ct8t,True,,usrnme878,,0,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/n7ct8t/computeraided_design_as_language/,all_ads,False,https://arxiv.org/abs/2105.02769,30199,1620432591.0,0,,False,https://arxiv.org/abs/2105.02769,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': '', 'author_fullname': 't2_hwj8c', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[R] Computer-Aided Design as Language', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'three', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_n7bjmo', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.78, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 13, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Research', 'can_mod_post': False, 'score': 13, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1620457451.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'arxiv.org', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://arxiv.org/abs/2105.02769', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'bb90e510-4e82-11e6-8635-0ee522e2349b', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'n7bjmo', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'hardmaru', 'discussion_type': None, 'num_comments': 2, 'send_replies': False, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/n7bjmo/r_computeraided_design_as_language/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://arxiv.org/abs/2105.02769', 'subreddit_subscribers': 1930700, 'created_utc': 1620428651.0, 'num_crossposts': 3, 'media': None, 'is_video': False}]",t3_n7bjmo,,,0
222,,LanguageTechnology,"Some of you might already know the [NLPCloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=23caa648-af1d-11eb-8529-0242ac130003) project I recently launched. The idea was to help developers and data scientists deploy spaCy models to production in a minute. It came from the fact that, as a developer, I spent much more time than I wanted on this DevOps part in my NLP projects. I was also seeing quite a lot of ML projects failing because teams didn't have the skills to deploy their new models to production... 

I had a lot of user requests asking me to support Hugging Face transformer-based models too, in addition to spaCy models. So I'm happy to announce that, after weeks of hard work, **it is now possible to deploy your own transformer-based models** to [https://nlpcloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=23caa648-af1d-11eb-8529-0242ac130003) , whether they are running on PyTorch or TensorFlow!

It can be useful in 2 situations:

* You developed your own models from scratch but you are having a hard time using them in production (because it takes an API, because resource consumption is very high, because you need high-availability, because server costs are too high, because you don't have advanced DevOps skills, etc.)
* You already use one of the Hugging Face pre-trained models on [NLPCloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=23caa648-af1d-11eb-8529-0242ac130003). It's not working so bad but you want to fine-tune them in order to adapt them to your own needs.

You can choose to either have your models run on CPU or GPU, depending on your requirements, and you can upload as many models as you want. Each new model has its own API endpoint, so you can use some of them in production while others are for testing only. It also makes it easy to urgently rollback to a previous model if needed.

Internally, everything is based on FastAPI and tons of Docker containers (if you are curious about our infra please don't hesitate to ask, I will be glad to comment).

For more details here is the API documentation: [https://docs.nlpcloud.io/#upload-your-transformers-based-model](https://docs.nlpcloud.io/#upload-your-transformers-based-model)

I really hope you will like it and find it useful!

I would love to have your opinion on this new feature. Please don't hesitate to answer this post!",t2_4z4m2qcs,False,,0,False,NLPCloud.io for transformer-based models in production (PyTorch and TensorFlow),[],r/LanguageTechnology,False,6,,0,,False,t3_n6vki5,False,dark,0.89,,public,19,0,{},,False,[],,False,False,,{},,False,19,,False,False,,False,,[],{},,True,,1620412874.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Some of you might already know the &lt;a href=""https://nlpcloud.io/?utm_source=reddit&amp;amp;utm_campaign=23caa648-af1d-11eb-8529-0242ac130003""&gt;NLPCloud.io&lt;/a&gt; project I recently launched. The idea was to help developers and data scientists deploy spaCy models to production in a minute. It came from the fact that, as a developer, I spent much more time than I wanted on this DevOps part in my NLP projects. I was also seeing quite a lot of ML projects failing because teams didn&amp;#39;t have the skills to deploy their new models to production... &lt;/p&gt;

&lt;p&gt;I had a lot of user requests asking me to support Hugging Face transformer-based models too, in addition to spaCy models. So I&amp;#39;m happy to announce that, after weeks of hard work, &lt;strong&gt;it is now possible to deploy your own transformer-based models&lt;/strong&gt; to &lt;a href=""https://nlpcloud.io/?utm_source=reddit&amp;amp;utm_campaign=23caa648-af1d-11eb-8529-0242ac130003""&gt;https://nlpcloud.io&lt;/a&gt; , whether they are running on PyTorch or TensorFlow!&lt;/p&gt;

&lt;p&gt;It can be useful in 2 situations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You developed your own models from scratch but you are having a hard time using them in production (because it takes an API, because resource consumption is very high, because you need high-availability, because server costs are too high, because you don&amp;#39;t have advanced DevOps skills, etc.)&lt;/li&gt;
&lt;li&gt;You already use one of the Hugging Face pre-trained models on &lt;a href=""https://nlpcloud.io/?utm_source=reddit&amp;amp;utm_campaign=23caa648-af1d-11eb-8529-0242ac130003""&gt;NLPCloud.io&lt;/a&gt;. It&amp;#39;s not working so bad but you want to fine-tune them in order to adapt them to your own needs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can choose to either have your models run on CPU or GPU, depending on your requirements, and you can upload as many models as you want. Each new model has its own API endpoint, so you can use some of them in production while others are for testing only. It also makes it easy to urgently rollback to a previous model if needed.&lt;/p&gt;

&lt;p&gt;Internally, everything is based on FastAPI and tons of Docker containers (if you are curious about our infra please don&amp;#39;t hesitate to ask, I will be glad to comment).&lt;/p&gt;

&lt;p&gt;For more details here is the API documentation: &lt;a href=""https://docs.nlpcloud.io/#upload-your-transformers-based-model""&gt;https://docs.nlpcloud.io/#upload-your-transformers-based-model&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I really hope you will like it and find it useful!&lt;/p&gt;

&lt;p&gt;I would love to have your opinion on this new feature. Please don&amp;#39;t hesitate to answer this post!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n6vki5,True,,juliensalinas,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n6vki5/nlpcloudio_for_transformerbased_models_in/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n6vki5/nlpcloudio_for_transformerbased_models_in/,30199,1620384074.0,0,,False,,,,,,2375
223,,LanguageTechnology,"Hello everyone,

Quick question: I am working on a low resource language that even large multilingual models such as [mBERT](https://huggingface.co/bert-base-multilingual-cased) fail to represent properly. So, can I fine-tune these models on MLM just like they were originally trained and then fine-tune it again on a specific task? In other words:

1. Fine-tune mBERT on the masked language modeling task (using a domain-specific corpus)
2. Fine-tune the resulting model on a different task (say semantic analysis)
3. Test the model

Does this make sense? Is this equivalent to training a BERT model from scratch using the same multilingual corpus in mBERT, with my corpus added to it, or is it different? If so, how's it different?

Thank you for your time. I really appreciate any knowledge on the matter.",t2_429oy0lo,False,,0,False,Does a sequential fine-tuning process for BERT make sense?,[],r/LanguageTechnology,False,6,,0,,False,t3_n709v8,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1620427819.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;

&lt;p&gt;Quick question: I am working on a low resource language that even large multilingual models such as &lt;a href=""https://huggingface.co/bert-base-multilingual-cased""&gt;mBERT&lt;/a&gt; fail to represent properly. So, can I fine-tune these models on MLM just like they were originally trained and then fine-tune it again on a specific task? In other words:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Fine-tune mBERT on the masked language modeling task (using a domain-specific corpus)&lt;/li&gt;
&lt;li&gt;Fine-tune the resulting model on a different task (say semantic analysis)&lt;/li&gt;
&lt;li&gt;Test the model&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Does this make sense? Is this equivalent to training a BERT model from scratch using the same multilingual corpus in mBERT, with my corpus added to it, or is it different? If so, how&amp;#39;s it different?&lt;/p&gt;

&lt;p&gt;Thank you for your time. I really appreciate any knowledge on the matter.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n709v8,True,,le-zakkaz,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n709v8/does_a_sequential_finetuning_process_for_bert/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n709v8/does_a_sequential_finetuning_process_for_bert/,30199,1620399019.0,0,,False,,,,,,808
224,,LanguageTechnology,"Is it okay to use Matthews Correlation Coefficient (phi coefficient) to compare the predictions of two different models?

&amp;#x200B;

Is this code correct:?

&amp;#x200B;

from sklearn.metrics import matthews\_corrcoef  

&amp;#x200B;

model4 = MultinomialNB() 

model4.fit(X\_train, y\_train) 

y\_pred4 = model4.predict(X\_test)  

&amp;#x200B;

model5 = BernoulliNB() 

model5.fit(X\_train, y\_train) 

y\_pred5 = model5.predict(X\_test)  

&amp;#x200B;

matthews\_corrcoef(y\_pred4, y\_pred5)",t2_581q2gxy,False,,0,False,Comparing individual predictions of two models,[],r/LanguageTechnology,False,6,,0,,False,t3_n6zncc,False,dark,1.0,,public,2,1,{},,False,[],,False,False,,{},,False,2,,False,False,,1620397831.0,,[],{},,True,,1620426119.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is it okay to use Matthews Correlation Coefficient (phi coefficient) to compare the predictions of two different models?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Is this code correct:?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;from sklearn.metrics import matthews_corrcoef  &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;model4 = MultinomialNB() &lt;/p&gt;

&lt;p&gt;model4.fit(X_train, y_train) &lt;/p&gt;

&lt;p&gt;y_pred4 = model4.predict(X_test)  &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;model5 = BernoulliNB() &lt;/p&gt;

&lt;p&gt;model5.fit(X_train, y_train) &lt;/p&gt;

&lt;p&gt;y_pred5 = model5.predict(X_test)  &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;matthews_corrcoef(y_pred4, y_pred5)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n6zncc,True,,jgj0707,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n6zncc/comparing_individual_predictions_of_two_models/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n6zncc/comparing_individual_predictions_of_two_models/,30199,1620397319.0,0,,False,,,,,,498
225,,LanguageTechnology,"Hey guys, I need some help. I need to find an open-source API for doing aspect-oriented sentiment analysis within this week. My brother is doing his first software project right now and I'd like to help him with his research. I was wondering if there are any sentiment analysis APIs or tools that can scan for the sentiment value of a specific topic word. Or maybe a sentiment analysis tool or trick that can be combined with a TF-IDF algorithm in order to do the same job.

Thanks in advance.",t2_5xfvbvw9,False,,0,False,Are there any open-source aspect-oriented sentiment analysis APIs out there?,[],r/LanguageTechnology,False,6,,0,,False,t3_n6xx98,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620421200.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys, I need some help. I need to find an open-source API for doing aspect-oriented sentiment analysis within this week. My brother is doing his first software project right now and I&amp;#39;d like to help him with his research. I was wondering if there are any sentiment analysis APIs or tools that can scan for the sentiment value of a specific topic word. Or maybe a sentiment analysis tool or trick that can be combined with a TF-IDF algorithm in order to do the same job.&lt;/p&gt;

&lt;p&gt;Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n6xx98,True,,ProperWait1,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n6xx98/are_there_any_opensource_aspectoriented_sentiment/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n6xx98/are_there_any_opensource_aspectoriented_sentiment/,30199,1620392400.0,0,,False,,,,,,493
226,,LanguageTechnology,"Normally, a recursive neural network is trained by back-propagating through every level of a tree (from the top to the bottom).

Did anybody encounter an example where a recursive neural network is trained by back-propagating on each level separately, calculating loss and updating weights on each level of the tree?

I am trying to do that on a ""pyramid"" binary tree, where the whole pyramid has the same consistent structure. I am looking for other similar examples so I can see how to improve my implementation.",t2_5ymrm,False,,0,False,Recursive neural networks,[],r/LanguageTechnology,False,6,,0,,False,t3_n6wsn0,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620417498.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Normally, a recursive neural network is trained by back-propagating through every level of a tree (from the top to the bottom).&lt;/p&gt;

&lt;p&gt;Did anybody encounter an example where a recursive neural network is trained by back-propagating on each level separately, calculating loss and updating weights on each level of the tree?&lt;/p&gt;

&lt;p&gt;I am trying to do that on a &amp;quot;pyramid&amp;quot; binary tree, where the whole pyramid has the same consistent structure. I am looking for other similar examples so I can see how to improve my implementation.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n6wsn0,True,,nox94,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n6wsn0/recursive_neural_networks/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n6wsn0/recursive_neural_networks/,30199,1620388698.0,0,,False,,,,,,514
227,,LanguageTechnology,"Created a custom NER model, which takes a blob of text as input. 

I’m trying to get the f1-score, recall, etc by using the Scorer.score method. This method requires a list of Example objects. 

Does anyone have or know of any code examples of how to accomplish this task? I looked for hours on the internet today without any luck, could definitely just be a silly mistake on my part as I’m still relatively new to the field !",t2_44md8gjv,False,,0,False,SpaCy NER Scorer/Example Question,[],r/LanguageTechnology,False,6,,0,,False,t3_n6i00e,False,dark,0.92,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1620364745.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Created a custom NER model, which takes a blob of text as input. &lt;/p&gt;

&lt;p&gt;I’m trying to get the f1-score, recall, etc by using the Scorer.score method. This method requires a list of Example objects. &lt;/p&gt;

&lt;p&gt;Does anyone have or know of any code examples of how to accomplish this task? I looked for hours on the internet today without any luck, could definitely just be a silly mistake on my part as I’m still relatively new to the field !&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n6i00e,True,,washknoxnash2255,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n6i00e/spacy_ner_scorerexample_question/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n6i00e/spacy_ner_scorerexample_question/,30199,1620335945.0,0,,False,,,,,,426
228,,LanguageTechnology,"Hello everyone, I need some help, or can I say, a second opinion on what I'm doing.

In the last few months I've been using spacy and nltk. Currently I'm working on a project in which I have to link multiple entities across multiple sentences. The main goal is to develop a knowledge graph. Now I'm using  mainly the dependency parser and other vanilla nlp techniques to filter out the relations between entities.

Now I'm seeking a second opinion,  after searching a few papers I'm finding a lot of work using machine learning, but I'm a total noob in that area. I'm feeling that I'm using the wrong approach.

Can someone help me out in the sense of, what I'm doing is actually not that bad or should I follow other approach, but again, I'm a total noob in the ML world.",t2_w9khn,False,,0,False,Help on Relation Extraction,[],r/LanguageTechnology,False,6,,0,,False,t3_n6ljt0,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1620374690.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone, I need some help, or can I say, a second opinion on what I&amp;#39;m doing.&lt;/p&gt;

&lt;p&gt;In the last few months I&amp;#39;ve been using spacy and nltk. Currently I&amp;#39;m working on a project in which I have to link multiple entities across multiple sentences. The main goal is to develop a knowledge graph. Now I&amp;#39;m using  mainly the dependency parser and other vanilla nlp techniques to filter out the relations between entities.&lt;/p&gt;

&lt;p&gt;Now I&amp;#39;m seeking a second opinion,  after searching a few papers I&amp;#39;m finding a lot of work using machine learning, but I&amp;#39;m a total noob in that area. I&amp;#39;m feeling that I&amp;#39;m using the wrong approach.&lt;/p&gt;

&lt;p&gt;Can someone help me out in the sense of, what I&amp;#39;m doing is actually not that bad or should I follow other approach, but again, I&amp;#39;m a total noob in the ML world.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n6ljt0,True,,costeirao,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n6ljt0/help_on_relation_extraction/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n6ljt0/help_on_relation_extraction/,30199,1620345890.0,0,,False,,,,,,772
229,,LanguageTechnology,[https://deepset.ai/germanquad](https://deepset.ai/germanquad),t2_b7hwxbvs,False,,0,False,German Question Answering and Dense Passage Retrieval Datasets,[],r/LanguageTechnology,False,6,,0,,False,t3_n6a94i,False,dark,0.72,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1620344646.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://deepset.ai/germanquad""&gt;https://deepset.ai/germanquad&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n6a94i,True,,bcdeepset,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n6a94i/german_question_answering_and_dense_passage/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n6a94i/german_question_answering_and_dense_passage/,30199,1620315846.0,0,,False,,,,,,62
230,,LanguageTechnology,"Hi All.

I am doing a project on multi-class text classification and could do with some advice.

I have a dataset of reviews which are classified into 7 product categories.

Firstly, I create a term document matrix using TF-IDF (tfidfvectorizer from sklearn). This generates a matrix of n x m where n in the number of reviews in my dataset and m is the number of features.

Then after splitting term document matrix into 80:20 train:test, I pass it through the K-Nearest Neighbours (KNN) algorithm and achieve an accuracy of 53%.

&amp;#x200B;

In another experiment, I used the Google News Word2Vec pretrained embedding (300 dimensional) and averaged all the word vectors for each review. So, each review consists of x words and each of the words has a 300 dimensional vector. Each of the vectors are averaged to produce one 300 dimensional vector per review.

&amp;#x200B;

Then I pass this matrix through KNN. I get an accuracy of 72%.

&amp;#x200B;

As for other classifiers that I tested on the same dataset, all of them performed better on the TF-IDF method of vectorization. However, KNN performed better on word2vec.

Can anyone help me understand why there is a jump in accuracy for KNN in using the word2vec method as compared to when using the tfidf method?",t2_581q2gxy,False,,0,False,KNN performs better on Word2Vec pretrained embedding than on TF-IDF vector representation,[],r/LanguageTechnology,False,6,,0,,False,t3_n65ktg,False,dark,0.9,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1620331086.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All.&lt;/p&gt;

&lt;p&gt;I am doing a project on multi-class text classification and could do with some advice.&lt;/p&gt;

&lt;p&gt;I have a dataset of reviews which are classified into 7 product categories.&lt;/p&gt;

&lt;p&gt;Firstly, I create a term document matrix using TF-IDF (tfidfvectorizer from sklearn). This generates a matrix of n x m where n in the number of reviews in my dataset and m is the number of features.&lt;/p&gt;

&lt;p&gt;Then after splitting term document matrix into 80:20 train:test, I pass it through the K-Nearest Neighbours (KNN) algorithm and achieve an accuracy of 53%.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;In another experiment, I used the Google News Word2Vec pretrained embedding (300 dimensional) and averaged all the word vectors for each review. So, each review consists of x words and each of the words has a 300 dimensional vector. Each of the vectors are averaged to produce one 300 dimensional vector per review.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Then I pass this matrix through KNN. I get an accuracy of 72%.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;As for other classifiers that I tested on the same dataset, all of them performed better on the TF-IDF method of vectorization. However, KNN performed better on word2vec.&lt;/p&gt;

&lt;p&gt;Can anyone help me understand why there is a jump in accuracy for KNN in using the word2vec method as compared to when using the tfidf method?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n65ktg,True,,jgj0707,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n65ktg/knn_performs_better_on_word2vec_pretrained/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n65ktg/knn_performs_better_on_word2vec_pretrained/,30199,1620302286.0,0,,False,,,,,,1268
231,,LanguageTechnology,"Hi. 

A trend I noticed in many papers that employ Autoencoders in text is that they use RNNs for encoding/decoding.

This might have something to do with data insufficiencies in fields that call for text autoencoders. (Since transformers are known to require a lot of training data). I am not sure.

I’d appreciate any insight into this!

Edit:

Some examples:
- [A Probabilistic Formulation of Unsupervised Text Style Transfer](https://arxiv.org/pdf/2002.03912.pdf)
- [Plug and Play Autoencoders for Conditional Text Generation](https://www.aclweb.org/anthology/2020.emnlp-main.491.pdf)
- [DialogWAE](https://arxiv.org/pdf/1805.12352.pdf)",t2_mwy6r93,False,,0,False,"Is there a reason text autoencoders mostly employ RNNs for encoder and decoder, instead of Transformers?",[],r/LanguageTechnology,False,6,,0,,False,t3_n62d5p,False,dark,0.93,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,False,,1620321672.0,,[],{},,True,,1620317657.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi. &lt;/p&gt;

&lt;p&gt;A trend I noticed in many papers that employ Autoencoders in text is that they use RNNs for encoding/decoding.&lt;/p&gt;

&lt;p&gt;This might have something to do with data insufficiencies in fields that call for text autoencoders. (Since transformers are known to require a lot of training data). I am not sure.&lt;/p&gt;

&lt;p&gt;I’d appreciate any insight into this!&lt;/p&gt;

&lt;p&gt;Edit:&lt;/p&gt;

&lt;p&gt;Some examples:
- &lt;a href=""https://arxiv.org/pdf/2002.03912.pdf""&gt;A Probabilistic Formulation of Unsupervised Text Style Transfer&lt;/a&gt;
- &lt;a href=""https://www.aclweb.org/anthology/2020.emnlp-main.491.pdf""&gt;Plug and Play Autoencoders for Conditional Text Generation&lt;/a&gt;
- &lt;a href=""https://arxiv.org/pdf/1805.12352.pdf""&gt;DialogWAE&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n62d5p,True,,boodleboodle,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n62d5p/is_there_a_reason_text_autoencoders_mostly_employ/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n62d5p/is_there_a_reason_text_autoencoders_mostly_employ/,30199,1620288857.0,0,,False,,,,,,640
232,,LanguageTechnology,"Hi everyone,

I am a second year Master's student and want to write my thesis about an application of NLP model, however, I am struggling to find an original idea. It would be great if you can give me some ideas/advices, because I am really nervous at this point.",t2_47i6g389,False,,0,False,I need help to find a NLP topic for my thesis,[],r/LanguageTechnology,False,6,,0,,False,t3_n65oq8,False,dark,0.75,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1620331416.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;I am a second year Master&amp;#39;s student and want to write my thesis about an application of NLP model, however, I am struggling to find an original idea. It would be great if you can give me some ideas/advices, because I am really nervous at this point.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n65oq8,True,,kutayoncuyilmaz,,12,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n65oq8/i_need_help_to_find_a_nlp_topic_for_my_thesis/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n65oq8/i_need_help_to_find_a_nlp_topic_for_my_thesis/,30199,1620302616.0,0,,False,,,,,,263
233,,LanguageTechnology,"This may be a silly question that is an existing topic already heavily explored by others more involved in NLP than me, but are there tools for taking a paragraph of text and transforming it into a machine-generated paragraph written in another person's style, but having similar content?  Essentially, I am looking for the NLP equivalent of what has already been done to death with GANs on image data (e.g. StyleGAN2).  Thanks in advance for any help!",t2_13wzhg,False,,0,False,Something like style transfer for language processing?,[],r/LanguageTechnology,False,6,,0,,False,t3_n5y6an,False,dark,1.0,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1620300607.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This may be a silly question that is an existing topic already heavily explored by others more involved in NLP than me, but are there tools for taking a paragraph of text and transforming it into a machine-generated paragraph written in another person&amp;#39;s style, but having similar content?  Essentially, I am looking for the NLP equivalent of what has already been done to death with GANs on image data (e.g. StyleGAN2).  Thanks in advance for any help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n5y6an,True,,newbie_lurker,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n5y6an/something_like_style_transfer_for_language/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n5y6an/something_like_style_transfer_for_language/,30199,1620271807.0,0,,False,,,,,,452
234,,LanguageTechnology,"Hey everyone!

I'm a master's student currently doing my research in the field of NLP and I've recently become heavily invested in the idea of implementing concepts such as emotion and empathy in dialogue systems. I was wondering if there was anybody else who was also interested in this topic and wanted to share our resources together? I've made a simple [list of papers](https://github.com/Sahandfer/Empathetic-COAI-Papers) I've read so far (not complete yet). Feel free to let me know if you're interested or if you have any opinions or suggestions. Thanks and wish you have a great day.",t2_aoiqtg2t,False,,0,False,Emotion and Empathy in NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_n5dbnc,False,dark,0.99,,public,34,0,{},,False,[],,False,False,,{},,False,34,,False,False,,False,,[],{},,True,,1620241554.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m a master&amp;#39;s student currently doing my research in the field of NLP and I&amp;#39;ve recently become heavily invested in the idea of implementing concepts such as emotion and empathy in dialogue systems. I was wondering if there was anybody else who was also interested in this topic and wanted to share our resources together? I&amp;#39;ve made a simple &lt;a href=""https://github.com/Sahandfer/Empathetic-COAI-Papers""&gt;list of papers&lt;/a&gt; I&amp;#39;ve read so far (not complete yet). Feel free to let me know if you&amp;#39;re interested or if you have any opinions or suggestions. Thanks and wish you have a great day.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n5dbnc,True,,Sahand_sab,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n5dbnc/emotion_and_empathy_in_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n5dbnc/emotion_and_empathy_in_nlp/,30199,1620212754.0,0,,False,,,,,,591
235,,LanguageTechnology,"
## 200+ State of the Art Medical Models for NER, Entity Resolution, Relation Extraction, Assertion, Spark 3 and Python 3.8 support in  NLU 3.0 Release and much more
We are incredibly excited to announce the release of `NLU 3.0.0` which makes most of John Snow Labs medical healthcare model available in just 1 line of code in NLU.
These models are the most accurate in their domains and highly scalable in Spark clusters.  
In addition, `Spark 3.0.X`  and `Spark 3.1.X ` is now supported, together with Python3.8

This is enabled by the amazing [Spark NLP3.0.1](https://nlp.johnsnowlabs.com/docs/en/release_notes#300) and [Spark NLP for Healthcare 3.0.1](https://nlp.johnsnowlabs.com/docs/en/licensed_release_notes#301) releases.

# New Features
- Over 200 new models for the `healthcare` domain
- 6 new classes of models, Assertion, Sentence/Chunk Resolvers, Relation Extractors, Medical NER models, De-Identificator Models
- Spark 3.0.X and 3.1.X support
- Python 3.8 Support
- New Output level `relation`
- 1 Line to install NLU  just run `!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash`
- [Various new EMR and Databricks versions supported](https://github.com/JohnSnowLabs/spark-nlp/releases/tag/3.0.0)
- GPU Mode, more then 600% speedup by enabling GPU mode.
- Authorized mode for licensed features

## New Documentation
- [NLU for Healthcare Examples](https://nlu.johnsnowlabs.com/docs/en/examples_hc#usage-examples-of-nluload)
- [Instrunctions to authorize your environment to use Licensed features](https://nlu.johnsnowlabs.com/docs/en/examples_hc#authorize-access-to-licensed-features-and-install-healthcare-dependencies)


## New Notebooks
- [Medical Named Entity Extraction (NER) notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/medical_named_entity_recognition/overview_medical_entity_recognizers.ipynb)
- [Relation extraction notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/relation_extraction/overview_relation.ipynb)
- [Entity Resolution overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/entity_resolution/entity_resolvers_overview.ipynb)
- [Assertion overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/assertion/assertion_overview.ipynb)
- [De-Identification overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/de_identification/DeIdentification_model_overview.ipynb)
- [Graph NLU tutorial](https://github.com/JohnSnowLabs/nlu/blob/3.0rc1/examples/webinars_conferences_etc/graph_ai_summit/Healthcare_Graph_NLU_COVID_Tigergraph.ipynb)


## AssertionDLModels

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [assert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html) | [assertion_dl](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html)                   |
| English  | [assert.biobert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html) | [assertion_dl_biobert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html)                   |
| English  | [assert.healthcare](https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html) | [assertion_dl_healthcare](https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html)                   |
| English  | [assert.large](https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html) | [assertion_dl_large](https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html)                   |

##  New Word Embeddings

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [embed.glove.clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html) | [embeddings_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html)                   |
| English  | [embed.glove.biovec](https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html) | [embeddings_biovec](https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html)                   |
| English  | [embed.glove.healthcare](https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html) | [embeddings_healthcare](https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html)                   |
| English  | [embed.glove.healthcare_100d](https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html) | [embeddings_healthcare_100d](https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html)                   |
| English  | en.embed.glove.icdoem | embeddings_icdoem          |
| English  | en.embed.glove.icdoem_2ng | embeddings_icdoem_2ng          |

## Sentence Entity resolvers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | embed_sentence.biobert.mli | sbiobert_base_cased_mli          |
| English  | resolve | sbiobertresolve_cpt          |
| English  | resolve.cpt | sbiobertresolve_cpt          |
| English  | resolve.cpt.augmented | sbiobertresolve_cpt_augmented          |
| English  | resolve.cpt.procedures_augmented | sbiobertresolve_cpt_procedures_augmented          |
| English  | resolve.hcc.augmented | sbiobertresolve_hcc_augmented          |
| English  | [resolve.icd10cm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html) | [sbiobertresolve_icd10cm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html)                   |
| English  | [resolve.icd10cm.augmented](https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html) | [sbiobertresolve_icd10cm_augmented](https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html)                   |
| English  | [resolve.icd10cm.augmented_billable](https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html) | [sbiobertresolve_icd10cm_augmented_billable_hcc](https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html)                   |
| English  | [resolve.icd10pcs](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html) | [sbiobertresolve_icd10pcs](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html)                   |
| English  | [resolve.icdo](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html) | [sbiobertresolve_icdo](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html)                   |
| English  | [resolve.rxcui](https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html) | [sbiobertresolve_rxcui](https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html)                   |
| English  | [resolve.rxnorm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html) | [sbiobertresolve_rxnorm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html)                   |
| English  | [resolve.snomed](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html) | [sbiobertresolve_snomed_auxConcepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html)                   |
| English  | [resolve.snomed.aux_concepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html) | [sbiobertresolve_snomed_auxConcepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html)                   |
| English  | [resolve.snomed.aux_concepts_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html) | [sbiobertresolve_snomed_auxConcepts_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html)                   |
| English  | [resolve.snomed.findings](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html) | [sbiobertresolve_snomed_findings](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html)                   |
| English  | [resolve.snomed.findings_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html) | [sbiobertresolve_snomed_findings_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html)                   |

## RelationExtractionModel

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | relation.posology | posology_re          |
| English  | [relation](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html) | [redl_bodypart_direction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html)                   |
| English  | [relation.bodypart.direction](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html) | [redl_bodypart_direction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html)                   |
| English  | [relation.bodypart.problem](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html) | [redl_bodypart_problem_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html)                   |
| English  | [relation.bodypart.procedure](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html) | [redl_bodypart_procedure_test_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html)                   |
| English  | [relation.chemprot](https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html) | [redl_chemprot_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html)                   |
| English  | [relation.clinical](https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html) | [redl_clinical_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html)                   |
| English  | [relation.date](https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls) | [redl_date_clinical_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls)                   |
| English  | [relation.drug_drug_interaction](https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html) | [redl_drug_drug_interaction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html)                   |
| English  | [relation.humen_phenotype_gene](https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html) | [redl_human_phenotype_gene_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html)                   |
| English  | [relation.temporal_events](https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html) | [redl_temporal_events_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html)                   |



## NERDLModels

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
|English  | [med_ner.ade.clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html) | [ner_ade_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html)                   |
| English  | [med_ner.ade.clinical_bert](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html) | [ner_ade_clinicalbert](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html)                   |
| English  | [med_ner.ade.ade_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html) | [ner_ade_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html)                   |
| English  | [med_ner.anatomy](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html) | [ner_anatomy](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html)                   |
| English  | [med_ner.anatomy.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html) | [ner_anatomy_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html)                   |
| English  | [med_ner.anatomy.coarse](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html) | [ner_anatomy_coarse](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html)                   |
| English  | [med_ner.anatomy.coarse_biobert](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html) | [ner_anatomy_coarse_biobert](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html)                   |
| English  | [med_ner.aspect_sentiment](https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html) | [ner_aspect_based_sentiment](https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html)                   |
| English  | [med_ner.bacterial_species](https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html) | [ner_bacterial_species](https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html)                   |
| English  | [med_ner.bionlp](https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html) | [ner_bionlp](https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html)                   |
| English  | [med_ner.bionlp.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html) | [ner_bionlp_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html)                   |
| English  | [med_ner.cancer](https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html) | [ner_cancer_genetics](https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html)                   |
| Englishs | [med_ner.cellular](https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html) | [ner_cellular](https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html)                   |
| English  | [med_ner.cellular.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html) | [ner_cellular_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html)                   |
| English  | [med_ner.chemicals](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html) | [ner_chemicals](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html)                   |
| English  | [med_ner.chemprot](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html) | [ner_chemprot_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html)           |
| English  | [med_ner.chemprot.clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html) | [ner_chemprot_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html)           |
| English  | [med_ner.clinical](https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html) | [ner_clinical](https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html)           |
| English  | [med_ner.clinical.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html) | [ner_clinical_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html)           |
| English  | med_ner.clinical.noncontrib | ner_clinical_noncontrib          |
| English  | [med_ner.diseases](https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html) | [ner_diseases](https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html)           |
| English  | [med_ner.diseases.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html) | [ner_diseases_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html)           |
| English  | [med_ner.diseases.large](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html) | [ner_diseases_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html)           |
| English  | [med_ner.drugs](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html) | [ner_drugs](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html)           |
| English  | [med_ner.drugsgreedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html) | [ner_drugs_greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html)           |
| English  | [med_ner.drugs.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html) | [ner_drugs_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html)           |
| English  | [med_ner.events_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html) | [ner_events_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html)           |
| English  | [med_ner.events_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html) | [ner_events_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html)           |
| English  | [med_ner.events_healthcre](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html) | [ner_events_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html)           |
| English  | [med_ner.financial_contract](https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html) | [ner_financial_contract](https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html)           |
| English  | [med_ner.healthcare](https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html) | [ner_healthcare](https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html)           |
| English  | [med_ner.human_phenotype.gene_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html) | [ner_human_phenotype_gene_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html)           |
| English  | [med_ner.human_phenotype.gene_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html) | [ner_human_phenotype_gene_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html)           |
| English  | [med_ner.human_phenotype.go_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html) | [ner_human_phenotype_go_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html)           |
| English  | [med_ner.human_phenotype.go_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html) | [ner_human_phenotype_go_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html)           |
| English  | [med_ner.jsl](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html) | [ner_jsl](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html)           |
| English  | [med_ner.jsl.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html) | [ner_jsl_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html)           |
| English  | [med_ner.jsl.enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html) | [ner_jsl_enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html)           |
| English  | [med_ner.jsl.enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html) | [ner_jsl_enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html)           |
| English  | [med_ner.measurements](https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html) | [ner_measurements_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html)           |
| English  | [med_ner.medmentions](https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html) | [ner_medmentions_coarse](https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html)           |
| English  | [med_ner.posology](https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html) | [ner_posology](https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html)           |
| English  | [med_ner.posology.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html) | [ner_posology_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html)           |
| English  | [med_ner.posology.greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html) | [ner_posology_greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html)           |
| English  | [med_ner.posology.healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html) | [ner_posology_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html)           |
| English  | [med_ner.posology.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html) | [ner_posology_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html)           |
| English  | [med_ner.posology.large_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html) | [ner_posology_large_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html)           |
| English  | [med_ner.posology.small](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html) | [ner_posology_small](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html)           |
| English  | [med_ner.radiology](https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html) | [ner_radiology](https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html)           |
| English  | [med_ner.radiology.wip_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html) | [ner_radiology_wip_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html)           |
| English  | [med_ner.risk_factors](https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html) | [ner_risk_factors](https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html)           |
| English  | [med_ner.risk_factors.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html) | [ner_risk_factors_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html)           |
| English  | med_ner.i2b2 | nerdl_i2b2          |
| English  | [med_ner.tumour](https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html) | [nerdl_tumour_demo](https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html)           |
| English  | med_ner.jsl.wip.clinical | jsl_ner_wip_clinical          |
| English  | [med_ner.jsl.wip.clinical.greedy](https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html) | [jsl_ner_wip_greedy_clinical](https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html)           |
| English  | [med_ner.jsl.wip.clinical.modifier](https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html) | [jsl_ner_wip_modifier_clinical](https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html)           |
| English  | [med_ner.jsl.wip.clinical.rd](https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html) | [jsl_rd_ner_wip_greedy_clinical](https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html)           |


## De-Identification Models

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [med_ner.deid.augmented](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html) | [ner_deid_augmented](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html)           |
| English  | [med_ner.deid.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html) | [ner_deid_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html)           |
| English  | [med_ner.deid.enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html) | [ner_deid_enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html)           |
| English  | [med_ner.deid.enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html) | [ner_deid_enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html)           |
| English  | [med_ner.deid.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html) | [ner_deid_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html)           |
| English  | [med_ner.deid.sd](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html) | [ner_deid_sd](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html)           |
| English  | [med_ner.deid.sd_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html) | [ner_deid_sd_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html)           |
| English  | med_ner.deid | nerdl_deid          |
| English  | med_ner.deid.synthetic | ner_deid_synthetic          |
| English  | [med_ner.deid.dl](https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html) | [ner_deidentify_dl](https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html)           |
| English  | [en.de_identify](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html) | [deidentify_rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html)           |
| English  | de_identify.rules | deid_rules          |
| English  | [de_identify.clinical](https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html) | [deidentify_enriched_clinical](https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html)           |
| English  | [de_identify.large](https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html) | [deidentify_large](https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html)           |
| English  | [de_identify.rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html) | [deidentify_rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html)           |
| English  | de_identify.rb_no_regex | deidentify_rb_no_regex          |



# Chunk resolvers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [resolve_chunk.athena_conditions](https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html) | [chunkresolve_athena_conditions_healthcare](https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html)           |
| English  | [resolve_chunk.cpt_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html) | [chunkresolve_cpt_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html) | [chunkresolve_icd10cm_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.diseases_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html) | [chunkresolve_icd10cm_diseases_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html)           |
| English  | resolve_chunk.icd10cm.hcc_clinical | chunkresolve_icd10cm_hcc_clinical          |
| English  | resolve_chunk.icd10cm.hcc_healthcare | chunkresolve_icd10cm_hcc_healthcare          |
| English  | [resolve_chunk.icd10cm.injuries](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html) | [chunkresolve_icd10cm_injuries_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.musculoskeletal](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html) | [chunkresolve_icd10cm_musculoskeletal_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.neoplasms](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html) | [chunkresolve_icd10cm_neoplasms_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.poison](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html) | [chunkresolve_icd10cm_poison_ext_clinical](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.puerile](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html) | [chunkresolve_icd10cm_puerile_clinical](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html)           |
| English  | resolve_chunk.icd10pcs.clinical | chunkresolve_icd10pcs_clinical          |
| English  | [resolve_chunk.icdo.clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html) | [chunkresolve_icdo_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html)           |
| English  | [resolve_chunk.loinc](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html) | [chunkresolve_loinc_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html)           |
| English  | [resolve_chunk.rxnorm.cd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html) | [chunkresolve_rxnorm_cd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html)           |
| English  | resolve_chunk.rxnorm.in | chunkresolve_rxnorm_in_clinical          |
| English  | resolve_chunk.rxnorm.in_healthcare | chunkresolve_rxnorm_in_healthcare          |
| English  | [resolve_chunk.rxnorm.sbd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html) | [chunkresolve_rxnorm_sbd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html)           |
| English  | [resolve_chunk.rxnorm.scd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html) | [chunkresolve_rxnorm_scd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html)           |
| English  | resolve_chunk.rxnorm.scdc | chunkresolve_rxnorm_scdc_clinical          |
| English  | resolve_chunk.rxnorm.scdc_healthcare | chunkresolve_rxnorm_scdc_healthcare          |
| English  | [resolve_chunk.rxnorm.xsmall.clinical](https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html) | [chunkresolve_rxnorm_xsmall_clinical](https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html)           |
| English  | [resolve_chunk.snomed.findings](https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html) | [chunkresolve_snomed_findings_clinical](https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html)           |


# New Classifiers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | classify.icd10.clinical | classifier_icd10cm_hcc_clinical          |
| English  | classify.icd10.healthcare | classifier_icd10cm_hcc_healthcare          |
| English  | [classify.ade.biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html) | [classifierdl_ade_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html)           |
| English  | [classify.ade.clinical](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html) | [classifierdl_ade_clinicalbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html)           |
| English  | [classify.ade.conversational](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html) | [classifierdl_ade_conversational_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html)           |
| English  | [classify.gender.biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html) | [classifierdl_gender_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html)           |
| English  | [classify.gender.sbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html) | [classifierdl_gender_sbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html)           |
| English  | classify.pico | classifierdl_pico_biobert          |


# German Medical models

| nlu.load() reference                                         | Spark NLP Model reference          |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [embed]    | w2v_cc_300d|
| [embed.w2v]    | w2v_cc_300d|
| [resolve_chunk]    | chunkresolve_ICD10GM|
| [resolve_chunk.icd10gm]    | chunkresolve_ICD10GM|
| resolve_chunk.icd10gm.2021    | chunkresolve_ICD10GM_2021|
| med_ner.legal   | ner_legal|
| med_ner    | ner_healthcare|
| med_ner.healthcare    | ner_healthcare|
| med_ner.healthcare_slim    | ner_healthcare_slim|
| med_ner.traffic    | ner_traffic|

# Spanish Medical models
| nlu.load() reference                                         | Spark NLP Model reference          |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [embed.scielo.150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html) | [embeddings_scielo_150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html)| 
| [embed.scielo.300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html)   | [embeddings_scielo_300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html)| 
| [embed.scielo.50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html)  | [embeddings_scielo_50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html)| 
| [embed.scielowiki.150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html)   | [embeddings_scielowiki_150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html)| 
| [embed.scielowiki.300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html)   | [embeddings_scielowiki_300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html)| 
| [embed.scielowiki.50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html)   | [embeddings_scielowiki_50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html)| 
| [embed.sciwiki.150d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html)   | [embeddings_sciwiki_150d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html)| 
| [embed.sciwiki.300d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html)   | [embeddings_sciwiki_300d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html)| 
| [embed.sciwiki.50d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html)   | [embeddings_sciwiki_50d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html)| 
| [med_ner](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)   |  [ner_diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)| 
| [med_ner.neoplasm](https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html)  | [ner_neoplasms](https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html)| 
| [med_ner.diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)  | [ner_diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)| 

# GPU Mode
You can now enable NLU GPU mode by setting `gpu=true` while loading a model. I.e. `nlu.load('train.sentiment' gpu=True)` . If must resart you kernel, if you already loaded a nlu pipeline withouth GPU mode.

# Output Level Relation
This new output level is used for relation extractors and will give you 1 row per relation extracted.


# Bug fixes
- Fixed a bug that caused loading NLU models in offline mode not to work in some occasions


# 1 line Install NLU
```!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash```

# Install via PIP 
```! pip install nlu pyspark==3.0.1```


## Additional NLU ressources

- [NLU Website](https://nlu.johnsnowlabs.com/)
- [All NLU Tutorial Notebooks](https://nlu.johnsnowlabs.com/docs/en/notebooks)
- [NLU Videos and Blogposts on NLU](https://nlp.johnsnowlabs.com/learn#pythons-nlu-library)
- [NLU on Github](https://github.com/JohnSnowLabs/nlu)
- [Suggestions or Questions? Contact us in Slack!](https://join.slack.com/t/spark-nlp/shared_invite/zt-lutct9gm-kuUazcyFKhuGY3_0AMkxqA)",t2_53n73cus,False,,0,False,"200+ State of the Art Medical Models for NER, Entity Resolution, Relation Extraction, Assertion, Spark 3 and Python 3.8 support - John Snow Labs NLU 3.0.0",[],r/LanguageTechnology,False,6,,0,,False,t3_n5gvel,False,dark,0.88,,public,13,0,{},,False,[],,False,False,,{},,False,13,,False,False,,False,,[],{},,True,,1620252798.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;h2&gt;200+ State of the Art Medical Models for NER, Entity Resolution, Relation Extraction, Assertion, Spark 3 and Python 3.8 support in  NLU 3.0 Release and much more&lt;/h2&gt;

&lt;p&gt;We are incredibly excited to announce the release of &lt;code&gt;NLU 3.0.0&lt;/code&gt; which makes most of John Snow Labs medical healthcare model available in just 1 line of code in NLU.
These models are the most accurate in their domains and highly scalable in Spark clusters.&lt;br/&gt;
In addition, &lt;code&gt;Spark 3.0.X&lt;/code&gt;  and &lt;code&gt;Spark 3.1.X&lt;/code&gt; is now supported, together with Python3.8&lt;/p&gt;

&lt;p&gt;This is enabled by the amazing &lt;a href=""https://nlp.johnsnowlabs.com/docs/en/release_notes#300""&gt;Spark NLP3.0.1&lt;/a&gt; and &lt;a href=""https://nlp.johnsnowlabs.com/docs/en/licensed_release_notes#301""&gt;Spark NLP for Healthcare 3.0.1&lt;/a&gt; releases.&lt;/p&gt;

&lt;h1&gt;New Features&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Over 200 new models for the &lt;code&gt;healthcare&lt;/code&gt; domain&lt;/li&gt;
&lt;li&gt;6 new classes of models, Assertion, Sentence/Chunk Resolvers, Relation Extractors, Medical NER models, De-Identificator Models&lt;/li&gt;
&lt;li&gt;Spark 3.0.X and 3.1.X support&lt;/li&gt;
&lt;li&gt;Python 3.8 Support&lt;/li&gt;
&lt;li&gt;New Output level &lt;code&gt;relation&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;1 Line to install NLU  just run &lt;code&gt;!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/spark-nlp/releases/tag/3.0.0""&gt;Various new EMR and Databricks versions supported&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GPU Mode, more then 600% speedup by enabling GPU mode.&lt;/li&gt;
&lt;li&gt;Authorized mode for licensed features&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;New Documentation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/docs/en/examples_hc#usage-examples-of-nluload""&gt;NLU for Healthcare Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/docs/en/examples_hc#authorize-access-to-licensed-features-and-install-healthcare-dependencies""&gt;Instrunctions to authorize your environment to use Licensed features&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;New Notebooks&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/medical_named_entity_recognition/overview_medical_entity_recognizers.ipynb""&gt;Medical Named Entity Extraction (NER) notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/relation_extraction/overview_relation.ipynb""&gt;Relation extraction notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/entity_resolution/entity_resolvers_overview.ipynb""&gt;Entity Resolution overview notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/assertion/assertion_overview.ipynb""&gt;Assertion overview notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/de_identification/DeIdentification_model_overview.ipynb""&gt;De-Identification overview notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/3.0rc1/examples/webinars_conferences_etc/graph_ai_summit/Healthcare_Graph_NLU_COVID_Tigergraph.ipynb""&gt;Graph NLU tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;AssertionDLModels&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html""&gt;assert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html""&gt;assertion_dl&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html""&gt;assert.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html""&gt;assertion_dl_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html""&gt;assert.healthcare&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html""&gt;assertion_dl_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html""&gt;assert.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html""&gt;assertion_dl_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;New Word Embeddings&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html""&gt;embed.glove.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html""&gt;embeddings_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html""&gt;embed.glove.biovec&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html""&gt;embeddings_biovec&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html""&gt;embed.glove.healthcare&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html""&gt;embeddings_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html""&gt;embed.glove.healthcare_100d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html""&gt;embeddings_healthcare_100d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;en.embed.glove.icdoem&lt;/td&gt;
&lt;td&gt;embeddings_icdoem&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;en.embed.glove.icdoem_2ng&lt;/td&gt;
&lt;td&gt;embeddings_icdoem_2ng&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;Sentence Entity resolvers&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;embed_sentence.biobert.mli&lt;/td&gt;
&lt;td&gt;sbiobert_base_cased_mli&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve&lt;/td&gt;
&lt;td&gt;sbiobertresolve_cpt&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve.cpt&lt;/td&gt;
&lt;td&gt;sbiobertresolve_cpt&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve.cpt.augmented&lt;/td&gt;
&lt;td&gt;sbiobertresolve_cpt_augmented&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve.cpt.procedures_augmented&lt;/td&gt;
&lt;td&gt;sbiobertresolve_cpt_procedures_augmented&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve.hcc.augmented&lt;/td&gt;
&lt;td&gt;sbiobertresolve_hcc_augmented&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html""&gt;resolve.icd10cm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html""&gt;sbiobertresolve_icd10cm&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html""&gt;resolve.icd10cm.augmented&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html""&gt;sbiobertresolve_icd10cm_augmented&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html""&gt;resolve.icd10cm.augmented_billable&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html""&gt;sbiobertresolve_icd10cm_augmented_billable_hcc&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html""&gt;resolve.icd10pcs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html""&gt;sbiobertresolve_icd10pcs&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html""&gt;resolve.icdo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html""&gt;sbiobertresolve_icdo&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html""&gt;resolve.rxcui&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html""&gt;sbiobertresolve_rxcui&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html""&gt;resolve.rxnorm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html""&gt;sbiobertresolve_rxnorm&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html""&gt;resolve.snomed&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html""&gt;sbiobertresolve_snomed_auxConcepts&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html""&gt;resolve.snomed.aux_concepts&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html""&gt;sbiobertresolve_snomed_auxConcepts&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html""&gt;resolve.snomed.aux_concepts_int&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html""&gt;sbiobertresolve_snomed_auxConcepts_int&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html""&gt;resolve.snomed.findings&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html""&gt;sbiobertresolve_snomed_findings&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html""&gt;resolve.snomed.findings_int&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html""&gt;sbiobertresolve_snomed_findings_int&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;RelationExtractionModel&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;relation.posology&lt;/td&gt;
&lt;td&gt;posology_re&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html""&gt;relation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html""&gt;redl_bodypart_direction_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html""&gt;relation.bodypart.direction&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html""&gt;redl_bodypart_direction_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html""&gt;relation.bodypart.problem&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html""&gt;redl_bodypart_problem_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html""&gt;relation.bodypart.procedure&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html""&gt;redl_bodypart_procedure_test_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html""&gt;relation.chemprot&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html""&gt;redl_chemprot_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html""&gt;relation.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html""&gt;redl_clinical_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls""&gt;relation.date&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls""&gt;redl_date_clinical_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html""&gt;relation.drug_drug_interaction&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html""&gt;redl_drug_drug_interaction_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html""&gt;relation.humen_phenotype_gene&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html""&gt;redl_human_phenotype_gene_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html""&gt;relation.temporal_events&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html""&gt;redl_temporal_events_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;NERDLModels&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html""&gt;med_ner.ade.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html""&gt;ner_ade_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html""&gt;med_ner.ade.clinical_bert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html""&gt;ner_ade_clinicalbert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html""&gt;med_ner.ade.ade_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html""&gt;ner_ade_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html""&gt;med_ner.anatomy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html""&gt;ner_anatomy&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html""&gt;med_ner.anatomy.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html""&gt;ner_anatomy_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html""&gt;med_ner.anatomy.coarse&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html""&gt;ner_anatomy_coarse&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html""&gt;med_ner.anatomy.coarse_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html""&gt;ner_anatomy_coarse_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html""&gt;med_ner.aspect_sentiment&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html""&gt;ner_aspect_based_sentiment&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html""&gt;med_ner.bacterial_species&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html""&gt;ner_bacterial_species&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html""&gt;med_ner.bionlp&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html""&gt;ner_bionlp&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html""&gt;med_ner.bionlp.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html""&gt;ner_bionlp_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html""&gt;med_ner.cancer&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html""&gt;ner_cancer_genetics&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Englishs&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html""&gt;med_ner.cellular&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html""&gt;ner_cellular&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html""&gt;med_ner.cellular.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html""&gt;ner_cellular_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html""&gt;med_ner.chemicals&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html""&gt;ner_chemicals&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html""&gt;med_ner.chemprot&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html""&gt;ner_chemprot_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html""&gt;med_ner.chemprot.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html""&gt;ner_chemprot_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html""&gt;med_ner.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html""&gt;ner_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html""&gt;med_ner.clinical.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html""&gt;ner_clinical_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;med_ner.clinical.noncontrib&lt;/td&gt;
&lt;td&gt;ner_clinical_noncontrib&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html""&gt;med_ner.diseases&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html""&gt;ner_diseases&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html""&gt;med_ner.diseases.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html""&gt;ner_diseases_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html""&gt;med_ner.diseases.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html""&gt;ner_diseases_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html""&gt;med_ner.drugs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html""&gt;ner_drugs&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html""&gt;med_ner.drugsgreedy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html""&gt;ner_drugs_greedy&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html""&gt;med_ner.drugs.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html""&gt;ner_drugs_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html""&gt;med_ner.events_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html""&gt;ner_events_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html""&gt;med_ner.events_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html""&gt;ner_events_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html""&gt;med_ner.events_healthcre&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html""&gt;ner_events_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html""&gt;med_ner.financial_contract&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html""&gt;ner_financial_contract&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html""&gt;med_ner.healthcare&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html""&gt;ner_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html""&gt;med_ner.human_phenotype.gene_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html""&gt;ner_human_phenotype_gene_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html""&gt;med_ner.human_phenotype.gene_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html""&gt;ner_human_phenotype_gene_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html""&gt;med_ner.human_phenotype.go_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html""&gt;ner_human_phenotype_go_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html""&gt;med_ner.human_phenotype.go_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html""&gt;ner_human_phenotype_go_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html""&gt;med_ner.jsl&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html""&gt;ner_jsl&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html""&gt;med_ner.jsl.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html""&gt;ner_jsl_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html""&gt;med_ner.jsl.enriched&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html""&gt;ner_jsl_enriched&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html""&gt;med_ner.jsl.enriched_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html""&gt;ner_jsl_enriched_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html""&gt;med_ner.measurements&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html""&gt;ner_measurements_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html""&gt;med_ner.medmentions&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html""&gt;ner_medmentions_coarse&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html""&gt;med_ner.posology&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html""&gt;ner_posology&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html""&gt;med_ner.posology.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html""&gt;ner_posology_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html""&gt;med_ner.posology.greedy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html""&gt;ner_posology_greedy&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html""&gt;med_ner.posology.healthcare&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html""&gt;ner_posology_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html""&gt;med_ner.posology.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html""&gt;ner_posology_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html""&gt;med_ner.posology.large_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html""&gt;ner_posology_large_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html""&gt;med_ner.posology.small&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html""&gt;ner_posology_small&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html""&gt;med_ner.radiology&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html""&gt;ner_radiology&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html""&gt;med_ner.radiology.wip_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html""&gt;ner_radiology_wip_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html""&gt;med_ner.risk_factors&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html""&gt;ner_risk_factors&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html""&gt;med_ner.risk_factors.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html""&gt;ner_risk_factors_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;med_ner.i2b2&lt;/td&gt;
&lt;td&gt;nerdl_i2b2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html""&gt;med_ner.tumour&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html""&gt;nerdl_tumour_demo&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;med_ner.jsl.wip.clinical&lt;/td&gt;
&lt;td&gt;jsl_ner_wip_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html""&gt;med_ner.jsl.wip.clinical.greedy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html""&gt;jsl_ner_wip_greedy_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html""&gt;med_ner.jsl.wip.clinical.modifier&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html""&gt;jsl_ner_wip_modifier_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html""&gt;med_ner.jsl.wip.clinical.rd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html""&gt;jsl_rd_ner_wip_greedy_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;De-Identification Models&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html""&gt;med_ner.deid.augmented&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html""&gt;ner_deid_augmented&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html""&gt;med_ner.deid.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html""&gt;ner_deid_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html""&gt;med_ner.deid.enriched&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html""&gt;ner_deid_enriched&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html""&gt;med_ner.deid.enriched_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html""&gt;ner_deid_enriched_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html""&gt;med_ner.deid.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html""&gt;ner_deid_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html""&gt;med_ner.deid.sd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html""&gt;ner_deid_sd&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html""&gt;med_ner.deid.sd_large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html""&gt;ner_deid_sd_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;med_ner.deid&lt;/td&gt;
&lt;td&gt;nerdl_deid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;med_ner.deid.synthetic&lt;/td&gt;
&lt;td&gt;ner_deid_synthetic&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html""&gt;med_ner.deid.dl&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html""&gt;ner_deidentify_dl&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html""&gt;en.de_identify&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html""&gt;deidentify_rb&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;de_identify.rules&lt;/td&gt;
&lt;td&gt;deid_rules&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html""&gt;de_identify.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html""&gt;deidentify_enriched_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html""&gt;de_identify.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html""&gt;deidentify_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html""&gt;de_identify.rb&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html""&gt;deidentify_rb&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;de_identify.rb_no_regex&lt;/td&gt;
&lt;td&gt;deidentify_rb_no_regex&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;Chunk resolvers&lt;/h1&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html""&gt;resolve_chunk.athena_conditions&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html""&gt;chunkresolve_athena_conditions_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html""&gt;resolve_chunk.cpt_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html""&gt;chunkresolve_cpt_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html""&gt;resolve_chunk.icd10cm.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html""&gt;chunkresolve_icd10cm_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html""&gt;resolve_chunk.icd10cm.diseases_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html""&gt;chunkresolve_icd10cm_diseases_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.icd10cm.hcc_clinical&lt;/td&gt;
&lt;td&gt;chunkresolve_icd10cm_hcc_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.icd10cm.hcc_healthcare&lt;/td&gt;
&lt;td&gt;chunkresolve_icd10cm_hcc_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html""&gt;resolve_chunk.icd10cm.injuries&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html""&gt;chunkresolve_icd10cm_injuries_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html""&gt;resolve_chunk.icd10cm.musculoskeletal&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html""&gt;chunkresolve_icd10cm_musculoskeletal_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html""&gt;resolve_chunk.icd10cm.neoplasms&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html""&gt;chunkresolve_icd10cm_neoplasms_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html""&gt;resolve_chunk.icd10cm.poison&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html""&gt;chunkresolve_icd10cm_poison_ext_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html""&gt;resolve_chunk.icd10cm.puerile&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html""&gt;chunkresolve_icd10cm_puerile_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.icd10pcs.clinical&lt;/td&gt;
&lt;td&gt;chunkresolve_icd10pcs_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html""&gt;resolve_chunk.icdo.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html""&gt;chunkresolve_icdo_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html""&gt;resolve_chunk.loinc&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html""&gt;chunkresolve_loinc_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html""&gt;resolve_chunk.rxnorm.cd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html""&gt;chunkresolve_rxnorm_cd_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.rxnorm.in&lt;/td&gt;
&lt;td&gt;chunkresolve_rxnorm_in_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.rxnorm.in_healthcare&lt;/td&gt;
&lt;td&gt;chunkresolve_rxnorm_in_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html""&gt;resolve_chunk.rxnorm.sbd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html""&gt;chunkresolve_rxnorm_sbd_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html""&gt;resolve_chunk.rxnorm.scd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html""&gt;chunkresolve_rxnorm_scd_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.rxnorm.scdc&lt;/td&gt;
&lt;td&gt;chunkresolve_rxnorm_scdc_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.rxnorm.scdc_healthcare&lt;/td&gt;
&lt;td&gt;chunkresolve_rxnorm_scdc_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html""&gt;resolve_chunk.rxnorm.xsmall.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html""&gt;chunkresolve_rxnorm_xsmall_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html""&gt;resolve_chunk.snomed.findings&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html""&gt;chunkresolve_snomed_findings_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;New Classifiers&lt;/h1&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;classify.icd10.clinical&lt;/td&gt;
&lt;td&gt;classifier_icd10cm_hcc_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;classify.icd10.healthcare&lt;/td&gt;
&lt;td&gt;classifier_icd10cm_hcc_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html""&gt;classify.ade.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html""&gt;classifierdl_ade_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html""&gt;classify.ade.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html""&gt;classifierdl_ade_clinicalbert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html""&gt;classify.ade.conversational&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html""&gt;classifierdl_ade_conversational_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html""&gt;classify.gender.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html""&gt;classifierdl_gender_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html""&gt;classify.gender.sbert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html""&gt;classifierdl_gender_sbert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;classify.pico&lt;/td&gt;
&lt;td&gt;classifierdl_pico_biobert&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;German Medical models&lt;/h1&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;[embed]&lt;/td&gt;
&lt;td&gt;w2v_cc_300d&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[embed.w2v]&lt;/td&gt;
&lt;td&gt;w2v_cc_300d&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[resolve_chunk]&lt;/td&gt;
&lt;td&gt;chunkresolve_ICD10GM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[resolve_chunk.icd10gm]&lt;/td&gt;
&lt;td&gt;chunkresolve_ICD10GM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;resolve_chunk.icd10gm.2021&lt;/td&gt;
&lt;td&gt;chunkresolve_ICD10GM_2021&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;med_ner.legal&lt;/td&gt;
&lt;td&gt;ner_legal&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;med_ner&lt;/td&gt;
&lt;td&gt;ner_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;med_ner.healthcare&lt;/td&gt;
&lt;td&gt;ner_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;med_ner.healthcare_slim&lt;/td&gt;
&lt;td&gt;ner_healthcare_slim&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;med_ner.traffic&lt;/td&gt;
&lt;td&gt;ner_traffic&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;Spanish Medical models&lt;/h1&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html""&gt;embed.scielo.150d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html""&gt;embeddings_scielo_150d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html""&gt;embed.scielo.300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html""&gt;embeddings_scielo_300d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html""&gt;embed.scielo.50d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html""&gt;embeddings_scielo_50d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html""&gt;embed.scielowiki.150d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html""&gt;embeddings_scielowiki_150d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html""&gt;embed.scielowiki.300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html""&gt;embeddings_scielowiki_300d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html""&gt;embed.scielowiki.50d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html""&gt;embeddings_scielowiki_50d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html""&gt;embed.sciwiki.150d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html""&gt;embeddings_sciwiki_150d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html""&gt;embed.sciwiki.300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html""&gt;embeddings_sciwiki_300d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html""&gt;embed.sciwiki.50d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html""&gt;embeddings_sciwiki_50d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html""&gt;med_ner&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html""&gt;ner_diag_proc&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html""&gt;med_ner.neoplasm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html""&gt;ner_neoplasms&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html""&gt;med_ner.diag_proc&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html""&gt;ner_diag_proc&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;GPU Mode&lt;/h1&gt;

&lt;p&gt;You can now enable NLU GPU mode by setting &lt;code&gt;gpu=true&lt;/code&gt; while loading a model. I.e. &lt;code&gt;nlu.load(&amp;#39;train.sentiment&amp;#39; gpu=True)&lt;/code&gt; . If must resart you kernel, if you already loaded a nlu pipeline withouth GPU mode.&lt;/p&gt;

&lt;h1&gt;Output Level Relation&lt;/h1&gt;

&lt;p&gt;This new output level is used for relation extractors and will give you 1 row per relation extracted.&lt;/p&gt;

&lt;h1&gt;Bug fixes&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Fixed a bug that caused loading NLU models in offline mode not to work in some occasions&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;1 line Install NLU&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash&lt;/code&gt;&lt;/p&gt;

&lt;h1&gt;Install via PIP&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;! pip install nlu pyspark==3.0.1&lt;/code&gt;&lt;/p&gt;

&lt;h2&gt;Additional NLU ressources&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/""&gt;NLU Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/docs/en/notebooks""&gt;All NLU Tutorial Notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlp.johnsnowlabs.com/learn#pythons-nlu-library""&gt;NLU Videos and Blogposts on NLU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu""&gt;NLU on Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://join.slack.com/t/spark-nlp/shared_invite/zt-lutct9gm-kuUazcyFKhuGY3_0AMkxqA""&gt;Suggestions or Questions? Contact us in Slack!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n5gvel,True,,CKL-IT,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n5gvel/200_state_of_the_art_medical_models_for_ner/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n5gvel/200_state_of_the_art_medical_models_for_ner/,30199,1620223998.0,0,,False,,,,,,38256
236,,LanguageTechnology,"Hi, everyone!

I posted this earlier to r/MachineLearning, but I just found out about this subreddit and thought people here might be more interested. Sorry if it's not allowed!

I'm a researcher from the Text Mining Unit at the Barcelona Supercomputing  Center, and I wanted to share with you some information about MEDDOPROF,  a Shared Task that we are currently organizing focused on the detection  and normalization of professions and employment status in clinical  texts in Spanish.

Even if these types of entities might seem really niche, every day we learn more and more about their importance. Just think about how someone's occupation  can have a radical impact in their physical and mental health, habits,  lifestyle choices, ... There is even an entire medical specialty,  occupational medicine, that is centered around this topic. In the context of the current pandemic, many people with specific occupations  have been specially affected (for instance, health professionals and  other essential workers). The detection of these terms will help  researchers to better characterize health risks of specific occupations.

Outside  medicine, we foresee that the systems resulting from MEDDOPROF may be  used in fields such as social care, human resources, legal NLP and even gender studies. Personally, I think one of the main contributions of the  task is the inclusion of employment status in a broad sense. We have  annotated unemployed and retired people, family caregivers, people who  are homeless, people who depend on government subsidies, etc, which strengthens the social side of this project.

Additionally,  each mention in the corpus (which includes 2000 documents from over 20  different medical specialties) has been normalized to either the  European Skills, Competences, Qualifications and Occupations  classification (ESCO) or SNOMED-CT. These are multilingual vocabularies,  which we hope might inspire similar tasks in other languages (to the  best of our knowledge, there haven't been any similar tasks yet).

We  released the training set some weeks ago, and on June 1st we will  release the test set. If you are interested in the task, want to see  some annotated examples, the data or the annotation guidelines, ...  please check out the task's website:[ https://temu.bsc.es/meddoprof/](https://temu.bsc.es/meddoprof/)

Thank you if you have read up to here, I hope to see at least some of you at the task!",t2_bffb2m2a,False,,0,False,Call for Participation in a Shared Task about occupations detection in clinical texts,[],r/LanguageTechnology,False,6,,0,,False,t3_n5kw8w,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1620263033.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, everyone!&lt;/p&gt;

&lt;p&gt;I posted this earlier to &lt;a href=""/r/MachineLearning""&gt;r/MachineLearning&lt;/a&gt;, but I just found out about this subreddit and thought people here might be more interested. Sorry if it&amp;#39;s not allowed!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m a researcher from the Text Mining Unit at the Barcelona Supercomputing  Center, and I wanted to share with you some information about MEDDOPROF,  a Shared Task that we are currently organizing focused on the detection  and normalization of professions and employment status in clinical  texts in Spanish.&lt;/p&gt;

&lt;p&gt;Even if these types of entities might seem really niche, every day we learn more and more about their importance. Just think about how someone&amp;#39;s occupation  can have a radical impact in their physical and mental health, habits,  lifestyle choices, ... There is even an entire medical specialty,  occupational medicine, that is centered around this topic. In the context of the current pandemic, many people with specific occupations  have been specially affected (for instance, health professionals and  other essential workers). The detection of these terms will help  researchers to better characterize health risks of specific occupations.&lt;/p&gt;

&lt;p&gt;Outside  medicine, we foresee that the systems resulting from MEDDOPROF may be  used in fields such as social care, human resources, legal NLP and even gender studies. Personally, I think one of the main contributions of the  task is the inclusion of employment status in a broad sense. We have  annotated unemployed and retired people, family caregivers, people who  are homeless, people who depend on government subsidies, etc, which strengthens the social side of this project.&lt;/p&gt;

&lt;p&gt;Additionally,  each mention in the corpus (which includes 2000 documents from over 20  different medical specialties) has been normalized to either the  European Skills, Competences, Qualifications and Occupations  classification (ESCO) or SNOMED-CT. These are multilingual vocabularies,  which we hope might inspire similar tasks in other languages (to the  best of our knowledge, there haven&amp;#39;t been any similar tasks yet).&lt;/p&gt;

&lt;p&gt;We  released the training set some weeks ago, and on June 1st we will  release the test set. If you are interested in the task, want to see  some annotated examples, the data or the annotation guidelines, ...  please check out the task&amp;#39;s website:&lt;a href=""https://temu.bsc.es/meddoprof/""&gt; https://temu.bsc.es/meddoprof/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thank you if you have read up to here, I hope to see at least some of you at the task!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n5kw8w,True,,s-lilo,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n5kw8w/call_for_participation_in_a_shared_task_about/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n5kw8w/call_for_participation_in_a_shared_task_about/,30199,1620234233.0,0,,False,,,,,,2440
237,,LanguageTechnology,"Hello everyone,

I'm currently working on a topic modelling paper and I in order to evaluate it, I need your help. Topic models are unsupervised clustering methods used to group related words together and extract topics. To evaluate such topic I will in part use the word intrusion task ([source](https://papers.nips.cc/paper/2009/hash/f92586a25bb3145facd64ab20fd554ff-Abstract.html)). The task consist in polluting a topic (group of words) with an intruder. The assumption is that better the topic the easier it is to find the intruder. If you want to help, follow the link to a small 2min survey. No personal information is needed.

Link to the survey : [https://forms.gle/svWu4XmR2PAMeiCD9](https://forms.gle/svWu4XmR2PAMeiCD9)

Thank you to all who will find the time to help :)

&amp;#x200B;

PS: don't hesitate if you have any questions",t2_bybd0cyd,False,,0,False,I am working on a topic modelling paper and I need your help,[],r/LanguageTechnology,False,6,,0,,False,t3_n5h677,False,dark,0.8,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,1620225120.0,,[],{},,True,,1620253589.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m currently working on a topic modelling paper and I in order to evaluate it, I need your help. Topic models are unsupervised clustering methods used to group related words together and extract topics. To evaluate such topic I will in part use the word intrusion task (&lt;a href=""https://papers.nips.cc/paper/2009/hash/f92586a25bb3145facd64ab20fd554ff-Abstract.html""&gt;source&lt;/a&gt;). The task consist in polluting a topic (group of words) with an intruder. The assumption is that better the topic the easier it is to find the intruder. If you want to help, follow the link to a small 2min survey. No personal information is needed.&lt;/p&gt;

&lt;p&gt;Link to the survey : &lt;a href=""https://forms.gle/svWu4XmR2PAMeiCD9""&gt;https://forms.gle/svWu4XmR2PAMeiCD9&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thank you to all who will find the time to help :)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;PS: don&amp;#39;t hesitate if you have any questions&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n5h677,True,,Addicted_to_math,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n5h677/i_am_working_on_a_topic_modelling_paper_and_i/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n5h677/i_am_working_on_a_topic_modelling_paper_and_i/,30199,1620224789.0,0,,False,,,,,,842
238,,LanguageTechnology,"Is there a curated list of Advanced Natural Language Processing (NLP) Resources (Model Zoo, GitHub Repositories, Datasets, etc.)  
I think Advanced NLP is progress in the last 3 years since BERT and GPT which are SOTA versatile models and also great for Transfer Learning (Zero-Shot and Few-Shot Learning) like Hugging Face models. Essentially, they've changed the way we approach NLP problems today.

I could not find it on the internet (including on GitHub, Kaggle, Medium, or Reddit.) And, I know about [NLP Progress](http://nlpprogress.com/) and [The Super Duper NLP Repo](https://notebooks.quantumstat.com/).

I need it for a project, and any help is greatly appreciated.",t2_32tnn9,False,,0,False,[Request] Curated Advanced NLP Resources,[],r/LanguageTechnology,False,6,,0,,False,t3_n5h4hy,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620253464.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there a curated list of Advanced Natural Language Processing (NLP) Resources (Model Zoo, GitHub Repositories, Datasets, etc.)&lt;br/&gt;
I think Advanced NLP is progress in the last 3 years since BERT and GPT which are SOTA versatile models and also great for Transfer Learning (Zero-Shot and Few-Shot Learning) like Hugging Face models. Essentially, they&amp;#39;ve changed the way we approach NLP problems today.&lt;/p&gt;

&lt;p&gt;I could not find it on the internet (including on GitHub, Kaggle, Medium, or Reddit.) And, I know about &lt;a href=""http://nlpprogress.com/""&gt;NLP Progress&lt;/a&gt; and &lt;a href=""https://notebooks.quantumstat.com/""&gt;The Super Duper NLP Repo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I need it for a project, and any help is greatly appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n5h4hy,True,,TheGupta,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n5h4hy/request_curated_advanced_nlp_resources/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n5h4hy/request_curated_advanced_nlp_resources/,30199,1620224664.0,0,,False,,,,,,676
239,,LanguageTechnology,,t2_icvu8,False,,0,False,Tagalog: a text labeling platform for teams,[],r/LanguageTechnology,False,6,,0,,False,t3_n5h2jx,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1620253327.0,text,6,,,text,tagalog.ai,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n5h2jx,True,,yvespeirsman,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n5h2jx/tagalog_a_text_labeling_platform_for_teams/,all_ads,False,https://www.tagalog.ai/tagalog/,30199,1620224527.0,0,,False,https://www.tagalog.ai/tagalog/,,,,,0
240,,LanguageTechnology,,t2_hkv9s,False,,0,False,EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks,[],r/LanguageTechnology,False,6,,0,,False,t3_n59z8e,False,dark,0.81,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,False,,1620227697.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n59z8e,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n59z8e/eda_easy_data_augmentation_techniques_for/,all_ads,False,https://link.medium.com/YkCBPe5n0fb,30199,1620198897.0,0,,False,https://link.medium.com/YkCBPe5n0fb,,,,,0
241,,LanguageTechnology,"
## 200+ State of the Art Medical Models for NER, Entity Resolution, Relation Extraction, Assertion, Spark 3 and Python 3.8 support in  NLU 3.0 Release and much more
We are incredibly excited to announce the release of `NLU 3.0.0` which makes most of John Snow Labs medical healthcare model available in just 1 line of code in NLU.
These models are the most accurate in their domains and highly scalable in Spark clusters.  
In addition, `Spark 3.0.X`  and `Spark 3.1.X ` is now supported, together with Python3.8

This is enabled by the amazing [Spark NLP3.0.1](https://nlp.johnsnowlabs.com/docs/en/release_notes#300) and [Spark NLP for Healthcare 3.0.1](https://nlp.johnsnowlabs.com/docs/en/licensed_release_notes#301) releases.

# New Features
- Over 200 new models for the `healthcare` domain
- 6 new classes of models, Assertion, Sentence/Chunk Resolvers, Relation Extractors, Medical NER models, De-Identificator Models
- Spark 3.0.X and 3.1.X support
- Python 3.8 Support
- New Output level `relation`
- 1 Line to install NLU  just run `!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash`
- [Various new EMR and Databricks versions supported](https://github.com/JohnSnowLabs/spark-nlp/releases/tag/3.0.0)
- GPU Mode, more then 600% speedup by enabling GPU mode.
- Authorized mode for licensed features

## New Documentation
- [NLU for Healthcare Examples](https://nlu.johnsnowlabs.com/docs/en/examples_hc#usage-examples-of-nluload)
- [Instrunctions to authorize your environment to use Licensed features](https://nlu.johnsnowlabs.com/docs/en/examples_hc#authorize-access-to-licensed-features-and-install-healthcare-dependencies)


## New Notebooks
- [Medical Named Entity Extraction (NER) notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/medical_named_entity_recognition/overview_medical_entity_recognizers.ipynb)
- [Relation extraction notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/relation_extraction/overview_relation.ipynb)
- [Entity Resolution overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/entity_resolution/entity_resolvers_overview.ipynb)
- [Assertion overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/assertion/assertion_overview.ipynb)
- [De-Identification overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/de_identification/DeIdentification_model_overview.ipynb)
- [Graph NLU tutorial](https://github.com/JohnSnowLabs/nlu/blob/3.0rc1/examples/webinars_conferences_etc/graph_ai_summit/Healthcare_Graph_NLU_COVID_Tigergraph.ipynb)


## AssertionDLModels

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [assert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html) | [assertion_dl](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html)                   |
| English  | [assert.biobert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html) | [assertion_dl_biobert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html)                   |
| English  | [assert.healthcare](https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html) | [assertion_dl_healthcare](https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html)                   |
| English  | [assert.large](https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html) | [assertion_dl_large](https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html)                   |

##  New Word Embeddings

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [embed.glove.clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html) | [embeddings_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html)                   |
| English  | [embed.glove.biovec](https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html) | [embeddings_biovec](https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html)                   |
| English  | [embed.glove.healthcare](https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html) | [embeddings_healthcare](https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html)                   |
| English  | [embed.glove.healthcare_100d](https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html) | [embeddings_healthcare_100d](https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html)                   |
| English  | en.embed.glove.icdoem | embeddings_icdoem          |
| English  | en.embed.glove.icdoem_2ng | embeddings_icdoem_2ng          |

## Sentence Entity resolvers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | embed_sentence.biobert.mli | sbiobert_base_cased_mli          |
| English  | resolve | sbiobertresolve_cpt          |
| English  | resolve.cpt | sbiobertresolve_cpt          |
| English  | resolve.cpt.augmented | sbiobertresolve_cpt_augmented          |
| English  | resolve.cpt.procedures_augmented | sbiobertresolve_cpt_procedures_augmented          |
| English  | resolve.hcc.augmented | sbiobertresolve_hcc_augmented          |
| English  | [resolve.icd10cm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html) | [sbiobertresolve_icd10cm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html)                   |
| English  | [resolve.icd10cm.augmented](https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html) | [sbiobertresolve_icd10cm_augmented](https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html)                   |
| English  | [resolve.icd10cm.augmented_billable](https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html) | [sbiobertresolve_icd10cm_augmented_billable_hcc](https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html)                   |
| English  | [resolve.icd10pcs](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html) | [sbiobertresolve_icd10pcs](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html)                   |
| English  | [resolve.icdo](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html) | [sbiobertresolve_icdo](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html)                   |
| English  | [resolve.rxcui](https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html) | [sbiobertresolve_rxcui](https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html)                   |
| English  | [resolve.rxnorm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html) | [sbiobertresolve_rxnorm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html)                   |
| English  | [resolve.snomed](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html) | [sbiobertresolve_snomed_auxConcepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html)                   |
| English  | [resolve.snomed.aux_concepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html) | [sbiobertresolve_snomed_auxConcepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html)                   |
| English  | [resolve.snomed.aux_concepts_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html) | [sbiobertresolve_snomed_auxConcepts_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html)                   |
| English  | [resolve.snomed.findings](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html) | [sbiobertresolve_snomed_findings](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html)                   |
| English  | [resolve.snomed.findings_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html) | [sbiobertresolve_snomed_findings_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html)                   |

## RelationExtractionModel

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | relation.posology | posology_re          |
| English  | [relation](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html) | [redl_bodypart_direction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html)                   |
| English  | [relation.bodypart.direction](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html) | [redl_bodypart_direction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html)                   |
| English  | [relation.bodypart.problem](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html) | [redl_bodypart_problem_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html)                   |
| English  | [relation.bodypart.procedure](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html) | [redl_bodypart_procedure_test_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html)                   |
| English  | [relation.chemprot](https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html) | [redl_chemprot_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html)                   |
| English  | [relation.clinical](https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html) | [redl_clinical_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html)                   |
| English  | [relation.date](https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls) | [redl_date_clinical_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls)                   |
| English  | [relation.drug_drug_interaction](https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html) | [redl_drug_drug_interaction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html)                   |
| English  | [relation.humen_phenotype_gene](https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html) | [redl_human_phenotype_gene_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html)                   |
| English  | [relation.temporal_events](https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html) | [redl_temporal_events_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html)                   |



## NERDLModels

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
|English  | [med_ner.ade.clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html) | [ner_ade_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html)                   |
| English  | [med_ner.ade.clinical_bert](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html) | [ner_ade_clinicalbert](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html)                   |
| English  | [med_ner.ade.ade_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html) | [ner_ade_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html)                   |
| English  | [med_ner.anatomy](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html) | [ner_anatomy](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html)                   |
| English  | [med_ner.anatomy.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html) | [ner_anatomy_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html)                   |
| English  | [med_ner.anatomy.coarse](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html) | [ner_anatomy_coarse](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html)                   |
| English  | [med_ner.anatomy.coarse_biobert](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html) | [ner_anatomy_coarse_biobert](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html)                   |
| English  | [med_ner.aspect_sentiment](https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html) | [ner_aspect_based_sentiment](https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html)                   |
| English  | [med_ner.bacterial_species](https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html) | [ner_bacterial_species](https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html)                   |
| English  | [med_ner.bionlp](https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html) | [ner_bionlp](https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html)                   |
| English  | [med_ner.bionlp.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html) | [ner_bionlp_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html)                   |
| English  | [med_ner.cancer](https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html) | [ner_cancer_genetics](https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html)                   |
| Englishs | [med_ner.cellular](https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html) | [ner_cellular](https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html)                   |
| English  | [med_ner.cellular.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html) | [ner_cellular_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html)                   |
| English  | [med_ner.chemicals](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html) | [ner_chemicals](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html)                   |
| English  | [med_ner.chemprot](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html) | [ner_chemprot_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html)           |
| English  | [med_ner.chemprot.clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html) | [ner_chemprot_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html)           |
| English  | [med_ner.clinical](https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html) | [ner_clinical](https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html)           |
| English  | [med_ner.clinical.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html) | [ner_clinical_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html)           |
| English  | med_ner.clinical.noncontrib | ner_clinical_noncontrib          |
| English  | [med_ner.diseases](https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html) | [ner_diseases](https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html)           |
| English  | [med_ner.diseases.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html) | [ner_diseases_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html)           |
| English  | [med_ner.diseases.large](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html) | [ner_diseases_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html)           |
| English  | [med_ner.drugs](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html) | [ner_drugs](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html)           |
| English  | [med_ner.drugsgreedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html) | [ner_drugs_greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html)           |
| English  | [med_ner.drugs.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html) | [ner_drugs_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html)           |
| English  | [med_ner.events_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html) | [ner_events_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html)           |
| English  | [med_ner.events_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html) | [ner_events_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html)           |
| English  | [med_ner.events_healthcre](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html) | [ner_events_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html)           |
| English  | [med_ner.financial_contract](https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html) | [ner_financial_contract](https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html)           |
| English  | [med_ner.healthcare](https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html) | [ner_healthcare](https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html)           |
| English  | [med_ner.human_phenotype.gene_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html) | [ner_human_phenotype_gene_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html)           |
| English  | [med_ner.human_phenotype.gene_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html) | [ner_human_phenotype_gene_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html)           |
| English  | [med_ner.human_phenotype.go_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html) | [ner_human_phenotype_go_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html)           |
| English  | [med_ner.human_phenotype.go_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html) | [ner_human_phenotype_go_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html)           |
| English  | [med_ner.jsl](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html) | [ner_jsl](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html)           |
| English  | [med_ner.jsl.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html) | [ner_jsl_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html)           |
| English  | [med_ner.jsl.enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html) | [ner_jsl_enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html)           |
| English  | [med_ner.jsl.enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html) | [ner_jsl_enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html)           |
| English  | [med_ner.measurements](https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html) | [ner_measurements_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html)           |
| English  | [med_ner.medmentions](https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html) | [ner_medmentions_coarse](https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html)           |
| English  | [med_ner.posology](https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html) | [ner_posology](https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html)           |
| English  | [med_ner.posology.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html) | [ner_posology_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html)           |
| English  | [med_ner.posology.greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html) | [ner_posology_greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html)           |
| English  | [med_ner.posology.healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html) | [ner_posology_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html)           |
| English  | [med_ner.posology.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html) | [ner_posology_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html)           |
| English  | [med_ner.posology.large_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html) | [ner_posology_large_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html)           |
| English  | [med_ner.posology.small](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html) | [ner_posology_small](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html)           |
| English  | [med_ner.radiology](https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html) | [ner_radiology](https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html)           |
| English  | [med_ner.radiology.wip_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html) | [ner_radiology_wip_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html)           |
| English  | [med_ner.risk_factors](https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html) | [ner_risk_factors](https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html)           |
| English  | [med_ner.risk_factors.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html) | [ner_risk_factors_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html)           |
| English  | med_ner.i2b2 | nerdl_i2b2          |
| English  | [med_ner.tumour](https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html) | [nerdl_tumour_demo](https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html)           |
| English  | med_ner.jsl.wip.clinical | jsl_ner_wip_clinical          |
| English  | [med_ner.jsl.wip.clinical.greedy](https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html) | [jsl_ner_wip_greedy_clinical](https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html)           |
| English  | [med_ner.jsl.wip.clinical.modifier](https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html) | [jsl_ner_wip_modifier_clinical](https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html)           |
| English  | [med_ner.jsl.wip.clinical.rd](https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html) | [jsl_rd_ner_wip_greedy_clinical](https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html)           |


## De-Identification Models

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [med_ner.deid.augmented](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html) | [ner_deid_augmented](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html)           |
| English  | [med_ner.deid.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html) | [ner_deid_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html)           |
| English  | [med_ner.deid.enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html) | [ner_deid_enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html)           |
| English  | [med_ner.deid.enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html) | [ner_deid_enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html)           |
| English  | [med_ner.deid.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html) | [ner_deid_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html)           |
| English  | [med_ner.deid.sd](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html) | [ner_deid_sd](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html)           |
| English  | [med_ner.deid.sd_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html) | [ner_deid_sd_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html)           |
| English  | med_ner.deid | nerdl_deid          |
| English  | med_ner.deid.synthetic | ner_deid_synthetic          |
| English  | [med_ner.deid.dl](https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html) | [ner_deidentify_dl](https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html)           |
| English  | [en.de_identify](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html) | [deidentify_rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html)           |
| English  | de_identify.rules | deid_rules          |
| English  | [de_identify.clinical](https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html) | [deidentify_enriched_clinical](https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html)           |
| English  | [de_identify.large](https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html) | [deidentify_large](https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html)           |
| English  | [de_identify.rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html) | [deidentify_rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html)           |
| English  | de_identify.rb_no_regex | deidentify_rb_no_regex          |



# Chunk resolvers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [resolve_chunk.athena_conditions](https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html) | [chunkresolve_athena_conditions_healthcare](https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html)           |
| English  | [resolve_chunk.cpt_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html) | [chunkresolve_cpt_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html) | [chunkresolve_icd10cm_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.diseases_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html) | [chunkresolve_icd10cm_diseases_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html)           |
| English  | resolve_chunk.icd10cm.hcc_clinical | chunkresolve_icd10cm_hcc_clinical          |
| English  | resolve_chunk.icd10cm.hcc_healthcare | chunkresolve_icd10cm_hcc_healthcare          |
| English  | [resolve_chunk.icd10cm.injuries](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html) | [chunkresolve_icd10cm_injuries_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.musculoskeletal](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html) | [chunkresolve_icd10cm_musculoskeletal_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.neoplasms](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html) | [chunkresolve_icd10cm_neoplasms_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.poison](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html) | [chunkresolve_icd10cm_poison_ext_clinical](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.puerile](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html) | [chunkresolve_icd10cm_puerile_clinical](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html)           |
| English  | resolve_chunk.icd10pcs.clinical | chunkresolve_icd10pcs_clinical          |
| English  | [resolve_chunk.icdo.clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html) | [chunkresolve_icdo_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html)           |
| English  | [resolve_chunk.loinc](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html) | [chunkresolve_loinc_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html)           |
| English  | [resolve_chunk.rxnorm.cd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html) | [chunkresolve_rxnorm_cd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html)           |
| English  | resolve_chunk.rxnorm.in | chunkresolve_rxnorm_in_clinical          |
| English  | resolve_chunk.rxnorm.in_healthcare | chunkresolve_rxnorm_in_healthcare          |
| English  | [resolve_chunk.rxnorm.sbd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html) | [chunkresolve_rxnorm_sbd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html)           |
| English  | [resolve_chunk.rxnorm.scd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html) | [chunkresolve_rxnorm_scd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html)           |
| English  | resolve_chunk.rxnorm.scdc | chunkresolve_rxnorm_scdc_clinical          |
| English  | resolve_chunk.rxnorm.scdc_healthcare | chunkresolve_rxnorm_scdc_healthcare          |
| English  | [resolve_chunk.rxnorm.xsmall.clinical](https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html) | [chunkresolve_rxnorm_xsmall_clinical](https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html)           |
| English  | [resolve_chunk.snomed.findings](https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html) | [chunkresolve_snomed_findings_clinical](https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html)           |


# New Classifiers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | classify.icd10.clinical | classifier_icd10cm_hcc_clinical          |
| English  | classify.icd10.healthcare | classifier_icd10cm_hcc_healthcare          |
| English  | [classify.ade.biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html) | [classifierdl_ade_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html)           |
| English  | [classify.ade.clinical](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html) | [classifierdl_ade_clinicalbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html)           |
| English  | [classify.ade.conversational](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html) | [classifierdl_ade_conversational_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html)           |
| English  | [classify.gender.biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html) | [classifierdl_gender_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html)           |
| English  | [classify.gender.sbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html) | [classifierdl_gender_sbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html)           |
| English  | classify.pico | classifierdl_pico_biobert          |


# German Medical models

| nlu.load() reference                                         | Spark NLP Model reference          |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [embed]    | w2v_cc_300d|
| [embed.w2v]    | w2v_cc_300d|
| [resolve_chunk]    | chunkresolve_ICD10GM|
| [resolve_chunk.icd10gm]    | chunkresolve_ICD10GM|
| resolve_chunk.icd10gm.2021    | chunkresolve_ICD10GM_2021|
| med_ner.legal   | ner_legal|
| med_ner    | ner_healthcare|
| med_ner.healthcare    | ner_healthcare|
| med_ner.healthcare_slim    | ner_healthcare_slim|
| med_ner.traffic    | ner_traffic|

# Spanish Medical models
| nlu.load() reference                                         | Spark NLP Model reference          |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [embed.scielo.150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html) | [embeddings_scielo_150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html)| 
| [embed.scielo.300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html)   | [embeddings_scielo_300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html)| 
| [embed.scielo.50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html)  | [embeddings_scielo_50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html)| 
| [embed.scielowiki.150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html)   | [embeddings_scielowiki_150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html)| 
| [embed.scielowiki.300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html)   | [embeddings_scielowiki_300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html)| 
| [embed.scielowiki.50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html)   | [embeddings_scielowiki_50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html)| 
| [embed.sciwiki.150d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html)   | [embeddings_sciwiki_150d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html)| 
| [embed.sciwiki.300d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html)   | [embeddings_sciwiki_300d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html)| 
| [embed.sciwiki.50d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html)   | [embeddings_sciwiki_50d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html)| 
| [med_ner](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)   |  [ner_diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)| 
| [med_ner.neoplasm](https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html)  | [ner_neoplasms](https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html)| 
| [med_ner.diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)  | [ner_diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)| 

# GPU Mode
You can now enable NLU GPU mode by setting `gpu=true` while loading a model. I.e. `nlu.load('train.sentiment' gpu=True)` . If must resart you kernel, if you already loaded a nlu pipeline withouth GPU mode.

# Output Level Relation
This new output level is used for relation extractors and will give you 1 row per relation extracted.


# Bug fixes
- Fixed a bug that caused loading NLU models in offline mode not to work in some occasions


# 1 line Install NLU
```!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash```

# Install via PIP 
```! pip install nlu pyspark==3.0.1```


## Additional NLU ressources

- [NLU Website](https://nlu.johnsnowlabs.com/)
- [All NLU Tutorial Notebooks](https://nlu.johnsnowlabs.com/docs/en/notebooks)
- [NLU Videos and Blogposts on NLU](https://nlp.johnsnowlabs.com/learn#pythons-nlu-library)
- [NLU on Github](https://github.com/JohnSnowLabs/nlu)
- [Suggestions or Questions? Contact us in Slack!](https://join.slack.com/t/spark-nlp/shared_invite/zt-lutct9gm-kuUazcyFKhuGY3_0AMkxqA)",t2_53n73cus,False,,0,False,"200+ State of the Art Medical Models for NER, Entity Resolution, Relation Extraction, Assertion, Spark 3 and Python 3.8 support - John Snow Labs NLU 3.0.0",[],r/LanguageTechnology,False,6,,0,,False,t3_n5guxs,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1620252763.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;h2&gt;200+ State of the Art Medical Models for NER, Entity Resolution, Relation Extraction, Assertion, Spark 3 and Python 3.8 support in  NLU 3.0 Release and much more&lt;/h2&gt;

&lt;p&gt;We are incredibly excited to announce the release of &lt;code&gt;NLU 3.0.0&lt;/code&gt; which makes most of John Snow Labs medical healthcare model available in just 1 line of code in NLU.
These models are the most accurate in their domains and highly scalable in Spark clusters.&lt;br/&gt;
In addition, &lt;code&gt;Spark 3.0.X&lt;/code&gt;  and &lt;code&gt;Spark 3.1.X&lt;/code&gt; is now supported, together with Python3.8&lt;/p&gt;

&lt;p&gt;This is enabled by the amazing &lt;a href=""https://nlp.johnsnowlabs.com/docs/en/release_notes#300""&gt;Spark NLP3.0.1&lt;/a&gt; and &lt;a href=""https://nlp.johnsnowlabs.com/docs/en/licensed_release_notes#301""&gt;Spark NLP for Healthcare 3.0.1&lt;/a&gt; releases.&lt;/p&gt;

&lt;h1&gt;New Features&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Over 200 new models for the &lt;code&gt;healthcare&lt;/code&gt; domain&lt;/li&gt;
&lt;li&gt;6 new classes of models, Assertion, Sentence/Chunk Resolvers, Relation Extractors, Medical NER models, De-Identificator Models&lt;/li&gt;
&lt;li&gt;Spark 3.0.X and 3.1.X support&lt;/li&gt;
&lt;li&gt;Python 3.8 Support&lt;/li&gt;
&lt;li&gt;New Output level &lt;code&gt;relation&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;1 Line to install NLU  just run &lt;code&gt;!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/spark-nlp/releases/tag/3.0.0""&gt;Various new EMR and Databricks versions supported&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GPU Mode, more then 600% speedup by enabling GPU mode.&lt;/li&gt;
&lt;li&gt;Authorized mode for licensed features&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;New Documentation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/docs/en/examples_hc#usage-examples-of-nluload""&gt;NLU for Healthcare Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/docs/en/examples_hc#authorize-access-to-licensed-features-and-install-healthcare-dependencies""&gt;Instrunctions to authorize your environment to use Licensed features&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;New Notebooks&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/medical_named_entity_recognition/overview_medical_entity_recognizers.ipynb""&gt;Medical Named Entity Extraction (NER) notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/relation_extraction/overview_relation.ipynb""&gt;Relation extraction notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/entity_resolution/entity_resolvers_overview.ipynb""&gt;Entity Resolution overview notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/assertion/assertion_overview.ipynb""&gt;Assertion overview notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/de_identification/DeIdentification_model_overview.ipynb""&gt;De-Identification overview notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/3.0rc1/examples/webinars_conferences_etc/graph_ai_summit/Healthcare_Graph_NLU_COVID_Tigergraph.ipynb""&gt;Graph NLU tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;AssertionDLModels&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html""&gt;assert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html""&gt;assertion_dl&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html""&gt;assert.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html""&gt;assertion_dl_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html""&gt;assert.healthcare&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html""&gt;assertion_dl_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html""&gt;assert.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html""&gt;assertion_dl_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;New Word Embeddings&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html""&gt;embed.glove.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html""&gt;embeddings_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html""&gt;embed.glove.biovec&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html""&gt;embeddings_biovec&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html""&gt;embed.glove.healthcare&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html""&gt;embeddings_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html""&gt;embed.glove.healthcare_100d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html""&gt;embeddings_healthcare_100d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;en.embed.glove.icdoem&lt;/td&gt;
&lt;td&gt;embeddings_icdoem&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;en.embed.glove.icdoem_2ng&lt;/td&gt;
&lt;td&gt;embeddings_icdoem_2ng&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;Sentence Entity resolvers&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;embed_sentence.biobert.mli&lt;/td&gt;
&lt;td&gt;sbiobert_base_cased_mli&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve&lt;/td&gt;
&lt;td&gt;sbiobertresolve_cpt&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve.cpt&lt;/td&gt;
&lt;td&gt;sbiobertresolve_cpt&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve.cpt.augmented&lt;/td&gt;
&lt;td&gt;sbiobertresolve_cpt_augmented&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve.cpt.procedures_augmented&lt;/td&gt;
&lt;td&gt;sbiobertresolve_cpt_procedures_augmented&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve.hcc.augmented&lt;/td&gt;
&lt;td&gt;sbiobertresolve_hcc_augmented&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html""&gt;resolve.icd10cm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html""&gt;sbiobertresolve_icd10cm&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html""&gt;resolve.icd10cm.augmented&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html""&gt;sbiobertresolve_icd10cm_augmented&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html""&gt;resolve.icd10cm.augmented_billable&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html""&gt;sbiobertresolve_icd10cm_augmented_billable_hcc&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html""&gt;resolve.icd10pcs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html""&gt;sbiobertresolve_icd10pcs&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html""&gt;resolve.icdo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html""&gt;sbiobertresolve_icdo&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html""&gt;resolve.rxcui&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html""&gt;sbiobertresolve_rxcui&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html""&gt;resolve.rxnorm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html""&gt;sbiobertresolve_rxnorm&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html""&gt;resolve.snomed&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html""&gt;sbiobertresolve_snomed_auxConcepts&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html""&gt;resolve.snomed.aux_concepts&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html""&gt;sbiobertresolve_snomed_auxConcepts&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html""&gt;resolve.snomed.aux_concepts_int&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html""&gt;sbiobertresolve_snomed_auxConcepts_int&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html""&gt;resolve.snomed.findings&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html""&gt;sbiobertresolve_snomed_findings&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html""&gt;resolve.snomed.findings_int&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html""&gt;sbiobertresolve_snomed_findings_int&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;RelationExtractionModel&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;relation.posology&lt;/td&gt;
&lt;td&gt;posology_re&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html""&gt;relation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html""&gt;redl_bodypart_direction_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html""&gt;relation.bodypart.direction&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html""&gt;redl_bodypart_direction_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html""&gt;relation.bodypart.problem&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html""&gt;redl_bodypart_problem_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html""&gt;relation.bodypart.procedure&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html""&gt;redl_bodypart_procedure_test_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html""&gt;relation.chemprot&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html""&gt;redl_chemprot_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html""&gt;relation.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html""&gt;redl_clinical_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls""&gt;relation.date&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls""&gt;redl_date_clinical_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html""&gt;relation.drug_drug_interaction&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html""&gt;redl_drug_drug_interaction_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html""&gt;relation.humen_phenotype_gene&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html""&gt;redl_human_phenotype_gene_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html""&gt;relation.temporal_events&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html""&gt;redl_temporal_events_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;NERDLModels&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html""&gt;med_ner.ade.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html""&gt;ner_ade_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html""&gt;med_ner.ade.clinical_bert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html""&gt;ner_ade_clinicalbert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html""&gt;med_ner.ade.ade_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html""&gt;ner_ade_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html""&gt;med_ner.anatomy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html""&gt;ner_anatomy&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html""&gt;med_ner.anatomy.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html""&gt;ner_anatomy_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html""&gt;med_ner.anatomy.coarse&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html""&gt;ner_anatomy_coarse&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html""&gt;med_ner.anatomy.coarse_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html""&gt;ner_anatomy_coarse_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html""&gt;med_ner.aspect_sentiment&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html""&gt;ner_aspect_based_sentiment&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html""&gt;med_ner.bacterial_species&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html""&gt;ner_bacterial_species&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html""&gt;med_ner.bionlp&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html""&gt;ner_bionlp&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html""&gt;med_ner.bionlp.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html""&gt;ner_bionlp_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html""&gt;med_ner.cancer&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html""&gt;ner_cancer_genetics&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Englishs&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html""&gt;med_ner.cellular&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html""&gt;ner_cellular&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html""&gt;med_ner.cellular.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html""&gt;ner_cellular_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html""&gt;med_ner.chemicals&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html""&gt;ner_chemicals&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html""&gt;med_ner.chemprot&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html""&gt;ner_chemprot_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html""&gt;med_ner.chemprot.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html""&gt;ner_chemprot_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html""&gt;med_ner.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html""&gt;ner_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html""&gt;med_ner.clinical.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html""&gt;ner_clinical_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;med_ner.clinical.noncontrib&lt;/td&gt;
&lt;td&gt;ner_clinical_noncontrib&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html""&gt;med_ner.diseases&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html""&gt;ner_diseases&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html""&gt;med_ner.diseases.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html""&gt;ner_diseases_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html""&gt;med_ner.diseases.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html""&gt;ner_diseases_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html""&gt;med_ner.drugs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html""&gt;ner_drugs&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html""&gt;med_ner.drugsgreedy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html""&gt;ner_drugs_greedy&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html""&gt;med_ner.drugs.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html""&gt;ner_drugs_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html""&gt;med_ner.events_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html""&gt;ner_events_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html""&gt;med_ner.events_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html""&gt;ner_events_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html""&gt;med_ner.events_healthcre&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html""&gt;ner_events_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html""&gt;med_ner.financial_contract&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html""&gt;ner_financial_contract&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html""&gt;med_ner.healthcare&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html""&gt;ner_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html""&gt;med_ner.human_phenotype.gene_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html""&gt;ner_human_phenotype_gene_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html""&gt;med_ner.human_phenotype.gene_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html""&gt;ner_human_phenotype_gene_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html""&gt;med_ner.human_phenotype.go_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html""&gt;ner_human_phenotype_go_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html""&gt;med_ner.human_phenotype.go_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html""&gt;ner_human_phenotype_go_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html""&gt;med_ner.jsl&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html""&gt;ner_jsl&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html""&gt;med_ner.jsl.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html""&gt;ner_jsl_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html""&gt;med_ner.jsl.enriched&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html""&gt;ner_jsl_enriched&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html""&gt;med_ner.jsl.enriched_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html""&gt;ner_jsl_enriched_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html""&gt;med_ner.measurements&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html""&gt;ner_measurements_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html""&gt;med_ner.medmentions&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html""&gt;ner_medmentions_coarse&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html""&gt;med_ner.posology&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html""&gt;ner_posology&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html""&gt;med_ner.posology.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html""&gt;ner_posology_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html""&gt;med_ner.posology.greedy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html""&gt;ner_posology_greedy&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html""&gt;med_ner.posology.healthcare&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html""&gt;ner_posology_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html""&gt;med_ner.posology.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html""&gt;ner_posology_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html""&gt;med_ner.posology.large_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html""&gt;ner_posology_large_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html""&gt;med_ner.posology.small&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html""&gt;ner_posology_small&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html""&gt;med_ner.radiology&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html""&gt;ner_radiology&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html""&gt;med_ner.radiology.wip_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html""&gt;ner_radiology_wip_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html""&gt;med_ner.risk_factors&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html""&gt;ner_risk_factors&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html""&gt;med_ner.risk_factors.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html""&gt;ner_risk_factors_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;med_ner.i2b2&lt;/td&gt;
&lt;td&gt;nerdl_i2b2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html""&gt;med_ner.tumour&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html""&gt;nerdl_tumour_demo&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;med_ner.jsl.wip.clinical&lt;/td&gt;
&lt;td&gt;jsl_ner_wip_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html""&gt;med_ner.jsl.wip.clinical.greedy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html""&gt;jsl_ner_wip_greedy_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html""&gt;med_ner.jsl.wip.clinical.modifier&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html""&gt;jsl_ner_wip_modifier_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html""&gt;med_ner.jsl.wip.clinical.rd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html""&gt;jsl_rd_ner_wip_greedy_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;De-Identification Models&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html""&gt;med_ner.deid.augmented&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html""&gt;ner_deid_augmented&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html""&gt;med_ner.deid.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html""&gt;ner_deid_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html""&gt;med_ner.deid.enriched&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html""&gt;ner_deid_enriched&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html""&gt;med_ner.deid.enriched_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html""&gt;ner_deid_enriched_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html""&gt;med_ner.deid.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html""&gt;ner_deid_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html""&gt;med_ner.deid.sd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html""&gt;ner_deid_sd&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html""&gt;med_ner.deid.sd_large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html""&gt;ner_deid_sd_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;med_ner.deid&lt;/td&gt;
&lt;td&gt;nerdl_deid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;med_ner.deid.synthetic&lt;/td&gt;
&lt;td&gt;ner_deid_synthetic&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html""&gt;med_ner.deid.dl&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html""&gt;ner_deidentify_dl&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html""&gt;en.de_identify&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html""&gt;deidentify_rb&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;de_identify.rules&lt;/td&gt;
&lt;td&gt;deid_rules&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html""&gt;de_identify.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html""&gt;deidentify_enriched_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html""&gt;de_identify.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html""&gt;deidentify_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html""&gt;de_identify.rb&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html""&gt;deidentify_rb&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;de_identify.rb_no_regex&lt;/td&gt;
&lt;td&gt;deidentify_rb_no_regex&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;Chunk resolvers&lt;/h1&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html""&gt;resolve_chunk.athena_conditions&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html""&gt;chunkresolve_athena_conditions_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html""&gt;resolve_chunk.cpt_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html""&gt;chunkresolve_cpt_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html""&gt;resolve_chunk.icd10cm.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html""&gt;chunkresolve_icd10cm_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html""&gt;resolve_chunk.icd10cm.diseases_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html""&gt;chunkresolve_icd10cm_diseases_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.icd10cm.hcc_clinical&lt;/td&gt;
&lt;td&gt;chunkresolve_icd10cm_hcc_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.icd10cm.hcc_healthcare&lt;/td&gt;
&lt;td&gt;chunkresolve_icd10cm_hcc_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html""&gt;resolve_chunk.icd10cm.injuries&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html""&gt;chunkresolve_icd10cm_injuries_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html""&gt;resolve_chunk.icd10cm.musculoskeletal&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html""&gt;chunkresolve_icd10cm_musculoskeletal_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html""&gt;resolve_chunk.icd10cm.neoplasms&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html""&gt;chunkresolve_icd10cm_neoplasms_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html""&gt;resolve_chunk.icd10cm.poison&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html""&gt;chunkresolve_icd10cm_poison_ext_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html""&gt;resolve_chunk.icd10cm.puerile&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html""&gt;chunkresolve_icd10cm_puerile_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.icd10pcs.clinical&lt;/td&gt;
&lt;td&gt;chunkresolve_icd10pcs_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html""&gt;resolve_chunk.icdo.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html""&gt;chunkresolve_icdo_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html""&gt;resolve_chunk.loinc&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html""&gt;chunkresolve_loinc_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html""&gt;resolve_chunk.rxnorm.cd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html""&gt;chunkresolve_rxnorm_cd_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.rxnorm.in&lt;/td&gt;
&lt;td&gt;chunkresolve_rxnorm_in_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.rxnorm.in_healthcare&lt;/td&gt;
&lt;td&gt;chunkresolve_rxnorm_in_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html""&gt;resolve_chunk.rxnorm.sbd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html""&gt;chunkresolve_rxnorm_sbd_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html""&gt;resolve_chunk.rxnorm.scd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html""&gt;chunkresolve_rxnorm_scd_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.rxnorm.scdc&lt;/td&gt;
&lt;td&gt;chunkresolve_rxnorm_scdc_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.rxnorm.scdc_healthcare&lt;/td&gt;
&lt;td&gt;chunkresolve_rxnorm_scdc_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html""&gt;resolve_chunk.rxnorm.xsmall.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html""&gt;chunkresolve_rxnorm_xsmall_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html""&gt;resolve_chunk.snomed.findings&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html""&gt;chunkresolve_snomed_findings_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;New Classifiers&lt;/h1&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;classify.icd10.clinical&lt;/td&gt;
&lt;td&gt;classifier_icd10cm_hcc_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;classify.icd10.healthcare&lt;/td&gt;
&lt;td&gt;classifier_icd10cm_hcc_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html""&gt;classify.ade.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html""&gt;classifierdl_ade_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html""&gt;classify.ade.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html""&gt;classifierdl_ade_clinicalbert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html""&gt;classify.ade.conversational&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html""&gt;classifierdl_ade_conversational_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html""&gt;classify.gender.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html""&gt;classifierdl_gender_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html""&gt;classify.gender.sbert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html""&gt;classifierdl_gender_sbert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;classify.pico&lt;/td&gt;
&lt;td&gt;classifierdl_pico_biobert&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;German Medical models&lt;/h1&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;[embed]&lt;/td&gt;
&lt;td&gt;w2v_cc_300d&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[embed.w2v]&lt;/td&gt;
&lt;td&gt;w2v_cc_300d&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[resolve_chunk]&lt;/td&gt;
&lt;td&gt;chunkresolve_ICD10GM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[resolve_chunk.icd10gm]&lt;/td&gt;
&lt;td&gt;chunkresolve_ICD10GM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;resolve_chunk.icd10gm.2021&lt;/td&gt;
&lt;td&gt;chunkresolve_ICD10GM_2021&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;med_ner.legal&lt;/td&gt;
&lt;td&gt;ner_legal&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;med_ner&lt;/td&gt;
&lt;td&gt;ner_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;med_ner.healthcare&lt;/td&gt;
&lt;td&gt;ner_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;med_ner.healthcare_slim&lt;/td&gt;
&lt;td&gt;ner_healthcare_slim&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;med_ner.traffic&lt;/td&gt;
&lt;td&gt;ner_traffic&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;Spanish Medical models&lt;/h1&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html""&gt;embed.scielo.150d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html""&gt;embeddings_scielo_150d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html""&gt;embed.scielo.300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html""&gt;embeddings_scielo_300d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html""&gt;embed.scielo.50d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html""&gt;embeddings_scielo_50d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html""&gt;embed.scielowiki.150d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html""&gt;embeddings_scielowiki_150d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html""&gt;embed.scielowiki.300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html""&gt;embeddings_scielowiki_300d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html""&gt;embed.scielowiki.50d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html""&gt;embeddings_scielowiki_50d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html""&gt;embed.sciwiki.150d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html""&gt;embeddings_sciwiki_150d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html""&gt;embed.sciwiki.300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html""&gt;embeddings_sciwiki_300d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html""&gt;embed.sciwiki.50d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html""&gt;embeddings_sciwiki_50d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html""&gt;med_ner&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html""&gt;ner_diag_proc&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html""&gt;med_ner.neoplasm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html""&gt;ner_neoplasms&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html""&gt;med_ner.diag_proc&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html""&gt;ner_diag_proc&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;GPU Mode&lt;/h1&gt;

&lt;p&gt;You can now enable NLU GPU mode by setting &lt;code&gt;gpu=true&lt;/code&gt; while loading a model. I.e. &lt;code&gt;nlu.load(&amp;#39;train.sentiment&amp;#39; gpu=True)&lt;/code&gt; . If must resart you kernel, if you already loaded a nlu pipeline withouth GPU mode.&lt;/p&gt;

&lt;h1&gt;Output Level Relation&lt;/h1&gt;

&lt;p&gt;This new output level is used for relation extractors and will give you 1 row per relation extracted.&lt;/p&gt;

&lt;h1&gt;Bug fixes&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Fixed a bug that caused loading NLU models in offline mode not to work in some occasions&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;1 line Install NLU&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash&lt;/code&gt;&lt;/p&gt;

&lt;h1&gt;Install via PIP&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;! pip install nlu pyspark==3.0.1&lt;/code&gt;&lt;/p&gt;

&lt;h2&gt;Additional NLU ressources&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/""&gt;NLU Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/docs/en/notebooks""&gt;All NLU Tutorial Notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlp.johnsnowlabs.com/learn#pythons-nlu-library""&gt;NLU Videos and Blogposts on NLU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu""&gt;NLU on Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://join.slack.com/t/spark-nlp/shared_invite/zt-lutct9gm-kuUazcyFKhuGY3_0AMkxqA""&gt;Suggestions or Questions? Contact us in Slack!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n5guxs,True,,CKL-IT,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n5guxs/200_state_of_the_art_medical_models_for_ner/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n5guxs/200_state_of_the_art_medical_models_for_ner/,30199,1620223963.0,0,,False,,,,,,38256
242,,LanguageTechnology,"Hello there,

im currently doing my research for my bachelor thesis.  
What is a nice technique to extract keywords from one or multiple documents. Currently i've tried TF-IDF, RAKE and YAKE. The hard part somehow is to find the Least Common Multiple of the documents.

Could someone help me out?

&amp;#x200B;

appreciate it",t2_8it6p6no,False,,0,False,Q: Multi-Document Keyword Extraction,[],r/LanguageTechnology,False,6,,0,,False,t3_n5e67a,False,dark,0.99,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1620244623.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello there,&lt;/p&gt;

&lt;p&gt;im currently doing my research for my bachelor thesis.&lt;br/&gt;
What is a nice technique to extract keywords from one or multiple documents. Currently i&amp;#39;ve tried TF-IDF, RAKE and YAKE. The hard part somehow is to find the Least Common Multiple of the documents.&lt;/p&gt;

&lt;p&gt;Could someone help me out?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;appreciate it&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n5e67a,True,,Logical-Necessary524,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n5e67a/q_multidocument_keyword_extraction/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n5e67a/q_multidocument_keyword_extraction/,30199,1620215823.0,0,,False,,,,,,325
243,,LanguageTechnology,"Hello NLP redditors.

I'm currently working on a project where I need to find the similarity between sentences. At the moment I'm using cosine similarity with [fastText word embeddings](https://fasttext.cc/docs/en/pretrained-vectors.html) and TF-IDF weights. On single languages results are pretty good so far, my boss is happy.

The next step is to solve the same problem with multiple languages. My idea is to use the same technology but with [aligned fastText word embeddings](https://fasttext.cc/docs/en/aligned-vectors.html). In theory, using vocabularies with aligned word vectors should mean that words with similar meanings (in different languages) have a similar vector. Thus, comparing ""the car runs fast"" and ""l'auto corre veloce"" (Italian) should bring a high cosine similarity.

In practice, results are disappointing. Even a comparison between simple particles such as ""yes"" and ""sì"", or even ""no"" and ""no"", bring a cosine similarity well below any acceptable threshold.

Is there anyone here that had some experience with aligned word vectors and can tell me if it's a lost war or there is something I can do to improve the results?

We currently train our own vocabularies on Wikipedia and other sources, and we align the vocabularies using [MUSE](https://github.com/facebookresearch/MUSE) with default settings (0-5000 dictionary for training, 5000-6500 dictionary for evaluation and 5 refinements).

Any help will be appreciated. Thanks in advance.

**Edit: Thanks all for the answers! Will take a look at each suggestion.**",t2_5pyizy4o,False,,0,False,Help with aligned word embeddings,[],r/LanguageTechnology,False,6,,0,,False,t3_n4wx6v,False,dark,0.95,,public,14,0,{},,False,[],,False,False,,{},,False,14,,False,False,,1620203830.0,,[],{},,True,,1620187863.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello NLP redditors.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m currently working on a project where I need to find the similarity between sentences. At the moment I&amp;#39;m using cosine similarity with &lt;a href=""https://fasttext.cc/docs/en/pretrained-vectors.html""&gt;fastText word embeddings&lt;/a&gt; and TF-IDF weights. On single languages results are pretty good so far, my boss is happy.&lt;/p&gt;

&lt;p&gt;The next step is to solve the same problem with multiple languages. My idea is to use the same technology but with &lt;a href=""https://fasttext.cc/docs/en/aligned-vectors.html""&gt;aligned fastText word embeddings&lt;/a&gt;. In theory, using vocabularies with aligned word vectors should mean that words with similar meanings (in different languages) have a similar vector. Thus, comparing &amp;quot;the car runs fast&amp;quot; and &amp;quot;l&amp;#39;auto corre veloce&amp;quot; (Italian) should bring a high cosine similarity.&lt;/p&gt;

&lt;p&gt;In practice, results are disappointing. Even a comparison between simple particles such as &amp;quot;yes&amp;quot; and &amp;quot;sì&amp;quot;, or even &amp;quot;no&amp;quot; and &amp;quot;no&amp;quot;, bring a cosine similarity well below any acceptable threshold.&lt;/p&gt;

&lt;p&gt;Is there anyone here that had some experience with aligned word vectors and can tell me if it&amp;#39;s a lost war or there is something I can do to improve the results?&lt;/p&gt;

&lt;p&gt;We currently train our own vocabularies on Wikipedia and other sources, and we align the vocabularies using &lt;a href=""https://github.com/facebookresearch/MUSE""&gt;MUSE&lt;/a&gt; with default settings (0-5000 dictionary for training, 5000-6500 dictionary for evaluation and 5 refinements).&lt;/p&gt;

&lt;p&gt;Any help will be appreciated. Thanks in advance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Edit: Thanks all for the answers! Will take a look at each suggestion.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n4wx6v,True,,Next-Initiative,,12,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n4wx6v/help_with_aligned_word_embeddings/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n4wx6v/help_with_aligned_word_embeddings/,30199,1620159063.0,0,,False,,,,,,1542
244,,LanguageTechnology,"Hi everyone! I have started here a blog for discussing various papers in the field of NLP/CompLing: https://selfassuredpaperreads.medium.com/
Do give it a read and let me know what you think!",t2_67r0a3tc,False,,0,False,Blog for papers in NLP/CompLing,[],r/LanguageTechnology,False,6,,0,,False,t3_n4nt8n,False,dark,0.9,,public,17,1,{},,False,[],,False,False,,{},,False,17,,False,False,,1620190043.0,,[],{},,True,,1620162169.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone! I have started here a blog for discussing various papers in the field of NLP/CompLing: &lt;a href=""https://selfassuredpaperreads.medium.com/""&gt;https://selfassuredpaperreads.medium.com/&lt;/a&gt;
Do give it a read and let me know what you think!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 1000, 'id': 'award_35c78e6e-507b-4f1d-b3d8-ed43840909a8', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 800, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_128.png', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'The treasure at the end of the rainbow. Gives the author 800 Coins to do with as they please.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': ""Pot o' Coins"", 'resized_static_icons': [{'url': 'https://external-preview.redd.it?width=16&amp;height=16&amp;auto=webp&amp;s=d88c9a453f8ac38850b7a8241cfe5804b7b4905d', 'width': 16, 'height': 16}, {'url': 'https://external-preview.redd.it?width=32&amp;height=32&amp;auto=webp&amp;s=96a25019eb75878bdec4f6c012540f3baffbb1b2', 'width': 32, 'height': 32}, {'url': 'https://external-preview.redd.it?width=48&amp;height=48&amp;auto=webp&amp;s=1a51d27d75afde3fbde8bba84f9338f511211461', 'width': 48, 'height': 48}, {'url': 'https://external-preview.redd.it?width=64&amp;height=64&amp;auto=webp&amp;s=96af5ec460b05669ed60224cb0619bb8884abe27', 'width': 64, 'height': 64}, {'url': 'https://external-preview.redd.it?width=128&amp;height=128&amp;auto=webp&amp;s=2d3e648ed2302e6258673051ca5291f57beb29d4', 'width': 128, 'height': 128}], 'icon_format': 'APNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n4nt8n,True,,patrickmelroseisi,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n4nt8n/blog_for_papers_in_nlpcompling/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n4nt8n/blog_for_papers_in_nlpcompling/,30199,1620133369.0,0,,False,,,,,,191
245,,LanguageTechnology,,t2_hkv9s,False,,0,False,EmbedRank: Simple Unsupervised Keyphrase Extraction using Sentence Embeddings,[],r/LanguageTechnology,False,6,,0,,False,t3_n4sbzu,False,dark,0.75,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,False,,1620173859.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n4sbzu,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n4sbzu/embedrank_simple_unsupervised_keyphrase/,all_ads,False,https://link.medium.com/YGm9rNXlZfb,30199,1620145059.0,0,,False,https://link.medium.com/YGm9rNXlZfb,,,,,0
246,,LanguageTechnology,In most standard applications of text classification - using algorithms like word2vec/glove/bert ... is text data automatically converted into vectors of numbers? Can you then use regression models right away once the text has been converted into numbers?,t2_3f0i9m72,False,,0,False,Text classification: words to numbers,[],r/LanguageTechnology,False,6,,0,,False,t3_n4v4bu,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1620183586.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In most standard applications of text classification - using algorithms like word2vec/glove/bert ... is text data automatically converted into vectors of numbers? Can you then use regression models right away once the text has been converted into numbers?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n4v4bu,True,,SQL_beginner,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n4v4bu/text_classification_words_to_numbers/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n4v4bu/text_classification_words_to_numbers/,30199,1620154786.0,0,,False,,,,,,255
247,,LanguageTechnology,"Hi everyone!  


I need to create a Bot that uses NLP with R!  


has anyone ever done this?  


Thank you so much!",t2_7fjpeklw,False,,0,False,Chatbot with R,[],r/LanguageTechnology,False,6,,0,,False,t3_n4p8ur,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1620166225.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone!  &lt;/p&gt;

&lt;p&gt;I need to create a Bot that uses NLP with R!  &lt;/p&gt;

&lt;p&gt;has anyone ever done this?  &lt;/p&gt;

&lt;p&gt;Thank you so much!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n4p8ur,True,,DanielaSMPereira,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n4p8ur/chatbot_with_r/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n4p8ur/chatbot_with_r/,30199,1620137425.0,0,,False,,,,,,115
248,,LanguageTechnology,"1. How is it possible to extract a single word (may be in textgrid) from the textgrid file of a sentence through a code in praat?

&amp;#x200B;

Suppose, from a long file I have put boundary and created one textgrid containing 4 tokens sentences containing the same key word life. The sentence is ""He loves life and laughter."" The tokens are:

&amp;#x200B;

S1\_life1-life, S1\_life2-life, S1\_life3-life, S1\_life4-life. I need to write a praat code that will separate the ""life"" from the sentence text grid.

&amp;#x200B;

2. Will the same code be applicable for the same function in a different sentence where the positioning of the keyword is different. For instance, ""Life has a different meaning in the mountains."" In here, they keyword is at the beginning of the sentence.",t2_8frpvpx5,False,,0,False,How is it possible to extract a single word (may be in textgrid) from the textgrid file of a sentence through a code in praat?,[],r/LanguageTechnology,False,6,,0,,False,t3_n4og06,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1620164044.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;ol&gt;
&lt;li&gt;How is it possible to extract a single word (may be in textgrid) from the textgrid file of a sentence through a code in praat?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Suppose, from a long file I have put boundary and created one textgrid containing 4 tokens sentences containing the same key word life. The sentence is &amp;quot;He loves life and laughter.&amp;quot; The tokens are:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;S1_life1-life, S1_life2-life, S1_life3-life, S1_life4-life. I need to write a praat code that will separate the &amp;quot;life&amp;quot; from the sentence text grid.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Will the same code be applicable for the same function in a different sentence where the positioning of the keyword is different. For instance, &amp;quot;Life has a different meaning in the mountains.&amp;quot; In here, they keyword is at the beginning of the sentence.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n4og06,True,,phonomonal,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n4og06/how_is_it_possible_to_extract_a_single_word_may/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n4og06/how_is_it_possible_to_extract_a_single_word_may/,30199,1620135244.0,0,,False,,,,,,779
249,,LanguageTechnology,"I have feeling that not many people are willing to admit - but ultimately, is a significant part of many data mining projects (e.g. checking data quality, parsing through data, etc.) still done manually? 

For example here is an example I just made up relating to Supervised NLP (Natural Language Processing) Classification : Suppose I have 1000 medical reports of patients, containing unstructured text made by a doctor during a hospital visit. For a given patient, each report contains all the text notes that the doctor made for that patient, for visits between 2010 and 2020. These reports make mention of the patients bio data (e.g. age, gender, medical history, etc.) and the details of the symptoms that the patient is experiencing over a long period of time (e.g. let's say that these reports are 2000 words on average). The problem is, different doctors have different styles of writing - each of these 1000 reports is different from another. If a human were to read the report, the human could figure out what happened to the patient - did the patient have a serious condition (let's call this ""class 1"") or a non-serious condition (let's call this ""class 0""). This is what we are interested in predicting for future patients based on the limited medical notes made by doctors for these future patients. 

The problem is - there is no clear and fast way (not that I know of) to take the 1000 medical reports that are available, and label each report as ""class 1"" or ""class 0"". For example, for ""class 0"" : one of the doctors could clearly write at the end of a report ""all medical tests were conducted and the results and were all negative"", and another doctor could end the report by saying ""the patient should seriously consider changing their lifestyle and eat healthier food. benign."" . 

In this example, how would someone assign labels to all these 1000 cases, without manually reading them and deciding if the information in the report corresponds to a ""serious condition"" or a ""non-serious condition""? I was thinking of using something like ""sentiment analysis"" to capture the ""mood"" of these reports, and use sentiment analysis a method to informally gauge if the tone of the report is ""dark"" (serious condition) or ""light"" (non serious condition). But I am not sure if this is the best way to approach this problem. Is there a way to do this without reading all the reports and manually deciding labels?

In the end - this is what I am interested in doing : suppose a new patient comes in and on the first visit, the doctor makes some quick notes (e.g. patient is male, 30 years old, 180 cm, 100 kg, non-smoker, frequently complains of chest pains, no high blood pressure, works a construction worker and takes daily medicine for acid reflex). Just based on these quick notes and the 1000 reports available (NOTE: I am trying to illustrate a point here, that the medical notes for the new patient and the 1000 reports DO NOT have the same format), can a researcher predict (supervised classification, e.g. decision tree) if this patient will have a ""serious"" or a ""non-serious"" condition?

PS: suppose the doctors have a very detailed medical encyclopedia on their computers - can this medical encyclopedia be used alongside the 1000 medical reports to improve the prediction results?",t2_xtuyc,False,,0,False,Inevitable Manual Work involved in NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_n4i0le,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1620140092.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have feeling that not many people are willing to admit - but ultimately, is a significant part of many data mining projects (e.g. checking data quality, parsing through data, etc.) still done manually? &lt;/p&gt;

&lt;p&gt;For example here is an example I just made up relating to Supervised NLP (Natural Language Processing) Classification : Suppose I have 1000 medical reports of patients, containing unstructured text made by a doctor during a hospital visit. For a given patient, each report contains all the text notes that the doctor made for that patient, for visits between 2010 and 2020. These reports make mention of the patients bio data (e.g. age, gender, medical history, etc.) and the details of the symptoms that the patient is experiencing over a long period of time (e.g. let&amp;#39;s say that these reports are 2000 words on average). The problem is, different doctors have different styles of writing - each of these 1000 reports is different from another. If a human were to read the report, the human could figure out what happened to the patient - did the patient have a serious condition (let&amp;#39;s call this &amp;quot;class 1&amp;quot;) or a non-serious condition (let&amp;#39;s call this &amp;quot;class 0&amp;quot;). This is what we are interested in predicting for future patients based on the limited medical notes made by doctors for these future patients. &lt;/p&gt;

&lt;p&gt;The problem is - there is no clear and fast way (not that I know of) to take the 1000 medical reports that are available, and label each report as &amp;quot;class 1&amp;quot; or &amp;quot;class 0&amp;quot;. For example, for &amp;quot;class 0&amp;quot; : one of the doctors could clearly write at the end of a report &amp;quot;all medical tests were conducted and the results and were all negative&amp;quot;, and another doctor could end the report by saying &amp;quot;the patient should seriously consider changing their lifestyle and eat healthier food. benign.&amp;quot; . &lt;/p&gt;

&lt;p&gt;In this example, how would someone assign labels to all these 1000 cases, without manually reading them and deciding if the information in the report corresponds to a &amp;quot;serious condition&amp;quot; or a &amp;quot;non-serious condition&amp;quot;? I was thinking of using something like &amp;quot;sentiment analysis&amp;quot; to capture the &amp;quot;mood&amp;quot; of these reports, and use sentiment analysis a method to informally gauge if the tone of the report is &amp;quot;dark&amp;quot; (serious condition) or &amp;quot;light&amp;quot; (non serious condition). But I am not sure if this is the best way to approach this problem. Is there a way to do this without reading all the reports and manually deciding labels?&lt;/p&gt;

&lt;p&gt;In the end - this is what I am interested in doing : suppose a new patient comes in and on the first visit, the doctor makes some quick notes (e.g. patient is male, 30 years old, 180 cm, 100 kg, non-smoker, frequently complains of chest pains, no high blood pressure, works a construction worker and takes daily medicine for acid reflex). Just based on these quick notes and the 1000 reports available (NOTE: I am trying to illustrate a point here, that the medical notes for the new patient and the 1000 reports DO NOT have the same format), can a researcher predict (supervised classification, e.g. decision tree) if this patient will have a &amp;quot;serious&amp;quot; or a &amp;quot;non-serious&amp;quot; condition?&lt;/p&gt;

&lt;p&gt;PS: suppose the doctors have a very detailed medical encyclopedia on their computers - can this medical encyclopedia be used alongside the 1000 medical reports to improve the prediction results?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n4i0le,True,,ottawalanguages,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n4i0le/inevitable_manual_work_involved_in_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n4i0le/inevitable_manual_work_involved_in_nlp/,30199,1620111292.0,0,,False,,,,,,3304
250,,LanguageTechnology,"I am trying to solve multi-class classification problem using BERT.

My training accuracy is way higher than the validation and test sets so I assume that is clearly overfitting.

&amp;#x200B;

This is the configuration for my BERT Classifier

    ""class_name"": ""bert_classifier"",
            ""n_classes"": 5,
            ""return_probas"": True,
            ""one_hot_labels"": True,
            ""bert_config_file"": ""/content/ru_conversational_cased_L-12_H-768_A-12/bert_config.json"",
            ""pretrained_bert"": ""/content/ru_conversational_cased_L-12_H-768_A-12/bert_model.ckpt"",
            ""save_path"": ""sst_bert_model/model"",
            ""load_path"": ""sst_bert_model/model"",
            ""keep_prob"": 0.5,
            ""learning_rate"": 1e-05,
            ""weight_decay_rate"": 0.001, #this line added after the overfitting
            ""learning_rate_drop_patience"": 5,
            ""learning_rate_drop_div"": 2.0,
            ""in"": [
              ""bert_features""
                ],
            ""in_y"": [
              ""y_onehot""
            ],
            ""out"": [
              ""y_pred_probas""
            ]

Here is my plan to overcome the issue of overfitting.

1. I added a little `more data` to the current one, (just a bit, at least something that I could find)
2. I added `""weight_decay_rate"": 0.001`, to basically regularize the weight of the model
3. I increased the `batch size to 32`. It was 16 before ( I don't know if that helps)
4. I set a `""learning_rate_drop_patience"": 2,` so it stops training after 2 epochs if there no is an improvement.

&amp;#x200B;

The last thing I wanna do is to increase my dropout

    DropOut = 0.5

&amp;#x200B;

**QUESTIONS:** 

1. However, I can't figure out which parameter is actually ""dropout"" as there is no parameter called directly dropout.  Mainly because the language I am targeting is Russian and I need to use Deep Pavlov [http://docs.deeppavlov.ai/en/master/apiref/models/bert.html](http://docs.deeppavlov.ai/en/master/apiref/models/bert.html) not the hugging face directly so Deep Pavlov Bert Classifier has only these 3 things that are kinda close to what I am looking for (`dropout function`)

&amp;#8203;

    keep_prob             -     dropout keep_prob for non-Bert layers
    attention_probs_keep_prob – keep_prob for Bert self-attention layers
    hidden_keep_prob –           keep_prob for Bert hidden layers

which one is the `Dropout`? which function I need to make 0.5 out of these three?

   2.    I have 3 `epochs` in total, is it correct that I am making `patience rate` (it stops training after 2 epochs if there is no improvement) ==  2, instead of 1? 

   3.    I am thinking to minimize the `learning rate` as well. Now it is currently `1e-05.`  Should I make it maybe 1e-3? 

   4. And overall, what else can you suggest to me to overcome the issue of overfitting? is the plan good enough to tackle this problem?",t2_6c0lef9b,False,,0,False,Overfitting,[],r/LanguageTechnology,False,6,,0,,False,t3_n4ei50,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,1620106976.0,,[],{},,True,,1620126051.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to solve multi-class classification problem using BERT.&lt;/p&gt;

&lt;p&gt;My training accuracy is way higher than the validation and test sets so I assume that is clearly overfitting.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;This is the configuration for my BERT Classifier&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;class_name&amp;quot;: &amp;quot;bert_classifier&amp;quot;,
        &amp;quot;n_classes&amp;quot;: 5,
        &amp;quot;return_probas&amp;quot;: True,
        &amp;quot;one_hot_labels&amp;quot;: True,
        &amp;quot;bert_config_file&amp;quot;: &amp;quot;/content/ru_conversational_cased_L-12_H-768_A-12/bert_config.json&amp;quot;,
        &amp;quot;pretrained_bert&amp;quot;: &amp;quot;/content/ru_conversational_cased_L-12_H-768_A-12/bert_model.ckpt&amp;quot;,
        &amp;quot;save_path&amp;quot;: &amp;quot;sst_bert_model/model&amp;quot;,
        &amp;quot;load_path&amp;quot;: &amp;quot;sst_bert_model/model&amp;quot;,
        &amp;quot;keep_prob&amp;quot;: 0.5,
        &amp;quot;learning_rate&amp;quot;: 1e-05,
        &amp;quot;weight_decay_rate&amp;quot;: 0.001, #this line added after the overfitting
        &amp;quot;learning_rate_drop_patience&amp;quot;: 5,
        &amp;quot;learning_rate_drop_div&amp;quot;: 2.0,
        &amp;quot;in&amp;quot;: [
          &amp;quot;bert_features&amp;quot;
            ],
        &amp;quot;in_y&amp;quot;: [
          &amp;quot;y_onehot&amp;quot;
        ],
        &amp;quot;out&amp;quot;: [
          &amp;quot;y_pred_probas&amp;quot;
        ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is my plan to overcome the issue of overfitting.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I added a little &lt;code&gt;more data&lt;/code&gt; to the current one, (just a bit, at least something that I could find)&lt;/li&gt;
&lt;li&gt;I added &lt;code&gt;&amp;quot;weight_decay_rate&amp;quot;: 0.001&lt;/code&gt;, to basically regularize the weight of the model&lt;/li&gt;
&lt;li&gt;I increased the &lt;code&gt;batch size to 32&lt;/code&gt;. It was 16 before ( I don&amp;#39;t know if that helps)&lt;/li&gt;
&lt;li&gt;I set a &lt;code&gt;&amp;quot;learning_rate_drop_patience&amp;quot;: 2,&lt;/code&gt; so it stops training after 2 epochs if there no is an improvement.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The last thing I wanna do is to increase my dropout&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DropOut = 0.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;QUESTIONS:&lt;/strong&gt; &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;However, I can&amp;#39;t figure out which parameter is actually &amp;quot;dropout&amp;quot; as there is no parameter called directly dropout.  Mainly because the language I am targeting is Russian and I need to use Deep Pavlov &lt;a href=""http://docs.deeppavlov.ai/en/master/apiref/models/bert.html""&gt;http://docs.deeppavlov.ai/en/master/apiref/models/bert.html&lt;/a&gt; not the hugging face directly so Deep Pavlov Bert Classifier has only these 3 things that are kinda close to what I am looking for (&lt;code&gt;dropout function&lt;/code&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;#8203;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;keep_prob             -     dropout keep_prob for non-Bert layers
attention_probs_keep_prob – keep_prob for Bert self-attention layers
hidden_keep_prob –           keep_prob for Bert hidden layers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which one is the &lt;code&gt;Dropout&lt;/code&gt;? which function I need to make 0.5 out of these three?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;I have 3 &lt;code&gt;epochs&lt;/code&gt; in total, is it correct that I am making &lt;code&gt;patience rate&lt;/code&gt; (it stops training after 2 epochs if there is no improvement) ==  2, instead of 1? &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I am thinking to minimize the &lt;code&gt;learning rate&lt;/code&gt; as well. Now it is currently &lt;code&gt;1e-05.&lt;/code&gt;  Should I make it maybe 1e-3? &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;And overall, what else can you suggest to me to overcome the issue of overfitting? is the plan good enough to tackle this problem?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n4ei50,True,,strangeguy111,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n4ei50/overfitting/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n4ei50/overfitting/,30199,1620097251.0,0,,False,,,,,,2890
251,,LanguageTechnology,"In my last post [https://www.reddit.com/r/learnmachinelearning/comments/n4ejdp/text\_classification\_problem\_overfitting/](https://www.reddit.com/r/learnmachinelearning/comments/n4ejdp/text_classification_problem_overfitting/)  I already said that I am trying to solve the multiclass text classification problem using BERT.

&amp;#x200B;

After the first training which took 13 hours, I noticed that my model is overfitting. I did some changes to my model, and now I wanna test it out. I can't afford training the whole model everytime because each time it takes huge amount of time, so how can i quickly test it to see how my changes affected my model? Maybe I can test it on a smaller dataset? I heard this is a way of doing so, however, i am not sure if I should use my test set to do so? if yes, should I break my test into 3 pieces (train,test,val ?) again? My test set is not that big (around 3000 rows of data) and I am wondering if such a small data can do the job? 

&amp;#x200B;

I am new in this field, and this is the first me doing this, so it would be highly appreciated if anyone could guide me through it. What are the best practices that allow me to quickly test my model?",t2_6c0lef9b,False,,0,False,How to quickly test my model?,[],r/LanguageTechnology,False,6,,0,,False,t3_n4eocz,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1620126662.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In my last post &lt;a href=""https://www.reddit.com/r/learnmachinelearning/comments/n4ejdp/text_classification_problem_overfitting/""&gt;https://www.reddit.com/r/learnmachinelearning/comments/n4ejdp/text_classification_problem_overfitting/&lt;/a&gt;  I already said that I am trying to solve the multiclass text classification problem using BERT.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;After the first training which took 13 hours, I noticed that my model is overfitting. I did some changes to my model, and now I wanna test it out. I can&amp;#39;t afford training the whole model everytime because each time it takes huge amount of time, so how can i quickly test it to see how my changes affected my model? Maybe I can test it on a smaller dataset? I heard this is a way of doing so, however, i am not sure if I should use my test set to do so? if yes, should I break my test into 3 pieces (train,test,val ?) again? My test set is not that big (around 3000 rows of data) and I am wondering if such a small data can do the job? &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I am new in this field, and this is the first me doing this, so it would be highly appreciated if anyone could guide me through it. What are the best practices that allow me to quickly test my model?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n4eocz,True,,strangeguy111,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n4eocz/how_to_quickly_test_my_model/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n4eocz/how_to_quickly_test_my_model/,30199,1620097862.0,0,,False,,,,,,1190
252,,LanguageTechnology,"Hello,

I am a undergraduate student writing my final thesis right now. I am exploring links between different social media platforms in regards to sentiment. How does Sentiment „flow“ from thematically and temporally near posts. 

One of the platforms that I explore is 4chan and I am having a lot of problems with the special vocabulary there. Most resources I can find solve twitter specific problems where a character limit is involved. The lack of such a limit presents me a plethora of variations of the same token. Elongated words and written out laughter with typos really messes up my work. 

Additionally I can‘t correct typos easily because there are many special words that aren‘t written in any dictionary. Some of these I could identify through urban dictionary, but many others are falsely corrected by a spellchecker. 

With the FastText-Algorithm I could identify a couple hundred tokens and normalize them, but I suspect there is a bunch more. Right now I am reading Dr. Farrell‘s work on jargon detection: http://oro.open.ac.uk/70529/

I wonder if you guys could give me some pointers. Thank you very much!",t2_6oph4mve,False,,0,False,Jargon Detection and Normalization,[],r/LanguageTechnology,False,6,,0,,False,t3_n3u6uh,False,dark,0.87,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,True,,1620071959.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am a undergraduate student writing my final thesis right now. I am exploring links between different social media platforms in regards to sentiment. How does Sentiment „flow“ from thematically and temporally near posts. &lt;/p&gt;

&lt;p&gt;One of the platforms that I explore is 4chan and I am having a lot of problems with the special vocabulary there. Most resources I can find solve twitter specific problems where a character limit is involved. The lack of such a limit presents me a plethora of variations of the same token. Elongated words and written out laughter with typos really messes up my work. &lt;/p&gt;

&lt;p&gt;Additionally I can‘t correct typos easily because there are many special words that aren‘t written in any dictionary. Some of these I could identify through urban dictionary, but many others are falsely corrected by a spellchecker. &lt;/p&gt;

&lt;p&gt;With the FastText-Algorithm I could identify a couple hundred tokens and normalize them, but I suspect there is a bunch more. Right now I am reading Dr. Farrell‘s work on jargon detection: &lt;a href=""http://oro.open.ac.uk/70529/""&gt;http://oro.open.ac.uk/70529/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I wonder if you guys could give me some pointers. Thank you very much!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n3u6uh,True,,gamboty,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n3u6uh/jargon_detection_and_normalization/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n3u6uh/jargon_detection_and_normalization/,30199,1620043159.0,0,,False,,,,,,1125
253,,LanguageTechnology,"I  am trying to find a software that could tell   
\-if the letter ""y"" in a word is a vowel or a consonant.  
\-Or if ""ti"" should be read as ""sh""

I found multiple tool that return a list of phoneme but none that tell me which letter in the original word match each phoneme (an alignment).  
I assume this is doable because this is essentially what speech-to-text tool are doing.  


But I would like a tool that give me a list of matching pair (grapheme/phoneme) so I display the annotation on the the correct range of letter in the original word.",t2_38k7g,False,,0,False,Any software that can annotate (grapheme/phonogram) in a word with the matching phoneme?,[],r/LanguageTechnology,False,6,,0,,False,t3_n458pt,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620099541.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I  am trying to find a software that could tell&lt;br/&gt;
-if the letter &amp;quot;y&amp;quot; in a word is a vowel or a consonant.&lt;br/&gt;
-Or if &amp;quot;ti&amp;quot; should be read as &amp;quot;sh&amp;quot;&lt;/p&gt;

&lt;p&gt;I found multiple tool that return a list of phoneme but none that tell me which letter in the original word match each phoneme (an alignment).&lt;br/&gt;
I assume this is doable because this is essentially what speech-to-text tool are doing.  &lt;/p&gt;

&lt;p&gt;But I would like a tool that give me a list of matching pair (grapheme/phoneme) so I display the annotation on the the correct range of letter in the original word.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n458pt,True,,skyde,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n458pt/any_software_that_can_annotate_graphemephonogram/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n458pt/any_software_that_can_annotate_graphemephonogram/,30199,1620070741.0,0,,False,,,,,,548
254,,LanguageTechnology,"I am learning about NLP and trying to understand how to tokenize text and remove stop words.

I tried the following line of code (from ""quanteada) and got the following error:

first_method &lt;- tokens(tdm) %&gt;% tokens_remove(stopwords(""en""), pad = TRUE)

Error: tokens() only works on character, corpus, list, tokens objects.

Has anyone ever gotten this error before?

I posted the full details to my question over here: https://stackoverflow.com/questions/67376045/r-error-only-works-with-character-objects

Thanks",t2_o4xj9,False,,0,False,Stop Word and Tokenization (with R),[],r/LanguageTechnology,False,6,,0,,False,t3_n494bh,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1620109386.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am learning about NLP and trying to understand how to tokenize text and remove stop words.&lt;/p&gt;

&lt;p&gt;I tried the following line of code (from &amp;quot;quanteada) and got the following error:&lt;/p&gt;

&lt;p&gt;first_method &amp;lt;- tokens(tdm) %&amp;gt;% tokens_remove(stopwords(&amp;quot;en&amp;quot;), pad = TRUE)&lt;/p&gt;

&lt;p&gt;Error: tokens() only works on character, corpus, list, tokens objects.&lt;/p&gt;

&lt;p&gt;Has anyone ever gotten this error before?&lt;/p&gt;

&lt;p&gt;I posted the full details to my question over here: &lt;a href=""https://stackoverflow.com/questions/67376045/r-error-only-works-with-character-objects""&gt;https://stackoverflow.com/questions/67376045/r-error-only-works-with-character-objects&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n494bh,True,,blueest,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n494bh/stop_word_and_tokenization_with_r/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n494bh/stop_word_and_tokenization_with_r/,30199,1620080586.0,0,,False,,,,,True,520
255,,LanguageTechnology,,t2_97mkm0fp,False,,0,False,The giant leaps in language technology -- and who's left behind,[],r/LanguageTechnology,False,6,,0,,False,t3_n3e5en,False,dark,0.81,,public,13,0,{},,False,[],,False,False,,{},,False,13,,False,True,,False,,[],{},,False,,1620013241.0,text,6,,,text,ted.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n3e5en,True,,Madame_President_,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n3e5en/the_giant_leaps_in_language_technology_and_whos/,all_ads,False,https://www.ted.com/talks/kalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind?utm_campaign=tedspread&amp;utm_medium=referral&amp;utm_source=tedcomshare,30199,1619984441.0,0,,False,https://www.ted.com/talks/kalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind?utm_campaign=tedspread&amp;utm_medium=referral&amp;utm_source=tedcomshare,"[{'approved_at_utc': None, 'subreddit': 'AskWomenOfColorOver30', 'selftext': '', 'author_fullname': 't2_97mkm0fp', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ""The giant leaps in language technology -- and who's left behind"", 'link_flair_richtext': [{'e': 'text', 't': 'ARTS &amp; CULTURE'}], 'subreddit_name_prefixed': 'r/AskWomenOfColorOver30', 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_n2wqof', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.83, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {'content': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fembed.ted.com%2Ftalks%2Fkalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind&amp;display_name=TED&amp;url=https%3A%2F%2Fwww.ted.com%2Ftalks%2Fkalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind&amp;image=https%3A%2F%2Fpi.tedcdn.com%2Fr%2Ftalkstar-photos.s3.amazonaws.com%2Fuploads%2Ff546d184-50e3-4226-bea6-ae8c09647d0b%2FKalikaBali_2020X-embed.jpg%3Fh%3D316%26w%3D560&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=ted"" width=""560"" height=""316"" scrolling=""no"" title=""TED embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'width': 560, 'scrolling': False, 'height': 316}, 'author_flair_template_id': '00bbbe72-36a9-11eb-8a5e-0e573b3e90cb', 'is_original_content': False, 'user_reports': [], 'secure_media': {'type': 'ted.com', 'oembed': {'provider_url': 'https://www.ted.com', 'description': 'Thousands of languages thrive across the globe, yet modern speech technology -- with all of its benefits -- supports just over a hundred. Computational linguist Kalika Bali dreams of a day when technology acts as a bridge instead of a barrier, working passionately to build new and inclusive systems for the millions who speak low-resource languages.', 'title': ""Kalika Bali: The giant leaps in language technology -- and who's left behind"", 'url': 'https://www.ted.com/talks/kalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind', 'type': 'video', 'author_name': 'Kalika Bali', 'height': 316, 'width': 560, 'html': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fembed.ted.com%2Ftalks%2Fkalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind&amp;display_name=TED&amp;url=https%3A%2F%2Fwww.ted.com%2Ftalks%2Fkalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind&amp;image=https%3A%2F%2Fpi.tedcdn.com%2Fr%2Ftalkstar-photos.s3.amazonaws.com%2Fuploads%2Ff546d184-50e3-4226-bea6-ae8c09647d0b%2FKalikaBali_2020X-embed.jpg%3Fh%3D316%26w%3D560&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=ted"" width=""560"" height=""316"" scrolling=""no"" title=""TED embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'thumbnail_width': 560, 'version': '1.0', 'provider_name': 'TED', 'cache_age': 300, 'thumbnail_url': 'https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/f546d184-50e3-4226-bea6-ae8c09647d0b/KalikaBali_2020X-embed.jpg?h=316&amp;w=560', 'thumbnail_height': 315, 'author_url': 'https://www.ted.com/speakers/kalika_bali'}}, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {'content': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fembed.ted.com%2Ftalks%2Fkalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind&amp;display_name=TED&amp;url=https%3A%2F%2Fwww.ted.com%2Ftalks%2Fkalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind&amp;image=https%3A%2F%2Fpi.tedcdn.com%2Fr%2Ftalkstar-photos.s3.amazonaws.com%2Fuploads%2Ff546d184-50e3-4226-bea6-ae8c09647d0b%2FKalikaBali_2020X-embed.jpg%3Fh%3D316%26w%3D560&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=ted"" width=""560"" height=""316"" scrolling=""no"" title=""TED embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'width': 560, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/n2wqof', 'height': 316}, 'link_flair_text': 'ARTS &amp; CULTURE', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': True, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [{'e': 'text', 't': 'Identifies as a WOC over 30'}], 'gildings': {}, 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1619949621.0, 'link_flair_type': 'richtext', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'richtext', 'domain': 'ted.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': 'new', 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.ted.com/talks/kalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind?utm_campaign=tedspread&amp;utm_medium=referral&amp;utm_source=tedcomshare', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'e6c4278a-3b39-11eb-8025-0e048a754485', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Identifies as a WOC over 30', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_3ihx1z', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0266b3', 'id': 'n2wqof', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Madame_President_', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/AskWomenOfColorOver30/comments/n2wqof/the_giant_leaps_in_language_technology_and_whos/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.ted.com/talks/kalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind?utm_campaign=tedspread&amp;utm_medium=referral&amp;utm_source=tedcomshare', 'subreddit_subscribers': 702, 'created_utc': 1619920821.0, 'num_crossposts': 1, 'media': {'type': 'ted.com', 'oembed': {'provider_url': 'https://www.ted.com', 'description': 'Thousands of languages thrive across the globe, yet modern speech technology -- with all of its benefits -- supports just over a hundred. Computational linguist Kalika Bali dreams of a day when technology acts as a bridge instead of a barrier, working passionately to build new and inclusive systems for the millions who speak low-resource languages.', 'title': ""Kalika Bali: The giant leaps in language technology -- and who's left behind"", 'url': 'https://www.ted.com/talks/kalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind', 'type': 'video', 'author_name': 'Kalika Bali', 'height': 316, 'width': 560, 'html': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fembed.ted.com%2Ftalks%2Fkalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind&amp;display_name=TED&amp;url=https%3A%2F%2Fwww.ted.com%2Ftalks%2Fkalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind&amp;image=https%3A%2F%2Fpi.tedcdn.com%2Fr%2Ftalkstar-photos.s3.amazonaws.com%2Fuploads%2Ff546d184-50e3-4226-bea6-ae8c09647d0b%2FKalikaBali_2020X-embed.jpg%3Fh%3D316%26w%3D560&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=ted"" width=""560"" height=""316"" scrolling=""no"" title=""TED embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'thumbnail_width': 560, 'version': '1.0', 'provider_name': 'TED', 'cache_age': 300, 'thumbnail_url': 'https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/f546d184-50e3-4226-bea6-ae8c09647d0b/KalikaBali_2020X-embed.jpg?h=316&amp;w=560', 'thumbnail_height': 315, 'author_url': 'https://www.ted.com/speakers/kalika_bali'}}, 'is_video': False}]",t3_n2wqof,,,0
256,,LanguageTechnology,,t2_hkv9s,False,,0,False,LINE: Large-scale Information Network Embedding (Machine Learning with Graphs),[],r/LanguageTechnology,False,6,,0,,False,t3_n31jh9,False,dark,0.88,,public,20,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/VuqvD3qp76M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'LINE: Large-scale Information Network Embedding (Machine Learning with Graphs)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/VuqvD3qp76M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/VuqvD3qp76M/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/VuqvD3qp76M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/n31jh9', 'height': 200}",,False,20,,False,False,,False,,[],{},,False,,1619970053.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n31jh9,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n31jh9/line_largescale_information_network_embedding/,all_ads,False,https://youtu.be/VuqvD3qp76M,30199,1619941253.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'LINE: Large-scale Information Network Embedding (Machine Learning with Graphs)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/VuqvD3qp76M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/VuqvD3qp76M/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/VuqvD3qp76M,,,,,0
257,,LanguageTechnology,,t2_5fsp2x6v,False,,0,False,CrossWeigh: Training NER with Imperfect Annotations | Research Papers Summary 016,[],r/LanguageTechnology,False,6,,0,,False,t3_n3d0kn,False,dark,1.0,,public,1,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/IXrwYWgnijQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'CrossWeigh: Training NER with Imperfect Annotations | Research Papers Summary 016', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/IXrwYWgnijQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/IXrwYWgnijQ/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/IXrwYWgnijQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/n3d0kn', 'height': 200}",,False,1,,False,False,,False,,[],{},,False,,1620010184.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n3d0kn,True,,RyanAI100,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n3d0kn/crossweigh_training_ner_with_imperfect/,all_ads,False,https://youtu.be/IXrwYWgnijQ,30199,1619981384.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'CrossWeigh: Training NER with Imperfect Annotations | Research Papers Summary 016', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/IXrwYWgnijQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/IXrwYWgnijQ/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,https://youtu.be/IXrwYWgnijQ,,,,,0
258,,LanguageTechnology,"Hello,
I'm new to this field.
What model would you recommend for creating single sentences? I'm looking for something a bit more advanced than a markov chain. I would like to have it choose a random word using a word probability distribution, and then give probability distributions for generating the next and previous words, repeating this until the start and end of a sentence.",t2_4irxns4h,False,,0,False,What model to create a sentence generator?,[],r/LanguageTechnology,False,6,,0,,False,t3_n2quai,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1619930124.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,
I&amp;#39;m new to this field.
What model would you recommend for creating single sentences? I&amp;#39;m looking for something a bit more advanced than a markov chain. I would like to have it choose a random word using a word probability distribution, and then give probability distributions for generating the next and previous words, repeating this until the start and end of a sentence.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n2quai,True,,qlpxumni,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n2quai/what_model_to_create_a_sentence_generator/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n2quai/what_model_to_create_a_sentence_generator/,30199,1619901324.0,0,,False,,,,,,380
259,,LanguageTechnology,,t2_apv7o2y8,False,,0,False,"Clinical Natural Language Processing – Challenges, Tasks and Datasets #NLProc",[],r/LanguageTechnology,False,6,,0,,False,t3_n2dl4b,False,dark,0.88,,public,13,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/yrN8VqkCWWc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Clinical Natural Language Processing – Challenges, Tasks and Datasets #NLProc', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/yrN8VqkCWWc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/yrN8VqkCWWc/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCPAfv1GeraR1WM0608He6ow'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/yrN8VqkCWWc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/n2dl4b', 'height': 200}",,False,13,,False,False,,False,,[],{},,False,,1619883256.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n2dl4b,True,,rsree123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n2dl4b/clinical_natural_language_processing_challenges/,all_ads,False,https://youtu.be/yrN8VqkCWWc,30199,1619854456.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Clinical Natural Language Processing – Challenges, Tasks and Datasets #NLProc', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/yrN8VqkCWWc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/yrN8VqkCWWc/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCPAfv1GeraR1WM0608He6ow'}}",False,https://youtu.be/yrN8VqkCWWc,,,,,0
260,,LanguageTechnology,"I have a collection of order forms from different manufacturers. Each manufacturer will generally use the same standardized order form but there is some variance where for whatever reason, a manufacturer might have 5-6 different order forms that they use.

Since each document contains a ton of legal language, I need to create a specific pipeline for each order form type, but I don't know how many different types there are in my corpus to begin with.

Can someone recommend a clustering algorithm to that I can use to figure out all the different document ""types"" that are in the corpus?

Thanks!",t2_bcdq12gz,False,,0,False,Advice on a clustering algorithm for a corpus of order forms that will sort documents into 'document' types without knowing how many different kinds there are.,[],r/LanguageTechnology,False,6,,0,,False,t3_n26mty,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1619855780.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a collection of order forms from different manufacturers. Each manufacturer will generally use the same standardized order form but there is some variance where for whatever reason, a manufacturer might have 5-6 different order forms that they use.&lt;/p&gt;

&lt;p&gt;Since each document contains a ton of legal language, I need to create a specific pipeline for each order form type, but I don&amp;#39;t know how many different types there are in my corpus to begin with.&lt;/p&gt;

&lt;p&gt;Can someone recommend a clustering algorithm to that I can use to figure out all the different document &amp;quot;types&amp;quot; that are in the corpus?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n26mty,True,,fastgoatboy,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n26mty/advice_on_a_clustering_algorithm_for_a_corpus_of/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n26mty/advice_on_a_clustering_algorithm_for_a_corpus_of/,30199,1619826980.0,0,,False,,,,,,599
261,,LanguageTechnology,"Hello everyone,

As part of my dissertation, I want to examine the general question of ""What do people talk about on stock market-related subreddits?"" To this end, I have identified around 200 subreddits that I would like to examine, and I plan to examine both the posts and comments to these subreddits. I am trying to decide the best and most efficient way of extracting the content of thesis subreddit discussions.

Some features of the data that I think are relevant to my decision:

1. Some subreddits are naturally focused around a certain topic or set of topics (e.g., r/technicalanalysis or r/thcinvesting) while others are more general (e.g., r/StockMarket).
2. The size of the subreddits (i.e., # of posts and comments) varies dramatically (e.g., r/wallstreetbets with millions of members vs r/stockanalysis with a couple of hundred members).
3. The size of my dataset is going to be extremely large (unsure of exact size yet, but just r/wallstreetbets is a large amount of data).

&amp;#x200B;

A couple of options that I am considering include LDA, author-topic LDA (with ""author"" being the subreddit) ([https://radimrehurek.com/gensim/models/atmodel.html](https://radimrehurek.com/gensim/models/atmodel.html)), and BERTopic ([https://github.com/MaartenGr/BERTopic](https://github.com/MaartenGr/BERTopic)).

I am also wondering whether I should manually group subreddits by category (e.g., technical analysis subreddits, fundamental analysis subreddits, penny stock subreddits, general subreddits, etc.) first and then run topic analysis separately for each group?

Any and all thoughts are greatly appreciated, and I am definitely open to hearing about alternative approaches or concerns that I haven't discussed. I am trying to weigh the pros and cons of each approach to make sure that my methodology is not obviously sub-optimal to some alternative.

Thanks in advance!",t2_1ch4r0dd,False,,0,False,topic modeling over many subreddits,[],r/LanguageTechnology,False,6,,0,,False,t3_n1x8vo,False,dark,0.78,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1619828058.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;

&lt;p&gt;As part of my dissertation, I want to examine the general question of &amp;quot;What do people talk about on stock market-related subreddits?&amp;quot; To this end, I have identified around 200 subreddits that I would like to examine, and I plan to examine both the posts and comments to these subreddits. I am trying to decide the best and most efficient way of extracting the content of thesis subreddit discussions.&lt;/p&gt;

&lt;p&gt;Some features of the data that I think are relevant to my decision:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Some subreddits are naturally focused around a certain topic or set of topics (e.g., &lt;a href=""/r/technicalanalysis""&gt;r/technicalanalysis&lt;/a&gt; or &lt;a href=""/r/thcinvesting""&gt;r/thcinvesting&lt;/a&gt;) while others are more general (e.g., &lt;a href=""/r/StockMarket""&gt;r/StockMarket&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;The size of the subreddits (i.e., # of posts and comments) varies dramatically (e.g., &lt;a href=""/r/wallstreetbets""&gt;r/wallstreetbets&lt;/a&gt; with millions of members vs &lt;a href=""/r/stockanalysis""&gt;r/stockanalysis&lt;/a&gt; with a couple of hundred members).&lt;/li&gt;
&lt;li&gt;The size of my dataset is going to be extremely large (unsure of exact size yet, but just &lt;a href=""/r/wallstreetbets""&gt;r/wallstreetbets&lt;/a&gt; is a large amount of data).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;A couple of options that I am considering include LDA, author-topic LDA (with &amp;quot;author&amp;quot; being the subreddit) (&lt;a href=""https://radimrehurek.com/gensim/models/atmodel.html""&gt;https://radimrehurek.com/gensim/models/atmodel.html&lt;/a&gt;), and BERTopic (&lt;a href=""https://github.com/MaartenGr/BERTopic""&gt;https://github.com/MaartenGr/BERTopic&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;I am also wondering whether I should manually group subreddits by category (e.g., technical analysis subreddits, fundamental analysis subreddits, penny stock subreddits, general subreddits, etc.) first and then run topic analysis separately for each group?&lt;/p&gt;

&lt;p&gt;Any and all thoughts are greatly appreciated, and I am definitely open to hearing about alternative approaches or concerns that I haven&amp;#39;t discussed. I am trying to weigh the pros and cons of each approach to make sure that my methodology is not obviously sub-optimal to some alternative.&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n1x8vo,True,,acctphd,,14,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n1x8vo/topic_modeling_over_many_subreddits/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n1x8vo/topic_modeling_over_many_subreddits/,30199,1619799258.0,0,,False,,,,,,1885
262,,LanguageTechnology,"Hi Everybody, 

Wondering if anyone has experience with Stanford's NLP project Chirpy Cardinal. Looking to find a freelance consultant that has experience with Chirpy architecture. Any suggestions on where I might find some good leads?

Thank you.",t2_buw4jm5s,False,,0,False,Stanford's Chirpy Cardinal,[],r/LanguageTechnology,False,6,,0,,False,t3_n1yt21,False,dark,0.76,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619832380.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi Everybody, &lt;/p&gt;

&lt;p&gt;Wondering if anyone has experience with Stanford&amp;#39;s NLP project Chirpy Cardinal. Looking to find a freelance consultant that has experience with Chirpy architecture. Any suggestions on where I might find some good leads?&lt;/p&gt;

&lt;p&gt;Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n1yt21,True,,Pro_Mgmt,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n1yt21/stanfords_chirpy_cardinal/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n1yt21/stanfords_chirpy_cardinal/,30199,1619803580.0,0,,False,,,,,,247
263,,LanguageTechnology,"I asked you guys about the opinion of choosing which domain to 

choose between NLP and CV few weeks ago.

&amp;#x200B;

I finally decided to study NLP area more deeply.

&amp;#x200B;

But I have concerns. It is really difficult to get accepted in 

NLP Labs and artificial intelligence graduate school.

&amp;#x200B;

So I made two plans if I failed to have master degree in graduate school.

&amp;#x200B;

Plan A

Studying NLP in data science graduate school

This plan have risk; there are no lab in this graduate school 

so I have to study NLP by myself.

&amp;#x200B;

Plan B

Studying CL(Computational Linguistics) in language graduate school

Also this plan have risk; the same reason with Plan A

&amp;#x200B;

There are not that many NLP labs in Korea, so I wonder if I go to

plan A and B, I can work in NLP field.

&amp;#x200B;

The best way is to read articles published by data and language graduate school, 

and NLP lab in AI graduate school and compare it, but the sad thing is

I don't have enough time left to contact labs and graduate schools above.

&amp;#x200B;

I guess that there are some people who graduated those graduate school 

that I mentioned, or at least worked with the people who went to Plan A and Plan B.

&amp;#x200B;

I am looking forward to have any advice from you guys.

I appreciate to the people who gave me advice last time when I asked about

the prospective domain to choose.

&amp;#x200B;

Thank you for reading my post and wish you guys all have good days today !",t2_bpsj0h63,False,,0,False,Concerns about studying NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_n1x2c6,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619827549.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I asked you guys about the opinion of choosing which domain to &lt;/p&gt;

&lt;p&gt;choose between NLP and CV few weeks ago.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I finally decided to study NLP area more deeply.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;But I have concerns. It is really difficult to get accepted in &lt;/p&gt;

&lt;p&gt;NLP Labs and artificial intelligence graduate school.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So I made two plans if I failed to have master degree in graduate school.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Plan A&lt;/p&gt;

&lt;p&gt;Studying NLP in data science graduate school&lt;/p&gt;

&lt;p&gt;This plan have risk; there are no lab in this graduate school &lt;/p&gt;

&lt;p&gt;so I have to study NLP by myself.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Plan B&lt;/p&gt;

&lt;p&gt;Studying CL(Computational Linguistics) in language graduate school&lt;/p&gt;

&lt;p&gt;Also this plan have risk; the same reason with Plan A&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;There are not that many NLP labs in Korea, so I wonder if I go to&lt;/p&gt;

&lt;p&gt;plan A and B, I can work in NLP field.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The best way is to read articles published by data and language graduate school, &lt;/p&gt;

&lt;p&gt;and NLP lab in AI graduate school and compare it, but the sad thing is&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t have enough time left to contact labs and graduate schools above.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I guess that there are some people who graduated those graduate school &lt;/p&gt;

&lt;p&gt;that I mentioned, or at least worked with the people who went to Plan A and Plan B.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I am looking forward to have any advice from you guys.&lt;/p&gt;

&lt;p&gt;I appreciate to the people who gave me advice last time when I asked about&lt;/p&gt;

&lt;p&gt;the prospective domain to choose.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thank you for reading my post and wish you guys all have good days today !&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n1x2c6,True,,Korean_Arabic_AI,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n1x2c6/concerns_about_studying_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n1x2c6/concerns_about_studying_nlp/,30199,1619798749.0,0,,False,,,,,,1512
264,,LanguageTechnology,"Hey NLPeople,

&amp;#x200B;

at the end of the year, I’m planning on writing my Master’s Thesis. My Master’s program is a combination of Computational Linguistics and Cognitive Science. I already know that I want to focus on the broad area of computational phonology.

Because we live in a productivity-driven dystopia apparently I’m already thinking about what would look good on my CV which I will embroider with the topic I will have spent a lot of time working on.

&amp;#x200B;

Do you guys have any general ideas for open research topics in phonologically driven speech recognition for example? Or would you nudge me towards literature in that field that comes to mind?

&amp;#x200B;

I’m really only looking for general inspiration right now.

&amp;#x200B;

Thanks a lot!",t2_12vsbb,False,,0,False,Master’s Thesis in Computational Phonology,[],r/LanguageTechnology,False,6,,0,,False,t3_n1yw5t,False,dark,0.6,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1619832615.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey NLPeople,&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;at the end of the year, I’m planning on writing my Master’s Thesis. My Master’s program is a combination of Computational Linguistics and Cognitive Science. I already know that I want to focus on the broad area of computational phonology.&lt;/p&gt;

&lt;p&gt;Because we live in a productivity-driven dystopia apparently I’m already thinking about what would look good on my CV which I will embroider with the topic I will have spent a lot of time working on.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Do you guys have any general ideas for open research topics in phonologically driven speech recognition for example? Or would you nudge me towards literature in that field that comes to mind?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I’m really only looking for general inspiration right now.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks a lot!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n1yw5t,True,,bronchialbalsam,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n1yw5t/masters_thesis_in_computational_phonology/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n1yw5t/masters_thesis_in_computational_phonology/,30199,1619803815.0,0,,False,,,,,,778
265,,LanguageTechnology,"Say I have a research paper (or papers) and I want to see how they  define a common term. For example, the papers might be on the topic of ""engagement"" and have definitions for the term they use in the paper. I was wondering if it would be possible to extract those definitions given the tools of NLP.

From some light research this seems to be a ""Terminology extraction"" problem. There are some papers on the topic, but my problem seems easier than that. I already know what to look for (e.g., the term ""engagement""), I just want to capture a definition of that from a text.

An idea I had was to search the text for the term I'm looking for (e.g., engagement) and then capture the text around it for further processing; For example, a sentence or two near the ""engagement"" term. But it's the additional processing I'm not sure about.

Any ideas? Thanks.",t2_h1axq,False,,0,False,Extracting term definitions from research papers,[],r/LanguageTechnology,False,6,,0,,False,t3_n1h5px,False,dark,1.0,,public,22,0,{},,False,[],,False,False,,{},,False,22,,False,False,,False,,[],{},,True,,1619767565.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Say I have a research paper (or papers) and I want to see how they  define a common term. For example, the papers might be on the topic of &amp;quot;engagement&amp;quot; and have definitions for the term they use in the paper. I was wondering if it would be possible to extract those definitions given the tools of NLP.&lt;/p&gt;

&lt;p&gt;From some light research this seems to be a &amp;quot;Terminology extraction&amp;quot; problem. There are some papers on the topic, but my problem seems easier than that. I already know what to look for (e.g., the term &amp;quot;engagement&amp;quot;), I just want to capture a definition of that from a text.&lt;/p&gt;

&lt;p&gt;An idea I had was to search the text for the term I&amp;#39;m looking for (e.g., engagement) and then capture the text around it for further processing; For example, a sentence or two near the &amp;quot;engagement&amp;quot; term. But it&amp;#39;s the additional processing I&amp;#39;m not sure about.&lt;/p&gt;

&lt;p&gt;Any ideas? Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n1h5px,True,,GetsTrimAPlenty,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n1h5px/extracting_term_definitions_from_research_papers/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n1h5px/extracting_term_definitions_from_research_papers/,30199,1619738765.0,0,,False,,,,,,855
266,,LanguageTechnology,"How do you deal with large text when using sentiment analysis? Even if SA is often used to analyze small pieces of text like tweets and reviews, it's often complex to encode a sentence because negative and positive phrases modify each other and end up to modify the overall sentiment of the review. This is even more true when dealing with texts made out of multiple sentences. A reasonable way to handle that would be to analyze the sentiment of each sentence and then get the average score for that text. Are there better ways to improve the accuracy of our analysis with large texts?
I found an article of Shocher and colleagues where they classify sentiment phrase-by-phrase  rather than on the sentence level. 

This is the paper
https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf",t2_b31us5mn,False,,0,False,Sentiment analysis of long text,[],r/LanguageTechnology,False,6,,0,,False,t3_n1gkh4,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619765802.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How do you deal with large text when using sentiment analysis? Even if SA is often used to analyze small pieces of text like tweets and reviews, it&amp;#39;s often complex to encode a sentence because negative and positive phrases modify each other and end up to modify the overall sentiment of the review. This is even more true when dealing with texts made out of multiple sentences. A reasonable way to handle that would be to analyze the sentiment of each sentence and then get the average score for that text. Are there better ways to improve the accuracy of our analysis with large texts?
I found an article of Shocher and colleagues where they classify sentiment phrase-by-phrase  rather than on the sentence level. &lt;/p&gt;

&lt;p&gt;This is the paper
&lt;a href=""https://nlp.stanford.edu/%7Esocherr/EMNLP2013_RNTN.pdf""&gt;https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n1gkh4,True,,Dr_Funkmachine,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n1gkh4/sentiment_analysis_of_long_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n1gkh4/sentiment_analysis_of_long_text/,30199,1619737002.0,0,,False,,,,,,787
267,,LanguageTechnology,"While LDA in Gensim supports both multiprocessing and distributed computation, the author-topic model implementation does not at the moment. Much of the infrastructure that allows both multiprocessing and distributed computation should however already be in place (class models.ldamulticore), as the model inherits it from LDA. Therefore, I was asking myself if these functionalities could be enabled also for atmodel without major issues",t2_9p25lyud,False,,0,False,Parallelization for Author-topic models (atmodel) in Gensim,[],r/LanguageTechnology,False,6,,0,,False,t3_n17myt,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1619741069.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;While LDA in Gensim supports both multiprocessing and distributed computation, the author-topic model implementation does not at the moment. Much of the infrastructure that allows both multiprocessing and distributed computation should however already be in place (class models.ldamulticore), as the model inherits it from LDA. Therefore, I was asking myself if these functionalities could be enabled also for atmodel without major issues&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n17myt,True,,Senior_Time_2928,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n17myt/parallelization_for_authortopic_models_atmodel_in/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n17myt/parallelization_for_authortopic_models_atmodel_in/,30199,1619712269.0,0,,False,,,,,,438
268,,LanguageTechnology,"My dataset is a little unbalanced:

    smile        4852 
    kind         2027 
    angry        1926 
    surprised     979 
    sad           698 

pretty same for the validation and test sets. My goal was to predict the emotion of user tweets, which I already did, however now I am wondering what are the best evaluation metrics for this type of problem?

My initial metrics score is as following

    {""train"": {""eval_examples_count"": 10481, ""metrics"": {""f1_weighted"": 0.8869, ""f1_macro"": 0.8401, ""accuracy"": 0.8884, ""roc_auc"": 0.9686}, ""time_spent"": ""1:27:10""}} {""valid"": {""eval_examples_count"": 3493, ""metrics"": {""f1_weighted"": 0.6112, ""f1_macro"": 0.5257, ""accuracy"": 0.6184, ""roc_auc"": 0.8177}, ""time_spent"": ""0:28:37""}} {""test"": {""eval_examples_count"": 3494, ""metrics"": {""f1_weighted"": 0.6191, ""f1_macro"": 0.5282, ""accuracy"": 0.6259, ""roc_auc"": 0.8271}, ""time_spent"": ""0:28:26""}} 

I am pretty new to ML and NLP in general so I am kinda confused. I have several questions here:

1. Why train f1\_macro is so high but valid and test sets are not that high? How to interpret it?
2. How to interpret the above result in general?
3. I've seen someone using Matthews Correlation Coefficient for the kinda similar task, but I heard that is only for the binary class problem, How can i use it for the multiclass problem?
4. How valuable roc\_auc information in a multi-class classification problem?

In general, even though metrics above are kinda giving me not really result, but i ve been testing the model manually and it just work pretty good.",t2_6c0lef9b,False,,0,False,Best evaluation metrics for the BERT model,[],r/LanguageTechnology,False,6,,0,,False,t3_n0yido,False,dark,1.0,,public,16,0,{},,False,[],,False,False,,{},,False,16,,False,False,,False,,[],{},,True,,1619706977.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My dataset is a little unbalanced:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;smile        4852 
kind         2027 
angry        1926 
surprised     979 
sad           698 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pretty same for the validation and test sets. My goal was to predict the emotion of user tweets, which I already did, however now I am wondering what are the best evaluation metrics for this type of problem?&lt;/p&gt;

&lt;p&gt;My initial metrics score is as following&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;train&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 10481, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.8869, &amp;quot;f1_macro&amp;quot;: 0.8401, &amp;quot;accuracy&amp;quot;: 0.8884, &amp;quot;roc_auc&amp;quot;: 0.9686}, &amp;quot;time_spent&amp;quot;: &amp;quot;1:27:10&amp;quot;}} {&amp;quot;valid&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 3493, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.6112, &amp;quot;f1_macro&amp;quot;: 0.5257, &amp;quot;accuracy&amp;quot;: 0.6184, &amp;quot;roc_auc&amp;quot;: 0.8177}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:28:37&amp;quot;}} {&amp;quot;test&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 3494, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.6191, &amp;quot;f1_macro&amp;quot;: 0.5282, &amp;quot;accuracy&amp;quot;: 0.6259, &amp;quot;roc_auc&amp;quot;: 0.8271}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:28:26&amp;quot;}} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I am pretty new to ML and NLP in general so I am kinda confused. I have several questions here:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Why train f1_macro is so high but valid and test sets are not that high? How to interpret it?&lt;/li&gt;
&lt;li&gt;How to interpret the above result in general?&lt;/li&gt;
&lt;li&gt;I&amp;#39;ve seen someone using Matthews Correlation Coefficient for the kinda similar task, but I heard that is only for the binary class problem, How can i use it for the multiclass problem?&lt;/li&gt;
&lt;li&gt;How valuable roc_auc information in a multi-class classification problem?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In general, even though metrics above are kinda giving me not really result, but i ve been testing the model manually and it just work pretty good.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n0yido,True,,strangeguy111,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n0yido/best_evaluation_metrics_for_the_bert_model/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n0yido/best_evaluation_metrics_for_the_bert_model/,30199,1619678177.0,0,,False,,,,,,1550
269,,LanguageTechnology,I am supposed to be doing a research paper(my first time) and thought nlp related topics would be good. Can you suggest some easy to understand research papers that I can refer to understand how this research paper thing actually works?,t2_6k07isk4,False,,0,False,What are some easy research topics for beginners related to nlp?,[],r/LanguageTechnology,False,6,,0,,False,t3_n1103z,False,dark,0.83,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1619718809.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am supposed to be doing a research paper(my first time) and thought nlp related topics would be good. Can you suggest some easy to understand research papers that I can refer to understand how this research paper thing actually works?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n1103z,True,,artsymin,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n1103z/what_are_some_easy_research_topics_for_beginners/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n1103z/what_are_some_easy_research_topics_for_beginners/,30199,1619690009.0,0,,False,,,,,,236
270,,LanguageTechnology,"I am working on the **CUAD**(Contract Understanding Atticus Dataset) which is a Q&amp;A based dataset. But training 80% of the dataset in one go is impossible due to resource constraints. I am using the boilerplate code provided by HuggingFace Transformer docs for Q&amp;A task [here](https://huggingface.co/transformers/examples.html). My hands are tied with Google Colab Pro. So, it's not possible for me to use multiple GPU's in training the dataset. Inspite of using the hyperparameters below, I'm unable to avoid errors due to memory constraints like ""CUDA out of Memory"" etc.

```
args = TrainingArguments(
    'cuad-roberta',
    evaluation_strategy = ""epoch"",
    learning_rate=3e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=2,
    weight_decay=0.01,
    save_steps=5000,
    logging_steps=5000,
    save_total_limit=100,
    gradient_accumulation_steps = 12,
    eval_accumulation_steps = 4,
)
```
Under these circumstances, I have divided my training set(80%) into 4 parts with each part holding 25% data. So, using any Q&amp;A supported pretrained model from Transformers, I've trained the first 25% of the training data and then saved the model in a directory of my drive. Then, I have loaded that tokenizer and model from the saved directory and trained the next 25% of my training data on the same model as shown below. 

```
tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/models/cuad-25%-roberta-base')
model = AutoModelForQuestionAnswering.from_pretrained('/content/drive/MyDrive/models/cuad-25%-roberta-base')
```

I repeated the step two more times to complete training the model on the entire training data.

Now, my question is that, **Is this approach correct in terms of training a model when I have resource constraints? If it is correct, will this approach hurt the performance of my model?** I'm relatively new to ML and NLP so please kindly consider any silly mistakes.

Also, any sources for understanding, visualising or implementing the Q&amp;A task through HuggingFace Transformers would be really helpful.",t2_3f58jdna,False,,0,False,Training a model on an entire dataset by dividing the dataset into chunks &amp; loading the model back again untill all chunks of the dataset are trained,[],r/LanguageTechnology,False,6,,0,,False,t3_n0xasz,False,dark,1.0,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,True,,1619701531.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am working on the &lt;strong&gt;CUAD&lt;/strong&gt;(Contract Understanding Atticus Dataset) which is a Q&amp;amp;A based dataset. But training 80% of the dataset in one go is impossible due to resource constraints. I am using the boilerplate code provided by HuggingFace Transformer docs for Q&amp;amp;A task &lt;a href=""https://huggingface.co/transformers/examples.html""&gt;here&lt;/a&gt;. My hands are tied with Google Colab Pro. So, it&amp;#39;s not possible for me to use multiple GPU&amp;#39;s in training the dataset. Inspite of using the hyperparameters below, I&amp;#39;m unable to avoid errors due to memory constraints like &amp;quot;CUDA out of Memory&amp;quot; etc.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
args = TrainingArguments(
    &amp;#39;cuad-roberta&amp;#39;,
    evaluation_strategy = &amp;quot;epoch&amp;quot;,
    learning_rate=3e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=2,
    weight_decay=0.01,
    save_steps=5000,
    logging_steps=5000,
    save_total_limit=100,
    gradient_accumulation_steps = 12,
    eval_accumulation_steps = 4,
)
&lt;/code&gt;
Under these circumstances, I have divided my training set(80%) into 4 parts with each part holding 25% data. So, using any Q&amp;amp;A supported pretrained model from Transformers, I&amp;#39;ve trained the first 25% of the training data and then saved the model in a directory of my drive. Then, I have loaded that tokenizer and model from the saved directory and trained the next 25% of my training data on the same model as shown below. &lt;/p&gt;

&lt;p&gt;&lt;code&gt;
tokenizer = AutoTokenizer.from_pretrained(&amp;#39;/content/drive/MyDrive/models/cuad-25%-roberta-base&amp;#39;)
model = AutoModelForQuestionAnswering.from_pretrained(&amp;#39;/content/drive/MyDrive/models/cuad-25%-roberta-base&amp;#39;)
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I repeated the step two more times to complete training the model on the entire training data.&lt;/p&gt;

&lt;p&gt;Now, my question is that, &lt;strong&gt;Is this approach correct in terms of training a model when I have resource constraints? If it is correct, will this approach hurt the performance of my model?&lt;/strong&gt; I&amp;#39;m relatively new to ML and NLP so please kindly consider any silly mistakes.&lt;/p&gt;

&lt;p&gt;Also, any sources for understanding, visualising or implementing the Q&amp;amp;A task through HuggingFace Transformers would be really helpful.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n0xasz,True,,MohammedRakib,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n0xasz/training_a_model_on_an_entire_dataset_by_dividing/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n0xasz/training_a_model_on_an_entire_dataset_by_dividing/,30199,1619672731.0,0,,False,,,,,,2104
271,,LanguageTechnology,"Dear all, I'm relatively new to NLP and ML. Currently I'm working on projects in which I have to compare and score two audio clips. Original clip (single sentence) will be from movie character (animated or real) and in second clip human will try to mimic it. I have to come up with my model to determine similarly and score human clip out of 5.
Factors which I have considered to be used are:
1. Getting sentence from speech and comparing with original
2. Evaluating similarly from audio spectrogram (spectral centroid and zero crossing rate)
3. Identification of emotion from speech and using emotions embedding (assuming it's available on internet) to measure similarly (will probably use cosine similarly)
I couldn't come up with more factors. Can you please help me come up with new comparison factors or suggest how can I approach this problem in better way?
Thanking you in anticipation...",t2_7n480jd4,False,,0,False,A model to evaluate audio clips similarly,[],r/LanguageTechnology,False,6,,0,,False,t3_n0x12t,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,1619709772.0,,[],{},,True,,1619700365.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Dear all, I&amp;#39;m relatively new to NLP and ML. Currently I&amp;#39;m working on projects in which I have to compare and score two audio clips. Original clip (single sentence) will be from movie character (animated or real) and in second clip human will try to mimic it. I have to come up with my model to determine similarly and score human clip out of 5.
Factors which I have considered to be used are:
1. Getting sentence from speech and comparing with original
2. Evaluating similarly from audio spectrogram (spectral centroid and zero crossing rate)
3. Identification of emotion from speech and using emotions embedding (assuming it&amp;#39;s available on internet) to measure similarly (will probably use cosine similarly)
I couldn&amp;#39;t come up with more factors. Can you please help me come up with new comparison factors or suggest how can I approach this problem in better way?
Thanking you in anticipation...&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n0x12t,True,,Drakshh,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n0x12t/a_model_to_evaluate_audio_clips_similarly/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n0x12t/a_model_to_evaluate_audio_clips_similarly/,30199,1619671565.0,0,,False,,,,,,895
272,,LanguageTechnology,"Hi everyone, 

I am in the hunt for open-domain datasets on Meronyms and Hypernyms. There are a few available online, but so far I've found only sets of entity pairs and **my research requires full sentences.** 

Any suggestion on where to search for datasets like this will be very much appreciated. Also if you happen to have one, please Dm me!

Thanks",t2_88cohh7w,False,,0,False,Looking for Datasets on Meronyms and Hypernyms - Full sentences,[],r/LanguageTechnology,False,6,,0,,False,t3_n0ntyi,False,dark,0.88,,public,6,1,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1619670018.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;

&lt;p&gt;I am in the hunt for open-domain datasets on Meronyms and Hypernyms. There are a few available online, but so far I&amp;#39;ve found only sets of entity pairs and &lt;strong&gt;my research requires full sentences.&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;Any suggestion on where to search for datasets like this will be very much appreciated. Also if you happen to have one, please Dm me!&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n0ntyi,True,,legendary_child,,11,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n0ntyi/looking_for_datasets_on_meronyms_and_hypernyms/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n0ntyi/looking_for_datasets_on_meronyms_and_hypernyms/,30199,1619641218.0,0,,False,,,,,,354
273,,LanguageTechnology,"Hi NLP lovers,

I found this exciting Teachable NLP Challenge! Is there anyone who wants to participate with me?

Teachable NLP Challenge is free and open to everyone interested in training their own AI without coding! All you need to be prepared for is good ideas and datasets.

* When: 05/05/2021 - 05/18/2021 11:59 EDT
* How: You just need to submit your AI model link and explanations on your AI (Good example: [https://forum.ainetwork.ai/c/ai-showcase/11](https://forum.ainetwork.ai/c/ai-showcase/11))
* Prizes: Apple Store gift cards, Winners' interviews will be broadcasted through AI Network Youtube Channel(1.48K subscribers)

To participate, submit your info via [https://forms.gle/XfUuNSS2heAn7JtH7](https://forms.gle/XfUuNSS2heAn7JtH7). You will receive an invitation email!

Check how Teachable NLP works: [https://forum.ainetwork.ai/t/teachable-nlp-how-to-use-teachable-nlp/65](https://forum.ainetwork.ai/t/teachable-nlp-how-to-use-teachable-nlp/65)Or watch a 1-minute tutorial video: [https://youtu.be/hzujZOT1qz8](https://youtu.be/hzujZOT1qz8)",t2_bsm6d0e4,False,,0,False,Call for Teachable NLP Challenge,[],r/LanguageTechnology,False,6,,0,,False,t3_n0fzcb,False,dark,0.86,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1619648734.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi NLP lovers,&lt;/p&gt;

&lt;p&gt;I found this exciting Teachable NLP Challenge! Is there anyone who wants to participate with me?&lt;/p&gt;

&lt;p&gt;Teachable NLP Challenge is free and open to everyone interested in training their own AI without coding! All you need to be prepared for is good ideas and datasets.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;When: 05/05/2021 - 05/18/2021 11:59 EDT&lt;/li&gt;
&lt;li&gt;How: You just need to submit your AI model link and explanations on your AI (Good example: &lt;a href=""https://forum.ainetwork.ai/c/ai-showcase/11""&gt;https://forum.ainetwork.ai/c/ai-showcase/11&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Prizes: Apple Store gift cards, Winners&amp;#39; interviews will be broadcasted through AI Network Youtube Channel(1.48K subscribers)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To participate, submit your info via &lt;a href=""https://forms.gle/XfUuNSS2heAn7JtH7""&gt;https://forms.gle/XfUuNSS2heAn7JtH7&lt;/a&gt;. You will receive an invitation email!&lt;/p&gt;

&lt;p&gt;Check how Teachable NLP works: &lt;a href=""https://forum.ainetwork.ai/t/teachable-nlp-how-to-use-teachable-nlp/65""&gt;https://forum.ainetwork.ai/t/teachable-nlp-how-to-use-teachable-nlp/65&lt;/a&gt;Or watch a 1-minute tutorial video: &lt;a href=""https://youtu.be/hzujZOT1qz8""&gt;https://youtu.be/hzujZOT1qz8&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n0fzcb,True,,Candid-Wishbone-692,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n0fzcb/call_for_teachable_nlp_challenge/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n0fzcb/call_for_teachable_nlp_challenge/,30199,1619619934.0,0,,False,,,,,,1059
274,,LanguageTechnology,"I’m building a classifier to predict whether or not a mechanical part is or is not automotive based on its description using a training dataset of approx. 13k labeled data points. After preprocessing, vectorizing, and TF-IDF the array is (12,918, 16,230). 

After training, I tried predicting on a new dataset of 173 descriptions. After preprocessing, vectorizing, and TF-IDF the new data array is (173, 492) and I’m getting a “dimension mismatch” error when passing it through my models’ predict function. 

Can anyone help with how to get the new unlabeled test data shaped to fit the training data?",t2_cb6se,False,,0,False,Classification dimension mismatch,[],r/LanguageTechnology,False,6,,0,,False,t3_n0lrrl,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1619664360.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m building a classifier to predict whether or not a mechanical part is or is not automotive based on its description using a training dataset of approx. 13k labeled data points. After preprocessing, vectorizing, and TF-IDF the array is (12,918, 16,230). &lt;/p&gt;

&lt;p&gt;After training, I tried predicting on a new dataset of 173 descriptions. After preprocessing, vectorizing, and TF-IDF the new data array is (173, 492) and I’m getting a “dimension mismatch” error when passing it through my models’ predict function. &lt;/p&gt;

&lt;p&gt;Can anyone help with how to get the new unlabeled test data shaped to fit the training data?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n0lrrl,True,,mikess314,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n0lrrl/classification_dimension_mismatch/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n0lrrl/classification_dimension_mismatch/,30199,1619635560.0,0,,False,,,,,,601
275,,LanguageTechnology,,t2_5sx1ux80,False,,0,False,3 Things I Learned About SPACs Using Knowledge Graphs,[],r/LanguageTechnology,False,6,,0,,False,t3_n0fx7y,False,dark,0.73,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,False,,1619648570.0,text,6,,,text,medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n0fx7y,True,,Glittering_Show_3414,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n0fx7y/3_things_i_learned_about_spacs_using_knowledge/,all_ads,False,https://medium.com/kgbase-blog/3-things-i-learned-about-spacs-using-knowledge-graphs-2ad0a7c12cf9,30199,1619619770.0,0,,False,https://medium.com/kgbase-blog/3-things-i-learned-about-spacs-using-knowledge-graphs-2ad0a7c12cf9,,,,,0
276,,LanguageTechnology,"Hi!

I have 2 separately trained word2vec models. I want to be able to compare the same word in both models and see how they relate to each other and their surroundings through a metric. Now of course, I can't use the cosine similarity as the word vectors are completely different. I also want to avoid using the `wv.most_similar()` method as I would like to effectively capture the similarity of the same word in both models through some kind of metric if it were possible (as if i were using the cosine similarity between 2 words in the same model), but i am not sure of any that may exist! 

Does anybody know of any such metrics or ways of comparing two word vectors like this?

thank you!",t2_2yirhe6m,False,,0,False,[D] metrics for measuring similarity of word vectors between different models,[],r/LanguageTechnology,False,6,,0,,False,t3_n0h695,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1619651996.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;I have 2 separately trained word2vec models. I want to be able to compare the same word in both models and see how they relate to each other and their surroundings through a metric. Now of course, I can&amp;#39;t use the cosine similarity as the word vectors are completely different. I also want to avoid using the &lt;code&gt;wv.most_similar()&lt;/code&gt; method as I would like to effectively capture the similarity of the same word in both models through some kind of metric if it were possible (as if i were using the cosine similarity between 2 words in the same model), but i am not sure of any that may exist! &lt;/p&gt;

&lt;p&gt;Does anybody know of any such metrics or ways of comparing two word vectors like this?&lt;/p&gt;

&lt;p&gt;thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n0h695,True,,amjass12,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n0h695/d_metrics_for_measuring_similarity_of_word/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n0h695/d_metrics_for_measuring_similarity_of_word/,30199,1619623196.0,0,,False,,,,,,693
277,,LanguageTechnology,"Wanted to introduce [chai](https://chai.ml) to y'all - it's a free, open-source platform for developers to build and deploy their own chat AIs. Currently over 100 AIs are hosted on the platform, which has over 12,000 conversations 😱

Following the tutorials at [https://chai.ml/docs](https://chai.ml/docs) you can deploy your own in under 10 minutes. I think this is such a cool way to learn NLP and it's really fun seeing your score go up on the [developer platform](https://chai.ml/dev).",t2_4o04wt,False,,0,False,Chai - Build your own chat AI in 10 minutes,[],r/LanguageTechnology,False,6,,0,,False,t3_n08m10,False,dark,0.89,,public,15,0,{},,False,[],,False,False,,{},,False,15,,False,False,,False,,[],{},,True,,1619620026.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Wanted to introduce &lt;a href=""https://chai.ml""&gt;chai&lt;/a&gt; to y&amp;#39;all - it&amp;#39;s a free, open-source platform for developers to build and deploy their own chat AIs. Currently over 100 AIs are hosted on the platform, which has over 12,000 conversations 😱&lt;/p&gt;

&lt;p&gt;Following the tutorials at &lt;a href=""https://chai.ml/docs""&gt;https://chai.ml/docs&lt;/a&gt; you can deploy your own in under 10 minutes. I think this is such a cool way to learn NLP and it&amp;#39;s really fun seeing your score go up on the &lt;a href=""https://chai.ml/dev""&gt;developer platform&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n08m10,True,,zecharias99,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n08m10/chai_build_your_own_chat_ai_in_10_minutes/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n08m10/chai_build_your_own_chat_ai_in_10_minutes/,30199,1619591226.0,0,,False,,,,,,489
278,,LanguageTechnology,"Hi guys, 

I’m working on a student assignment that essentially involves determining whether the sentiment is good/bad for certain keywords.

For example if a review is: 

‘I love the facilities and staff here, however the bathrooms were dirty.’

I need to be able to determine that the facilities and staff were positive and the cleanliness was negative. 

Does anyone have any suggestions for how to go about this? 

Relatively new to NLP!!!",t2_437mget6,False,,0,False,Does anyone have any suggestions for determining sentiment associated with key words in online reviews? Pls help,[],r/LanguageTechnology,False,6,,0,,False,t3_n0e2wq,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619643019.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys, &lt;/p&gt;

&lt;p&gt;I’m working on a student assignment that essentially involves determining whether the sentiment is good/bad for certain keywords.&lt;/p&gt;

&lt;p&gt;For example if a review is: &lt;/p&gt;

&lt;p&gt;‘I love the facilities and staff here, however the bathrooms were dirty.’&lt;/p&gt;

&lt;p&gt;I need to be able to determine that the facilities and staff were positive and the cleanliness was negative. &lt;/p&gt;

&lt;p&gt;Does anyone have any suggestions for how to go about this? &lt;/p&gt;

&lt;p&gt;Relatively new to NLP!!!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n0e2wq,True,,Fancy-Shelter7149,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n0e2wq/does_anyone_have_any_suggestions_for_determining/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n0e2wq/does_anyone_have_any_suggestions_for_determining/,30199,1619614219.0,0,,False,,,,,,443
279,,LanguageTechnology,"Want to introduce “The NLP Index”, a new asset in NLP code discovery. It's free and open to the public.

It houses over 3,000 code repositories that one can search including a side bar with some of the most important topics in NLP today. The engine is search as you type and typo tolerant (it’s crazy fast). The index includes the arxiv research paper PDF, ConnectedPapers link, and its GitHub repo.

[https://index.quantumstat.com/](https://index.quantumstat.com/)",t2_27omm5ij,False,,0,False,"The NLP Index: 3,000+ code repos for hackers and researchers. [self-promotion]",[],r/LanguageTechnology,False,6,,0,,False,t3_mzosly,False,dark,0.99,,public,80,1,{},,False,[],,False,False,,{},,False,80,,False,False,,False,,[],{'gid_1': 1},,True,,1619559938.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Want to introduce “The NLP Index”, a new asset in NLP code discovery. It&amp;#39;s free and open to the public.&lt;/p&gt;

&lt;p&gt;It houses over 3,000 code repositories that one can search including a side bar with some of the most important topics in NLP today. The engine is search as you type and typo tolerant (it’s crazy fast). The index includes the arxiv research paper PDF, ConnectedPapers link, and its GitHub repo.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://index.quantumstat.com/""&gt;https://index.quantumstat.com/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mzosly,True,,Quantum_Stat,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mzosly/the_nlp_index_3000_code_repos_for_hackers_and/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mzosly/the_nlp_index_3000_code_repos_for_hackers_and/,30199,1619531138.0,0,,False,,,,,,465
280,,LanguageTechnology,,t2_5zb7u,False,,0,False,Facebook's Internal Report About Its Role In The Capitol Insurrection -- Interested in Thoughts about the analysts' approach to text analytics as an adjunct to their social network and time analytics,[],r/LanguageTechnology,False,6,,0,,False,t3_mzn432,False,dark,1.0,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,False,,1619554517.0,text,6,,,text,buzzfeednews.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mzn432,True,,androbot,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mzn432/facebooks_internal_report_about_its_role_in_the/,all_ads,False,https://www.buzzfeednews.com/article/ryanmac/full-facebook-stop-the-steal-internal-report,30199,1619525717.0,0,,False,https://www.buzzfeednews.com/article/ryanmac/full-facebook-stop-the-steal-internal-report,,,,,0
281,,LanguageTechnology,As the title suggests I'm looking for some reading material (or videos) on Topic Modelling (esp. Hierarchical Topic Modelling). My aim is to understand the concept.,t2_sx582e5,False,,0,False,Reading material for Topic Modelling,[],r/LanguageTechnology,False,6,,0,,False,t3_mzk8y4,False,dark,1.0,,public,14,0,{},,False,[],,False,False,,{},,False,14,,False,False,,False,,[],{},,True,,1619543082.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;As the title suggests I&amp;#39;m looking for some reading material (or videos) on Topic Modelling (esp. Hierarchical Topic Modelling). My aim is to understand the concept.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mzk8y4,True,,mayanknagda,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mzk8y4/reading_material_for_topic_modelling/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mzk8y4/reading_material_for_topic_modelling/,30199,1619514282.0,0,,False,,,,,,164
282,,LanguageTechnology,"Hello!

Many users were asking me to add transformer-based translation models to the [NLPCloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=f80a8332-aaaf-11eb-bcbc-0242ac130002)'s API. So I added the 12 following NLP models based on Hugging Face transformers and Helsinki NLP's Opus MT:

* English to French
* French to English
* English to Spanish
* Spanish to English
* English to German
* German to English
* English to Dutch
* Dutch to English
* English to Chinese
* Chinese to English
* English to Russian
* Russian to English

I personally find the accuracy of these translation models very good, and the latency is pretty good too. Here's the link to the docs: [https://docs.nlpcloud.io/#translation](https://docs.nlpcloud.io/#translation)

**I would LOVE to have your opinion on this guys!**  Do you think that quality is good enough for  production use? Are there other languages you would like to see?

If you want to have a try, the API is free for up to 3 requests per minute, but if it's not enough please don't hesitate to ping me so I can grant you more requests.

Thanks!",t2_4z4m2qcs,False,,0,False,"I added translation models to the NLPCloud.io API, based on Helsinki NLP's Opus MT",[],r/LanguageTechnology,False,6,,0,,False,t3_mzqdgi,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,1619895876.0,,[],{},,True,,1619564409.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;Many users were asking me to add transformer-based translation models to the &lt;a href=""https://nlpcloud.io/?utm_source=reddit&amp;amp;utm_campaign=f80a8332-aaaf-11eb-bcbc-0242ac130002""&gt;NLPCloud.io&lt;/a&gt;&amp;#39;s API. So I added the 12 following NLP models based on Hugging Face transformers and Helsinki NLP&amp;#39;s Opus MT:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;English to French&lt;/li&gt;
&lt;li&gt;French to English&lt;/li&gt;
&lt;li&gt;English to Spanish&lt;/li&gt;
&lt;li&gt;Spanish to English&lt;/li&gt;
&lt;li&gt;English to German&lt;/li&gt;
&lt;li&gt;German to English&lt;/li&gt;
&lt;li&gt;English to Dutch&lt;/li&gt;
&lt;li&gt;Dutch to English&lt;/li&gt;
&lt;li&gt;English to Chinese&lt;/li&gt;
&lt;li&gt;Chinese to English&lt;/li&gt;
&lt;li&gt;English to Russian&lt;/li&gt;
&lt;li&gt;Russian to English&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I personally find the accuracy of these translation models very good, and the latency is pretty good too. Here&amp;#39;s the link to the docs: &lt;a href=""https://docs.nlpcloud.io/#translation""&gt;https://docs.nlpcloud.io/#translation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;I would LOVE to have your opinion on this guys!&lt;/strong&gt;  Do you think that quality is good enough for  production use? Are there other languages you would like to see?&lt;/p&gt;

&lt;p&gt;If you want to have a try, the API is free for up to 3 requests per minute, but if it&amp;#39;s not enough please don&amp;#39;t hesitate to ping me so I can grant you more requests.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mzqdgi,True,,juliensalinas,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mzqdgi/i_added_translation_models_to_the_nlpcloudio_api/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mzqdgi/i_added_translation_models_to_the_nlpcloudio_api/,30199,1619535609.0,0,,False,,,,,,1102
283,,LanguageTechnology,"I imagine in voice recognition training data there are simple nouns which you can hear clearly, and with no similar words?",t2_dfvmc,False,,0,False,"I often help companies do branding, and I'm wanting to find words that are simple to hear, are there words that are unique and easy to hear?",[],r/LanguageTechnology,False,6,,0,,False,t3_mzybpa,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1619585537.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I imagine in voice recognition training data there are simple nouns which you can hear clearly, and with no similar words?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mzybpa,True,,jaybestnz,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mzybpa/i_often_help_companies_do_branding_and_im_wanting/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mzybpa/i_often_help_companies_do_branding_and_im_wanting/,30199,1619556737.0,0,,False,,,,,,122
284,,LanguageTechnology,"Hi everyone, I'm really interested in building chatbots. I'm wondering what kinds of chatbot people need. 

If you can build your own AI chatbots, what kinds of chatbots do you want to build? (e.g., BTS chatbot, the chatbot that brings your loved one back from the dead, and business chatbot, etc.) Why you need those chatbots?",t2_bsm6d0e4,False,,0,False,"If you can build AI chatbots, what kinds of chatbots do you want to build?",[],r/LanguageTechnology,False,6,,0,,False,t3_mzoos1,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619559619.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, I&amp;#39;m really interested in building chatbots. I&amp;#39;m wondering what kinds of chatbot people need. &lt;/p&gt;

&lt;p&gt;If you can build your own AI chatbots, what kinds of chatbots do you want to build? (e.g., BTS chatbot, the chatbot that brings your loved one back from the dead, and business chatbot, etc.) Why you need those chatbots?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mzoos1,True,,Candid-Wishbone-692,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mzoos1/if_you_can_build_ai_chatbots_what_kinds_of/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mzoos1/if_you_can_build_ai_chatbots_what_kinds_of/,30199,1619530819.0,0,,False,,,,,,327
285,,LanguageTechnology,Need suggestions to fine tune Gan-Bert model on mechanical design dataset. Any leads would be appreciated. I can share the detailed problem if you can suggest some insights.,t2_5cjqpkl9,False,,0,False,Gan-Bert for Topic modelling,[],r/LanguageTechnology,False,6,,0,,False,t3_mzm7kp,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1619551263.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Need suggestions to fine tune Gan-Bert model on mechanical design dataset. Any leads would be appreciated. I can share the detailed problem if you can suggest some insights.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mzm7kp,True,,thecrimsonhead,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mzm7kp/ganbert_for_topic_modelling/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mzm7kp/ganbert_for_topic_modelling/,30199,1619522463.0,0,,False,,,,,,173
286,,LanguageTechnology,"I have an upcoming project that has a speech recognition component, but I'm pretty unfamiliar with the basics of this branch of NLP.

I suspect that I will need to create my own training dataset because the speech will involve a lot of niche terminology. From [what little I've seen on the tooling side](https://prodi.gy/docs/audio-video#transcribe), it looks like people segment audio into rough sentence equivalents, and then just type in a transcript to an input field. 

Is this best practice for creating speech recognition models? I would think you would need to provide word-level alignments to the audio, and that the models would be word level sequence models. But the training data tools seem mostly to facilitate capture of whole sentence transcripts.

Any help is appreciated",t2_9ozjj,False,,0,False,Speech Recognition Training Data Tools?,[],r/LanguageTechnology,False,6,,0,,False,t3_mzf2ee,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1619521455.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have an upcoming project that has a speech recognition component, but I&amp;#39;m pretty unfamiliar with the basics of this branch of NLP.&lt;/p&gt;

&lt;p&gt;I suspect that I will need to create my own training dataset because the speech will involve a lot of niche terminology. From &lt;a href=""https://prodi.gy/docs/audio-video#transcribe""&gt;what little I&amp;#39;ve seen on the tooling side&lt;/a&gt;, it looks like people segment audio into rough sentence equivalents, and then just type in a transcript to an input field. &lt;/p&gt;

&lt;p&gt;Is this best practice for creating speech recognition models? I would think you would need to provide word-level alignments to the audio, and that the models would be word level sequence models. But the training data tools seem mostly to facilitate capture of whole sentence transcripts.&lt;/p&gt;

&lt;p&gt;Any help is appreciated&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mzf2ee,True,,SurplusPopulation,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mzf2ee/speech_recognition_training_data_tools/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mzf2ee/speech_recognition_training_data_tools/,30199,1619492655.0,0,,False,,,,,,787
287,,LanguageTechnology,,t2_5smymm7p,False,,0,False,Into NLP - Text Normalization,[],r/LanguageTechnology,False,6,,0,,False,t3_myv82r,False,dark,0.96,,public,23,0,{},,False,[],,False,False,,{},,False,23,,False,False,,False,,[],{},,False,,1619464233.0,text,6,,,text,qualicen.de,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,myv82r,True,,QualicenDS,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/myv82r/into_nlp_text_normalization/,all_ads,False,https://www.qualicen.de/into-nlp-4-normal-language-perfection-text-normalization/,30199,1619435433.0,0,,False,https://www.qualicen.de/into-nlp-4-normal-language-perfection-text-normalization/,,,,,0
288,,LanguageTechnology,"As part of an internship, I have to build a biomedical knowledge graph from textual data, to do this I have to go through the tasks of named entity extraction and relation extraction as well as coreference resolution using BERT variant models. My problem is the availability of fine tune data for the three tasks.

Is there any open access datasets that I can use to fine tune my models in the three previous tasks in the biomedical domain?",t2_55lweil3,False,,0,False,Biomedical datasets suggestions for fine tuning Bert variant models on three tasks,[],r/LanguageTechnology,False,6,,0,,False,t3_mywo9c,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619469427.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;As part of an internship, I have to build a biomedical knowledge graph from textual data, to do this I have to go through the tasks of named entity extraction and relation extraction as well as coreference resolution using BERT variant models. My problem is the availability of fine tune data for the three tasks.&lt;/p&gt;

&lt;p&gt;Is there any open access datasets that I can use to fine tune my models in the three previous tasks in the biomedical domain?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mywo9c,True,,theweirdinstruction,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mywo9c/biomedical_datasets_suggestions_for_fine_tuning/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mywo9c/biomedical_datasets_suggestions_for_fine_tuning/,30199,1619440627.0,0,,False,,,,,,440
289,,LanguageTechnology,"Please state specific math subjects in the format used below (these are simply examples).

* Calculus 1

* Calculus 2

* Calculus 3

* Calculus 4

* Discrete Math

* Introduction to Probability

* Introduction to Statistics

* Statistical Inference

* Linear Algebra

Thank You",t2_botme4lw,False,,0,False,What specific math subjects do I need to fully understand every NLP/Computational Linguistics journal article (espeically the most important ones)?,[],r/LanguageTechnology,False,6,,0,,False,t3_myqlea,False,dark,0.75,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1619444355.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Please state specific math subjects in the format used below (these are simply examples).&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Calculus 1&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Calculus 2&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Calculus 3&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Calculus 4&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Discrete Math&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Introduction to Probability&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Introduction to Statistics&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Statistical Inference&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Linear Algebra&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank You&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,myqlea,True,,RockHardAndVeryHorny,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/myqlea/what_specific_math_subjects_do_i_need_to_fully/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/myqlea/what_specific_math_subjects_do_i_need_to_fully/,30199,1619415555.0,0,,False,,,,,,277
290,,LanguageTechnology,"There's a dataset which has conversational data with customer &amp; sales persons &amp; I have to incorporate that data into a sales forecasting model. 

The conversational data is mainly in sentiment - related topics (For reference)

What NLP technique can be applied here to maybe get a feature out that can act as a dependent variable when I am predicting my final sales output?

PS. I have other variables that can help to predict sales, but I need to use the text data as well. How am  I supposed to move forward with this?",t2_8j8sr5mz,False,,0,False,Incorporating Text Data for a Sales Forecasting Model,[],r/LanguageTechnology,False,6,,0,,False,t3_myqvhj,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619445566.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;There&amp;#39;s a dataset which has conversational data with customer &amp;amp; sales persons &amp;amp; I have to incorporate that data into a sales forecasting model. &lt;/p&gt;

&lt;p&gt;The conversational data is mainly in sentiment - related topics (For reference)&lt;/p&gt;

&lt;p&gt;What NLP technique can be applied here to maybe get a feature out that can act as a dependent variable when I am predicting my final sales output?&lt;/p&gt;

&lt;p&gt;PS. I have other variables that can help to predict sales, but I need to use the text data as well. How am  I supposed to move forward with this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,myqvhj,True,,Educational-Bid-5263,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/myqvhj/incorporating_text_data_for_a_sales_forecasting/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/myqvhj/incorporating_text_data_for_a_sales_forecasting/,30199,1619416766.0,0,,False,,,,,,528
291,,LanguageTechnology,I'm pretty new to NLP so I hope you can help me out. I have a file with \~700 paragraphs on a similar topic and I want to make a semantic search engine so that the user can input a query and it will return the paragraphs that match the closest. Thanks!,t2_2scuv8zh,False,,0,False,Any good tutorials for creating a semantic search engine?,[],r/LanguageTechnology,False,6,,0,,False,t3_myk81n,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1619421444.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m pretty new to NLP so I hope you can help me out. I have a file with ~700 paragraphs on a similar topic and I want to make a semantic search engine so that the user can input a query and it will return the paragraphs that match the closest. Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,myk81n,True,,beardedjoy,,10,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/myk81n/any_good_tutorials_for_creating_a_semantic_search/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/myk81n/any_good_tutorials_for_creating_a_semantic_search/,30199,1619392644.0,0,,False,,,,,,252
292,,LanguageTechnology,,t2_5fsp2x6v,False,,0,False,BERTweet (SOTA) for Named Entity Recognition in Social Media | Research Papers Summary 015,[],r/LanguageTechnology,False,6,,0,,False,t3_myf69x,False,dark,0.89,,public,7,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/npxjcPhFLKE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'BERTweet (SOTA) for Named Entity Recognition in Social Media | Research Papers Summary 015', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/npxjcPhFLKE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/npxjcPhFLKE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/npxjcPhFLKE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/myf69x', 'height': 200}",,False,7,,False,False,,False,,[],{},,False,,1619406598.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,myf69x,True,,RyanAI100,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/myf69x/bertweet_sota_for_named_entity_recognition_in/,all_ads,False,https://youtu.be/npxjcPhFLKE,30199,1619377798.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'BERTweet (SOTA) for Named Entity Recognition in Social Media | Research Papers Summary 015', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/npxjcPhFLKE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/npxjcPhFLKE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,https://youtu.be/npxjcPhFLKE,,,,,0
293,,LanguageTechnology,"I was wondering whether there are models that can handle text input with two or more languages.

For instance, a multilingualBert can probably understand both English and Spanish:

`(en) This is a test for language model`

`(es)Esta es una prueba para el modelo de lenguaje`

But it fails in understanding the following :

`(en-es) This is a prueba for language modelo`

`(en-jp) This is a テスト for language モデル`

A text that has both English and Spanish or Japanese(where even the script is different than Latin).",t2_nwzpkmq,False,,0,False,Handling Multi-lingual text,[],r/LanguageTechnology,False,6,,0,,False,t3_my6w2t,False,dark,1.0,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,False,,False,,[],{},,True,,1619380828.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was wondering whether there are models that can handle text input with two or more languages.&lt;/p&gt;

&lt;p&gt;For instance, a multilingualBert can probably understand both English and Spanish:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;(en) This is a test for language model&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;(es)Esta es una prueba para el modelo de lenguaje&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;But it fails in understanding the following :&lt;/p&gt;

&lt;p&gt;&lt;code&gt;(en-es) This is a prueba for language modelo&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;(en-jp) This is a テスト for language モデル&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;A text that has both English and Spanish or Japanese(where even the script is different than Latin).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,my6w2t,True,,10zin_,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/my6w2t/handling_multilingual_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/my6w2t/handling_multilingual_text/,30199,1619352028.0,0,,False,,,,,,513
294,,LanguageTechnology,,t2_hkv9s,False,,0,False,Aspect-based Document Similarity for Research Papers (Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_my6c2k,False,dark,0.89,,public,14,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ZO6QWG7-Ye0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Aspect-based Document Similarity for Research Papers (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ZO6QWG7-Ye0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/ZO6QWG7-Ye0/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ZO6QWG7-Ye0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/my6c2k', 'height': 200}",,False,14,,False,False,,False,,[],{},,False,,1619378514.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,my6c2k,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/my6c2k/aspectbased_document_similarity_for_research/,all_ads,False,https://youtu.be/ZO6QWG7-Ye0,30199,1619349714.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Aspect-based Document Similarity for Research Papers (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ZO6QWG7-Ye0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/ZO6QWG7-Ye0/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/ZO6QWG7-Ye0,,,,,0
295,,LanguageTechnology,"I'm building a open source government complaint redressal system. For the purpose of complaint classification (for routing the complaints to a department), I need a existing government complaint dataset. If anyone has come across any such datasets, please help by commenting.

It basically needs to have a complaint column and a column with department name to which the complaints belong",t2_8n5d56qx,False,,0,False,Government complaint dataset,[],r/LanguageTechnology,False,6,,0,,False,t3_myc2ee,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1619397755.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m building a open source government complaint redressal system. For the purpose of complaint classification (for routing the complaints to a department), I need a existing government complaint dataset. If anyone has come across any such datasets, please help by commenting.&lt;/p&gt;

&lt;p&gt;It basically needs to have a complaint column and a column with department name to which the complaints belong&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,myc2ee,True,,ChandlerBingggggggg,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/myc2ee/government_complaint_dataset/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/myc2ee/government_complaint_dataset/,30199,1619368955.0,0,,False,,,,,,387
296,,LanguageTechnology," Hi all,

My  name is Kushel Ramanayake, and I am a current 4th undergraduate at  Informatics Institute of Technology affiliated with University of  Westminster UK. As part of my Final Year Thesis, I am required to gather some feedback on my project, from experts within the domain. The project I am working on is similar to an automatic question generation system, and main technology is NLP.

Ideally we'll get on a 1 on 1 zoom call (\~15 minutes), where I'd go over my project and you'd give me feedback on;

* The Scope
* The Architecture of the Solution
* The Implementation of the Solution

The data collected will only be used for my thesis and will be discarded with afterwards. No personal information other than maybe name and occupation will be collected. 

Thank you in advance,

Kushel Ramanayake",t2_ic2y7,False,,0,False,Feedback for NLP project,[],r/LanguageTechnology,False,6,,0,,False,t3_my41vc,False,dark,0.89,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1619368300.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;My  name is Kushel Ramanayake, and I am a current 4th undergraduate at  Informatics Institute of Technology affiliated with University of  Westminster UK. As part of my Final Year Thesis, I am required to gather some feedback on my project, from experts within the domain. The project I am working on is similar to an automatic question generation system, and main technology is NLP.&lt;/p&gt;

&lt;p&gt;Ideally we&amp;#39;ll get on a 1 on 1 zoom call (~15 minutes), where I&amp;#39;d go over my project and you&amp;#39;d give me feedback on;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Scope&lt;/li&gt;
&lt;li&gt;The Architecture of the Solution&lt;/li&gt;
&lt;li&gt;The Implementation of the Solution&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The data collected will only be used for my thesis and will be discarded with afterwards. No personal information other than maybe name and occupation will be collected. &lt;/p&gt;

&lt;p&gt;Thank you in advance,&lt;/p&gt;

&lt;p&gt;Kushel Ramanayake&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,my41vc,True,,kushel,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/my41vc/feedback_for_nlp_project/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/my41vc/feedback_for_nlp_project/,30199,1619339500.0,0,,False,,,,,,809
297,,LanguageTechnology,"[https://preview.redd.it/t60n4t6z08v61.gif?format=mp4&amp;s=b3e2748ba96ec723266ee5c8e14bd49081cb6c57](https://preview.redd.it/t60n4t6z08v61.gif?format=mp4&amp;s=b3e2748ba96ec723266ee5c8e14bd49081cb6c57)

Information:  
For my dissertation project I fine-tuned a pre-trained language model on a self-mined dataset of ""left"" and ""right"" leaning subreddits to classify comments and subreddit's.

I mined the data over a few months using praw, I used a list of around 20-25 different subreddits taking between 10-20,000 comments from each from within the past year, so the model is quite American election biased but the model was fine tuned a few weeks ago so the comments you are seeing the gif it has not seen before.

I used DistilBert to fine-tune the model on pre-processed text, I spent a few months fine-tuning different models on different versions of the data set until I minimised overfitting and got a decent validation to training trade-off.

I also made a fun venn diagram tool to help find similar subreddits, I used this tool with a much larger sample size to help find similar leaning subreddits to help remove my personal bias although I am certain the left-wing subreddits tend to the far left more than the right which is why you may see a fair bit of negative biden commentary leading more left than right.

Disclaimer:  
The venn diagram tool and the subreddit classifier tool utilise praw which has a decent rate limit so may take 10-20 seconds before it returns a result, I have moved to psaw although loading times have not improved much.

View this project: [https://reddit-political-analysis.com/](https://reddit-political-analysis.com/)",t2_6l9u9,False,,0,False,I fine-tuned a language model on left and right leaning political commentary on Reddit,[],r/LanguageTechnology,False,6,,0,,False,t3_mxxwlc,False,dark,0.9,,public,17,0,{},,False,[],,False,False,,{},,False,17,,False,False,,False,,[],{},,True,,1619343228.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://preview.redd.it/t60n4t6z08v61.gif?format=mp4&amp;amp;s=b3e2748ba96ec723266ee5c8e14bd49081cb6c57""&gt;https://preview.redd.it/t60n4t6z08v61.gif?format=mp4&amp;amp;s=b3e2748ba96ec723266ee5c8e14bd49081cb6c57&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Information:&lt;br/&gt;
For my dissertation project I fine-tuned a pre-trained language model on a self-mined dataset of &amp;quot;left&amp;quot; and &amp;quot;right&amp;quot; leaning subreddits to classify comments and subreddit&amp;#39;s.&lt;/p&gt;

&lt;p&gt;I mined the data over a few months using praw, I used a list of around 20-25 different subreddits taking between 10-20,000 comments from each from within the past year, so the model is quite American election biased but the model was fine tuned a few weeks ago so the comments you are seeing the gif it has not seen before.&lt;/p&gt;

&lt;p&gt;I used DistilBert to fine-tune the model on pre-processed text, I spent a few months fine-tuning different models on different versions of the data set until I minimised overfitting and got a decent validation to training trade-off.&lt;/p&gt;

&lt;p&gt;I also made a fun venn diagram tool to help find similar subreddits, I used this tool with a much larger sample size to help find similar leaning subreddits to help remove my personal bias although I am certain the left-wing subreddits tend to the far left more than the right which is why you may see a fair bit of negative biden commentary leading more left than right.&lt;/p&gt;

&lt;p&gt;Disclaimer:&lt;br/&gt;
The venn diagram tool and the subreddit classifier tool utilise praw which has a decent rate limit so may take 10-20 seconds before it returns a result, I have moved to psaw although loading times have not improved much.&lt;/p&gt;

&lt;p&gt;View this project: &lt;a href=""https://reddit-political-analysis.com/""&gt;https://reddit-political-analysis.com/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mxxwlc,True,,rockwilly,,12,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mxxwlc/i_finetuned_a_language_model_on_left_and_right/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mxxwlc/i_finetuned_a_language_model_on_left_and_right/,30199,1619314428.0,0,,False,,,,,,1660
298,,LanguageTechnology,"I'm researching language mixing and English borrowings and I need to compare two txt corpora (one in English, one in Polish) to find words that overlap between them, excluding stop words, and preferably the frequency of overlap as well. I'm using Python. One difficulty is that Polish is an inflectional language, so e.g. ""to ghost"" is ""ghostOWAĆ"" in Polish. How can I account for that in my code so the program to find the instances of these inflected words as well?

Thanks!",t2_60py1dqs,False,,0,False,Corpus overlap and inflection,[],r/LanguageTechnology,False,6,,0,,False,t3_my4vju,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619372094.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m researching language mixing and English borrowings and I need to compare two txt corpora (one in English, one in Polish) to find words that overlap between them, excluding stop words, and preferably the frequency of overlap as well. I&amp;#39;m using Python. One difficulty is that Polish is an inflectional language, so e.g. &amp;quot;to ghost&amp;quot; is &amp;quot;ghostOWAĆ&amp;quot; in Polish. How can I account for that in my code so the program to find the instances of these inflected words as well?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,my4vju,True,,Complex-Intention256,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/my4vju/corpus_overlap_and_inflection/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/my4vju/corpus_overlap_and_inflection/,30199,1619343294.0,0,,False,,,,,,476
299,,LanguageTechnology,,t2_10efjmjx,False,,0,False,Anyone know of any papers about training with a traditional pretraining task (MLM) simultaneously with a finetuning task; as opposed to first doing pretraining then finetuning ?,[],r/LanguageTechnology,False,6,,0,,False,t3_my1ruz,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619357749.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,my1ruz,True,,BatmantoshReturns,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/my1ruz/anyone_know_of_any_papers_about_training_with_a/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/my1ruz/anyone_know_of_any_papers_about_training_with_a/,30199,1619328949.0,0,,False,,,,,,0
300,,LanguageTechnology,"I have data which is basically a group of paragraphs that I know could be related.

Inside a group of paragraphs (let’s say 100 paragraphs). There could be a sentence of two that are almost/exactly the same text across 60% of the paragraphs. Another completely different similar text across 20% of the paragraphs while the other 20% might be unrelated. 

I have billions of these groups of 100 paragraphs and have no idea where to start! Thanks for any pointers",t2_8dmgu,False,,0,False,How to approach finding Similar wording in paragraph,[],r/LanguageTechnology,False,6,,0,,False,t3_mxm06o,False,dark,0.94,,public,13,1,{},,False,[],,False,False,,{},,False,13,,False,False,,False,,[],{},,True,,1619306339.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have data which is basically a group of paragraphs that I know could be related.&lt;/p&gt;

&lt;p&gt;Inside a group of paragraphs (let’s say 100 paragraphs). There could be a sentence of two that are almost/exactly the same text across 60% of the paragraphs. Another completely different similar text across 20% of the paragraphs while the other 20% might be unrelated. &lt;/p&gt;

&lt;p&gt;I have billions of these groups of 100 paragraphs and have no idea where to start! Thanks for any pointers&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mxm06o,True,,johne898,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mxm06o/how_to_approach_finding_similar_wording_in/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mxm06o/how_to_approach_finding_similar_wording_in/,30199,1619277539.0,0,,False,,,,,,461
301,,LanguageTechnology,"What would be a good method to find, for example, the best method to compare two sentences for having the same or a similar meaning? I mean something like a overview or a search engine, based on a problem/solution pattern. Finding the best ways to solve a problem, without asking around a lot or looking into abstracts for papers.",t2_6jxf653x,False,,0,False,Best way to find the (best) solution to any task?,[],r/LanguageTechnology,False,6,,0,,False,t3_mxodes,False,dark,0.81,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1619313468.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What would be a good method to find, for example, the best method to compare two sentences for having the same or a similar meaning? I mean something like a overview or a search engine, based on a problem/solution pattern. Finding the best ways to solve a problem, without asking around a lot or looking into abstracts for papers.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mxodes,True,,botfiddler,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mxodes/best_way_to_find_the_best_solution_to_any_task/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mxodes/best_way_to_find_the_best_solution_to_any_task/,30199,1619284668.0,0,,False,,,,,,330
302,,LanguageTechnology,"Any expert in the domain here?

I had learnt about LDA/ LSA, and it seems that BERT artchitecture is not a fit if there's no pretagged labels on the topics. What are some of the latst trends? Are there any good teams/ companies working on this sector?",t2_11dikh,False,,0,False,Latest trends in topic modelling?,[],r/LanguageTechnology,False,6,,0,,False,t3_mxcvxk,False,dark,1.0,,public,21,0,{},,False,[],,False,False,,{},,False,21,,False,False,,False,,[],{},,True,,1619268532.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Any expert in the domain here?&lt;/p&gt;

&lt;p&gt;I had learnt about LDA/ LSA, and it seems that BERT artchitecture is not a fit if there&amp;#39;s no pretagged labels on the topics. What are some of the latst trends? Are there any good teams/ companies working on this sector?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mxcvxk,True,,reddithurc,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mxcvxk/latest_trends_in_topic_modelling/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mxcvxk/latest_trends_in_topic_modelling/,30199,1619239732.0,0,,False,,,,,,251
303,,LanguageTechnology,How do I fine tune another language? Who will tell you?,t2_82cn9rut,False,,0,False,Transformers Pegasus - how do I fine tune another language?,[],r/LanguageTechnology,False,6,,0,,False,t3_mxj2j1,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1619296396.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How do I fine tune another language? Who will tell you?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mxj2j1,True,,Careless-Shift-4100,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mxj2j1/transformers_pegasus_how_do_i_fine_tune_another/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mxj2j1/transformers_pegasus_how_do_i_fine_tune_another/,30199,1619267596.0,0,,False,,,,,,55
304,,LanguageTechnology,"Hello,

I'm currently working on a NER that extracts products and capabilities of company based on texts from their website. At the moment I don't have any training data that I could use to train my NLP Model. I need German training data.

I found this farmework: [https://github.com/NorskRegnesentral/skweak](https://github.com/NorskRegnesentral/skweak) and it looks great to automatically label data, but I would still need some kind of structured data in form of gazetters or another ML model to automatically annotate words.

One thing I tried is using Masked Language Model to predict missing words. I found this approach here: [https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a](https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a)  
If one of the predicted words is in my target dictionary then I tag this word with the corresponding entity. This approach works in some cases, but it is not very accurate and I'm not sure how to deal with products that consist of multiple words.

Do any of you have an idea where I could find training data in German with labeled products or how i could generate the training data myself?

Thanks",t2_6eol9zo6,False,,0,False,How to get Training data for NER?,[],r/LanguageTechnology,False,6,,0,,False,t3_mxg8pj,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1619283923.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m currently working on a NER that extracts products and capabilities of company based on texts from their website. At the moment I don&amp;#39;t have any training data that I could use to train my NLP Model. I need German training data.&lt;/p&gt;

&lt;p&gt;I found this farmework: &lt;a href=""https://github.com/NorskRegnesentral/skweak""&gt;https://github.com/NorskRegnesentral/skweak&lt;/a&gt; and it looks great to automatically label data, but I would still need some kind of structured data in form of gazetters or another ML model to automatically annotate words.&lt;/p&gt;

&lt;p&gt;One thing I tried is using Masked Language Model to predict missing words. I found this approach here: &lt;a href=""https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a""&gt;https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a&lt;/a&gt;&lt;br/&gt;
If one of the predicted words is in my target dictionary then I tag this word with the corresponding entity. This approach works in some cases, but it is not very accurate and I&amp;#39;m not sure how to deal with products that consist of multiple words.&lt;/p&gt;

&lt;p&gt;Do any of you have an idea where I could find training data in German with labeled products or how i could generate the training data myself?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mxg8pj,True,,SWEQuestion123,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mxg8pj/how_to_get_training_data_for_ner/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mxg8pj/how_to_get_training_data_for_ner/,30199,1619255123.0,0,,False,,,,,True,1179
305,,LanguageTechnology,"I'm coming from the world of computer vision and was wondering whether there's the YOLOv5 model equivalent for NLP? I'm looking to train a classification model to categorize short strings (ex: classifying area-related words - Acreage, Land Area, acres, estimated size, lot size, deeded acres, and land area - all as ""area""). I'm thinking of using transfer learning on a custom dataset.

Are there industry-standard models that you'd recommend looking into? Or, put differently, models that offer a good tradeoff between performance and ease-of-use?

Edit: Looking for deep learning approaches to NLP. Perhaps with a [fast.ai](https://fast.ai) flavour",t2_8qjmpb1m,False,,0,False,YOLOv5 equivalent for NLP classification?,[],r/LanguageTechnology,False,6,,0,,False,t3_mxae9h,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,1619229900.0,,[],{},,True,,1619258437.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m coming from the world of computer vision and was wondering whether there&amp;#39;s the YOLOv5 model equivalent for NLP? I&amp;#39;m looking to train a classification model to categorize short strings (ex: classifying area-related words - Acreage, Land Area, acres, estimated size, lot size, deeded acres, and land area - all as &amp;quot;area&amp;quot;). I&amp;#39;m thinking of using transfer learning on a custom dataset.&lt;/p&gt;

&lt;p&gt;Are there industry-standard models that you&amp;#39;d recommend looking into? Or, put differently, models that offer a good tradeoff between performance and ease-of-use?&lt;/p&gt;

&lt;p&gt;Edit: Looking for deep learning approaches to NLP. Perhaps with a &lt;a href=""https://fast.ai""&gt;fast.ai&lt;/a&gt; flavour&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mxae9h,True,,kaia_1527,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mxae9h/yolov5_equivalent_for_nlp_classification/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mxae9h/yolov5_equivalent_for_nlp_classification/,30199,1619229637.0,0,,False,,,,,,650
306,,LanguageTechnology,,t2_apv7o2y8,False,,0,False,Text Classification using Convolutional Neural Network with TensorFlow 2.1 in Python | #NLProc,[],r/LanguageTechnology,False,6,,0,,False,t3_mxdj43,False,dark,0.67,,public,1,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/MsL79ZIqWpg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Text Classification using Convolutional Neural Network with TensorFlow 2.1  in Python |  #NLProc', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/MsL79ZIqWpg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/MsL79ZIqWpg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/MsL79ZIqWpg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mxdj43', 'height': 200}",,False,1,,False,False,,False,,[],{},,False,,1619271379.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mxdj43,True,,rsree123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mxdj43/text_classification_using_convolutional_neural/,all_ads,False,https://youtu.be/MsL79ZIqWpg,30199,1619242579.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Text Classification using Convolutional Neural Network with TensorFlow 2.1  in Python |  #NLProc', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/MsL79ZIqWpg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/MsL79ZIqWpg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,https://youtu.be/MsL79ZIqWpg,,,,,0
307,,LanguageTechnology,"Hi everybody!

&amp;#x200B;

A couple of years ago, I made a [post to announce the release of mlconjug](https://www.reddit.com/r/Python/comments/bb8400/mlconjug_a_python_library_to_conjugate_verbs_in/),  a Python package/library to conjugate verbs (even made-up verbs, or verbs coming from slang and not covered in traditional conjugation tables) in French, English, Spanish, Italian, Portuguese and Romanian using Machine Learning techniques.

Since then, mlconjug has had a lot of success with thousands of students of foreign languages using it as a standalone application to improve their conjugation skills, but it has also been incorporated as a library dependency in more than a dozen different python software projects ranging from traditional NLP tasks using Machine Learning, to twitter bots, Voice Assistants, and even games.

It has also been used in several Academic publications, for example: [""Generative Grading: Near Human-level Accuracy for Automated Feedback on Richly Structured Problems""](https://arxiv.org/abs/1905.09916) where it is used for automatically grading students essays, and  United States citizenship exam questions.

I released a new and improved version of the software, called [mlconjug3](https://github.com/SekouDiaoNlp/mlconjug3) as it is only compatible with Python 3.x, and had many enhancements and bug fixes. The accuracy of the conjugations models has improved a lot and I am in the process of implementing regional European languages (in beta version for now), like Catalan,  Valencian , Basque language, etc... as well as slavic languages (Czech and Polish for now).

Those new languages should be available in the beginning of summer and I am looking for native speakers of the languages that are in beta status to test the software and check that the conjugated forms are correct.

You can install mlconjug3 from [PyPi](https://pypi.python.org/pypi/mlconjug3) or [Anaconda](https://anaconda.org/conda-forge/mlconjug3).

Some of the features of mlconjug3 are the following:

* Easy to use API.
* Includes pre-trained language models with 99% + accuracy in predicting conjugation class of unknown verbs.
* Easily train new models or add new languages.
* Easily integrate MLConjug in your own projects.
* Can be used as a command line tool.

I invite everyone to try it out and if you are a native speaker of Catalan,  Valencian , Basque, Czech or Polish and are willing to beta-test the software, please pm me, you are would be greatly appreciated, and it will make mlconjug3 more versatile and therefore more useful.

Thanks Everyone,

Peace,

SekouDiaoNlp",t2_46ggkdhi,False,,0,False,"Introducing mlconjug3. A Python library to conjugate verbs in French, English, Spanish, Italian, Portuguese and Romanian (with more in beta version) using Machine Learning techniques.",[],r/LanguageTechnology,False,6,,0,,False,t3_mwsymt,False,dark,1.0,,public,31,0,{},,False,[],,False,False,,{},,False,31,,False,False,,1619292255.0,,[],{},,True,,1619205397.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everybody!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;A couple of years ago, I made a &lt;a href=""https://www.reddit.com/r/Python/comments/bb8400/mlconjug_a_python_library_to_conjugate_verbs_in/""&gt;post to announce the release of mlconjug&lt;/a&gt;,  a Python package/library to conjugate verbs (even made-up verbs, or verbs coming from slang and not covered in traditional conjugation tables) in French, English, Spanish, Italian, Portuguese and Romanian using Machine Learning techniques.&lt;/p&gt;

&lt;p&gt;Since then, mlconjug has had a lot of success with thousands of students of foreign languages using it as a standalone application to improve their conjugation skills, but it has also been incorporated as a library dependency in more than a dozen different python software projects ranging from traditional NLP tasks using Machine Learning, to twitter bots, Voice Assistants, and even games.&lt;/p&gt;

&lt;p&gt;It has also been used in several Academic publications, for example: &lt;a href=""https://arxiv.org/abs/1905.09916""&gt;&amp;quot;Generative Grading: Near Human-level Accuracy for Automated Feedback on Richly Structured Problems&amp;quot;&lt;/a&gt; where it is used for automatically grading students essays, and  United States citizenship exam questions.&lt;/p&gt;

&lt;p&gt;I released a new and improved version of the software, called &lt;a href=""https://github.com/SekouDiaoNlp/mlconjug3""&gt;mlconjug3&lt;/a&gt; as it is only compatible with Python 3.x, and had many enhancements and bug fixes. The accuracy of the conjugations models has improved a lot and I am in the process of implementing regional European languages (in beta version for now), like Catalan,  Valencian , Basque language, etc... as well as slavic languages (Czech and Polish for now).&lt;/p&gt;

&lt;p&gt;Those new languages should be available in the beginning of summer and I am looking for native speakers of the languages that are in beta status to test the software and check that the conjugated forms are correct.&lt;/p&gt;

&lt;p&gt;You can install mlconjug3 from &lt;a href=""https://pypi.python.org/pypi/mlconjug3""&gt;PyPi&lt;/a&gt; or &lt;a href=""https://anaconda.org/conda-forge/mlconjug3""&gt;Anaconda&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Some of the features of mlconjug3 are the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Easy to use API.&lt;/li&gt;
&lt;li&gt;Includes pre-trained language models with 99% + accuracy in predicting conjugation class of unknown verbs.&lt;/li&gt;
&lt;li&gt;Easily train new models or add new languages.&lt;/li&gt;
&lt;li&gt;Easily integrate MLConjug in your own projects.&lt;/li&gt;
&lt;li&gt;Can be used as a command line tool.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I invite everyone to try it out and if you are a native speaker of Catalan,  Valencian , Basque, Czech or Polish and are willing to beta-test the software, please pm me, you are would be greatly appreciated, and it will make mlconjug3 more versatile and therefore more useful.&lt;/p&gt;

&lt;p&gt;Thanks Everyone,&lt;/p&gt;

&lt;p&gt;Peace,&lt;/p&gt;

&lt;p&gt;SekouDiaoNlp&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mwsymt,True,,BlackPythonGuru,,19,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mwsymt/introducing_mlconjug3_a_python_library_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mwsymt/introducing_mlconjug3_a_python_library_to/,30199,1619176597.0,2,,False,,,,,,2603
308,,LanguageTechnology,"Hello guys, I am living in korea and majored arabic in HUFS(Hankook university of foreign studies).

I got interest on singularity concept since 2013, and I got into this field

from learning machine learning and big data in the academy funded by government.

&amp;#x200B;

Now I am preparing to go artificial intelligence in graduate school, which is master degree

but there are not that many information in korea that which part; computer vision, natural language processing, have more good prospect.

&amp;#x200B;

I was actually thinking about studying nlp, because I majored language in university; english and arabic, and korean speaker, so I applied to the program that only teach nlp for 6 months.

&amp;#x200B;

But I saw one opinion in the forum that nlp have no future after gpt3, and if gpt4 comes out,

there will be no future for nlp.

&amp;#x200B;

So I felt that I have to consider which part will survive for long.

&amp;#x200B;

One of the member in A.I group recommended me that Medical image analysis is hot trend now,

so it is better to study computer vision for the future.

&amp;#x200B;

From now, I have to decide which part I should study more deeply in order to appeal graduate school that I had interest on specific field.

&amp;#x200B;

Briefly, the question is two.

&amp;#x200B;

1.Which part of A.I will survive long? CV or NLP? And which part is more prospective regarding

job opportunity and studying ph.D?

&amp;#x200B;

2.I applied to two kind of government program, first one is teaching general artificial intelligence

including brief lesson of CV and NLP. Second one is the program that only teaches NLP. I am 

wondering which program is better for me entering A.I field.

My second question will be depend on the first question, because if the future of NLP is dark,

I will apply to general A.I program and then study medical image analysis and contact the graduate school in CV lab.

&amp;#x200B;

thank you for reading all of my stories and questions.",t2_bpsj0h63,False,,0,False,Which domain in master degree to choose,[],r/LanguageTechnology,False,6,,0,,False,t3_mwwkk0,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1619217103.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello guys, I am living in korea and majored arabic in HUFS(Hankook university of foreign studies).&lt;/p&gt;

&lt;p&gt;I got interest on singularity concept since 2013, and I got into this field&lt;/p&gt;

&lt;p&gt;from learning machine learning and big data in the academy funded by government.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Now I am preparing to go artificial intelligence in graduate school, which is master degree&lt;/p&gt;

&lt;p&gt;but there are not that many information in korea that which part; computer vision, natural language processing, have more good prospect.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I was actually thinking about studying nlp, because I majored language in university; english and arabic, and korean speaker, so I applied to the program that only teach nlp for 6 months.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;But I saw one opinion in the forum that nlp have no future after gpt3, and if gpt4 comes out,&lt;/p&gt;

&lt;p&gt;there will be no future for nlp.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So I felt that I have to consider which part will survive for long.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;One of the member in A.I group recommended me that Medical image analysis is hot trend now,&lt;/p&gt;

&lt;p&gt;so it is better to study computer vision for the future.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;From now, I have to decide which part I should study more deeply in order to appeal graduate school that I had interest on specific field.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Briefly, the question is two.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;1.Which part of A.I will survive long? CV or NLP? And which part is more prospective regarding&lt;/p&gt;

&lt;p&gt;job opportunity and studying ph.D?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;2.I applied to two kind of government program, first one is teaching general artificial intelligence&lt;/p&gt;

&lt;p&gt;including brief lesson of CV and NLP. Second one is the program that only teaches NLP. I am &lt;/p&gt;

&lt;p&gt;wondering which program is better for me entering A.I field.&lt;/p&gt;

&lt;p&gt;My second question will be depend on the first question, because if the future of NLP is dark,&lt;/p&gt;

&lt;p&gt;I will apply to general A.I program and then study medical image analysis and contact the graduate school in CV lab.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;thank you for reading all of my stories and questions.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mwwkk0,True,,Korean_Arabic_AI,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mwwkk0/which_domain_in_master_degree_to_choose/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mwwkk0/which_domain_in_master_degree_to_choose/,30199,1619188303.0,0,,False,,,,,,1998
309,,LanguageTechnology,"Hi everyone, been a lurker here for a while. I'd like some advice on an NLP task I'm working on.
 I have a list of paragraphs, and a list of possible key words/phrases to choose from. I would like to assign the most probable keyword from the list for each paragraph, based on semantic meaning. 
Currently I am just computing an embedding for the keyword and the paragraph and comparing the two, but the results aren't great. What would be the best way to do this? Is there some preprocessing that would improve results? Thanks in advance",t2_4b0mttq8,False,,0,False,Best way to choose topic keyword for text?,[],r/LanguageTechnology,False,6,,0,,False,t3_mx1um3,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619231446.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, been a lurker here for a while. I&amp;#39;d like some advice on an NLP task I&amp;#39;m working on.
 I have a list of paragraphs, and a list of possible key words/phrases to choose from. I would like to assign the most probable keyword from the list for each paragraph, based on semantic meaning. 
Currently I am just computing an embedding for the keyword and the paragraph and comparing the two, but the results aren&amp;#39;t great. What would be the best way to do this? Is there some preprocessing that would improve results? Thanks in advance&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mx1um3,True,,anihm136,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mx1um3/best_way_to_choose_topic_keyword_for_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mx1um3/best_way_to_choose_topic_keyword_for_text/,30199,1619202646.0,0,,False,,,,,,537
310,,LanguageTechnology,"**\*\*\* CFP2  MESINESP2 track: Medical Semantic Indexing (BioASQ – CLEF 2021) \*\*\***

[https://temu.bsc.es/mesinesp2/](https://temu.bsc.es/mesinesp2/) 

**MESINESP2 Awards by BSC-Plan TL \[2,700€\]**

**Test sets and additional data are now available**

There is a pressing need for advanced multilingual semantic search strategies for health related content like literature, patents and clinical trials (cross-genre). The use of semantic search techniques in combination with structured vocabularies is critical for sophisticated searches or content analysis as needed by healthcare professionals, researchers, the pharmaceutical industry, patient groups and private citizens.

Following the impact of past BioASQ tracks for benchmarking studies (e.g. BioBERT) and organization of other initiatives like BioCreative or IberLEF, we propose three semantic labelling subtracks using the widely used DeCS vocabulary (similar to MeSH terms):

**MESINESP-L – Scientific Literature**: for automatic labelling of medical literature abstracts in Spanish (including recent COVID-19 literature).

**MESINESP-T – Clinical trials**: for automatic labelling of clinical trials summaries.

**MESINESP-P – Patents:** for automatic labelling of health-related patents in Spanish to improve patent intelligence.

**Key information**

**Web**:[ https://temu.bsc.es/mesinesp2](https://temu.bsc.es/mesinesp2) 

**Registration**:[ http://clef2021-labs-registration.dei.unipd.it/](http://clef2021-labs-registration.dei.unipd.it/) (BioASQ **Task 3 - MESINESP**)

**Data**: [https://doi.org/10.5281/zenodo.4707104](https://doi.org/10.5281/zenodo.4707104)

MESINESP2 is organized in close collaboration with widely used multilingual medical literature databases (BIREME/WHO, ISCIII/Spain), which expressed a direct need for advanced technologies to accelerate manual indexing efforts for the contents in Spanish (spoken globally by over 572 million people). They do face a challenge to keep up with the increasing number of published medical papers when using purely manual indexing.

A large manually indexed collection of training documents will be provided. These documents have already been automatically annotated (&gt; 1.5 million entity mentions) with  medical entities such as diseases, medical procedures, drugs and symptoms to facilitate the use of complementary strategies like *multi-label classification*, *multilingual transformers*, *graph matching*, *text similarity,* *advanced term matching* or *named entity recognition components*. 

Participating systems will be directly useful for ongoing medical literature indexing efforts, and thus improve competitive intelligence/prior art searches, enable complex search queries needed for evidence-based medicine, clinical decision making, or elaboration of clinical practice guidelines and serve as base for future tasks on semantic indexing of medical records or content in other languages. 

**Important dates**

* April 19: Updated Train, Validation and Test sets release
* April 19: Additional datasets release (Medical entities present in documents)
* April , 30: BioASQ9 Lab u/CLEF 2021 Registration Deadline
* May, 7: Start of the evaluation period
* May, 17: End of the evaluation period
* May,28 :Submission of Participant Papers at CLEF2021
* July, 2: Camera ready paper submission.
* Sep 21-24: CLEF 2021 Conference

**Publications and BioASQ/CLEF2021 workshop**

Teams participating in MESINESP2 will be invited to contribute a systems description paper for the BioASQ (CLEF 2021) Working Notes proceedings, and a short presentation of their approach at the BioASQ 2021 workshop.

**Main Track organizers**

* **Martin Krallinger**, Barcelona Supercomputing Center (BSC), Spain.
* **Luis Gascó**, Barcelona Supercomputing Center (BSC), Spain.
* **Anastasios Nentidis**, National Center for Scientific Research Demokritos, Greece.
* **Elena Primo-Peña,** Biblioteca Nacional de Ciencias de Salud. Instituto de Salud Carlos III, Spain.
* **Cristina Bojo Canales,** Biblioteca Nacional de Ciencias de la Salud. Instituto de Salud Carlos III, Spain.
* **George Paliouras**, National Center for Scientific Research Demokritos, Greece.
* **Anastasia Krithara**, National Center for Scientific Research Demokritos, Greece.
* **Renato Murasaki,** BIREME – Organización Panamericana de la Salud (WHO), Brasil.

**Scientific Committee**

* **Tristan Naumann,** Microsoft Research (USA)
* **Prof. Xavier Tannier,** Sorbonne Université and LIMICS (France)
* **Lucy Lu Wang,** Allen Institute for AI (AI2) (USA)
* **Prof. David Camacho,** Applied Intelligence and Data Analysis Research Group, Universidad Politécnica de Madrid (Spain)
* **Prof. Oscar Corcho,** Ontology Engineering Group, Universidad Politécnica de Madrid (Spain)
* **Parminder Batia,** Amazon Health AI (USA)
* **Prof. Irena Spasic,** School of Computer Science &amp; Informatics, co-Director of the Data Innovation Research Institute, Cardiff University (UK)
* **Jose Luis Redondo García,** Amazon Alexa, Amazon (UK)
* **Carlos Badenes-Olmedo,** Ontology Engineering Group, Universidad Politécnica de Madrid (Spain)
* **Prof. Allan Hanbury,**  E-Commerce Research Unit in the Faculty of Informatics, TU Wien (Austria)
* **Prof. Alfonso Valencia,** Barcelona Supercomputing Center (Spain)
* **Prof. Stefan J. Darmoni,** Department of Biomedical Informatics, Rouen University Hospital (France) and LIMICS (France)
* **Rezarta Islamaj,** National Center for Biotechnology Information (USA)
* **Prof. Rafael Berlanga Llavori,** Universidad Jaume I (Spain)
* **Prof. Henning Müller,** University of Applied Sciences Western Switzerland – Valais (Switzerland)
* **Prof. Gareth J.F. Jones,** School of Computing at Dublin City University (Ireland)
* **Georg Rehm,** Deutsches Forschungszentrum für Künstliche Intelligenz (Germany)
* **Petr Knoth,** Research Studios Austria Forschungsgesellschaft mbH (Austria)
* **Natalia Manola,** CEO at OpenAIRE AMKE (Greece)
* **Prof. Jesús Tramullas,** Departamento de Ciencias de la Documentación e Historia de la Ciencia, Universidad de Zaragoza (Spain)",t2_y7ny0,False,,0,False,"[Call For Participants] MESINESP2 (BioASQ / CLEF2021 shared task) on semantic indexing of heterogenous health content: literature, clinical trials and patents",[],r/LanguageTechnology,False,6,,0,,False,t3_mwqxru,False,dark,0.83,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1619196565.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;strong&gt;*** CFP2  MESINESP2 track: Medical Semantic Indexing (BioASQ – CLEF 2021) **\&lt;/strong&gt;*&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://temu.bsc.es/mesinesp2/""&gt;https://temu.bsc.es/mesinesp2/&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MESINESP2 Awards by BSC-Plan TL [2,700€]&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Test sets and additional data are now available&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There is a pressing need for advanced multilingual semantic search strategies for health related content like literature, patents and clinical trials (cross-genre). The use of semantic search techniques in combination with structured vocabularies is critical for sophisticated searches or content analysis as needed by healthcare professionals, researchers, the pharmaceutical industry, patient groups and private citizens.&lt;/p&gt;

&lt;p&gt;Following the impact of past BioASQ tracks for benchmarking studies (e.g. BioBERT) and organization of other initiatives like BioCreative or IberLEF, we propose three semantic labelling subtracks using the widely used DeCS vocabulary (similar to MeSH terms):&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MESINESP-L – Scientific Literature&lt;/strong&gt;: for automatic labelling of medical literature abstracts in Spanish (including recent COVID-19 literature).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MESINESP-T – Clinical trials&lt;/strong&gt;: for automatic labelling of clinical trials summaries.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MESINESP-P – Patents:&lt;/strong&gt; for automatic labelling of health-related patents in Spanish to improve patent intelligence.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Key information&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Web&lt;/strong&gt;:&lt;a href=""https://temu.bsc.es/mesinesp2""&gt; https://temu.bsc.es/mesinesp2&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Registration&lt;/strong&gt;:&lt;a href=""http://clef2021-labs-registration.dei.unipd.it/""&gt; http://clef2021-labs-registration.dei.unipd.it/&lt;/a&gt; (BioASQ &lt;strong&gt;Task 3 - MESINESP&lt;/strong&gt;)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;: &lt;a href=""https://doi.org/10.5281/zenodo.4707104""&gt;https://doi.org/10.5281/zenodo.4707104&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;MESINESP2 is organized in close collaboration with widely used multilingual medical literature databases (BIREME/WHO, ISCIII/Spain), which expressed a direct need for advanced technologies to accelerate manual indexing efforts for the contents in Spanish (spoken globally by over 572 million people). They do face a challenge to keep up with the increasing number of published medical papers when using purely manual indexing.&lt;/p&gt;

&lt;p&gt;A large manually indexed collection of training documents will be provided. These documents have already been automatically annotated (&amp;gt; 1.5 million entity mentions) with  medical entities such as diseases, medical procedures, drugs and symptoms to facilitate the use of complementary strategies like &lt;em&gt;multi-label classification&lt;/em&gt;, &lt;em&gt;multilingual transformers&lt;/em&gt;, &lt;em&gt;graph matching&lt;/em&gt;, &lt;em&gt;text similarity,&lt;/em&gt; &lt;em&gt;advanced term matching&lt;/em&gt; or &lt;em&gt;named entity recognition components&lt;/em&gt;. &lt;/p&gt;

&lt;p&gt;Participating systems will be directly useful for ongoing medical literature indexing efforts, and thus improve competitive intelligence/prior art searches, enable complex search queries needed for evidence-based medicine, clinical decision making, or elaboration of clinical practice guidelines and serve as base for future tasks on semantic indexing of medical records or content in other languages. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Important dates&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;April 19: Updated Train, Validation and Test sets release&lt;/li&gt;
&lt;li&gt;April 19: Additional datasets release (Medical entities present in documents)&lt;/li&gt;
&lt;li&gt;April , 30: BioASQ9 Lab &lt;a href=""/u/CLEF""&gt;u/CLEF&lt;/a&gt; 2021 Registration Deadline&lt;/li&gt;
&lt;li&gt;May, 7: Start of the evaluation period&lt;/li&gt;
&lt;li&gt;May, 17: End of the evaluation period&lt;/li&gt;
&lt;li&gt;May,28 :Submission of Participant Papers at CLEF2021&lt;/li&gt;
&lt;li&gt;July, 2: Camera ready paper submission.&lt;/li&gt;
&lt;li&gt;Sep 21-24: CLEF 2021 Conference&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Publications and BioASQ/CLEF2021 workshop&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Teams participating in MESINESP2 will be invited to contribute a systems description paper for the BioASQ (CLEF 2021) Working Notes proceedings, and a short presentation of their approach at the BioASQ 2021 workshop.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Main Track organizers&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Martin Krallinger&lt;/strong&gt;, Barcelona Supercomputing Center (BSC), Spain.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Luis Gascó&lt;/strong&gt;, Barcelona Supercomputing Center (BSC), Spain.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Anastasios Nentidis&lt;/strong&gt;, National Center for Scientific Research Demokritos, Greece.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Elena Primo-Peña,&lt;/strong&gt; Biblioteca Nacional de Ciencias de Salud. Instituto de Salud Carlos III, Spain.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cristina Bojo Canales,&lt;/strong&gt; Biblioteca Nacional de Ciencias de la Salud. Instituto de Salud Carlos III, Spain.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;George Paliouras&lt;/strong&gt;, National Center for Scientific Research Demokritos, Greece.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Anastasia Krithara&lt;/strong&gt;, National Center for Scientific Research Demokritos, Greece.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Renato Murasaki,&lt;/strong&gt; BIREME – Organización Panamericana de la Salud (WHO), Brasil.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Scientific Committee&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tristan Naumann,&lt;/strong&gt; Microsoft Research (USA)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Xavier Tannier,&lt;/strong&gt; Sorbonne Université and LIMICS (France)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lucy Lu Wang,&lt;/strong&gt; Allen Institute for AI (AI2) (USA)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. David Camacho,&lt;/strong&gt; Applied Intelligence and Data Analysis Research Group, Universidad Politécnica de Madrid (Spain)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Oscar Corcho,&lt;/strong&gt; Ontology Engineering Group, Universidad Politécnica de Madrid (Spain)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parminder Batia,&lt;/strong&gt; Amazon Health AI (USA)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Irena Spasic,&lt;/strong&gt; School of Computer Science &amp;amp; Informatics, co-Director of the Data Innovation Research Institute, Cardiff University (UK)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jose Luis Redondo García,&lt;/strong&gt; Amazon Alexa, Amazon (UK)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Carlos Badenes-Olmedo,&lt;/strong&gt; Ontology Engineering Group, Universidad Politécnica de Madrid (Spain)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Allan Hanbury,&lt;/strong&gt;  E-Commerce Research Unit in the Faculty of Informatics, TU Wien (Austria)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Alfonso Valencia,&lt;/strong&gt; Barcelona Supercomputing Center (Spain)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Stefan J. Darmoni,&lt;/strong&gt; Department of Biomedical Informatics, Rouen University Hospital (France) and LIMICS (France)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rezarta Islamaj,&lt;/strong&gt; National Center for Biotechnology Information (USA)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Rafael Berlanga Llavori,&lt;/strong&gt; Universidad Jaume I (Spain)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Henning Müller,&lt;/strong&gt; University of Applied Sciences Western Switzerland – Valais (Switzerland)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Gareth J.F. Jones,&lt;/strong&gt; School of Computing at Dublin City University (Ireland)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Georg Rehm,&lt;/strong&gt; Deutsches Forschungszentrum für Künstliche Intelligenz (Germany)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Petr Knoth,&lt;/strong&gt; Research Studios Austria Forschungsgesellschaft mbH (Austria)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Natalia Manola,&lt;/strong&gt; CEO at OpenAIRE AMKE (Greece)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Jesús Tramullas,&lt;/strong&gt; Departamento de Ciencias de la Documentación e Historia de la Ciencia, Universidad de Zaragoza (Spain)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mwqxru,True,,luisgasco,,0,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/mwqxru/call_for_participants_mesinesp2_bioasq_clef2021/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mwqxru/call_for_participants_mesinesp2_bioasq_clef2021/,30199,1619167765.0,0,,False,,,,,,6098
311,,LanguageTechnology,"I didn't build it. But I found it interesting. What could be other alternative approach to solve this problem?

[Research Paper](https://publications.waset.org/10011676/ai-tutor-a-computer-science-domain-knowledge-graph-based-qa-system-on-jade-platform)",t2_auwgbh53,False,,0,False,AI Tutor: A Computer Science Domain Knowledge Graph-Based QA System,[],r/LanguageTechnology,False,6,,0,,False,t3_mwakja,False,dark,0.92,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,True,,False,,[],{},,True,,1619142933.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I didn&amp;#39;t build it. But I found it interesting. What could be other alternative approach to solve this problem?&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://publications.waset.org/10011676/ai-tutor-a-computer-science-domain-knowledge-graph-based-qa-system-on-jade-platform""&gt;Research Paper&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mwakja,True,,opensourcecolumbus,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mwakja/ai_tutor_a_computer_science_domain_knowledge/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mwakja/ai_tutor_a_computer_science_domain_knowledge/,30199,1619114133.0,0,,False,,,,,,253
312,,LanguageTechnology,"I was reading reviews for a food outlet and saw tags like ""wrap"" ""hummus"" ""chicken"" ""lady"" ""platter"" ""sauce""",t2_5r4mo7fh,False,,0,False,Any idea what technology google map uses to tag reviews into categories,[],r/LanguageTechnology,False,6,,0,,False,t3_mw41p3,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1619124744.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was reading reviews for a food outlet and saw tags like &amp;quot;wrap&amp;quot; &amp;quot;hummus&amp;quot; &amp;quot;chicken&amp;quot; &amp;quot;lady&amp;quot; &amp;quot;platter&amp;quot; &amp;quot;sauce&amp;quot;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mw41p3,True,,Epiphany925,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mw41p3/any_idea_what_technology_google_map_uses_to_tag/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mw41p3/any_idea_what_technology_google_map_uses_to_tag/,30199,1619095944.0,0,,False,,,,,,108
313,,LanguageTechnology,"I am not sure if this question fit here, but I am working with a regression task on a songs dataset,  one of the variables is the name, I am trying to use it to get a better prediction.

What I did:
I use the python library langdetec to get the language of the name, transform this categorical variable to numerical, and it work good, but I want to improve it.

- Langdetec is not too good, I saw Spanish songs catalogued as italian, but others better libraries takes a lot of time to evaluate all the dataset (this dataset has 130000 rows).
- My transformation from categorical to numerical is basic (number=1, other=2, af=3, ..., en=13, etc).
- I think using a word embedded would be too much for this kind of problem, there is other 17 variables in the data set and the results are good, I just want to take advantage of the name too.

Does anybody recommend me something?
It would be great if anybody recommend me a paper to base on because this task is for an assignment.

Also, I am thinking in a future research about best way to have language categorical variable or language prediction based on frequency.

Thanks in advance for any comments.",t2_662s6zz6,False,,0,False,Recommendation about regression + language,[],r/LanguageTechnology,False,6,,0,,False,t3_mwdzeq,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1619152117.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am not sure if this question fit here, but I am working with a regression task on a songs dataset,  one of the variables is the name, I am trying to use it to get a better prediction.&lt;/p&gt;

&lt;p&gt;What I did:
I use the python library langdetec to get the language of the name, transform this categorical variable to numerical, and it work good, but I want to improve it.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Langdetec is not too good, I saw Spanish songs catalogued as italian, but others better libraries takes a lot of time to evaluate all the dataset (this dataset has 130000 rows).&lt;/li&gt;
&lt;li&gt;My transformation from categorical to numerical is basic (number=1, other=2, af=3, ..., en=13, etc).&lt;/li&gt;
&lt;li&gt;I think using a word embedded would be too much for this kind of problem, there is other 17 variables in the data set and the results are good, I just want to take advantage of the name too.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Does anybody recommend me something?
It would be great if anybody recommend me a paper to base on because this task is for an assignment.&lt;/p&gt;

&lt;p&gt;Also, I am thinking in a future research about best way to have language categorical variable or language prediction based on frequency.&lt;/p&gt;

&lt;p&gt;Thanks in advance for any comments.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mwdzeq,True,,alandragonrojo,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mwdzeq/recommendation_about_regression_language/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mwdzeq/recommendation_about_regression_language/,30199,1619123317.0,0,,False,,,,,,1151
314,,LanguageTechnology,"If anyone is teaching NLP this summer, feel free to use any of these free assignments from OpenClass: [https://openclass.ai/catalog/nlp](https://openclass.ai/catalog/nlp)

These assignments leverage learning principles proven to optimize knowledge retention, and identify &amp; bridge knowledge gaps to personalize the experience for learners. The idea is to provide a low-stakes environment for learners to master fundamental concepts at their own pace.

Our mission with this project is to improve &amp; democratize education. We're looking to grow our community of NLP educators, so if you're interested in contributing, feel free to join on. (Note: you can also keep your assignments private.)",t2_2uzfmau,False,,0,False,Free NLP assignments,[],r/LanguageTechnology,False,6,,0,,False,t3_mvjzhp,False,dark,0.98,,public,47,0,{},,False,[],,False,False,,{},,False,47,,False,False,,False,,[],{},,True,,1619052836.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If anyone is teaching NLP this summer, feel free to use any of these free assignments from OpenClass: &lt;a href=""https://openclass.ai/catalog/nlp""&gt;https://openclass.ai/catalog/nlp&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;These assignments leverage learning principles proven to optimize knowledge retention, and identify &amp;amp; bridge knowledge gaps to personalize the experience for learners. The idea is to provide a low-stakes environment for learners to master fundamental concepts at their own pace.&lt;/p&gt;

&lt;p&gt;Our mission with this project is to improve &amp;amp; democratize education. We&amp;#39;re looking to grow our community of NLP educators, so if you&amp;#39;re interested in contributing, feel free to join on. (Note: you can also keep your assignments private.)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mvjzhp,True,,galalalal,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mvjzhp/free_nlp_assignments/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mvjzhp/free_nlp_assignments/,30199,1619024036.0,0,,False,,,,,,697
315,,LanguageTechnology,"I am trying to do multi classification for sentences.

What are the best ways to make sure my clusters are homogenous?

(I am having a coverage of aprox 80%))

Edit; More context as requested;

I am working on clustering methods for textual data (sentences). Implemented an unsupervised clustering method . When I go through the output, it makes sense. I went through literatures to see what metrics would to tell us ""how good the clusters are"" and got confused. This will help me compare my methods to other methods out there and maybe tweak my method to perform better. I would like to know from the fellow researchers if there are methods which worked best for you which :  
 1. gives a score on homogeneity of clusters  
 2. gives a score on what's the optimal inter-cluster distance.  
 3. gives significance of a cluster  
 4. gives a number on ""optimal number of clusters""",t2_b7kbdmn4,False,,0,False,Measuring accuracy of my clusters,[],r/LanguageTechnology,False,6,,0,,False,t3_mw4a6p,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,1619150550.0,,[],{},,True,,1619125494.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to do multi classification for sentences.&lt;/p&gt;

&lt;p&gt;What are the best ways to make sure my clusters are homogenous?&lt;/p&gt;

&lt;p&gt;(I am having a coverage of aprox 80%))&lt;/p&gt;

&lt;p&gt;Edit; More context as requested;&lt;/p&gt;

&lt;p&gt;I am working on clustering methods for textual data (sentences). Implemented an unsupervised clustering method . When I go through the output, it makes sense. I went through literatures to see what metrics would to tell us &amp;quot;how good the clusters are&amp;quot; and got confused. This will help me compare my methods to other methods out there and maybe tweak my method to perform better. I would like to know from the fellow researchers if there are methods which worked best for you which :&lt;br/&gt;
 1. gives a score on homogeneity of clusters&lt;br/&gt;
 2. gives a score on what&amp;#39;s the optimal inter-cluster distance.&lt;br/&gt;
 3. gives significance of a cluster&lt;br/&gt;
 4. gives a number on &amp;quot;optimal number of clusters&amp;quot;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mw4a6p,True,,TadpoleAware1774,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mw4a6p/measuring_accuracy_of_my_clusters/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mw4a6p/measuring_accuracy_of_my_clusters/,30199,1619096694.0,0,,False,,,,,,879
316,,LanguageTechnology,,t2_b3tr954r,False,,0,False,How to make a multilingual translator like Google Translate,[],r/LanguageTechnology,False,6,,0,,False,t3_mvlh37,False,dark,1.0,,public,15,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/HuZq5KkLx8Q?list=PL_49VD9KwQ_ObGMW5g9hMOLnDY01NHv91"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': ""Let's Recreate Google Translate! | Neural Machine Translation"", 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/HuZq5KkLx8Q?list=PL_49VD9KwQ_ObGMW5g9hMOLnDY01NHv91"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Slick Blue', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/HuZq5KkLx8Q/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC66Ggxy8MHX9DCDohdRYDTA'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/HuZq5KkLx8Q?list=PL_49VD9KwQ_ObGMW5g9hMOLnDY01NHv91"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mvlh37', 'height': 200}",,False,15,,False,False,,False,,[],{},,False,,1619056919.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mvlh37,True,,SlickBlueML,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mvlh37/how_to_make_a_multilingual_translator_like_google/,all_ads,False,https://www.youtube.com/watch?v=HuZq5KkLx8Q&amp;list=PL_49VD9KwQ_ObGMW5g9hMOLnDY01NHv91&amp;index=2&amp;t=1s,30199,1619028119.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': ""Let's Recreate Google Translate! | Neural Machine Translation"", 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/HuZq5KkLx8Q?list=PL_49VD9KwQ_ObGMW5g9hMOLnDY01NHv91"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Slick Blue', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/HuZq5KkLx8Q/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC66Ggxy8MHX9DCDohdRYDTA'}}",False,https://www.youtube.com/watch?v=HuZq5KkLx8Q&amp;list=PL_49VD9KwQ_ObGMW5g9hMOLnDY01NHv91&amp;index=2&amp;t=1s,,,,,0
317,,LanguageTechnology,"My dataset looks like following:

    smile        7957
    kind         3399
    angry        3288
    surprised    1702
    sad          1124

My goal is to classify an emotion using BERT.

and I decided to use SMOTE to balance it back

    train['numeric'] = train['emotions'].replace(to_replace=['smile', 'kind','angry','surprised','sad'], value=[0,1,2,3,4])
    
    smote = SMOTE()
    tv = TfidfVectorizer(stop_words=None, max_features=100000)
    tfidf = tv.fit_transform(train['content'].values.astype('U'))
    X_SMOTE, y_SMOTE = smote.fit_resample(tfidf, train['numeric'])
    

Should I even worry about balancing this dataset when using the BERT? or it is handled by BERT already?",t2_6c0lef9b,False,,0,False,Do we need to balance a dataset when using BERT,[],r/LanguageTechnology,False,6,,0,,False,t3_mvmujf,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1619060656.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My dataset looks like following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;smile        7957
kind         3399
angry        3288
surprised    1702
sad          1124
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My goal is to classify an emotion using BERT.&lt;/p&gt;

&lt;p&gt;and I decided to use SMOTE to balance it back&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train[&amp;#39;numeric&amp;#39;] = train[&amp;#39;emotions&amp;#39;].replace(to_replace=[&amp;#39;smile&amp;#39;, &amp;#39;kind&amp;#39;,&amp;#39;angry&amp;#39;,&amp;#39;surprised&amp;#39;,&amp;#39;sad&amp;#39;], value=[0,1,2,3,4])

smote = SMOTE()
tv = TfidfVectorizer(stop_words=None, max_features=100000)
tfidf = tv.fit_transform(train[&amp;#39;content&amp;#39;].values.astype(&amp;#39;U&amp;#39;))
X_SMOTE, y_SMOTE = smote.fit_resample(tfidf, train[&amp;#39;numeric&amp;#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Should I even worry about balancing this dataset when using the BERT? or it is handled by BERT already?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mvmujf,True,,strangeguy111,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mvmujf/do_we_need_to_balance_a_dataset_when_using_bert/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mvmujf/do_we_need_to_balance_a_dataset_when_using_bert/,30199,1619031856.0,0,,False,,,,,,693
318,,LanguageTechnology,,t2_9lo3x1bz,False,,1,False,How do you guys find/ keep up to date with the latest NLP papers?,[],r/LanguageTechnology,False,6,,0,,False,t3_mv8vn3,False,dark,1.0,,public,35,1,{},,False,[],,False,False,,{},,False,35,,False,False,,False,,[],{'gid_2': 1},,True,,1619011104.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 500, 'id': 'gid_2', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 100, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/gold_512.png', 'days_of_premium': 7, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/gold_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Gold', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/gold_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/gold_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mv8vn3,True,,wherll,,14,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mv8vn3/how_do_you_guys_find_keep_up_to_date_with_the/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mv8vn3/how_do_you_guys_find_keep_up_to_date_with_the/,30199,1618982304.0,0,,False,,,,,,0
319,,LanguageTechnology,,t2_l6ugg,False,,0,False,"The Conversational AI &amp; NLP Summit is hosting speakers from DeepMind, Facebook, Nestle, McDonald's and more next week. See the full list of talks, networking opps and speakers below!",[],r/LanguageTechnology,False,6,,0,,False,t3_mvdqql,False,dark,1.0,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,False,,1619033792.0,text,6,,,text,re-work.co,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mvdqql,True,,nikitaljohnson,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mvdqql/the_conversational_ai_nlp_summit_is_hosting/,all_ads,False,https://www.re-work.co/events/conversational-ai-nlp-summit-2021?utm_source=LK&amp;utm_medium=Promo&amp;utm_campaign=LK_Conv_NLP_LC,30199,1619004992.0,0,,False,https://www.re-work.co/events/conversational-ai-nlp-summit-2021?utm_source=LK&amp;utm_medium=Promo&amp;utm_campaign=LK_Conv_NLP_LC,,,,,0
320,,LanguageTechnology,"Hello

I am a student and I am working on emotion analysis with deep learning. My supervisor asked me to extract semantic features from the text ( convert raw data into useful semantic features ) before using deep learning. But I am confused when I read research about text classification with deep learning. DL does not need feature extraction only using different types of word embedding to convert data, I need to clarify this. Is it possible to extract features before applying Deep Learning.? Is there any research that can help me with that? Any suggestions tools or techniques to extract semantic features from the text.",t2_tx187ox,False,,0,False,Using Deep Learning Based On Semantic Features for Emotion Classification in Tweets,[],r/LanguageTechnology,False,6,,0,,False,t3_mvlr8c,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619057685.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello&lt;/p&gt;

&lt;p&gt;I am a student and I am working on emotion analysis with deep learning. My supervisor asked me to extract semantic features from the text ( convert raw data into useful semantic features ) before using deep learning. But I am confused when I read research about text classification with deep learning. DL does not need feature extraction only using different types of word embedding to convert data, I need to clarify this. Is it possible to extract features before applying Deep Learning.? Is there any research that can help me with that? Any suggestions tools or techniques to extract semantic features from the text.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mvlr8c,True,,Tahani_cs,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mvlr8c/using_deep_learning_based_on_semantic_features/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mvlr8c/using_deep_learning_based_on_semantic_features/,30199,1619028885.0,0,,False,,,,,,627
321,,LanguageTechnology,"The [huggingface documentation](https://huggingface.co/transformers/v2.5.0/examples.html) states:

&gt;GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss.

Is this a common distinction you'd find in the NLP literature? Is it a sensible distinction in your opinion? While I totally agree with the first category, I don't understand why you would call BERT &amp; co. ""masked language models"", since causal language models do the actual masking in next token prediction.",t2_113mvs,False,,0,False,Is causal language modeling (CLM) vs masked language modeling (MLM) a common distinction in NLP research?,[],r/LanguageTechnology,False,6,,0,,False,t3_mvepoi,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1619037275.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The &lt;a href=""https://huggingface.co/transformers/v2.5.0/examples.html""&gt;huggingface documentation&lt;/a&gt; states:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Is this a common distinction you&amp;#39;d find in the NLP literature? Is it a sensible distinction in your opinion? While I totally agree with the first category, I don&amp;#39;t understand why you would call BERT &amp;amp; co. &amp;quot;masked language models&amp;quot;, since causal language models do the actual masking in next token prediction.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mvepoi,True,,EntropyGoAway,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mvepoi/is_causal_language_modeling_clm_vs_masked/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mvepoi/is_causal_language_modeling_clm_vs_masked/,30199,1619008475.0,0,,False,,,,,,571
322,,LanguageTechnology,"Hey everyone! Made a post and started delving into this space a few months ago when i decided to take this on for my Final Year Project. Find my post here: [https://www.reddit.com/r/LanguageTechnology/comments/jvo6ec/what\_is\_the\_technology\_behind\_automated\_insights/?utm\_source=share&amp;utm\_medium=web2x&amp;context=3](https://www.reddit.com/r/LanguageTechnology/comments/jvo6ec/what_is_the_technology_behind_automated_insights/?utm_source=share&amp;utm_medium=web2x&amp;context=3)

&amp;#x200B;

I've been building and simultaneously reading papers and enhancing my designs. Wanted to ask if anyone else is or has been working on this type of projects. Would love to know if there is a community in this space or any open source projects happening because from what i see, people working on this would be mainly also working in the companies that are building these systems :/ Would love to hear what challenges have been faced while building these systems. Coding is hard enough, creating a system that generates text that makes grammatical sense and fluent is SUPER difficult.",t2_1lcmbnjy,False,,0,False,Natural Language Generation Systems - are there any communities or open source projects around here?,[],r/LanguageTechnology,False,6,,0,,False,t3_mv94ps,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619012264.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone! Made a post and started delving into this space a few months ago when i decided to take this on for my Final Year Project. Find my post here: &lt;a href=""https://www.reddit.com/r/LanguageTechnology/comments/jvo6ec/what_is_the_technology_behind_automated_insights/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3""&gt;https://www.reddit.com/r/LanguageTechnology/comments/jvo6ec/what_is_the_technology_behind_automated_insights/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been building and simultaneously reading papers and enhancing my designs. Wanted to ask if anyone else is or has been working on this type of projects. Would love to know if there is a community in this space or any open source projects happening because from what i see, people working on this would be mainly also working in the companies that are building these systems :/ Would love to hear what challenges have been faced while building these systems. Coding is hard enough, creating a system that generates text that makes grammatical sense and fluent is SUPER difficult.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mv94ps,True,,jffryteo,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mv94ps/natural_language_generation_systems_are_there_any/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mv94ps/natural_language_generation_systems_are_there_any/,30199,1618983464.0,0,,False,,,,,,1088
323,,LanguageTechnology,,t2_21qaqh1p,False,,0,False,AI-Generated Blog Content with GPT-Neo (GPT-3 Alternative) - Python Web App,[],r/LanguageTechnology,False,6,,0,,False,t3_muzgu1,False,dark,0.8,,public,6,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/d_xRYyy2LFM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'AI-Generated Blog Content with GPT-Neo (GPT-3 Alternative) + Gradio | Python ML Web App', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/d_xRYyy2LFM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/d_xRYyy2LFM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/d_xRYyy2LFM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/muzgu1', 'height': 200}",,False,6,,False,False,,False,,[],{},,False,,1618978969.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,muzgu1,True,,dulldata,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/muzgu1/aigenerated_blog_content_with_gptneo_gpt3/,all_ads,False,https://youtu.be/d_xRYyy2LFM,30199,1618950169.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'AI-Generated Blog Content with GPT-Neo (GPT-3 Alternative) + Gradio | Python ML Web App', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/d_xRYyy2LFM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/d_xRYyy2LFM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,https://youtu.be/d_xRYyy2LFM,,,,,0
324,,LanguageTechnology,"I am following this tutorial  [dp\_tutorials/Tutorial\_3\_RU\_Fine\_tuning\_BERT\_classifier.ipynb at master · deepmipt/dp\_tutorials · GitHub](https://github.com/deepmipt/dp_tutorials/blob/master/russian_tutorials/Tutorial_3_RU_Fine_tuning_BERT_classifier.ipynb)

However, I am constantly running out of memory so I decided to use Gradient Accumulation to reduce the memory size as it was suggested in my previous post. How can I add it to my model? I can’t find any tutorial that explains it well.

&amp;#x200B;

I already reduced the batch size however it did not helped that much

Thank you.",t2_6c0lef9b,False,,0,False,How to add the Gradient Accumulation to my model?,[],r/LanguageTechnology,False,6,,0,,False,t3_mv2tzf,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1618988604.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am following this tutorial  &lt;a href=""https://github.com/deepmipt/dp_tutorials/blob/master/russian_tutorials/Tutorial_3_RU_Fine_tuning_BERT_classifier.ipynb""&gt;dp_tutorials/Tutorial_3_RU_Fine_tuning_BERT_classifier.ipynb at master · deepmipt/dp_tutorials · GitHub&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;However, I am constantly running out of memory so I decided to use Gradient Accumulation to reduce the memory size as it was suggested in my previous post. How can I add it to my model? I can’t find any tutorial that explains it well.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I already reduced the batch size however it did not helped that much&lt;/p&gt;

&lt;p&gt;Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mv2tzf,True,,strangeguy111,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mv2tzf/how_to_add_the_gradient_accumulation_to_my_model/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mv2tzf/how_to_add_the_gradient_accumulation_to_my_model/,30199,1618959804.0,0,,False,,,,,,595
325,,LanguageTechnology,"Hi everyone! I'm currently trying to develop a text classification program that detects emotions and social instances from written text of individuals diagnosed with depression. I thought it'd be a good idea to use lists of words for specific categories in order to build a vocabulary, like for social words I used ""family, friends, girlfriend"" and so on. This classification task is still simple though because it doesn't take into account context and negation (""not happy"" = unhappy). For example, the program can detect a word like ""bad"" but that word could be unrelated, like ""bad driver"". That doesn't give me any hint about the emotional state of that person but ""bad"" still gets detected as part of ""bad driver"". The downside of making a vocabulary using a list is that it can't contain complex words such as ""pissed off"" because they're two words and the text is tokenized at the beginning of the process, so they are passed in as two different words, ""piss"" and ""off"".  I'm trying to implement bigrams to add a little bit of context. I'm open to any suggestion and ready to hear from you.",t2_b31us5mn,False,,0,False,Use NLP to detect and classify emotion in text,[],r/LanguageTechnology,False,6,,0,,False,t3_mv2f32,False,dark,0.8,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1618987362.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone! I&amp;#39;m currently trying to develop a text classification program that detects emotions and social instances from written text of individuals diagnosed with depression. I thought it&amp;#39;d be a good idea to use lists of words for specific categories in order to build a vocabulary, like for social words I used &amp;quot;family, friends, girlfriend&amp;quot; and so on. This classification task is still simple though because it doesn&amp;#39;t take into account context and negation (&amp;quot;not happy&amp;quot; = unhappy). For example, the program can detect a word like &amp;quot;bad&amp;quot; but that word could be unrelated, like &amp;quot;bad driver&amp;quot;. That doesn&amp;#39;t give me any hint about the emotional state of that person but &amp;quot;bad&amp;quot; still gets detected as part of &amp;quot;bad driver&amp;quot;. The downside of making a vocabulary using a list is that it can&amp;#39;t contain complex words such as &amp;quot;pissed off&amp;quot; because they&amp;#39;re two words and the text is tokenized at the beginning of the process, so they are passed in as two different words, &amp;quot;piss&amp;quot; and &amp;quot;off&amp;quot;.  I&amp;#39;m trying to implement bigrams to add a little bit of context. I&amp;#39;m open to any suggestion and ready to hear from you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mv2f32,True,,Dr_Funkmachine,,25,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mv2f32/use_nlp_to_detect_and_classify_emotion_in_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mv2f32/use_nlp_to_detect_and_classify_emotion_in_text/,30199,1618958562.0,0,,False,,,,,,1097
326,,LanguageTechnology,,t2_i12ut,False,,0,False,Bitextor: a tool for generating translation memories from multilingual websites,[],r/LanguageTechnology,False,6,,0,,False,t3_mur9ki,False,dark,0.93,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,False,,1618956883.0,text,6,,,text,github.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mur9ki,True,,leops,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mur9ki/bitextor_a_tool_for_generating_translation/,all_ads,False,https://github.com/bitextor/bitextor,30199,1618928083.0,0,,False,https://github.com/bitextor/bitextor,,,,,0
327,,LanguageTechnology,"I'm in the process of labeling data that I'll be using for an NLP binary classification project.  Some of my text is longer and some of it is shorter.  

If I have some text (call it two paragraphs), that exhibits traits of both classifications (label A and label not-A), is it better to break the text apart and apply the specific labels to each?  

Or is it better to keep the text together and apply just one of the labels (label A, since it does include some text that is ""A"")?  

If I keep the paragraphs together, I don't want the model to just learn that long text = A.  But if I break it apart, will the model struggle with longer text when it was only trained on single paragraphs?",t2_bkxsovae,False,,0,False,"For binary classification, is it better to train with more focused examples?",[],r/LanguageTechnology,False,6,,0,,False,t3_muzfp7,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1618978881.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m in the process of labeling data that I&amp;#39;ll be using for an NLP binary classification project.  Some of my text is longer and some of it is shorter.  &lt;/p&gt;

&lt;p&gt;If I have some text (call it two paragraphs), that exhibits traits of both classifications (label A and label not-A), is it better to break the text apart and apply the specific labels to each?  &lt;/p&gt;

&lt;p&gt;Or is it better to keep the text together and apply just one of the labels (label A, since it does include some text that is &amp;quot;A&amp;quot;)?  &lt;/p&gt;

&lt;p&gt;If I keep the paragraphs together, I don&amp;#39;t want the model to just learn that long text = A.  But if I break it apart, will the model struggle with longer text when it was only trained on single paragraphs?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,muzfp7,True,,NLMNOP,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/muzfp7/for_binary_classification_is_it_better_to_train/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/muzfp7/for_binary_classification_is_it_better_to_train/,30199,1618950081.0,0,,False,,,,,,690
328,,LanguageTechnology,"Large-scale transformer-based language models have produced significant gains in natural language processing (NLP) in recent years. However, training such models is difficult because no single GPU has enough memory to accommodate parameter totals that have grown exponentially in recent years. Even if these parameters could be trained on a single GPU, limited computing power would result in longer training times without model parallelism. 

In a [paper](https://arxiv.org/pdf/2104.04473.pdf) by NVIDIA, Stanford University, and Microsoft Research, a research team has proposed a new parallelization schedule that improves throughput by more than 10 percent with a comparable memory footprint. The paper demonstrated that such strategies could be composed to achieve high aggregate throughput when training large models with nearly a trillion parameters. 

Summary: [https://www.marktechpost.com/2021/04/19/researchers-from-nvidia-stanford-university-and-microsoft-research-propose-efficient-trillion-parameter-language-model-training-on-gpu-clusters/](https://www.marktechpost.com/2021/04/19/researchers-from-nvidia-stanford-university-and-microsoft-research-propose-efficient-trillion-parameter-language-model-training-on-gpu-clusters/) 

Paper: https://arxiv.org/pdf/2104.04473.pdf

Github: https://github.com/nvidia/megatron-lm",t2_4wudjgid,False,,0,False,"Researchers From NVIDIA, Stanford University and Microsoft Research Propose Efficient Trillion-Parameter Language Model Training on GPU Clusters (Paper and Github link included)",[],r/LanguageTechnology,False,6,,0,,False,t3_mufmdd,False,dark,0.99,,public,25,0,{},,False,[],,False,False,,{},,False,25,,False,False,,False,,[],{},,True,,1618910929.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Large-scale transformer-based language models have produced significant gains in natural language processing (NLP) in recent years. However, training such models is difficult because no single GPU has enough memory to accommodate parameter totals that have grown exponentially in recent years. Even if these parameters could be trained on a single GPU, limited computing power would result in longer training times without model parallelism. &lt;/p&gt;

&lt;p&gt;In a &lt;a href=""https://arxiv.org/pdf/2104.04473.pdf""&gt;paper&lt;/a&gt; by NVIDIA, Stanford University, and Microsoft Research, a research team has proposed a new parallelization schedule that improves throughput by more than 10 percent with a comparable memory footprint. The paper demonstrated that such strategies could be composed to achieve high aggregate throughput when training large models with nearly a trillion parameters. &lt;/p&gt;

&lt;p&gt;Summary: &lt;a href=""https://www.marktechpost.com/2021/04/19/researchers-from-nvidia-stanford-university-and-microsoft-research-propose-efficient-trillion-parameter-language-model-training-on-gpu-clusters/""&gt;https://www.marktechpost.com/2021/04/19/researchers-from-nvidia-stanford-university-and-microsoft-research-propose-efficient-trillion-parameter-language-model-training-on-gpu-clusters/&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/pdf/2104.04473.pdf""&gt;https://arxiv.org/pdf/2104.04473.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Github: &lt;a href=""https://github.com/nvidia/megatron-lm""&gt;https://github.com/nvidia/megatron-lm&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mufmdd,True,,techsucker,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mufmdd/researchers_from_nvidia_stanford_university_and/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mufmdd/researchers_from_nvidia_stanford_university_and/,30199,1618882129.0,0,,False,,,,,,1333
329,,LanguageTechnology,,t2_hkv9s,False,,0,False,Automatic Title Generation for Text with Transformer Language Model (Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_muo3fn,False,dark,1.0,,public,2,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/KhxHo_OQGtI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Automatic Title Generation for Text with Transformer Language Model (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/KhxHo_OQGtI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/KhxHo_OQGtI/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/KhxHo_OQGtI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/muo3fn', 'height': 200}",,False,2,,False,False,,False,,[],{},,False,,1618946304.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,muo3fn,True,,prakhar21,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/muo3fn/automatic_title_generation_for_text_with/,all_ads,False,https://youtu.be/KhxHo_OQGtI,30199,1618917504.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Automatic Title Generation for Text with Transformer Language Model (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/KhxHo_OQGtI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/KhxHo_OQGtI/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/KhxHo_OQGtI,,,,,0
330,,LanguageTechnology,"Hi everyone, 

I am currently doing a project where I am supposed to be using named entity recognition with models such as biobert to process clinical data and possibly link them to ICD (International Classification of Diseases) codes however I am currently extremely lost. 

I am planning on using spacy to do the pre-processing on the data however, I am unsure as to what format biobert needs the dataset in.

Would anyone be able to point me in the right direction?",t2_4utjfbbm,False,,0,False,nlp project with biobert,[],r/LanguageTechnology,False,6,,0,,False,t3_muc80y,False,dark,0.9,,public,7,1,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{'gid_1': 1},,True,,1618899882.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;

&lt;p&gt;I am currently doing a project where I am supposed to be using named entity recognition with models such as biobert to process clinical data and possibly link them to ICD (International Classification of Diseases) codes however I am currently extremely lost. &lt;/p&gt;

&lt;p&gt;I am planning on using spacy to do the pre-processing on the data however, I am unsure as to what format biobert needs the dataset in.&lt;/p&gt;

&lt;p&gt;Would anyone be able to point me in the right direction?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,muc80y,True,,ninjamurai009,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/muc80y/nlp_project_with_biobert/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/muc80y/nlp_project_with_biobert/,30199,1618871082.0,0,,False,,,,,,468
331,,LanguageTechnology,"Hey, people! Hope you are all well!

I've been reading some papers on NER and started to notice a development towards a more general and less strict definition of the task. Firstly, at MUC, only named mentions (proper nouns) would be considered. Then, at ACE, both nominal (common nouns) and pronominal phrases were added and also recursive mentions were considered. Further, other datasets kept exploring broader settings, like LitBank accepting personifications and recursive mentions in literary texts, HAREM enforcing ambiguity, etc.

As a part of this extension of the NER definition, do you believe that non-noun phrases can also be considered as mentions? Do you see other constructions being considered as mentions in the future?  

Also, if there are other interesting extensions you might know or believe may occur, please share!",t2_1uz63xco,False,,0,False,Named Entity Recognition and possible extensions such as Non-noun phrases,[],r/LanguageTechnology,False,6,,0,,False,t3_mud88u,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1618902961.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, people! Hope you are all well!&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been reading some papers on NER and started to notice a development towards a more general and less strict definition of the task. Firstly, at MUC, only named mentions (proper nouns) would be considered. Then, at ACE, both nominal (common nouns) and pronominal phrases were added and also recursive mentions were considered. Further, other datasets kept exploring broader settings, like LitBank accepting personifications and recursive mentions in literary texts, HAREM enforcing ambiguity, etc.&lt;/p&gt;

&lt;p&gt;As a part of this extension of the NER definition, do you believe that non-noun phrases can also be considered as mentions? Do you see other constructions being considered as mentions in the future?  &lt;/p&gt;

&lt;p&gt;Also, if there are other interesting extensions you might know or believe may occur, please share!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mud88u,True,,pauloamed,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mud88u/named_entity_recognition_and_possible_extensions/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mud88u/named_entity_recognition_and_possible_extensions/,30199,1618874161.0,0,,False,,,,,,839
332,,LanguageTechnology,"Hello everyone,

I am currently developing a system (for one of my thesis components) which analyses the mathematical relationships between numbers extracted from a given text and generates alternatives, unfortunately there are requirements such that the numbers can't be negative of a majority of 0.

I am currently making improvements on my current system however I think it would be wise to look at more feasible alternatives as this is for my thesis however I have not found any previous work in this niche. Does anyone have any ideas on this front please?",t2_37a99pnk,False,,0,False,NLP and Mathematical Generation?,[],r/LanguageTechnology,False,6,,0,,False,t3_mu5x5k,False,dark,0.81,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1618882583.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;

&lt;p&gt;I am currently developing a system (for one of my thesis components) which analyses the mathematical relationships between numbers extracted from a given text and generates alternatives, unfortunately there are requirements such that the numbers can&amp;#39;t be negative of a majority of 0.&lt;/p&gt;

&lt;p&gt;I am currently making improvements on my current system however I think it would be wise to look at more feasible alternatives as this is for my thesis however I have not found any previous work in this niche. Does anyone have any ideas on this front please?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mu5x5k,True,,Leli1024,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mu5x5k/nlp_and_mathematical_generation/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mu5x5k/nlp_and_mathematical_generation/,30199,1618853783.0,0,,False,,,,,,560
333,,LanguageTechnology,"Hello, 

I have a few hundred txt documents, each containing a few sentences about someone's history with substance abuse. 

Based on 2 types of substances, I am trying to go through each file and collect the frequencies of 4 entities: **status** (e.g., past, current, none), **method** (e.g., inhale, chew), **amount** (e.g., 2 packs, 3-4 glasses), and **frequency** (e.g., a day). 

Example txt file: ""He does not use tobacco. He sometimes drinks wine. He does not use drugs ever."" 

**My dilemma:** again, I need to scrape the frequencies of these entities (essentially attributes of some type of substance abuse), ***but the wording/sentence structure varies a lot*** \--&gt; the **status** entity, for example, could see 'past', 'currently', 'still', 'sometimes', 'on weekends', 'back in 1984', etc... just a whole lot of things. I think I need to employ some NLP technique to classify/annotate this stuff but I am not sure how. **Any ideas?**

Thank you so much for your consideration.",t2_2ich5u4y,False,,0,False,NLP Help | Scraping Question,[],r/LanguageTechnology,False,6,,0,,False,t3_mtvquw,False,dark,1.0,,public,16,0,{},,False,[],,False,False,,{},,False,16,,False,False,,False,,[],{},,True,,1618847724.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, &lt;/p&gt;

&lt;p&gt;I have a few hundred txt documents, each containing a few sentences about someone&amp;#39;s history with substance abuse. &lt;/p&gt;

&lt;p&gt;Based on 2 types of substances, I am trying to go through each file and collect the frequencies of 4 entities: &lt;strong&gt;status&lt;/strong&gt; (e.g., past, current, none), &lt;strong&gt;method&lt;/strong&gt; (e.g., inhale, chew), &lt;strong&gt;amount&lt;/strong&gt; (e.g., 2 packs, 3-4 glasses), and &lt;strong&gt;frequency&lt;/strong&gt; (e.g., a day). &lt;/p&gt;

&lt;p&gt;Example txt file: &amp;quot;He does not use tobacco. He sometimes drinks wine. He does not use drugs ever.&amp;quot; &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;My dilemma:&lt;/strong&gt; again, I need to scrape the frequencies of these entities (essentially attributes of some type of substance abuse), &lt;strong&gt;&lt;em&gt;but the wording/sentence structure varies a lot&lt;/em&gt;&lt;/strong&gt; --&amp;gt; the &lt;strong&gt;status&lt;/strong&gt; entity, for example, could see &amp;#39;past&amp;#39;, &amp;#39;currently&amp;#39;, &amp;#39;still&amp;#39;, &amp;#39;sometimes&amp;#39;, &amp;#39;on weekends&amp;#39;, &amp;#39;back in 1984&amp;#39;, etc... just a whole lot of things. I think I need to employ some NLP technique to classify/annotate this stuff but I am not sure how. &lt;strong&gt;Any ideas?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Thank you so much for your consideration.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mtvquw,True,,Stutoucan12,,10,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mtvquw/nlp_help_scraping_question/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mtvquw/nlp_help_scraping_question/,30199,1618818924.0,0,,False,,,,,,991
334,,LanguageTechnology,"I would like to start a career in NLP. Before investing my time in NLP, I would like to know if the skills needed in NLP can be transferred to another fields or not. For example, if in the future I want to work in DSP( digital signal processing), or Image processing, can NLP skills applied in these fields? 


Thank you very much! 

Note: I am asking because tomorrow I have an interview for  NLP job in a startup company. My background is CS, networking and a limited knowledge in Genetic algorithms  .

Edit: language mistakes",t2_zj4u90g,False,,0,False,NLP and Skill Transfer,[],r/LanguageTechnology,False,6,,0,,False,t3_mu3v2m,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,1618850666.0,,[],{},,True,,1618877001.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I would like to start a career in NLP. Before investing my time in NLP, I would like to know if the skills needed in NLP can be transferred to another fields or not. For example, if in the future I want to work in DSP( digital signal processing), or Image processing, can NLP skills applied in these fields? &lt;/p&gt;

&lt;p&gt;Thank you very much! &lt;/p&gt;

&lt;p&gt;Note: I am asking because tomorrow I have an interview for  NLP job in a startup company. My background is CS, networking and a limited knowledge in Genetic algorithms  .&lt;/p&gt;

&lt;p&gt;Edit: language mistakes&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mu3v2m,True,,Beginner4ever,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mu3v2m/nlp_and_skill_transfer/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mu3v2m/nlp_and_skill_transfer/,30199,1618848201.0,0,,False,,,,,,529
335,,LanguageTechnology,"Guys I am trying to develop a BERT model that basically predicts an emotion. (Emotion classifier)

I never got a chance to eventually finish my training because I every time run out of Memory Ram

I tried Google Colab (ran out of memory) then tried Kaggle Kernel (ran out of space as well), both 12 and 16 GB RAM.

I do not know what is wrong with it? how people even train this type of model?

&amp;#x200B;

    ......DATA LOADING AND PREPROCESSING...
    
    !pip install deeppavlov
    from deeppavlov.dataset_readers.basic_classification_reader import BasicClassificationDatasetReader
    data = BasicClassificationDatasetReader().read(
        data_path='./',
        train='../input/pikabucsva/train_pikabu_a.csv',
        valid=""../input/pikabucsva/validation_pikabu_a.csv"", 
        test=""../input/pikabucsva/test_pikabu_a.csv"",
        x = 'content',
        y = 'emotions'
    )
    
    #ITERATOR
    from deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator
    # initializing an iterator
    iterator = BasicClassificationDatasetIterator(data, seed=42, shuffle=True)
    
    #BERT PREPROCESSOR
    !python -m deeppavlov install squad_bert
    from deeppavlov.models.preprocessors.bert_preprocessor import BertPreprocessor
    bert_preprocessor = BertPreprocessor(vocab_file=""../input/bertmodel/vocab.txt"",
                                         do_lower_case=False,
                                         max_seq_length=64)
    
    #SIMPLE VOCABULARY
    from deeppavlov.core.data.simple_vocab import SimpleVocabulary
    vocab = SimpleVocabulary(save_path=""./binary_classes.dict"")
    
    
    #ONEHOTTER
    from deeppavlov.models.preprocessors.one_hotter import OneHotter
    one_hotter = OneHotter(depth=vocab.len, 
                           single_vector=True  # means we want to have one vector per sample
                          )
    #PROB TO LABELS
    from deeppavlov.models.classifiers.proba2labels import Proba2Labels
    prob2labels = Proba2Labels(max_proba=True)
    vocab(prob2labels([[0.6, 0.4], 
                       [0.2, 0.8],
                       [0.1, 0.9]]))
    
    
    #BERT CLASSIFIER
    from deeppavlov.models.bert.bert_classifier import BertClassifierModel
    from deeppavlov.metrics.accuracy import sets_accuracy
    
    bert_classifier = BertClassifierModel(
        n_classes=vocab.len,
        return_probas=True,
        one_hot_labels=True,
        bert_config_file=""../input/bertmodel/bert_config.json"",
        pretrained_bert=""../input/bertmodel/bert_model.ckpt"",
        save_path=""sst_bert_model/model"",
        load_path=""sst_bert_model/model"",
        keep_prob=0.5,
        learning_rate=1e-05,
        learning_rate_drop_patience=5,
        learning_rate_drop_div=2.0
        )
    
    #TRAINING
    # Method `get_instances` returns all the samples of particular data field
    x_valid, y_valid = iterator.get_instances(data_type=""valid"")
    # You need to save model only when validation score is higher than previous one.
    # This variable will contain the highest accuracy score
    best_score = 0.
    patience = 2
    impatience = 0
    
    # let's train for 3 epochs
    for ep in range(3):
      
        nbatches = 0
        for x, y in iterator.gen_batches(batch_size=256, 
                                         data_type=""train"", shuffle=True):
            x_feat = bert_preprocessor(x)
            y_onehot = one_hotter(vocab(y))
            bert_classifier.train_on_batch(x_feat, y_onehot)
            print(""Batch done\n"")
            nbatches += 1
            
            if nbatches % 1 == 0:
                # валидируемся каждые 100 батчей
                y_valid_pred = bert_classifier(bert_preprocessor(x_valid))
                score = sets_accuracy(y_valid, vocab(prob2labels(y_valid_pred)))
                print(""Batches done: {}. Valid Accuracy: {}"".format(nbatches, score))
                
        y_valid_pred = bert_classifier(bert_preprocessor(x_valid))
        score = sets_accuracy(y_valid, vocab(prob2labels(y_valid_pred)))
        print(""Epochs done: {}. Valid Accuracy: {}"".format(ep + 1, score))
        if score &gt; best_score:
            bert_classifier.save()
            print(""New best score. Saving model."")
            best_score = score    
            impatience = 0
        else:
            impatience += 1
            if impatience == patience:
                print(""Out of patience. Stop training."")
                break",t2_6c0lef9b,False,,0,False,WHY DO I KEEP RUNNING OUT OF MEMORY (RAM),[],r/LanguageTechnology,False,6,,0,,False,t3_muf1be,False,dark,0.25,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1618908914.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Guys I am trying to develop a BERT model that basically predicts an emotion. (Emotion classifier)&lt;/p&gt;

&lt;p&gt;I never got a chance to eventually finish my training because I every time run out of Memory Ram&lt;/p&gt;

&lt;p&gt;I tried Google Colab (ran out of memory) then tried Kaggle Kernel (ran out of space as well), both 12 and 16 GB RAM.&lt;/p&gt;

&lt;p&gt;I do not know what is wrong with it? how people even train this type of model?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;......DATA LOADING AND PREPROCESSING...

!pip install deeppavlov
from deeppavlov.dataset_readers.basic_classification_reader import BasicClassificationDatasetReader
data = BasicClassificationDatasetReader().read(
    data_path=&amp;#39;./&amp;#39;,
    train=&amp;#39;../input/pikabucsva/train_pikabu_a.csv&amp;#39;,
    valid=&amp;quot;../input/pikabucsva/validation_pikabu_a.csv&amp;quot;, 
    test=&amp;quot;../input/pikabucsva/test_pikabu_a.csv&amp;quot;,
    x = &amp;#39;content&amp;#39;,
    y = &amp;#39;emotions&amp;#39;
)

#ITERATOR
from deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator
# initializing an iterator
iterator = BasicClassificationDatasetIterator(data, seed=42, shuffle=True)

#BERT PREPROCESSOR
!python -m deeppavlov install squad_bert
from deeppavlov.models.preprocessors.bert_preprocessor import BertPreprocessor
bert_preprocessor = BertPreprocessor(vocab_file=&amp;quot;../input/bertmodel/vocab.txt&amp;quot;,
                                     do_lower_case=False,
                                     max_seq_length=64)

#SIMPLE VOCABULARY
from deeppavlov.core.data.simple_vocab import SimpleVocabulary
vocab = SimpleVocabulary(save_path=&amp;quot;./binary_classes.dict&amp;quot;)


#ONEHOTTER
from deeppavlov.models.preprocessors.one_hotter import OneHotter
one_hotter = OneHotter(depth=vocab.len, 
                       single_vector=True  # means we want to have one vector per sample
                      )
#PROB TO LABELS
from deeppavlov.models.classifiers.proba2labels import Proba2Labels
prob2labels = Proba2Labels(max_proba=True)
vocab(prob2labels([[0.6, 0.4], 
                   [0.2, 0.8],
                   [0.1, 0.9]]))


#BERT CLASSIFIER
from deeppavlov.models.bert.bert_classifier import BertClassifierModel
from deeppavlov.metrics.accuracy import sets_accuracy

bert_classifier = BertClassifierModel(
    n_classes=vocab.len,
    return_probas=True,
    one_hot_labels=True,
    bert_config_file=&amp;quot;../input/bertmodel/bert_config.json&amp;quot;,
    pretrained_bert=&amp;quot;../input/bertmodel/bert_model.ckpt&amp;quot;,
    save_path=&amp;quot;sst_bert_model/model&amp;quot;,
    load_path=&amp;quot;sst_bert_model/model&amp;quot;,
    keep_prob=0.5,
    learning_rate=1e-05,
    learning_rate_drop_patience=5,
    learning_rate_drop_div=2.0
    )

#TRAINING
# Method `get_instances` returns all the samples of particular data field
x_valid, y_valid = iterator.get_instances(data_type=&amp;quot;valid&amp;quot;)
# You need to save model only when validation score is higher than previous one.
# This variable will contain the highest accuracy score
best_score = 0.
patience = 2
impatience = 0

# let&amp;#39;s train for 3 epochs
for ep in range(3):

    nbatches = 0
    for x, y in iterator.gen_batches(batch_size=256, 
                                     data_type=&amp;quot;train&amp;quot;, shuffle=True):
        x_feat = bert_preprocessor(x)
        y_onehot = one_hotter(vocab(y))
        bert_classifier.train_on_batch(x_feat, y_onehot)
        print(&amp;quot;Batch done\n&amp;quot;)
        nbatches += 1

        if nbatches % 1 == 0:
            # валидируемся каждые 100 батчей
            y_valid_pred = bert_classifier(bert_preprocessor(x_valid))
            score = sets_accuracy(y_valid, vocab(prob2labels(y_valid_pred)))
            print(&amp;quot;Batches done: {}. Valid Accuracy: {}&amp;quot;.format(nbatches, score))

    y_valid_pred = bert_classifier(bert_preprocessor(x_valid))
    score = sets_accuracy(y_valid, vocab(prob2labels(y_valid_pred)))
    print(&amp;quot;Epochs done: {}. Valid Accuracy: {}&amp;quot;.format(ep + 1, score))
    if score &amp;gt; best_score:
        bert_classifier.save()
        print(&amp;quot;New best score. Saving model.&amp;quot;)
        best_score = score    
        impatience = 0
    else:
        impatience += 1
        if impatience == patience:
            print(&amp;quot;Out of patience. Stop training.&amp;quot;)
            break
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,muf1be,True,,strangeguy111,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/muf1be/why_do_i_keep_running_out_of_memory_ram/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/muf1be/why_do_i_keep_running_out_of_memory_ram/,30199,1618880114.0,0,,False,,,,,,4503
336,,LanguageTechnology,"Hi

about 3 years ago Google showed a demo in which a human-sounding ""AI assistant"" made a phone call to a shop (a hairdresser, if I remember well). Is anybody using this technology in production, for example to accept restaurant/takeaway orders? Is DialogFlow production-ready? What's the SOTA in this field?

Thanks!",t2_hdiwog1,False,,0,False,production-ready NLP/NLU for restaurants?,[],r/LanguageTechnology,False,6,,0,,False,t3_mu0rxw,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1618868252.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi&lt;/p&gt;

&lt;p&gt;about 3 years ago Google showed a demo in which a human-sounding &amp;quot;AI assistant&amp;quot; made a phone call to a shop (a hairdresser, if I remember well). Is anybody using this technology in production, for example to accept restaurant/takeaway orders? Is DialogFlow production-ready? What&amp;#39;s the SOTA in this field?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mu0rxw,True,,_aus_,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mu0rxw/productionready_nlpnlu_for_restaurants/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mu0rxw/productionready_nlpnlu_for_restaurants/,30199,1618839452.0,0,,False,,,,,,318
337,,LanguageTechnology,,t2_5fsp2x6v,False,,0,False,A Rigorous Study on Pretrained Model for NER | Research Papers Summary 014,[],r/LanguageTechnology,False,6,,0,,False,t3_mtixvt,False,dark,0.89,,public,14,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/rL6FvknTdKw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'A Rigorous Study on Pretrained Model for NER | Research Papers Summary 014', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/rL6FvknTdKw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/rL6FvknTdKw/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/rL6FvknTdKw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mtixvt', 'height': 200}",,False,14,,False,False,,False,,[],{},,False,,1618800917.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mtixvt,True,,RyanAI100,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mtixvt/a_rigorous_study_on_pretrained_model_for_ner/,all_ads,False,https://youtu.be/rL6FvknTdKw,30199,1618772117.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'A Rigorous Study on Pretrained Model for NER | Research Papers Summary 014', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/rL6FvknTdKw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/rL6FvknTdKw/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,https://youtu.be/rL6FvknTdKw,,,,,0
338,,LanguageTechnology,"Hey, I've created a tutorial on how to specify a large formula for a regression model using the R programming language. The tutorial shows different tips and tricks to make your code more efficient: [https://statisticsglobe.com/write-model-formula-with-many-variables-in-r](https://statisticsglobe.com/write-model-formula-with-many-variables-in-r)",t2_77cigax1,False,,0,False,Tutorial on how to specify a large formula for a regression model,[],r/LanguageTechnology,False,6,,0,,False,t3_mtvm0d,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1618846994.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, I&amp;#39;ve created a tutorial on how to specify a large formula for a regression model using the R programming language. The tutorial shows different tips and tricks to make your code more efficient: &lt;a href=""https://statisticsglobe.com/write-model-formula-with-many-variables-in-r""&gt;https://statisticsglobe.com/write-model-formula-with-many-variables-in-r&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mtvm0d,True,,JoachimSchork,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mtvm0d/tutorial_on_how_to_specify_a_large_formula_for_a/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mtvm0d/tutorial_on_how_to_specify_a_large_formula_for_a/,30199,1618818194.0,0,,False,,,,,,347
339,,LanguageTechnology,"In our most recent paper, we introduce ""Datasets from Instructions"" (DINO 🦕) and show how LMs can create entire datasets from scratch if provided with instructions. These datasets can be used to train much smaller models.

This is an early draft, so I'd be very happy to hear your thoughts 😊

📄 Paper: https://arxiv.org/abs/2104.07540

🖥️ Code: https://github.com/timoschick/dino",t2_383etasr,False,,0,False,Generating Datasets with Pretrained Language Models,[],r/LanguageTechnology,False,6,,0,,False,t3_mtacs2,False,dark,1.0,,public,15,1,{},,False,[],,False,False,,{},,False,15,,False,False,,False,,[],{},,True,,1618770646.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In our most recent paper, we introduce &amp;quot;Datasets from Instructions&amp;quot; (DINO 🦕) and show how LMs can create entire datasets from scratch if provided with instructions. These datasets can be used to train much smaller models.&lt;/p&gt;

&lt;p&gt;This is an early draft, so I&amp;#39;d be very happy to hear your thoughts 😊&lt;/p&gt;

&lt;p&gt;📄 Paper: &lt;a href=""https://arxiv.org/abs/2104.07540""&gt;https://arxiv.org/abs/2104.07540&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;🖥️ Code: &lt;a href=""https://github.com/timoschick/dino""&gt;https://github.com/timoschick/dino&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mtacs2,True,,timoschick,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mtacs2/generating_datasets_with_pretrained_language/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mtacs2/generating_datasets_with_pretrained_language/,30199,1618741846.0,0,,False,,,,,,379
340,,LanguageTechnology,,t2_hkv9s,False,,0,False,BART: Denoising Sequence-to-Sequence Pre-training for NLG (Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_mt7zun,False,dark,0.91,,public,26,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/BGWpNQHIcs4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'BART: Denoising Sequence-to-Sequence Pre-training for NLG (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/BGWpNQHIcs4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/BGWpNQHIcs4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/BGWpNQHIcs4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mt7zun', 'height': 200}",,False,26,,False,False,,False,,[],{},,False,,1618758176.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mt7zun,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mt7zun/bart_denoising_sequencetosequence_pretraining_for/,all_ads,False,https://youtu.be/BGWpNQHIcs4,30199,1618729376.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'BART: Denoising Sequence-to-Sequence Pre-training for NLG (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/BGWpNQHIcs4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/BGWpNQHIcs4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/BGWpNQHIcs4,,,,,0
341,,LanguageTechnology,"I'm doing some research on biomedical language models and would like to use search engines as a baseline. What I have in mind is something like the [Biomedical Entity Search Tool (BEST)](http://best.korea.ac.kr/). Polysearch is also a candidate, but for some reason it's not working anymore.

I'd appreciate it if someone who knows of other search engines like these ones would be able to provide some tips on where I could find more. Thanks!",t2_m8kccne,False,,0,False,Does anyone know of any biomedical entity search engines out there?,[],r/LanguageTechnology,False,6,,0,,False,t3_mta54d,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1618769584.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m doing some research on biomedical language models and would like to use search engines as a baseline. What I have in mind is something like the &lt;a href=""http://best.korea.ac.kr/""&gt;Biomedical Entity Search Tool (BEST)&lt;/a&gt;. Polysearch is also a candidate, but for some reason it&amp;#39;s not working anymore.&lt;/p&gt;

&lt;p&gt;I&amp;#39;d appreciate it if someone who knows of other search engines like these ones would be able to provide some tips on where I could find more. Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mta54d,True,,Seankala,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mta54d/does_anyone_know_of_any_biomedical_entity_search/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mta54d/does_anyone_know_of_any_biomedical_entity_search/,30199,1618740784.0,0,,False,,,,,,442
342,,LanguageTechnology,"Hello everyone!

Over the last few months I've been knee deep in the process for applying to grad school in compling/language technology, and now that decisions have come back I'm left with the choice between programs at Saarland University and the University of Gothenburg.

My research so far has favored Saarland: it's often mentioned in online forums and lists of the best compling universities, it ranks highly in the CSRankings for NLP in Europe (number 6 is you only consider English language master programs), and appears to be well regarded on this subreddit. Gothenburg, on the other hand, is rarely if ever mentioned, and ranks in the 50s in CSRankings for NLP in Europe. Indeed, I only applied for Gothenburg at the suggestion of a friend who goes there, when I was panicking after getting rejected from Edinburgh (which came as a surprise, given that it was my alma mater).

While this may appear to be an open and shut case, given the above, as far as I can tell Gothenburg does appear to have one major advantage over Saarland: job placement. According to my friend, Gothenburg works closely with Swedish tech firms to place its students, and indeed it actively encourages and facilitates collaborating with local firms on your dissertation. Saarland, on the other hand, does not appear to provide this level of support: according to the Saarland program's study coordinator, Saarland does not offer individual support for placement, and it appears that the university's relationship with local companies is not a strong as Gothenburg's appears to be.

My question is, does anyone here have experience with Saarland in terms of career placement? And is Saarland truly a better overall choice than Gothenburg?

Thank you in advance!",t2_1ybka8j,False,,0,False,Deciding between Saarland and Gothenburg,[],r/LanguageTechnology,False,6,,0,,False,t3_mtestc,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1618788017.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;

&lt;p&gt;Over the last few months I&amp;#39;ve been knee deep in the process for applying to grad school in compling/language technology, and now that decisions have come back I&amp;#39;m left with the choice between programs at Saarland University and the University of Gothenburg.&lt;/p&gt;

&lt;p&gt;My research so far has favored Saarland: it&amp;#39;s often mentioned in online forums and lists of the best compling universities, it ranks highly in the CSRankings for NLP in Europe (number 6 is you only consider English language master programs), and appears to be well regarded on this subreddit. Gothenburg, on the other hand, is rarely if ever mentioned, and ranks in the 50s in CSRankings for NLP in Europe. Indeed, I only applied for Gothenburg at the suggestion of a friend who goes there, when I was panicking after getting rejected from Edinburgh (which came as a surprise, given that it was my alma mater).&lt;/p&gt;

&lt;p&gt;While this may appear to be an open and shut case, given the above, as far as I can tell Gothenburg does appear to have one major advantage over Saarland: job placement. According to my friend, Gothenburg works closely with Swedish tech firms to place its students, and indeed it actively encourages and facilitates collaborating with local firms on your dissertation. Saarland, on the other hand, does not appear to provide this level of support: according to the Saarland program&amp;#39;s study coordinator, Saarland does not offer individual support for placement, and it appears that the university&amp;#39;s relationship with local companies is not a strong as Gothenburg&amp;#39;s appears to be.&lt;/p&gt;

&lt;p&gt;My question is, does anyone here have experience with Saarland in terms of career placement? And is Saarland truly a better overall choice than Gothenburg?&lt;/p&gt;

&lt;p&gt;Thank you in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mtestc,True,,Identifies-Birds,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mtestc/deciding_between_saarland_and_gothenburg/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mtestc/deciding_between_saarland_and_gothenburg/,30199,1618759217.0,0,,False,,,,,,1746
343,,LanguageTechnology,"Can someone help me understand what is the purpose of word embeddings in neural machine translation models? 

I understand (mostly) he word embeddings in the context of sentiment analysis and other tasks, but what purpose does it have in the context of neural machine translation? Doesn't the neural model learn everything itself without needing any embedding space?",t2_4m6jgrsb,False,,0,False,Word embeddings and neural machine translation,[],r/LanguageTechnology,False,6,,0,,False,t3_mtb0c7,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1618773780.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can someone help me understand what is the purpose of word embeddings in neural machine translation models? &lt;/p&gt;

&lt;p&gt;I understand (mostly) he word embeddings in the context of sentiment analysis and other tasks, but what purpose does it have in the context of neural machine translation? Doesn&amp;#39;t the neural model learn everything itself without needing any embedding space?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mtb0c7,True,,permadressed,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mtb0c7/word_embeddings_and_neural_machine_translation/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mtb0c7/word_embeddings_and_neural_machine_translation/,30199,1618744980.0,0,,False,,,,,,366
344,,LanguageTechnology,"Many of you were asking for the DUC 2004 dataset. Although it is available through their website, I have also uploaded it to GitHub and Kaggle for a better workflow. You can download it from here. 

&amp;#x200B;

 [UsmanNiazi/DUC-2004-Dataset: This Repo Contains the DUC 2004 Dataset (github.com)](https://github.com/UsmanNiazi/DUC-2004-Dataset)   


 [DUC 2004 Dataset | Kaggle](https://www.kaggle.com/usmanniazi/duc-2004-dataset)",t2_65fqlohg,False,,0,False,DUC 2004 Dataset,[],r/LanguageTechnology,False,6,,0,,False,t3_mt3228,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1618736440.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Many of you were asking for the DUC 2004 dataset. Although it is available through their website, I have also uploaded it to GitHub and Kaggle for a better workflow. You can download it from here. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/UsmanNiazi/DUC-2004-Dataset""&gt;UsmanNiazi/DUC-2004-Dataset: This Repo Contains the DUC 2004 Dataset (github.com)&lt;/a&gt;   &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.kaggle.com/usmanniazi/duc-2004-dataset""&gt;DUC 2004 Dataset | Kaggle&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mt3228,True,,usmannkhan,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mt3228/duc_2004_dataset/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mt3228/duc_2004_dataset/,30199,1618707640.0,0,,False,,,,,,431
345,,LanguageTechnology,"BERT is made up of Transformer encoders, and therefore uses layer normalization instead of batch normalization. Layer normalization normalizes each datapoint independently from other datapoints in the batch. So why would batch size matter for BERT's performance? Where else does it factor in?",t2_8gn71,False,,0,False,Why is batch size a relevant hyperparameter for BERT?,[],r/LanguageTechnology,False,6,,0,,False,t3_mt9ex7,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1618765846.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;BERT is made up of Transformer encoders, and therefore uses layer normalization instead of batch normalization. Layer normalization normalizes each datapoint independently from other datapoints in the batch. So why would batch size matter for BERT&amp;#39;s performance? Where else does it factor in?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mt9ex7,True,,boxdreper,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mt9ex7/why_is_batch_size_a_relevant_hyperparameter_for/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mt9ex7/why_is_batch_size_a_relevant_hyperparameter_for/,30199,1618737046.0,0,,False,,,,,,292
346,,LanguageTechnology,"I’m doing a school project which is building a chatbot to help students learn English and give tips for the TOEIC test (I dont come from English native speaking country), so far my ideas are using masked language models to help solving the reading test in TOEIC and I’m kinda stuck there. Can you give me some suggestions of what to implement and maybe some SOTA open source chatbots and how to tune it into specific domain (English and TOEIC) that would be super life saving. Thanks in advance",t2_10b2sjxx,False,,0,False,Please give me some suggestions.,[],r/LanguageTechnology,False,6,,0,,False,t3_mt3xp2,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1618739980.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m doing a school project which is building a chatbot to help students learn English and give tips for the TOEIC test (I dont come from English native speaking country), so far my ideas are using masked language models to help solving the reading test in TOEIC and I’m kinda stuck there. Can you give me some suggestions of what to implement and maybe some SOTA open source chatbots and how to tune it into specific domain (English and TOEIC) that would be super life saving. Thanks in advance&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mt3xp2,True,,SEMClovesYOU,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mt3xp2/please_give_me_some_suggestions/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mt3xp2/please_give_me_some_suggestions/,30199,1618711180.0,0,,False,,,,,,494
347,,LanguageTechnology,"Hi All, 

I am writing here because I wish to get in touch with those brilliant minds that are active in the group and learning from them :)   
In a course about Finance, I saw how I could use TextBlob and Scrapy on ICO whitepapers and how to exploit experts´ posts in [icobench.com](https://icobench.com) to determine a sentiment - based on Bayes Classifier. We have used Python. 

My goal is to demonstrate that I can be a point of reference for NLP and ML in a company (relative terms). I would proceed following: 

* Setup scraper/crawler to obtain data from several websites
* Store the data in google firebase /firestore
* Cleanup the data for ML
* Create a training set/test set
* Build a ML model (classification/prediction)
* Measure effectiveness
* Report on what I did 

A) Do you suggest me to be familiar first to Vue.js? I have no experience with HTML, CSS and java and I assume that crawl data &gt; store &gt; clean &gt; give label would take a relevant amount of time.

B) If I build a text corpus from wikipedia and use it for my data, which are the problems that I may encounter and take into account? Fx the text blob offers a classifier from movie database but if you apply it on another context like finance, then you need specific label for certain words. 

C) Which models can be more interesting to compare? 

I thank you in advance for the attention given to this post. 

  
Have a good weekend",t2_a80a17xm,False,,0,False,Project NLP / ML from scratch - Beginner,[],r/LanguageTechnology,False,6,,0,,False,t3_mssi5w,False,dark,0.9,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1618701167.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All, &lt;/p&gt;

&lt;p&gt;I am writing here because I wish to get in touch with those brilliant minds that are active in the group and learning from them :)&lt;br/&gt;
In a course about Finance, I saw how I could use TextBlob and Scrapy on ICO whitepapers and how to exploit experts´ posts in &lt;a href=""https://icobench.com""&gt;icobench.com&lt;/a&gt; to determine a sentiment - based on Bayes Classifier. We have used Python. &lt;/p&gt;

&lt;p&gt;My goal is to demonstrate that I can be a point of reference for NLP and ML in a company (relative terms). I would proceed following: &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Setup scraper/crawler to obtain data from several websites&lt;/li&gt;
&lt;li&gt;Store the data in google firebase /firestore&lt;/li&gt;
&lt;li&gt;Cleanup the data for ML&lt;/li&gt;
&lt;li&gt;Create a training set/test set&lt;/li&gt;
&lt;li&gt;Build a ML model (classification/prediction)&lt;/li&gt;
&lt;li&gt;Measure effectiveness&lt;/li&gt;
&lt;li&gt;Report on what I did &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A) Do you suggest me to be familiar first to Vue.js? I have no experience with HTML, CSS and java and I assume that crawl data &amp;gt; store &amp;gt; clean &amp;gt; give label would take a relevant amount of time.&lt;/p&gt;

&lt;p&gt;B) If I build a text corpus from wikipedia and use it for my data, which are the problems that I may encounter and take into account? Fx the text blob offers a classifier from movie database but if you apply it on another context like finance, then you need specific label for certain words. &lt;/p&gt;

&lt;p&gt;C) Which models can be more interesting to compare? &lt;/p&gt;

&lt;p&gt;I thank you in advance for the attention given to this post. &lt;/p&gt;

&lt;p&gt;Have a good weekend&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mssi5w,True,,risar16,,12,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mssi5w/project_nlp_ml_from_scratch_beginner/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mssi5w/project_nlp_ml_from_scratch_beginner/,30199,1618672367.0,0,,False,,,,,,1419
348,,LanguageTechnology,What are some good document intelligence resources to begin with?,t2_7tb2j,False,,0,False,Di resources,[],r/LanguageTechnology,False,6,,0,,False,t3_mswji6,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1618714151.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What are some good document intelligence resources to begin with?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mswji6,True,,gevezex,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mswji6/di_resources/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mswji6/di_resources/,30199,1618685351.0,0,,False,,,,,,65
349,,LanguageTechnology,"Hello community!

So basically what we want to achieve is a benchmarking of pretrained models in order to find out which ones are the most useful/efficient for calculating semantinc text similarity and whether one multi-lingual model would do, or separate models are better for this case (it's nothing really serious, this is for a scholar project but not the study per-se, just a way for us to argument a choice)

The context is not bound to a particular field. In practice, we will be comparing sentences from documents from any and all fields (a situation similar to plagiarism detection) so fine-tuning on a field-specific corpus is not an option and we will test the models as-is.

We are seeking recommendation:

* **For the corpora**: We will be benchmarking for languages other than english (but english is also included). Where can we find parallel corpora (for each language) that are suitable for this task? if not, what means can we use to build a small corpus of our own without doing manual paraphrase
* **For the metrics**: Is cosine distance good enough for the comparison? (by taking 1.0 as the perfect similarity score and the averages of all cosine distances accross the corpus, it will be pretty much like an accuracy  score).
   * What metrics/distances are better suited for the comparison, and what would you recommend for textual similarity in general?

oh and one last point that is not related to this benchmarking in particular. Supposing we have two documents (for instance 2 research papers) that we want to compare agaisnt eachother:

* What tokenization strategy would you recommend, considering we will compare the tokens from document 1 to those of document 2, then highlight the matching text in both documents (again, the usecase is very similar to plagiarism detection). The goal is minimal pairs loss.

Tokenization by sentence/punctuation doesn't seem to play nicely  as sentences in one document can be very long and can include many sentences of the second.

&amp;#x200B;",t2_a6fffcza,False,,0,False,Benchmarking of pretrained models for semantic textual similarity,[],r/LanguageTechnology,False,6,,0,,False,t3_msh1sk,False,dark,0.88,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,1618681863.0,,[],{},,True,,1618650949.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello community!&lt;/p&gt;

&lt;p&gt;So basically what we want to achieve is a benchmarking of pretrained models in order to find out which ones are the most useful/efficient for calculating semantinc text similarity and whether one multi-lingual model would do, or separate models are better for this case (it&amp;#39;s nothing really serious, this is for a scholar project but not the study per-se, just a way for us to argument a choice)&lt;/p&gt;

&lt;p&gt;The context is not bound to a particular field. In practice, we will be comparing sentences from documents from any and all fields (a situation similar to plagiarism detection) so fine-tuning on a field-specific corpus is not an option and we will test the models as-is.&lt;/p&gt;

&lt;p&gt;We are seeking recommendation:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;For the corpora&lt;/strong&gt;: We will be benchmarking for languages other than english (but english is also included). Where can we find parallel corpora (for each language) that are suitable for this task? if not, what means can we use to build a small corpus of our own without doing manual paraphrase&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;For the metrics&lt;/strong&gt;: Is cosine distance good enough for the comparison? (by taking 1.0 as the perfect similarity score and the averages of all cosine distances accross the corpus, it will be pretty much like an accuracy  score).

&lt;ul&gt;
&lt;li&gt;What metrics/distances are better suited for the comparison, and what would you recommend for textual similarity in general?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;oh and one last point that is not related to this benchmarking in particular. Supposing we have two documents (for instance 2 research papers) that we want to compare agaisnt eachother:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What tokenization strategy would you recommend, considering we will compare the tokens from document 1 to those of document 2, then highlight the matching text in both documents (again, the usecase is very similar to plagiarism detection). The goal is minimal pairs loss.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tokenization by sentence/punctuation doesn&amp;#39;t seem to play nicely  as sentences in one document can be very long and can include many sentences of the second.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,msh1sk,True,,Ok-Caregiver3587,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/msh1sk/benchmarking_of_pretrained_models_for_semantic/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/msh1sk/benchmarking_of_pretrained_models_for_semantic/,30199,1618622149.0,0,,False,,,,,,2011
350,,LanguageTechnology,"Can anyone recommend any projects, thesis available online in which nlp methods are applied to data from the legal/law domain? I am trying to learn more about this topic, and so far couldnt find anything other than the ""LEGALBert"" paper.

Does anyone recommend checking out any masters university projects where nlp was applied to data from the legal domain? I already know the basics *about word clouds, lda topic modelling and sentiment analysis - I am looking for a bit more intermediate level stuff about information extraction, transfer learning, summarization, fuzzy match, etc. 

Thanks",t2_3tosvccj,False,,0,False,"Links to nlp applied projects/thesis relating to ""law/legal""",[],r/LanguageTechnology,False,6,,0,,False,t3_msaync,False,dark,1.0,,public,15,0,{},,False,[],,False,False,,{},,False,15,,False,False,,False,,[],{},,True,,1618631340.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can anyone recommend any projects, thesis available online in which nlp methods are applied to data from the legal/law domain? I am trying to learn more about this topic, and so far couldnt find anything other than the &amp;quot;LEGALBert&amp;quot; paper.&lt;/p&gt;

&lt;p&gt;Does anyone recommend checking out any masters university projects where nlp was applied to data from the legal domain? I already know the basics *about word clouds, lda topic modelling and sentiment analysis - I am looking for a bit more intermediate level stuff about information extraction, transfer learning, summarization, fuzzy match, etc. &lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,msaync,True,,jj4646,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/msaync/links_to_nlp_applied_projectsthesis_relating_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/msaync/links_to_nlp_applied_projectsthesis_relating_to/,30199,1618602540.0,0,,False,,,,,,593
351,,LanguageTechnology,"Hello, fellow Subredditors!

I'm curious to know if any of you are familiar with this program and if you could share your opinion about it.

I have a BA in Linguistics and I've done some work during my degree with corpora and have some experience in programming, specifically in Python. Mostly computer applications for linguistic research and digital humanities, more than anything else. I'm interested in studying a master's degree and Germany (for many reasons) is my place of choice. I've already looked at the programs at Tübingen and Stuttgart and the info I've seen on this subreddit about those two has been very useful. However, I haven't found much about the program at Darmstadt, and I'm particularly interested in this program, given its digital humanities component.

I would really appreciate your input! Thanks a lot!",t2_xxvoz,False,,0,False,MA in Linguistic and Literary Computing (TU Darmstadt): Thoughts?,[],r/LanguageTechnology,False,6,,0,,False,t3_ms63a1,False,dark,0.83,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1618617230.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, fellow Subredditors!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m curious to know if any of you are familiar with this program and if you could share your opinion about it.&lt;/p&gt;

&lt;p&gt;I have a BA in Linguistics and I&amp;#39;ve done some work during my degree with corpora and have some experience in programming, specifically in Python. Mostly computer applications for linguistic research and digital humanities, more than anything else. I&amp;#39;m interested in studying a master&amp;#39;s degree and Germany (for many reasons) is my place of choice. I&amp;#39;ve already looked at the programs at Tübingen and Stuttgart and the info I&amp;#39;ve seen on this subreddit about those two has been very useful. However, I haven&amp;#39;t found much about the program at Darmstadt, and I&amp;#39;m particularly interested in this program, given its digital humanities component.&lt;/p&gt;

&lt;p&gt;I would really appreciate your input! Thanks a lot!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ms63a1,True,,Alchelinguist,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ms63a1/ma_in_linguistic_and_literary_computing_tu/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ms63a1/ma_in_linguistic_and_literary_computing_tu/,30199,1618588430.0,0,,False,,,,,,832
352,,LanguageTechnology,"Link to paper: https://arxiv.org/abs/2104.06644

Is BERT just a giant bag-of-words??

Relevant meme: https://imgur.com/a/RGUtOvt",t2_bkc0iaxi,False,,0,False,[D] WTF these results are incredible?!! What do you guys think of this? BERT seems to learn equally well on word-shuffled sentences?,[],r/LanguageTechnology,False,6,,0,,False,t3_mrrcda,False,dark,0.98,,public,53,1,{},,False,[],,False,False,,{},,False,53,,False,False,,False,,[],{},,True,,1618559980.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Link to paper: &lt;a href=""https://arxiv.org/abs/2104.06644""&gt;https://arxiv.org/abs/2104.06644&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Is BERT just a giant bag-of-words??&lt;/p&gt;

&lt;p&gt;Relevant meme: &lt;a href=""https://imgur.com/a/RGUtOvt""&gt;https://imgur.com/a/RGUtOvt&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mrrcda,True,,Choice_Willow1890,,10,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/mrrcda/d_wtf_these_results_are_incredible_what_do_you/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mrrcda/d_wtf_these_results_are_incredible_what_do_you/,30199,1618531180.0,0,,False,,,,,,128
353,,LanguageTechnology,"Hi everyone. I want to ask you if this is possible:

Consider we have a minimal text, or just a couple of keywords, say X. And we have another large text or corpus, Y, that we assume they are about similar topics. Assume that X is a sentence and Y is a book.

Is it possible to generate a text Z that may be to an answer or comment to X that using facts or style from Y?

E.g. consider X is a text or keywords like ""I like to swim in Maldives!"". Y is a news about Maldives. Then Z might be a generated text like ""Maldives has the most clean beaches in the world.""

Thank you all.",t2_3kwtnien,False,,0,False,Text Generation from Another Text,[],r/LanguageTechnology,False,6,,0,,False,t3_mrywon,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1618590633.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone. I want to ask you if this is possible:&lt;/p&gt;

&lt;p&gt;Consider we have a minimal text, or just a couple of keywords, say X. And we have another large text or corpus, Y, that we assume they are about similar topics. Assume that X is a sentence and Y is a book.&lt;/p&gt;

&lt;p&gt;Is it possible to generate a text Z that may be to an answer or comment to X that using facts or style from Y?&lt;/p&gt;

&lt;p&gt;E.g. consider X is a text or keywords like &amp;quot;I like to swim in Maldives!&amp;quot;. Y is a news about Maldives. Then Z might be a generated text like &amp;quot;Maldives has the most clean beaches in the world.&amp;quot;&lt;/p&gt;

&lt;p&gt;Thank you all.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mrywon,True,,emremrah,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mrywon/text_generation_from_another_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mrywon/text_generation_from_another_text/,30199,1618561833.0,0,,False,,,,,,579
354,,LanguageTechnology,,t2_16gftjf,False,,0,False,The first ever Gesture Generation Challenge. More info in comments,[],r/LanguageTechnology,False,6,,0,,False,t3_mrh9zp,False,dark,1.0,,public,16,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ja7IXGFrYGA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': ""[IUI'21] A large, crowdsourced evaluation of gesture generation systems on common data"", 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ja7IXGFrYGA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'GENEA Workshop', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/ja7IXGFrYGA/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCViP_2AQz55MsM3D_1OCbpw'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ja7IXGFrYGA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mrh9zp', 'height': 200}",,False,16,,False,False,,False,,[],{},,False,,1618529950.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mrh9zp,True,,Svito-zar,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mrh9zp/the_first_ever_gesture_generation_challenge_more/,all_ads,False,https://youtu.be/ja7IXGFrYGA,30199,1618501150.0,1,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': ""[IUI'21] A large, crowdsourced evaluation of gesture generation systems on common data"", 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ja7IXGFrYGA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'GENEA Workshop', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/ja7IXGFrYGA/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCViP_2AQz55MsM3D_1OCbpw'}}",False,https://youtu.be/ja7IXGFrYGA,,,,,0
355,,LanguageTechnology,,t2_13p32d,False,,0,False,NLP equivalent to speaker diarization?,[],r/LanguageTechnology,False,6,,0,,False,t3_mrmc87,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,False,,1618544331.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mrmc87,True,,tim_gabie,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mrmc87/nlp_equivalent_to_speaker_diarization/,all_ads,False,/r/learnmachinelearning/comments/mrmaym/equivalent_to_speaker_diarization_for_books/,30199,1618515531.0,0,,False,/r/learnmachinelearning/comments/mrmaym/equivalent_to_speaker_diarization_for_books/,"[{'approved_at_utc': None, 'subreddit': 'learnmachinelearning', 'selftext': ""I want to segment a novel based on which character in the novel said what. e.g. listing all text the character in the novel called Peter said.\n\n I think of it as the NLP equivalent for speaker diarization. I couldn't find papers on this yet. Anyone has a hint for me what I have to search for to find more about this topic?"", 'author_fullname': 't2_13p32d', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'equivalent to speaker diarization for books?', 'link_flair_richtext': [{'e': 'text', 't': 'Question'}], 'subreddit_name_prefixed': 'r/learnmachinelearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_mrmaym', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Question', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1618544223.0, 'link_flair_type': 'richtext', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.learnmachinelearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to segment a novel based on which character in the novel said what. e.g. listing all text the character in the novel called Peter said.&lt;/p&gt;\n\n&lt;p&gt;I think of it as the NLP equivalent for speaker diarization. I couldn&amp;#39;t find papers on this yet. Anyone has a hint for me what I have to search for to find more about this topic?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'ec81b8ee-accf-11e9-b8f8-0ebea2df7d78', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_3cqa1', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ffb000', 'id': 'mrmaym', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'tim_gabie', 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/learnmachinelearning/comments/mrmaym/equivalent_to_speaker_diarization_for_books/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/learnmachinelearning/comments/mrmaym/equivalent_to_speaker_diarization_for_books/', 'subreddit_subscribers': 232286, 'created_utc': 1618515423.0, 'num_crossposts': 4, 'media': None, 'is_video': False}]",t3_mrmaym,,,0
356,,LanguageTechnology,"I am working on a classification problem whose data includes both text and numerical features. My first approach to tacke this problem was to convert the text features into embeddings and append those as new features to original dataset. The problem with approach is that since embeddings are usually of high dimensions, they overwhelm the numerical features.

Second approach I used was append every feature together into a string, convert it into embedding and train the model. This gave me very poor accuracy.

Is there any other way I can handle this problem?",t2_5zyo23hz,False,,0,False,Classification problem with text and numerical features,[],r/LanguageTechnology,False,6,,0,,False,t3_mr8rsi,False,dark,0.83,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1618495482.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am working on a classification problem whose data includes both text and numerical features. My first approach to tacke this problem was to convert the text features into embeddings and append those as new features to original dataset. The problem with approach is that since embeddings are usually of high dimensions, they overwhelm the numerical features.&lt;/p&gt;

&lt;p&gt;Second approach I used was append every feature together into a string, convert it into embedding and train the model. This gave me very poor accuracy.&lt;/p&gt;

&lt;p&gt;Is there any other way I can handle this problem?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mr8rsi,True,,eagleandwolf,,15,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mr8rsi/classification_problem_with_text_and_numerical/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mr8rsi/classification_problem_with_text_and_numerical/,30199,1618466682.0,0,,False,,,,,,563
357,,LanguageTechnology,Best method for aspect analysis ??,t2_75yo37zi,False,,0,False,Aspect analysis,[],r/LanguageTechnology,False,6,,0,,False,t3_mrecsh,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1618521096.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Best method for aspect analysis ??&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mrecsh,True,,Snoo71755,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mrecsh/aspect_analysis/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mrecsh/aspect_analysis/,30199,1618492296.0,0,,False,,,,,,34
358,,LanguageTechnology,"I just graduated with a degree in CS and now I'm interested in doing research in NLP. The material available online, ever-changing technology and also the hype makes me overwhelmed. Also the real research happens at the PhD level which is far too above from an undergrad level. How do I approach things? Will reading more papers and learning more about the theory helps? Or should I just dive into the practical parts and start using pretrained models and libraries? 

The way I like to do things is first to have a solid theoretical foundations of a certain topic after which I look around for implementations. I am more of a theory person, but NLP is more practical than theory. 

My goal is to focus on only 1 field for my future studies and I've chosen Nlp. Now I want to get good at it, but it's not clear how?",t2_8s1tooht,False,,0,False,How to do undergrad research the right way?,[],r/LanguageTechnology,False,6,,0,,False,t3_mqwzpv,False,dark,0.93,,public,28,0,{},,False,[],,False,False,,{},,False,28,,False,False,,False,,[],{},,True,,1618454399.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I just graduated with a degree in CS and now I&amp;#39;m interested in doing research in NLP. The material available online, ever-changing technology and also the hype makes me overwhelmed. Also the real research happens at the PhD level which is far too above from an undergrad level. How do I approach things? Will reading more papers and learning more about the theory helps? Or should I just dive into the practical parts and start using pretrained models and libraries? &lt;/p&gt;

&lt;p&gt;The way I like to do things is first to have a solid theoretical foundations of a certain topic after which I look around for implementations. I am more of a theory person, but NLP is more practical than theory. &lt;/p&gt;

&lt;p&gt;My goal is to focus on only 1 field for my future studies and I&amp;#39;ve chosen Nlp. Now I want to get good at it, but it&amp;#39;s not clear how?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mqwzpv,True,,rapchickk,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mqwzpv/how_to_do_undergrad_research_the_right_way/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mqwzpv/how_to_do_undergrad_research_the_right_way/,30199,1618425599.0,0,,False,,,,,,815
359,,LanguageTechnology,"Sharing our new [interactive research graph for BERT](https://crossminds.ai/graphlist/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-60709500c8663c4cfa875fc4/), which maps important prior contextual representation work and various pre-trained language models derived from BERT.  Hope you find it helpful!

Here are the papers (and video presentations) included in the graph:

* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (core paper)
* Attention Is All You Need
* Semi-supervised Sequence Learning
* Deep contextualized word representations
* Universal Language Model Fine-tuning for Text Classification
* Improving Language Understanding by Generative Pre-Training
* RoBERTa: A Robustly Optimized BERT Pretraining Approach
* Defending Against Neural Fake News
* Language Models as Knowledge Bases?
* ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
* XLNet: Generalized Autoregressive Pretraining for Language Understanding
* Cross-lingual Language Model Pretraining
* 75 Languages, 1 Model: Parsing Universal Dependencies Universally
* Multi-Task Deep Neural Networks for Natural Language Understanding
* MASS: Masked Sequence to Sequence Pre-training for Language Generation
* Unified Language Model Pre-training for Natural Language Understanding and Generation
* SpanBERT: Improving Pre-training by Representing and Predicting Spans
* ERNIE: Enhanced Language Representation with Informative Entities
* ERNIE: Enhanced Representation through Knowledge Integration
* Knowledge Enhanced Contextual Word Representations
* VideoBERT: A Joint Model for Video and Language Representation Learning
* Contrastive Bidirectional Transformer for Temporal Representation Learning
* ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks
* VisualBERT: A Simple and Performant Baseline for Vision and Language
* Fusion of Detected Objects in Text for Visual Question Answering
* Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training
* LXMERT: Learning Cross-Modality Encoder Representations from Transformers
* VL-BERT: Pre-training of Generic Visual-Linguistic Representations
* UNITER: Learning UNiversal Image-TExt Representations
* Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks
* DeBERTa: Decoding-enhanced BERT with Disentangled Attention",t2_779ciqdy,False,,0,False,Research paper mapping for BERT: foundational work &amp; latest advancements,[],r/LanguageTechnology,False,6,,0,,False,t3_mqgqcz,False,dark,1.0,,public,39,1,{},,False,[],,False,False,,{},,False,39,,False,False,,False,,[],{'gid_1': 1},,True,,1618393923.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Sharing our new &lt;a href=""https://crossminds.ai/graphlist/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-60709500c8663c4cfa875fc4/""&gt;interactive research graph for BERT&lt;/a&gt;, which maps important prior contextual representation work and various pre-trained language models derived from BERT.  Hope you find it helpful!&lt;/p&gt;

&lt;p&gt;Here are the papers (and video presentations) included in the graph:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (core paper)&lt;/li&gt;
&lt;li&gt;Attention Is All You Need&lt;/li&gt;
&lt;li&gt;Semi-supervised Sequence Learning&lt;/li&gt;
&lt;li&gt;Deep contextualized word representations&lt;/li&gt;
&lt;li&gt;Universal Language Model Fine-tuning for Text Classification&lt;/li&gt;
&lt;li&gt;Improving Language Understanding by Generative Pre-Training&lt;/li&gt;
&lt;li&gt;RoBERTa: A Robustly Optimized BERT Pretraining Approach&lt;/li&gt;
&lt;li&gt;Defending Against Neural Fake News&lt;/li&gt;
&lt;li&gt;Language Models as Knowledge Bases?&lt;/li&gt;
&lt;li&gt;ALBERT: A Lite BERT for Self-supervised Learning of Language Representations&lt;/li&gt;
&lt;li&gt;XLNet: Generalized Autoregressive Pretraining for Language Understanding&lt;/li&gt;
&lt;li&gt;Cross-lingual Language Model Pretraining&lt;/li&gt;
&lt;li&gt;75 Languages, 1 Model: Parsing Universal Dependencies Universally&lt;/li&gt;
&lt;li&gt;Multi-Task Deep Neural Networks for Natural Language Understanding&lt;/li&gt;
&lt;li&gt;MASS: Masked Sequence to Sequence Pre-training for Language Generation&lt;/li&gt;
&lt;li&gt;Unified Language Model Pre-training for Natural Language Understanding and Generation&lt;/li&gt;
&lt;li&gt;SpanBERT: Improving Pre-training by Representing and Predicting Spans&lt;/li&gt;
&lt;li&gt;ERNIE: Enhanced Language Representation with Informative Entities&lt;/li&gt;
&lt;li&gt;ERNIE: Enhanced Representation through Knowledge Integration&lt;/li&gt;
&lt;li&gt;Knowledge Enhanced Contextual Word Representations&lt;/li&gt;
&lt;li&gt;VideoBERT: A Joint Model for Video and Language Representation Learning&lt;/li&gt;
&lt;li&gt;Contrastive Bidirectional Transformer for Temporal Representation Learning&lt;/li&gt;
&lt;li&gt;ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks&lt;/li&gt;
&lt;li&gt;VisualBERT: A Simple and Performant Baseline for Vision and Language&lt;/li&gt;
&lt;li&gt;Fusion of Detected Objects in Text for Visual Question Answering&lt;/li&gt;
&lt;li&gt;Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training&lt;/li&gt;
&lt;li&gt;LXMERT: Learning Cross-Modality Encoder Representations from Transformers&lt;/li&gt;
&lt;li&gt;VL-BERT: Pre-training of Generic Visual-Linguistic Representations&lt;/li&gt;
&lt;li&gt;UNITER: Learning UNiversal Image-TExt Representations&lt;/li&gt;
&lt;li&gt;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&lt;/li&gt;
&lt;li&gt;DeBERTa: Decoding-enhanced BERT with Disentangled Attention&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mqgqcz,True,,ccrbltscm,,2,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/mqgqcz/research_paper_mapping_for_bert_foundational_work/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mqgqcz/research_paper_mapping_for_bert_foundational_work/,30199,1618365123.0,0,,False,,,,,,2423
360,,LanguageTechnology,"Looking for other types or ways to use this data and come up with analysis on it. I feel like my project needs a little more but I’m unsure what else I can do with what I have. It takes in user reviews, like Yelp reviews, and tags them as either positive or negative.",t2_7wo3mdn,False,,0,False,"Finished an NLP based binary text classified for user reviews, what else can I do with this to add to my project?",[],r/LanguageTechnology,False,6,,0,,False,t3_mqtitw,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1618444545.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Looking for other types or ways to use this data and come up with analysis on it. I feel like my project needs a little more but I’m unsure what else I can do with what I have. It takes in user reviews, like Yelp reviews, and tags them as either positive or negative.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mqtitw,True,,edwardsrk,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mqtitw/finished_an_nlp_based_binary_text_classified_for/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mqtitw/finished_an_nlp_based_binary_text_classified_for/,30199,1618415745.0,0,,False,,,,,,267
361,,LanguageTechnology,"For example, how perplexed is our model assigning the topic that it assigned to a document. The reason I want to know this is because I want to weed out low frequency documents that were wrongly assigned to high frequency topics",t2_5r4mo7fh,False,,0,False,Is there a way to get document wise perplexity in gensim LDA,[],r/LanguageTechnology,False,6,,0,,False,t3_mqpuw5,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1618433337.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For example, how perplexed is our model assigning the topic that it assigned to a document. The reason I want to know this is because I want to weed out low frequency documents that were wrongly assigned to high frequency topics&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mqpuw5,True,,Epiphany925,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mqpuw5/is_there_a_way_to_get_document_wise_perplexity_in/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mqpuw5/is_there_a_way_to_get_document_wise_perplexity_in/,30199,1618404537.0,0,,False,,,,,,228
362,,LanguageTechnology,"*Long time lurker, first time poster!* 

*otso has just launched our Annotator, and have built with the needs of many in this sub!*

&amp;#x200B;

**A cloud-based Text Annotator built for Machine Learning Engineers and Data Scientists.**

We have been working on otso Annotator for over two years. It began as an internal tool, used to manage annotation and data labelling for our own machine learning projects. As the tool and the interface developed, we began providing it to select enterprise customers. We’ve received so much significant positive feedback from our clients, that we decided to launch the Annotator as a standalone product.

otso Annotator provides three key benefits; the user experience, which prioritises ease-of-use and understanding. The project management features, which let you allocate and manage annotation tasks. Finally, as a cloud-first tool, you no longer need to have annotators use a CLI to get started, which makes it a much easier tool for teams to use.

**Why a focus on the user experience of teams?**

Text annotation is best done in a team environment. Ideally, machine learning engineers and data scientists will set up and run projects, while subject matter experts provide annotations. We have built otso Annotator with these different user types in mind - enabling seamless project setup for project admins and an easy and keyboard enabled annotation experience for annotators.

With this public launch, we are granting users and teams that sign up during April an extended trial period of 30 days.

To check it out, head to [otso.ai/annotator](https://otso.ai/annotator).No credit card required.",t2_2wqhs6ll,False,,0,False,Checkout our team-based cloud-first text annotator.,[],r/LanguageTechnology,False,6,,0,,False,t3_mqcra9,False,dark,0.78,,public,5,1,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1618380371.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;em&gt;Long time lurker, first time poster!&lt;/em&gt; &lt;/p&gt;

&lt;p&gt;&lt;em&gt;otso has just launched our Annotator, and have built with the needs of many in this sub!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A cloud-based Text Annotator built for Machine Learning Engineers and Data Scientists.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We have been working on otso Annotator for over two years. It began as an internal tool, used to manage annotation and data labelling for our own machine learning projects. As the tool and the interface developed, we began providing it to select enterprise customers. We’ve received so much significant positive feedback from our clients, that we decided to launch the Annotator as a standalone product.&lt;/p&gt;

&lt;p&gt;otso Annotator provides three key benefits; the user experience, which prioritises ease-of-use and understanding. The project management features, which let you allocate and manage annotation tasks. Finally, as a cloud-first tool, you no longer need to have annotators use a CLI to get started, which makes it a much easier tool for teams to use.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why a focus on the user experience of teams?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Text annotation is best done in a team environment. Ideally, machine learning engineers and data scientists will set up and run projects, while subject matter experts provide annotations. We have built otso Annotator with these different user types in mind - enabling seamless project setup for project admins and an easy and keyboard enabled annotation experience for annotators.&lt;/p&gt;

&lt;p&gt;With this public launch, we are granting users and teams that sign up during April an extended trial period of 30 days.&lt;/p&gt;

&lt;p&gt;To check it out, head to &lt;a href=""https://otso.ai/annotator""&gt;otso.ai/annotator&lt;/a&gt;.No credit card required.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mqcra9,True,,otso_ai,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mqcra9/checkout_our_teambased_cloudfirst_text_annotator/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mqcra9/checkout_our_teambased_cloudfirst_text_annotator/,30199,1618351571.0,0,,False,,,,,,1641
363,,LanguageTechnology,"Hello,

I am developing a an extractive text summarizer for french language, I would like to explore the self-attention mechanism to see at which linguistic patterns it looks to by generating a heat map or? Any help?",t2_9e6mmc22,False,,0,False,At which linguistic patterns and features attention heads of BERT look to ?,[],r/LanguageTechnology,False,6,,0,,False,t3_mq3ov5,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1618353858.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am developing a an extractive text summarizer for french language, I would like to explore the self-attention mechanism to see at which linguistic patterns it looks to by generating a heat map or? Any help?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mq3ov5,True,,Ok_Inspection_5208,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mq3ov5/at_which_linguistic_patterns_and_features/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mq3ov5/at_which_linguistic_patterns_and_features/,30199,1618325058.0,0,,False,,,,,,216
364,,LanguageTechnology,,t2_21qaqh1p,False,,0,False,Youtube Video Transcript Summarization with Hugging Face,[],r/LanguageTechnology,False,6,,0,,False,t3_mq9rwa,False,dark,0.6,,public,1,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3V-MJhJvRWg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Youtube Video Transcript Summarization with Hugging Face Transformers | Python NLP Projects', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3V-MJhJvRWg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3V-MJhJvRWg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3V-MJhJvRWg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mq9rwa', 'height': 200}",,False,1,,False,False,,False,,[],{},,False,,1618371353.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mq9rwa,True,,dulldata,,0,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/mq9rwa/youtube_video_transcript_summarization_with/,all_ads,False,https://youtu.be/3V-MJhJvRWg,30199,1618342553.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Youtube Video Transcript Summarization with Hugging Face Transformers | Python NLP Projects', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3V-MJhJvRWg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3V-MJhJvRWg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,https://youtu.be/3V-MJhJvRWg,,,,,0
365,,LanguageTechnology,,t2_bhyxnyna,False,,0,False,hello I need DUC 2004 dataset for my master project .... if anyone share it with me i'll so thankful,[],r/LanguageTechnology,False,6,,0,,False,t3_mq18qz,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1618345759.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mq18qz,True,,Agile-Cartoonist-912,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mq18qz/hello_i_need_duc_2004_dataset_for_my_master/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mq18qz/hello_i_need_duc_2004_dataset_for_my_master/,30199,1618316959.0,0,,False,,,,,,0
366,,LanguageTechnology,"Hello,

I'm using BERT language model for predicting mental health issues. Later, I'll be using more language models for comparison. The dataset is big and the whole computation requires GPUs which I don't have. I was wondering if there's any quick way to make a training cluster with multiple CPU based computers to make training faster. I'm no knowledge of distributed computing so idk how to proceed with this. Do you guys have any idea?

Thanks.",t2_8s1tooht,False,,0,False,Deep learning on multiple computers,[],r/LanguageTechnology,False,6,,0,,False,t3_mpgwsl,False,dark,0.93,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,False,,False,,[],{},,True,,1618272058.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m using BERT language model for predicting mental health issues. Later, I&amp;#39;ll be using more language models for comparison. The dataset is big and the whole computation requires GPUs which I don&amp;#39;t have. I was wondering if there&amp;#39;s any quick way to make a training cluster with multiple CPU based computers to make training faster. I&amp;#39;m no knowledge of distributed computing so idk how to proceed with this. Do you guys have any idea?&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mpgwsl,True,,rapchickk,,21,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mpgwsl/deep_learning_on_multiple_computers/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mpgwsl/deep_learning_on_multiple_computers/,30199,1618243258.0,0,,False,,,,,,449
367,,LanguageTechnology,"Hello there,
Im currently working on my Bachelorthesis and have only little knowledge on NLP. 
My idea was to finetune a BERT Model on a task that is a mix of fill-mask and text-summarization. 

Basically i would scrape for short sentences about a specific topic and display those. The user would have to crate template sentences with masked entries. And finally the transformer guessing those masked entries.

Does this sound realistic?

Could someone provide any information about how to fine tune something like that? Is it eveb possible to focus the transformer on a specific text?

Thanks !",t2_9vkre7v4,False,,0,False,[Q:] MLM for short phrases,[],r/LanguageTechnology,False,6,,0,,False,t3_mpee92,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1618264658.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello there,
Im currently working on my Bachelorthesis and have only little knowledge on NLP. 
My idea was to finetune a BERT Model on a task that is a mix of fill-mask and text-summarization. &lt;/p&gt;

&lt;p&gt;Basically i would scrape for short sentences about a specific topic and display those. The user would have to crate template sentences with masked entries. And finally the transformer guessing those masked entries.&lt;/p&gt;

&lt;p&gt;Does this sound realistic?&lt;/p&gt;

&lt;p&gt;Could someone provide any information about how to fine tune something like that? Is it eveb possible to focus the transformer on a specific text?&lt;/p&gt;

&lt;p&gt;Thanks !&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mpee92,True,,Ieoure,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mpee92/q_mlm_for_short_phrases/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mpee92/q_mlm_for_short_phrases/,30199,1618235858.0,0,,False,,,,,,595
368,,LanguageTechnology,,t2_apv7o2y8,False,,0,False,Tweet Scraping using Twint and Sentiment Analysis using Hugging Face Transformers #python #NLProc,[],r/LanguageTechnology,False,6,,0,,False,t3_mp8fos,False,dark,0.85,,public,14,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/z-sFQVN7hgg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Tweet Scraping using Twint and Sentiment Analysis using Hugging Face Transformers #python #NLProc', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/z-sFQVN7hgg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/z-sFQVN7hgg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/z-sFQVN7hgg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mp8fos', 'height': 200}",,False,14,,False,False,,False,,[],{},,False,,1618238776.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mp8fos,True,,rsree123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mp8fos/tweet_scraping_using_twint_and_sentiment_analysis/,all_ads,False,https://youtu.be/z-sFQVN7hgg,30199,1618209976.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Tweet Scraping using Twint and Sentiment Analysis using Hugging Face Transformers #python #NLProc', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/z-sFQVN7hgg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/z-sFQVN7hgg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,https://youtu.be/z-sFQVN7hgg,,,,,0
369,,LanguageTechnology,"Hello,

I am going to make a simple sentiment analysis of Twitter posts. Basically searching for a word(name) and than retrieving n tweets and later returning the distribution positive, negative, neutral.",t2_9xraodz5,False,,0,False,Plan to make twitter sentiment analysis,[],r/LanguageTechnology,False,6,,0,,False,t3_mpbpel,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,1618520204.0,,[],{},,True,,1618255096.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am going to make a simple sentiment analysis of Twitter posts. Basically searching for a word(name) and than retrieving n tweets and later returning the distribution positive, negative, neutral.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mpbpel,True,,Overall_Flamingo_668,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mpbpel/plan_to_make_twitter_sentiment_analysis/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mpbpel/plan_to_make_twitter_sentiment_analysis/,30199,1618226296.0,0,,False,,,,,,204
370,,LanguageTechnology,"Suppose you have downloaded the pdf of every Shakespeare play on your computer. Suppose now you want to find the name of a Shakespeare play that you read in high school, but you can't remember it's name - however, you do remember the general plot of the play, e.g. ""a danish prince is visted by his father's ghost and holds a skull in his hand while delivering a speech"". (Btw this is the plot of hamlet)

Suppose you type this sentence in - could the cosine similarity be used to find out which play is most similar to this sentence? Is there a common way to solve this kind of problem?",t2_3f0i9m72,False,,0,False,[D] is this a correct application of the cosine similarity?,[],r/LanguageTechnology,False,6,,0,,False,t3_mp5ipq,False,dark,0.81,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1618225273.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Suppose you have downloaded the pdf of every Shakespeare play on your computer. Suppose now you want to find the name of a Shakespeare play that you read in high school, but you can&amp;#39;t remember it&amp;#39;s name - however, you do remember the general plot of the play, e.g. &amp;quot;a danish prince is visted by his father&amp;#39;s ghost and holds a skull in his hand while delivering a speech&amp;quot;. (Btw this is the plot of hamlet)&lt;/p&gt;

&lt;p&gt;Suppose you type this sentence in - could the cosine similarity be used to find out which play is most similar to this sentence? Is there a common way to solve this kind of problem?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mp5ipq,True,,SQL_beginner,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mp5ipq/d_is_this_a_correct_application_of_the_cosine/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mp5ipq/d_is_this_a_correct_application_of_the_cosine/,30199,1618196473.0,0,,False,,,,,,587
371,,LanguageTechnology,"Can somebody urgently suggest me the best method to extract a specified section from a .txt webpage.

Example-  [https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-000413.txt](https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-000413.txt)   

Suppose, I want to extract all text from the section ITEM 7. Management's Discussion and Analysis section. Now, the catch is that there are several such webpages and not all of them start with Item 7 \[they have Management's discussion and analysis common though\]. How to do this?",t2_5wcskabr,False,,0,False,Suggestion for web scraping,[],r/LanguageTechnology,False,6,,0,,False,t3_mpdt8m,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1618262787.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can somebody urgently suggest me the best method to extract a specified section from a .txt webpage.&lt;/p&gt;

&lt;p&gt;Example-  &lt;a href=""https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-000413.txt""&gt;https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-000413.txt&lt;/a&gt;   &lt;/p&gt;

&lt;p&gt;Suppose, I want to extract all text from the section ITEM 7. Management&amp;#39;s Discussion and Analysis section. Now, the catch is that there are several such webpages and not all of them start with Item 7 [they have Management&amp;#39;s discussion and analysis common though]. How to do this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mpdt8m,True,,debadri3,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mpdt8m/suggestion_for_web_scraping/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mpdt8m/suggestion_for_web_scraping/,30199,1618233987.0,0,,False,,,,,,540
372,,LanguageTechnology,"Hello!  For my MSc Artificial Intelligence thesis I would like to work on fine-tuning transformers for NLP. I will have 5 months to complete my thesis. I don't have too much experience in NLP because the course was more focused on Computer Vision. I find transformers very interesting and I want to work on fine-tuning for dialogue systems and text classification. I haven't fully decided on the dataset or what classification I want to do. 

1. Can you suggest a topic for text classification? I am looking for an interesting subject that is also relevant for employers. 
2. There are so many different transformer models that I can work on. Which ones do you think would be the best? 

Thank you in advance!",t2_63f2q1ee,False,,0,False,Can you suggest me a Transformer Model for fine-tuning for my thesis?,[],r/LanguageTechnology,False,6,,0,,False,t3_mpaood,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1618250465.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!  For my MSc Artificial Intelligence thesis I would like to work on fine-tuning transformers for NLP. I will have 5 months to complete my thesis. I don&amp;#39;t have too much experience in NLP because the course was more focused on Computer Vision. I find transformers very interesting and I want to work on fine-tuning for dialogue systems and text classification. I haven&amp;#39;t fully decided on the dataset or what classification I want to do. &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Can you suggest a topic for text classification? I am looking for an interesting subject that is also relevant for employers. &lt;/li&gt;
&lt;li&gt;There are so many different transformer models that I can work on. Which ones do you think would be the best? &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thank you in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mpaood,True,,bayraksc,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mpaood/can_you_suggest_me_a_transformer_model_for/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mpaood/can_you_suggest_me_a_transformer_model_for/,30199,1618221665.0,0,,False,,,,,,709
373,,LanguageTechnology,,t2_5fsp2x6v,False,,0,False,NER for Social Media Texts with Semantic Augmentation | Research Papers Summary 013,[],r/LanguageTechnology,False,6,,0,,False,t3_mow4oz,False,dark,0.95,,public,18,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/FmnbBL0ems8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'NER for Social Media Texts with Semantic Augmentation | Research Papers Summary 013', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/FmnbBL0ems8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/FmnbBL0ems8/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/FmnbBL0ems8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mow4oz', 'height': 200}",,False,18,,False,False,,False,,[],{},,False,,1618193991.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mow4oz,True,,RyanAI100,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mow4oz/ner_for_social_media_texts_with_semantic/,all_ads,False,https://youtu.be/FmnbBL0ems8,30199,1618165191.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'NER for Social Media Texts with Semantic Augmentation | Research Papers Summary 013', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/FmnbBL0ems8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/FmnbBL0ems8/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,https://youtu.be/FmnbBL0ems8,,,,,0
374,,LanguageTechnology,"
I have found various topic modelling algorithms, but they seem to depend on the interrelationships among words used in a document to determine the similarity. I want to achieve something similar to keyword clustering based on its meaning and retrieve top 5 most similar words of a query keyword. Any suggestions or path to solve this problem will be very helpful.

This community has been amazing for a fresher like me and thank you to each and everyone who takes out their time to help out people like me.",t2_8n5d56qx,False,,0,False,Given a corpus of about 10k keywords and another keyword x. How do I extract the top 5 most similar words to x.,[],r/LanguageTechnology,False,6,,0,,False,t3_moxlf4,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1618198422.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have found various topic modelling algorithms, but they seem to depend on the interrelationships among words used in a document to determine the similarity. I want to achieve something similar to keyword clustering based on its meaning and retrieve top 5 most similar words of a query keyword. Any suggestions or path to solve this problem will be very helpful.&lt;/p&gt;

&lt;p&gt;This community has been amazing for a fresher like me and thank you to each and everyone who takes out their time to help out people like me.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,moxlf4,True,,ChandlerBingggggggg,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/moxlf4/given_a_corpus_of_about_10k_keywords_and_another/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/moxlf4/given_a_corpus_of_about_10k_keywords_and_another/,30199,1618169622.0,0,,False,,,,,,507
375,,LanguageTechnology,"Have a product in the works which includes a quiz element. Would like to support fill-in-the-blank for entering text answers, but not require perfect spelling.

If correct answer is “Guatemala”, would like to give 100 pts for exact match, and (for example) 93 for Guatamela, and 45 for Gwattanala, and so on. 

Are there good examples of this in the wild?",t2_ophpfym,False,,0,False,How to award a score (1-100) for closeness to the right answer?,[],r/LanguageTechnology,False,6,,0,,False,t3_moqi8s,False,dark,0.75,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1618174027.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Have a product in the works which includes a quiz element. Would like to support fill-in-the-blank for entering text answers, but not require perfect spelling.&lt;/p&gt;

&lt;p&gt;If correct answer is “Guatemala”, would like to give 100 pts for exact match, and (for example) 93 for Guatamela, and 45 for Gwattanala, and so on. &lt;/p&gt;

&lt;p&gt;Are there good examples of this in the wild?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,moqi8s,True,,TheBraveProtonDuck,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/moqi8s/how_to_award_a_score_1100_for_closeness_to_the/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/moqi8s/how_to_award_a_score_1100_for_closeness_to_the/,30199,1618145227.0,0,,False,,,,,,355
376,,LanguageTechnology,"I'm a beginner in data science. I would like to have a roadmap to follow and understand the broad extent of this amazing field.

May be it could be difficult to find something specific for NLP. In that case, please share Machine Learning or Data Science roadmaps that you may have.

Beginners of this community would appreciate it!",t2_4df6a1hn,False,,0,False,Any good NLP roadmap?,[],r/LanguageTechnology,False,6,,0,,False,t3_moc365,False,dark,0.98,,public,37,0,{},,False,[],,False,False,,{},,False,37,,False,False,,False,,[],{},,True,,1618114317.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a beginner in data science. I would like to have a roadmap to follow and understand the broad extent of this amazing field.&lt;/p&gt;

&lt;p&gt;May be it could be difficult to find something specific for NLP. In that case, please share Machine Learning or Data Science roadmaps that you may have.&lt;/p&gt;

&lt;p&gt;Beginners of this community would appreciate it!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,moc365,True,,JotaPe-exe,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/moc365/any_good_nlp_roadmap/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/moc365/any_good_nlp_roadmap/,30199,1618085517.0,0,,False,,,,,,331
377,,LanguageTechnology,"I have a training set of news articles that I wish to condense into triples, and I actually can evaluate how well these triples describe the original text and use it as a training signal, but I'm new to the field of NLP and uncertain about my options.  Which ML algorithms would you consider to take a BERT embedding layer as input and learn to turn/decode it to triples based on the mentioned training signal?",t2_75k22yqr,False,,0,False,"Embedding2triples, which ML algorithm?",[],r/LanguageTechnology,False,6,,0,,False,t3_mopzoq,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1618171955.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a training set of news articles that I wish to condense into triples, and I actually can evaluate how well these triples describe the original text and use it as a training signal, but I&amp;#39;m new to the field of NLP and uncertain about my options.  Which ML algorithms would you consider to take a BERT embedding layer as input and learn to turn/decode it to triples based on the mentioned training signal?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mopzoq,True,,NoRexTreX,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mopzoq/embedding2triples_which_ml_algorithm/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mopzoq/embedding2triples_which_ml_algorithm/,30199,1618143155.0,1,,False,,,,,,410
378,,LanguageTechnology,,t2_3z8a2x6g,False,,0,False,Event extraction/Highlight detection from transcript,[],r/LanguageTechnology,False,6,,0,,False,t3_moc42e,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,False,,1618114403.0,text,6,,,text,self.MachineLearning,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,moc42e,True,,i_kurt_i,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/moc42e/event_extractionhighlight_detection_from/,all_ads,False,/r/MachineLearning/comments/moc0zc/r_techniques_for_nlp_event_extraction_from_large/,30199,1618085603.0,0,,False,/r/MachineLearning/comments/moc0zc/r_techniques_for_nlp_event_extraction_from_large/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': '[removed]', 'author_fullname': 't2_3z8a2x6g', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[R] Techniques for NLP event extraction from large text of commentator speech from football match?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'three', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_moc0zc', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Research', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1618114113.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': 'moderator', 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'moc0zc', 'is_robot_indexable': False, 'report_reasons': None, 'author': 'i_kurt_i', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/moc0zc/r_techniques_for_nlp_event_extraction_from_large/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/moc0zc/r_techniques_for_nlp_event_extraction_from_large/', 'subreddit_subscribers': 1930710, 'created_utc': 1618085313.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_moc0zc,,,0
379,,LanguageTechnology,"when doing NLP, do you usually augment your data with a ""pre defined corpus"" specific to the field your data comes from?

E.g. if you are working with medical data, do nlp procedures require you to use a predefined corpus from the medical domain?",t2_o4xj9,False,,0,False,"when doing NLP, do you usually augment your data with a ""pre defined corpus"" specific to the field your data comes from?",[],r/LanguageTechnology,False,6,,0,,False,t3_mnzami,False,dark,0.92,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1618063633.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;when doing NLP, do you usually augment your data with a &amp;quot;pre defined corpus&amp;quot; specific to the field your data comes from?&lt;/p&gt;

&lt;p&gt;E.g. if you are working with medical data, do nlp procedures require you to use a predefined corpus from the medical domain?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mnzami,True,,blueest,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mnzami/when_doing_nlp_do_you_usually_augment_your_data/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mnzami/when_doing_nlp_do_you_usually_augment_your_data/,30199,1618034833.0,0,,False,,,,,True,246
380,,LanguageTechnology,"This tutorial gives a step-by-step guide to implementing an RNN model (encoder-decoder sequence-to-sequence with attention mechanism) for French to English translation using Keras.

Additional topics covered include:

* The Problem With Sequence-to-Sequence Models for Neural Machine Translation
* An Introduction to Attention Mechanisms
* Categories of Attention Mechanisms
* Applications of Attention Mechanisms
* Neural Machine Translation Using an RNN With Attention Mechanism (Keras)

Tutorial link: [https://blog.paperspace.com/seq-to-seq-attention-mechanism-keras/](https://blog.paperspace.com/seq-to-seq-attention-mechanism-keras/)

Run all of the code on a free GPU: [https://ml-showcase.paperspace.com/projects/neural-machine-translation-with-keras](https://ml-showcase.paperspace.com/projects/neural-machine-translation-with-keras)",t2_15en0l,False,,0,False,[Tutorial] Neural Machine Translation With Attention With Keras,[],r/LanguageTechnology,False,6,,0,,False,t3_mnis25,False,dark,0.92,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1618008397.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This tutorial gives a step-by-step guide to implementing an RNN model (encoder-decoder sequence-to-sequence with attention mechanism) for French to English translation using Keras.&lt;/p&gt;

&lt;p&gt;Additional topics covered include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Problem With Sequence-to-Sequence Models for Neural Machine Translation&lt;/li&gt;
&lt;li&gt;An Introduction to Attention Mechanisms&lt;/li&gt;
&lt;li&gt;Categories of Attention Mechanisms&lt;/li&gt;
&lt;li&gt;Applications of Attention Mechanisms&lt;/li&gt;
&lt;li&gt;Neural Machine Translation Using an RNN With Attention Mechanism (Keras)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tutorial link: &lt;a href=""https://blog.paperspace.com/seq-to-seq-attention-mechanism-keras/""&gt;https://blog.paperspace.com/seq-to-seq-attention-mechanism-keras/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Run all of the code on a free GPU: &lt;a href=""https://ml-showcase.paperspace.com/projects/neural-machine-translation-with-keras""&gt;https://ml-showcase.paperspace.com/projects/neural-machine-translation-with-keras&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mnis25,True,,hellopaperspace,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mnis25/tutorial_neural_machine_translation_with/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mnis25/tutorial_neural_machine_translation_with/,30199,1617979597.0,0,,False,,,,,,842
381,,LanguageTechnology,"Hi everyone!

I got accepted into the Erasmus master in Language and Communications Technology and also into the University of Uppsala master in Language Technology. I need help deciding between the two.

If you don't know, the Erasmus program implies attending two universities, one year each, out of the seven universities of the consortium. They will tell me what universities i will attend once i say yes. Therefore, i don't know exactly what the course will be.

I feel more inclined to choose Uppsala because:
The logistics of living in Sweden for two years are easier than living in two different countries.
The program is free, as opposed to Erasmus in which i have to pay a fee.
I actually know the course of studies program, as opposed to Erasmus in which i will find out later.
I believe studying in Uppsala will make it easier for me to find a job and a life in Sweden.
I have a boyfriend living four hours away from Uppsala (i try not to let this be a factor but who am I kidding, it is).

On the other hand, i think maybe the Erasmus program has a better reputation and also living in two random countries sounds like fun.

If you have any advice it would be welcome. thanks!",t2_30vynpif,False,,0,False,Erasmus or Uppsala?,[],r/LanguageTechnology,False,6,,0,,False,t3_mnecov,False,dark,0.88,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,False,,False,,[],{},,True,,1617993283.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;

&lt;p&gt;I got accepted into the Erasmus master in Language and Communications Technology and also into the University of Uppsala master in Language Technology. I need help deciding between the two.&lt;/p&gt;

&lt;p&gt;If you don&amp;#39;t know, the Erasmus program implies attending two universities, one year each, out of the seven universities of the consortium. They will tell me what universities i will attend once i say yes. Therefore, i don&amp;#39;t know exactly what the course will be.&lt;/p&gt;

&lt;p&gt;I feel more inclined to choose Uppsala because:
The logistics of living in Sweden for two years are easier than living in two different countries.
The program is free, as opposed to Erasmus in which i have to pay a fee.
I actually know the course of studies program, as opposed to Erasmus in which i will find out later.
I believe studying in Uppsala will make it easier for me to find a job and a life in Sweden.
I have a boyfriend living four hours away from Uppsala (i try not to let this be a factor but who am I kidding, it is).&lt;/p&gt;

&lt;p&gt;On the other hand, i think maybe the Erasmus program has a better reputation and also living in two random countries sounds like fun.&lt;/p&gt;

&lt;p&gt;If you have any advice it would be welcome. thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mnecov,True,,imnowonderwoman,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mnecov/erasmus_or_uppsala/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mnecov/erasmus_or_uppsala/,30199,1617964483.0,0,,False,,,,,,1189
382,,LanguageTechnology,"I am completely to new to nlp/text mining and am interested in learning more about it (on an applied side). Suppose I have a dataset with 1000 doctor comments (no ""labels"", i.e. I don't know if they are positive or negative) for different comments. The question is now, what can I do? 

Just by doing some google searches, it seems like the two popular things to do with this data is LDA and sentiment analysis. Just by looking how to do this in R, this looks doable. But what are some other popular algorithms that can done on this kind of data? I assume that through LDA and sentiment analysis, you can ""cluster"" these doctor comments together? If you have a new comment, you can see which from clustering which cluster this comment belongs to? 

In this example, how would the newer BERT algorithm come into play? Is there a main way to extract ""insights"" from these comments? Based on the text in these comments, are there algorithms that can determine if its possible to see which medical conditions  are more lethal, which age groups/demographics of patients are healthier, which medications are people taking, etc. ? Or is this a very advanced problem?

Thanks",t2_o4xj9,False,,0,False,Intro to nlp and text mining,[],r/LanguageTechnology,False,6,,0,,False,t3_mnith5,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1618008505.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am completely to new to nlp/text mining and am interested in learning more about it (on an applied side). Suppose I have a dataset with 1000 doctor comments (no &amp;quot;labels&amp;quot;, i.e. I don&amp;#39;t know if they are positive or negative) for different comments. The question is now, what can I do? &lt;/p&gt;

&lt;p&gt;Just by doing some google searches, it seems like the two popular things to do with this data is LDA and sentiment analysis. Just by looking how to do this in R, this looks doable. But what are some other popular algorithms that can done on this kind of data? I assume that through LDA and sentiment analysis, you can &amp;quot;cluster&amp;quot; these doctor comments together? If you have a new comment, you can see which from clustering which cluster this comment belongs to? &lt;/p&gt;

&lt;p&gt;In this example, how would the newer BERT algorithm come into play? Is there a main way to extract &amp;quot;insights&amp;quot; from these comments? Based on the text in these comments, are there algorithms that can determine if its possible to see which medical conditions  are more lethal, which age groups/demographics of patients are healthier, which medications are people taking, etc. ? Or is this a very advanced problem?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mnith5,True,,blueest,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mnith5/intro_to_nlp_and_text_mining/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mnith5/intro_to_nlp_and_text_mining/,30199,1617979705.0,0,,False,,,,,True,1167
383,,LanguageTechnology," 

Hi all - I'm working on what I think could be an low-level NLP problem at work, and was wondering if anyone here had any ideas on modules I could/should look into that might provide a solution.

Problem: so, the company I work for deploys networked hardware into the field, and that hardware often requires repairs. Our repair folks take down a lot of unstructured data when they get the hardware back up and running, stored as a note on salesforce. I'm trying to figure out a way to process this information to get some meaning from it (e.g., are repair cases most often networking or electrical issues, or from weather damages...etc.,).

Solution: I've been trying to figure out a way to sort all of this information. One idea was to make a list of all strings in all notes, strip out all articles/conjunctions, and then create some word cloud. My hunch is that someone has probably solved something very similar to this, and I wanted to socialize it in this community to see if anyone had any thoughts!

I would massively appreciate any ideas, or pointers on what modules I should be looking into! Just to be clear, we use Python/Jupyter Notebooks.",t2_b5daqq4l,False,,0,False,NLP Problem,[],r/LanguageTechnology,False,6,,0,,False,t3_mn36ij,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1617949552.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all - I&amp;#39;m working on what I think could be an low-level NLP problem at work, and was wondering if anyone here had any ideas on modules I could/should look into that might provide a solution.&lt;/p&gt;

&lt;p&gt;Problem: so, the company I work for deploys networked hardware into the field, and that hardware often requires repairs. Our repair folks take down a lot of unstructured data when they get the hardware back up and running, stored as a note on salesforce. I&amp;#39;m trying to figure out a way to process this information to get some meaning from it (e.g., are repair cases most often networking or electrical issues, or from weather damages...etc.,).&lt;/p&gt;

&lt;p&gt;Solution: I&amp;#39;ve been trying to figure out a way to sort all of this information. One idea was to make a list of all strings in all notes, strip out all articles/conjunctions, and then create some word cloud. My hunch is that someone has probably solved something very similar to this, and I wanted to socialize it in this community to see if anyone had any thoughts!&lt;/p&gt;

&lt;p&gt;I would massively appreciate any ideas, or pointers on what modules I should be looking into! Just to be clear, we use Python/Jupyter Notebooks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mn36ij,True,,Ok_Caterpillar_7948,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mn36ij/nlp_problem/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mn36ij/nlp_problem/,30199,1617920752.0,0,,False,,,,,,1154
384,,LanguageTechnology,"Hello guys, 

I came here to ask you for a help. I'm writing my final thesis right now on Detecting plagiarism in text documents. And I have to be honest with you guys, It is over my head. Deadline is in 1 month and I don't know how to make a working piece of code. So I wanted to ask you if you are willing to help me or at least tell me if there is something I can do. 

So the task is to compare documents (preferably bunch of documents to one suspicious document) and find if the suspecious document is plagiarism to any of those documents in reference collection. Currently I have done loading multiple pdf files, making dataframe, preprocess the text data (tokenisation, lemmantisation, stemming, stopwords removal, punctuation removal, lowercased) and vectorization. Then I applied cosine similarity to it and it kinda works.

My question is if there is any possible way to apply Support Vector Machine or Naive Bayes to this task? and if so, how do I set it up? The main goal was to apply some Machine Learning algoritm but I took much bigger piece of pie than I was supposed to.

I am really really desperate and any information will help me. Thank you",t2_1q21txa3,False,,0,False,Means to detect plagiarism in textual documents,[],r/LanguageTechnology,False,6,,0,,False,t3_mn2vpx,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1617948681.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello guys, &lt;/p&gt;

&lt;p&gt;I came here to ask you for a help. I&amp;#39;m writing my final thesis right now on Detecting plagiarism in text documents. And I have to be honest with you guys, It is over my head. Deadline is in 1 month and I don&amp;#39;t know how to make a working piece of code. So I wanted to ask you if you are willing to help me or at least tell me if there is something I can do. &lt;/p&gt;

&lt;p&gt;So the task is to compare documents (preferably bunch of documents to one suspicious document) and find if the suspecious document is plagiarism to any of those documents in reference collection. Currently I have done loading multiple pdf files, making dataframe, preprocess the text data (tokenisation, lemmantisation, stemming, stopwords removal, punctuation removal, lowercased) and vectorization. Then I applied cosine similarity to it and it kinda works.&lt;/p&gt;

&lt;p&gt;My question is if there is any possible way to apply Support Vector Machine or Naive Bayes to this task? and if so, how do I set it up? The main goal was to apply some Machine Learning algoritm but I took much bigger piece of pie than I was supposed to.&lt;/p&gt;

&lt;p&gt;I am really really desperate and any information will help me. Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mn2vpx,True,,EnchLUL,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mn2vpx/means_to_detect_plagiarism_in_textual_documents/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mn2vpx/means_to_detect_plagiarism_in_textual_documents/,30199,1617919881.0,0,,False,,,,,,1161
385,,LanguageTechnology,"Hi, any leads on upcoming NLP summer schools.",t2_xf374,False,,0,False,Summer schools in NLP 2021,[],r/LanguageTechnology,False,6,,0,,False,t3_mmnuhk,False,dark,0.99,,public,30,0,{},,False,[],,False,False,,{},,False,30,,False,False,,False,,[],{},,True,,1617901990.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, any leads on upcoming NLP summer schools.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mmnuhk,True,,thak123,,16,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mmnuhk/summer_schools_in_nlp_2021/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mmnuhk/summer_schools_in_nlp_2021/,30199,1617873190.0,0,,False,,,,,,45
386,,LanguageTechnology,,t2_apv7o2y8,False,,0,False,Stanford NLP Stanza NLP package - Biomedical NLP models demo #NLproc #python #clincialNLP,[],r/LanguageTechnology,False,6,,0,,False,t3_mmqvao,False,dark,0.86,,public,5,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/iQ5kP2kOGNA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Stanford NLP  Stanza NLP package -  Biomedical NLP models demo #NLproc #python #clincialNLP', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/iQ5kP2kOGNA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/iQ5kP2kOGNA/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/iQ5kP2kOGNA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mmqvao', 'height': 200}",,False,5,,False,False,,False,,[],{},,False,,1617914337.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mmqvao,True,,rsree123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mmqvao/stanford_nlp_stanza_nlp_package_biomedical_nlp/,all_ads,False,https://youtu.be/iQ5kP2kOGNA,30199,1617885537.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Stanford NLP  Stanza NLP package -  Biomedical NLP models demo #NLproc #python #clincialNLP', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/iQ5kP2kOGNA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/iQ5kP2kOGNA/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,https://youtu.be/iQ5kP2kOGNA,,,,,0
387,,LanguageTechnology,,t2_a2fvpp34,False,,0,False,Question Answering (Seq2Seq) Visualization with Transformer Neural Networks,[],r/LanguageTechnology,False,6,,0,,False,t3_mmh89x,False,dark,0.94,,public,14,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/GTVgJhSlHEk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Animation of Sequence to Sequence Learning (Seq2Seq with Transformer Neural Networks)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/GTVgJhSlHEk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'learningcurve', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/GTVgJhSlHEk/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/learningcurveai'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/GTVgJhSlHEk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mmh89x', 'height': 200}",,False,14,,False,False,,False,,[],{},,False,,1617875085.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mmh89x,True,,No-Guard-5438,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mmh89x/question_answering_seq2seq_visualization_with/,all_ads,False,https://youtu.be/GTVgJhSlHEk,30199,1617846285.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Animation of Sequence to Sequence Learning (Seq2Seq with Transformer Neural Networks)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/GTVgJhSlHEk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'learningcurve', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/GTVgJhSlHEk/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/learningcurveai'}}",False,https://youtu.be/GTVgJhSlHEk,,,,,0
388,,LanguageTechnology,"It is a common consensus that words appearing in similar contexts are semantically similar. But this definition breaks drastically when we consider antonymy relationships - &gt; For. eg., the word ""positive"" and ""negative"" generally appear in similar contexts and are assigned spatially close vectors. In the inherent sense, it does not contain meaning. 

Word2vec was trained on contextual information is somehow not commonly referred to as contextualized word embeddings. However. embeddings realized through BERT and Elmo are termed contextualized word-embeddings. Is there a reason for this? Or perhaps I might be wrong here and it was already called contextual word embedding. The embeddings through BERT and ElMO can be called dynamic embeddings at best, but it is not difficult to construct dynamic embeddings through word2vec embeddings. Does BERT embedding inherently have any other semantic information (non-positional, non-context), other than a more complicated interaction layer (and perhaps more data)?",t2_a1hqdli,False,,0,False,"Are word embeddings semantically ""meaningful""?",[],r/LanguageTechnology,False,6,,0,,False,t3_mmknon,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1617887641.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;It is a common consensus that words appearing in similar contexts are semantically similar. But this definition breaks drastically when we consider antonymy relationships - &amp;gt; For. eg., the word &amp;quot;positive&amp;quot; and &amp;quot;negative&amp;quot; generally appear in similar contexts and are assigned spatially close vectors. In the inherent sense, it does not contain meaning. &lt;/p&gt;

&lt;p&gt;Word2vec was trained on contextual information is somehow not commonly referred to as contextualized word embeddings. However. embeddings realized through BERT and Elmo are termed contextualized word-embeddings. Is there a reason for this? Or perhaps I might be wrong here and it was already called contextual word embedding. The embeddings through BERT and ElMO can be called dynamic embeddings at best, but it is not difficult to construct dynamic embeddings through word2vec embeddings. Does BERT embedding inherently have any other semantic information (non-positional, non-context), other than a more complicated interaction layer (and perhaps more data)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mmknon,True,,flerakml,,5,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/mmknon/are_word_embeddings_semantically_meaningful/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mmknon/are_word_embeddings_semantically_meaningful/,30199,1617858841.0,0,,False,,,,,,1016
389,,LanguageTechnology,"Hi everyone,

is it possible to use MarianMT transformer models with spacy-transformers? Similar to [https://spacy.io/universe/project/spacy-transformers](https://spacy.io/universe/project/spacy-transformers)  
MarianMT: [https://huggingface.co/transformers/model\_doc/marian.html?highlight=mariantokenizer#multilingual-models](https://huggingface.co/transformers/model_doc/marian.html?highlight=mariantokenizer#multilingual-models)

Thanks!",t2_sdfge43,False,,0,False,MarianMT usage with Spacy possible?,[],r/LanguageTechnology,False,6,,0,,False,t3_mmnrmr,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1617901605.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;is it possible to use MarianMT transformer models with spacy-transformers? Similar to &lt;a href=""https://spacy.io/universe/project/spacy-transformers""&gt;https://spacy.io/universe/project/spacy-transformers&lt;/a&gt;&lt;br/&gt;
MarianMT: &lt;a href=""https://huggingface.co/transformers/model_doc/marian.html?highlight=mariantokenizer#multilingual-models""&gt;https://huggingface.co/transformers/model_doc/marian.html?highlight=mariantokenizer#multilingual-models&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mmnrmr,True,,ProKellaSK,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mmnrmr/marianmt_usage_with_spacy_possible/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mmnrmr/marianmt_usage_with_spacy_possible/,30199,1617872805.0,0,,False,,,,,,441
390,,LanguageTechnology,,t2_j0zwm,False,,0,False,Multi-Document Summarization: The Wikipedia Current Events Portal Dataset (Dataset and Colab notebook),[],r/LanguageTechnology,False,6,,0,,False,t3_mlynns,False,dark,1.0,,public,23,0,{},,False,[],,False,False,,{},,False,23,,False,False,,False,,[],{},,False,,1617818115.0,text,6,,,text,aylien.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mlynns,True,,MikeWally,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mlynns/multidocument_summarization_the_wikipedia_current/,all_ads,False,https://aylien.com/blog/multi-document-summarisation-and-the-wcep-dataset,30199,1617789315.0,0,,False,https://aylien.com/blog/multi-document-summarisation-and-the-wcep-dataset,,,,,0
391,,LanguageTechnology,"Using scikit learn, I managed to train my model but dont know how to use the model to predict new text passages. I have watched tons of tutorials but none of them go beyond training and testing. Below is the code Im using. Any help is appreciated.

        data_source_url = ""/path/to/file.csv""
        airline_tweets = pd.read_csv(data_source_url)
        
        features = airline_tweets.iloc[:, 10].values
        labels = airline_tweets.iloc[:, 1].values
        
        processed_features = []
        
          # I do some text processing here and then append the text to processed_features
        
                
        vectorizer = CountVectorizer(analyzer = 'word', lowercase = False)
        features = vectorizer.fit_transform(processed_features)
        features_nd = features.toarray() # for easy usage
        
        X_train, X_test, y_train, y_test  = train_test_split(features_nd, labels, train_size=0.80, random_state=1234)
        
        log_model = LogisticRegression()
        log_model = log_model.fit(X=X_train, y=y_train)
            
        predictions = log_model.predict(X_test)",t2_5r4mo7fh,False,,0,False,How do I predict sentiment of unseen text (After training and testing),[],r/LanguageTechnology,False,6,,0,,False,t3_mmfwr6,False,dark,0.33,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,1617847246.0,,[],{},,True,,1617870546.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Using scikit learn, I managed to train my model but dont know how to use the model to predict new text passages. I have watched tons of tutorials but none of them go beyond training and testing. Below is the code Im using. Any help is appreciated.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    data_source_url = &amp;quot;/path/to/file.csv&amp;quot;
    airline_tweets = pd.read_csv(data_source_url)

    features = airline_tweets.iloc[:, 10].values
    labels = airline_tweets.iloc[:, 1].values

    processed_features = []

      # I do some text processing here and then append the text to processed_features


    vectorizer = CountVectorizer(analyzer = &amp;#39;word&amp;#39;, lowercase = False)
    features = vectorizer.fit_transform(processed_features)
    features_nd = features.toarray() # for easy usage

    X_train, X_test, y_train, y_test  = train_test_split(features_nd, labels, train_size=0.80, random_state=1234)

    log_model = LogisticRegression()
    log_model = log_model.fit(X=X_train, y=y_train)

    predictions = log_model.predict(X_test)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mmfwr6,True,,Epiphany925,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mmfwr6/how_do_i_predict_sentiment_of_unseen_text_after/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mmfwr6/how_do_i_predict_sentiment_of_unseen_text_after/,30199,1617841746.0,0,,False,,,,,,1117
392,,LanguageTechnology,"Im working on predicting answers given question using seq2seq. So far it is as simple as can be . Encoder and decoder are just an LSTM each.

However, even if train loss decreases w epochs, validation loss is too high.

And the question answer train data has many misspellings and words from another language sometimes.

How can I help this? I already included Dropout layers with learning rate 0.2 to 0.5 for input, and 0.8 for output but validation loss still too high..

Accuracy also is so low. Like 5%",t2_445mnejc,False,,0,False,Overfitting? Input data with too many misspellings,[],r/LanguageTechnology,False,6,,0,,False,t3_mm12uc,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1617827773.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Im working on predicting answers given question using seq2seq. So far it is as simple as can be . Encoder and decoder are just an LSTM each.&lt;/p&gt;

&lt;p&gt;However, even if train loss decreases w epochs, validation loss is too high.&lt;/p&gt;

&lt;p&gt;And the question answer train data has many misspellings and words from another language sometimes.&lt;/p&gt;

&lt;p&gt;How can I help this? I already included Dropout layers with learning rate 0.2 to 0.5 for input, and 0.8 for output but validation loss still too high..&lt;/p&gt;

&lt;p&gt;Accuracy also is so low. Like 5%&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mm12uc,True,,phresia,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mm12uc/overfitting_input_data_with_too_many_misspellings/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mm12uc/overfitting_input_data_with_too_many_misspellings/,30199,1617798973.0,0,,False,,,,,,506
393,,LanguageTechnology,"Hi all,
Long time lurker, first time poster here. 

I’m turning to the hivemind because I’m at my wit’s end.

I’ve been tasked with providing a simple corpus service that returns tokens, sentences, etc. from a huge corpus based on certain criteria. These criteria are used to filter the corpus according to the annotations of the corpus entries. For example I should be able to return all nouns in the corpus back to the client. 

My first thought was to parse the corpus and load it into some sort of database which I could then query. But it didn’t work because of the technical limitations at my job. Also it seems like a lot of overhead to just do simple queries on the data. 

Then I thought I would load the corpus into a pandas dataframe, keeping it in memory for the lifecycle of the corpus service and querying it when client requests come in. But I find the solution a bit hacky. For example, it becomes very brittle when I try to map the schema of the corpus to my service’s own internal schema. 

Does anyone have experience with this problem? Is there a more straightforward approach that I haven’t thought of before? 

Thanks for taking the time to read this!",t2_kbj77jj,False,,0,False,Simple corpus service,[],r/LanguageTechnology,False,6,,0,,False,t3_mm48xk,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1617837037.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,
Long time lurker, first time poster here. &lt;/p&gt;

&lt;p&gt;I’m turning to the hivemind because I’m at my wit’s end.&lt;/p&gt;

&lt;p&gt;I’ve been tasked with providing a simple corpus service that returns tokens, sentences, etc. from a huge corpus based on certain criteria. These criteria are used to filter the corpus according to the annotations of the corpus entries. For example I should be able to return all nouns in the corpus back to the client. &lt;/p&gt;

&lt;p&gt;My first thought was to parse the corpus and load it into some sort of database which I could then query. But it didn’t work because of the technical limitations at my job. Also it seems like a lot of overhead to just do simple queries on the data. &lt;/p&gt;

&lt;p&gt;Then I thought I would load the corpus into a pandas dataframe, keeping it in memory for the lifecycle of the corpus service and querying it when client requests come in. But I find the solution a bit hacky. For example, it becomes very brittle when I try to map the schema of the corpus to my service’s own internal schema. &lt;/p&gt;

&lt;p&gt;Does anyone have experience with this problem? Is there a more straightforward approach that I haven’t thought of before? &lt;/p&gt;

&lt;p&gt;Thanks for taking the time to read this!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mm48xk,True,,L4teralu5,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mm48xk/simple_corpus_service/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mm48xk/simple_corpus_service/,30199,1617808237.0,0,,False,,,,,,1173
394,,LanguageTechnology,"Hey everyone!

I'm applying for my Master's right now and the threads in this sub have been tremendously helpful! I was wondering if anyone is studying / knows someone who's studying at Uni-Potsdam. I've seen next to nothing about the program in this sub. If I'm being honest, that program looks the most appealing of the European programs I've come across in my research. So, to anyone who can provide some insight, I ask:

* Is it as heavy on the Machine Learning / Deep Learning front as it would appear from the course plan?
* Is it technical and applied (as opposed to theoretical and scholarly)?
* What's the general vibe? How do you and/or the students like it?

You can totally stop reading now but in case anyone's interested:  
I've been accepted to UEF and Edinburgh so far, hearing from Uppsala and/or Gothenburg on Friday. I'm Canadian, so the Swedish schools are either scholarship or $$$$$$. German applications aren't due for a little while so I have this nice little intermission to pause and ponder. Any advice is welcome! Thanks!",t2_14nkh3,False,,0,False,What's the word on Potsdam's MSc Cognitive Systems?,[],r/LanguageTechnology,False,6,,0,,False,t3_mlrmmy,False,dark,0.88,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1617789585.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m applying for my Master&amp;#39;s right now and the threads in this sub have been tremendously helpful! I was wondering if anyone is studying / knows someone who&amp;#39;s studying at Uni-Potsdam. I&amp;#39;ve seen next to nothing about the program in this sub. If I&amp;#39;m being honest, that program looks the most appealing of the European programs I&amp;#39;ve come across in my research. So, to anyone who can provide some insight, I ask:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Is it as heavy on the Machine Learning / Deep Learning front as it would appear from the course plan?&lt;/li&gt;
&lt;li&gt;Is it technical and applied (as opposed to theoretical and scholarly)?&lt;/li&gt;
&lt;li&gt;What&amp;#39;s the general vibe? How do you and/or the students like it?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can totally stop reading now but in case anyone&amp;#39;s interested:&lt;br/&gt;
I&amp;#39;ve been accepted to UEF and Edinburgh so far, hearing from Uppsala and/or Gothenburg on Friday. I&amp;#39;m Canadian, so the Swedish schools are either scholarship or $$$$$$. German applications aren&amp;#39;t due for a little while so I have this nice little intermission to pause and ponder. Any advice is welcome! Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mlrmmy,True,,NooksnGrannies,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mlrmmy/whats_the_word_on_potsdams_msc_cognitive_systems/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mlrmmy/whats_the_word_on_potsdams_msc_cognitive_systems/,30199,1617760785.0,0,,False,,,,,,1048
395,,LanguageTechnology,,t2_21qaqh1p,False,,0,False,AI (GPT-2) Text Generation in Python with Kaggle Dataset | FineTuning GPT Model,[],r/LanguageTechnology,False,6,,0,,False,t3_mljed7,False,dark,0.85,,public,14,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/6HcGTQKfeXY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'AI (GPT-2) Text Generation in Python with Kaggle Dataset | FineTuning GPT Model', 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/6HcGTQKfeXY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'version': '1.0', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/6HcGTQKfeXY/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/6HcGTQKfeXY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mljed7', 'height': 200}",,False,14,,False,False,,False,,[],{},,False,,1617765459.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mljed7,True,,dulldata,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mljed7/ai_gpt2_text_generation_in_python_with_kaggle/,all_ads,False,https://youtu.be/6HcGTQKfeXY,30199,1617736659.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'AI (GPT-2) Text Generation in Python with Kaggle Dataset | FineTuning GPT Model', 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/6HcGTQKfeXY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'version': '1.0', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/6HcGTQKfeXY/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}, 'type': 'youtube.com'}",False,https://youtu.be/6HcGTQKfeXY,,,,,0
396,,LanguageTechnology,"I see several websites listing top X conferences in NLP (Natural Language Processing), but I am not sure if there is some kind of ranking for these conferences. It will be amazing if anyone has some clue about it. Thank you very much for any hints or pointers or answers. :)",t2_9ptho1m,False,,0,False,What are the top 15 conferences in Natural Language Processing?,[],r/LanguageTechnology,False,6,,0,,False,t3_mlce0h,False,dark,0.93,,public,25,1,{},,False,[],,False,False,,{},,False,25,,False,False,,False,,[],{},,True,,1617746640.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I see several websites listing top X conferences in NLP (Natural Language Processing), but I am not sure if there is some kind of ranking for these conferences. It will be amazing if anyone has some clue about it. Thank you very much for any hints or pointers or answers. :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mlce0h,True,,freaky_eater,,10,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mlce0h/what_are_the_top_15_conferences_in_natural/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mlce0h/what_are_the_top_15_conferences_in_natural/,30199,1617717840.0,0,,False,,,,,,274
397,,LanguageTechnology,,t2_dwfcl,False,,0,False,[N] Grammarly releases a grammatical error correction (GEC) dataset for the Ukrainian language,[],r/LanguageTechnology,False,6,,0,,False,t3_mld6v9,False,dark,1.0,,public,15,0,{},,False,[],,False,False,,{},,False,15,,False,False,,False,,[],{},,False,,1617748863.0,text,6,,,text,self.MachineLearning,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mld6v9,True,,asivokon,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mld6v9/n_grammarly_releases_a_grammatical_error/,all_ads,False,/r/MachineLearning/comments/mlcv28/n_grammarly_releases_a_grammatical_error/,30199,1617720063.0,0,,False,/r/MachineLearning/comments/mlcv28/n_grammarly_releases_a_grammatical_error/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'This dataset contains 20,715 sentences annotated for grammatical errors and fluency correction. The license is CC-BY 4.0.\n\nThis blog post provides some context: [https://www.grammarly.com/blog/engineering/announcing-ua-gec/](https://www.grammarly.com/blog/engineering/announcing-ua-gec/)\n\nThe data and code are on Github: [https://github.com/grammarly/ua-gec](https://github.com/grammarly/ua-gec)\n\nPaper (draft): [https://arxiv.org/abs/2103.16997](https://arxiv.org/abs/2103.16997)\n\nI am one of the authors. Happy to answer your questions :)', 'author_fullname': 't2_dwfcl', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[N] Grammarly releases a grammatical error correction (GEC) dataset for the Ukrainian language', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'two', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_mlcv28', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.94, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 68, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'News', 'can_mod_post': False, 'score': 68, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1617747941.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This dataset contains 20,715 sentences annotated for grammatical errors and fluency correction. The license is CC-BY 4.0.&lt;/p&gt;\n\n&lt;p&gt;This blog post provides some context: &lt;a href=""https://www.grammarly.com/blog/engineering/announcing-ua-gec/""&gt;https://www.grammarly.com/blog/engineering/announcing-ua-gec/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The data and code are on Github: &lt;a href=""https://github.com/grammarly/ua-gec""&gt;https://github.com/grammarly/ua-gec&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Paper (draft): &lt;a href=""https://arxiv.org/abs/2103.16997""&gt;https://arxiv.org/abs/2103.16997&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I am one of the authors. Happy to answer your questions :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'mlcv28', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'asivokon', 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/mlcv28/n_grammarly_releases_a_grammatical_error/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/mlcv28/n_grammarly_releases_a_grammatical_error/', 'subreddit_subscribers': 1930710, 'created_utc': 1617719141.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_mlcv28,,,0
398,,LanguageTechnology,"This sub has been very helpful in the past so I am hoping I can get some feedback. For my project, I am essentially trying to find a way to detect changes over time to a news narrative. At this stage I have applied an algorithm to successfully group together the stories that follow the development of the same event. So now I need to find a way to track, analyze, and maybe quantify how the events (and their coverage changes) . My current approach is using topic modeling to find important keywords in each of the articles. Then, I use those key words to map how the stories change overtime. So in the most basic of terms, I am identifying key words in the first article in each narrative chain and then comparing how those keywords change and are different from key words identified in subsequent articles about the same event.  Does this approach sound reasonable?  Is there anything else I should be trying instead?  Thanks everyone!",t2_4qae118k,False,,0,False,Requesting feedback on project related to tracking how news events change over time,[],r/LanguageTechnology,False,6,,0,,False,t3_mliti6,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1617763927.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This sub has been very helpful in the past so I am hoping I can get some feedback. For my project, I am essentially trying to find a way to detect changes over time to a news narrative. At this stage I have applied an algorithm to successfully group together the stories that follow the development of the same event. So now I need to find a way to track, analyze, and maybe quantify how the events (and their coverage changes) . My current approach is using topic modeling to find important keywords in each of the articles. Then, I use those key words to map how the stories change overtime. So in the most basic of terms, I am identifying key words in the first article in each narrative chain and then comparing how those keywords change and are different from key words identified in subsequent articles about the same event.  Does this approach sound reasonable?  Is there anything else I should be trying instead?  Thanks everyone!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mliti6,True,,Massive-Marzipan,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mliti6/requesting_feedback_on_project_related_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mliti6/requesting_feedback_on_project_related_to/,30199,1617735127.0,0,,False,,,,,,938
399,,LanguageTechnology,,t2_apv7o2y8,False,,0,False,LDA Topic Modelling Explained with implementation using gensim in Python -#NLPRoc tutorial,[],r/LanguageTechnology,False,6,,0,,False,t3_ml2m65,False,dark,0.96,,public,26,1,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/nNvPvvuPnGs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'LDA Topic Modelling Explained with implementation using gensim in Python -#NLPRoc tutorial', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/nNvPvvuPnGs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech - NLP/AI/ML simplified', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/nNvPvvuPnGs/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCPAfv1GeraR1WM0608He6ow'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/nNvPvvuPnGs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ml2m65', 'height': 200}",,False,26,,False,False,,False,,[],{},,False,,1617709023.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ml2m65,True,,rsree123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ml2m65/lda_topic_modelling_explained_with_implementation/,all_ads,False,https://youtu.be/nNvPvvuPnGs,30199,1617680223.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'LDA Topic Modelling Explained with implementation using gensim in Python -#NLPRoc tutorial', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/nNvPvvuPnGs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech - NLP/AI/ML simplified', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/nNvPvvuPnGs/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCPAfv1GeraR1WM0608He6ow'}}",False,https://youtu.be/nNvPvvuPnGs,,,,,0
400,,LanguageTechnology,What's your experience in NLP and what do you think are the common strategies and best practices to follow as a beginner?,t2_b31us5mn,False,,0,False,Best practices in NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_ml9g1l,False,dark,0.6,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1617737241.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What&amp;#39;s your experience in NLP and what do you think are the common strategies and best practices to follow as a beginner?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ml9g1l,True,,Dr_Funkmachine,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ml9g1l/best_practices_in_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ml9g1l/best_practices_in_nlp/,30199,1617708441.0,0,,False,,,,,,121
401,,LanguageTechnology,"Hello everyone.

I am currently thinking about possible dissertation ideas that would combine NLP and blockchain. I got solid background in both modern NLP and distributed systems using blockchain (knowledge about theory, cryptography, investing, solidity...). 

If you have any idea that you are willing to share, it is more than welcome.",t2_1n85cze8,False,,0,False,Dissertation Ideas - NLP + blockchain,[],r/LanguageTechnology,False,6,,0,,False,t3_ml97ld,False,dark,0.67,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1617736349.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone.&lt;/p&gt;

&lt;p&gt;I am currently thinking about possible dissertation ideas that would combine NLP and blockchain. I got solid background in both modern NLP and distributed systems using blockchain (knowledge about theory, cryptography, investing, solidity...). &lt;/p&gt;

&lt;p&gt;If you have any idea that you are willing to share, it is more than welcome.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ml97ld,True,,LBahnhof,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ml97ld/dissertation_ideas_nlp_blockchain/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ml97ld/dissertation_ideas_nlp_blockchain/,30199,1617707549.0,0,,False,,,,,,339
402,,LanguageTechnology," I have dense neural network for BERT, How do I change that to Conv1D, maxpooling, and flatten, before connected to dense layer. 

\`\`\`

 

    class BertBinaryClassifier(nn.Module): 
    def __init__(self, dropout=0.1): 
    super(BertBinaryClassifier, self).__init__()         
    self.bert = BertModel.from_pretrained('bert-base-uncased')         
    self.dropout = nn.Dropout(dropout)         
    self.linear = nn.Linear(768, 1)        
     self.sigmoid = nn.Sigmoid()                   
    def forward(self, tokens, masks=None):         
    _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)         dropout_output = self.dropout(pooled_output)         
    linear_output = self.linear(dropout_output)        
     prediction = self.sigmoid(linear_output)         
    return prediction
     # Config setting 
    BATCH_SIZE = 4
     EPOCHS = 5

\`\`\`",t2_9kmymntm,False,,0,False,[D] Can we add CNN on top of BERT,[],r/LanguageTechnology,False,6,,0,,False,t3_ml5pe7,False,dark,0.75,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1617720947.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have dense neural network for BERT, How do I change that to Conv1D, maxpooling, and flatten, before connected to dense layer. &lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class BertBinaryClassifier(nn.Module): 
def __init__(self, dropout=0.1): 
super(BertBinaryClassifier, self).__init__()         
self.bert = BertModel.from_pretrained(&amp;#39;bert-base-uncased&amp;#39;)         
self.dropout = nn.Dropout(dropout)         
self.linear = nn.Linear(768, 1)        
 self.sigmoid = nn.Sigmoid()                   
def forward(self, tokens, masks=None):         
_, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)         dropout_output = self.dropout(pooled_output)         
linear_output = self.linear(dropout_output)        
 prediction = self.sigmoid(linear_output)         
return prediction
 # Config setting 
BATCH_SIZE = 4
 EPOCHS = 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;```&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ml5pe7,True,,jeromeharper,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ml5pe7/d_can_we_add_cnn_on_top_of_bert/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ml5pe7/d_can_we_add_cnn_on_top_of_bert/,30199,1617692147.0,0,,False,,,,,,906
403,,LanguageTechnology,"Is it possible for one to add keywords on a classification model?  (eg different sentences present in a dataset that may relate to -- company, school , Monuments, temples etc)  
So if I have a sentence that is classified as 'school' --&gt; maybe I can map the keyword - 'School' , 'Boarding school' , ' classroom' etc in the model so it can give better accuracy.

This is just a hypothetical thought I had, nothing pertaining to any existing dataset/ problem",t2_8j8sr5mz,False,,0,False,Adding Keywords to a Classification Model,[],r/LanguageTechnology,False,6,,0,,False,t3_ml5w7a,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1617721735.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is it possible for one to add keywords on a classification model?  (eg different sentences present in a dataset that may relate to -- company, school , Monuments, temples etc)&lt;br/&gt;
So if I have a sentence that is classified as &amp;#39;school&amp;#39; --&amp;gt; maybe I can map the keyword - &amp;#39;School&amp;#39; , &amp;#39;Boarding school&amp;#39; , &amp;#39; classroom&amp;#39; etc in the model so it can give better accuracy.&lt;/p&gt;

&lt;p&gt;This is just a hypothetical thought I had, nothing pertaining to any existing dataset/ problem&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ml5w7a,True,,Educational-Bid-5263,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ml5w7a/adding_keywords_to_a_classification_model/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ml5w7a/adding_keywords_to_a_classification_model/,30199,1617692935.0,0,,False,,,,,,458
404,,LanguageTechnology,"Hi, so I'm working on a topic modelling project whereby I'm trying to extract relevant job skills from a corpus of UX Researcher job postings. 

I've gone through all the standard steps of pre-processing and EDA and have already achieved some meaningful results by using TF-IDF with K-means clustering. With trigrams I've been able to return several clusters of skills which have things like; 

* qualitative quantitative research
* quantitative research method
* quantitative qualitative research
* user centre design
* psychology human computer
* brand include vogue
* deliver high quality
* user behaviour attitude
* interview usability test
* communicate research result

So I'm quite happy so far but it can be better no doubt. I've made word vectors of the dataset using word2vec, and the model is able to provide meaningful similarities between words for example; 

    model = Word2Vec(gensim_input_set, min_count=10, size=100, workers=3, window=5, sg=1)
    print(model.wv.most_similar('psychology')[:5])
    
    &gt;&gt;&gt; [('cognitive', 0.9959733486175537), ('computer', 0.9905843734741211), ('experimental', 0.9882711172103882), ('hci', 0.9872062802314758), ('anthropology', 0.9846541285514832)]

But now I'd like to know if it's possible to take the word embeddings from the model and cluster them somehow?",t2_48eyl33r,False,,0,False,Use word2vec for topic modelling,[],r/LanguageTechnology,False,6,,0,,False,t3_mkum00,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1617684788.0,text,6,,,text,self.LanguageTechnology,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, so I&amp;#39;m working on a topic modelling project whereby I&amp;#39;m trying to extract relevant job skills from a corpus of UX Researcher job postings. &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve gone through all the standard steps of pre-processing and EDA and have already achieved some meaningful results by using TF-IDF with K-means clustering. With trigrams I&amp;#39;ve been able to return several clusters of skills which have things like; &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;qualitative quantitative research&lt;/li&gt;
&lt;li&gt;quantitative research method&lt;/li&gt;
&lt;li&gt;quantitative qualitative research&lt;/li&gt;
&lt;li&gt;user centre design&lt;/li&gt;
&lt;li&gt;psychology human computer&lt;/li&gt;
&lt;li&gt;brand include vogue&lt;/li&gt;
&lt;li&gt;deliver high quality&lt;/li&gt;
&lt;li&gt;user behaviour attitude&lt;/li&gt;
&lt;li&gt;interview usability test&lt;/li&gt;
&lt;li&gt;communicate research result&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So I&amp;#39;m quite happy so far but it can be better no doubt. I&amp;#39;ve made word vectors of the dataset using word2vec, and the model is able to provide meaningful similarities between words for example; &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model = Word2Vec(gensim_input_set, min_count=10, size=100, workers=3, window=5, sg=1)
print(model.wv.most_similar(&amp;#39;psychology&amp;#39;)[:5])

&amp;gt;&amp;gt;&amp;gt; [(&amp;#39;cognitive&amp;#39;, 0.9959733486175537), (&amp;#39;computer&amp;#39;, 0.9905843734741211), (&amp;#39;experimental&amp;#39;, 0.9882711172103882), (&amp;#39;hci&amp;#39;, 0.9872062802314758), (&amp;#39;anthropology&amp;#39;, 0.9846541285514832)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But now I&amp;#39;d like to know if it&amp;#39;s possible to take the word embeddings from the model and cluster them somehow?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mkum00,True,,crowpup783,,13,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mkum00/use_word2vec_for_topic_modelling/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mkum00/use_word2vec_for_topic_modelling/,30199,1617655988.0,0,,False,,,,,,1322
405,,LanguageTechnology,Am I correct in thinking that BERT uses wordpiece to create a unigram type vector that is then used as the input into the BERT system. But the embeddings we use after the pre-training is done is not just those initial embeddings it is the weights of all the parts of the network. Is this correct?,t2_q97pg,False,,0,False,Question about BERT Embeddings,[],r/LanguageTechnology,False,6,,0,,False,t3_mkr306,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1617675320.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Am I correct in thinking that BERT uses wordpiece to create a unigram type vector that is then used as the input into the BERT system. But the embeddings we use after the pre-training is done is not just those initial embeddings it is the weights of all the parts of the network. Is this correct?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mkr306,True,,10Exahertz,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mkr306/question_about_bert_embeddings/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mkr306/question_about_bert_embeddings/,30199,1617646520.0,0,,False,,,,,,296
406,,LanguageTechnology,"We’ve been seeing a surge in the requests for Fuzzy Matching/Logic techniques. We tried to learn more about these techniques and the implementation process and have written about it. We have also covered some complex scenarios and their solution.  


Here’s a detailed view on the concept, its utility, and the implementation: [https://nanonets.com/blog/fuzzy-matching-fuzzy-logic/](https://nanonets.com/blog/fuzzy-matching-fuzzy-logic/)  


Can anyone share their experience on implementing Fuzzy Matching in their product or solution?",t2_13ggwo57,False,,0,False,"Fuzzy Matching/Logic - Concept, Utility, Implementation, and Complex Scenarios",[],r/LanguageTechnology,False,6,,0,,False,t3_mkgoiy,False,dark,0.88,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1617643715.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We’ve been seeing a surge in the requests for Fuzzy Matching/Logic techniques. We tried to learn more about these techniques and the implementation process and have written about it. We have also covered some complex scenarios and their solution.  &lt;/p&gt;

&lt;p&gt;Here’s a detailed view on the concept, its utility, and the implementation: &lt;a href=""https://nanonets.com/blog/fuzzy-matching-fuzzy-logic/""&gt;https://nanonets.com/blog/fuzzy-matching-fuzzy-logic/&lt;/a&gt;  &lt;/p&gt;

&lt;p&gt;Can anyone share their experience on implementing Fuzzy Matching in their product or solution?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mkgoiy,True,,nanonets,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mkgoiy/fuzzy_matchinglogic_concept_utility/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mkgoiy/fuzzy_matchinglogic_concept_utility/,30199,1617614915.0,0,,False,,,,,,536
407,,LanguageTechnology,"Hi

I m trying to set up a Spacy pipeline with my one transformer but it does not work as the transformer I generated is not compatible.

Dows anyone have an updated example of this pipeline set up, including the compatible method of creating the Transformer?

Thanks!",t2_8ozl7,False,,0,False,How to use spacy and a tailor-made transformer in sentiment analysis?,[],r/LanguageTechnology,False,6,,0,,False,t3_mjzxhu,False,dark,1.0,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,True,,1617583801.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi&lt;/p&gt;

&lt;p&gt;I m trying to set up a Spacy pipeline with my one transformer but it does not work as the transformer I generated is not compatible.&lt;/p&gt;

&lt;p&gt;Dows anyone have an updated example of this pipeline set up, including the compatible method of creating the Transformer?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mjzxhu,True,,raffbr2,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mjzxhu/how_to_use_spacy_and_a_tailormade_transformer_in/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mjzxhu/how_to_use_spacy_and_a_tailormade_transformer_in/,30199,1617555001.0,0,,False,,,,,,268
408,,LanguageTechnology,"Hey,   
I'm trying to create a chatbot website. The chatbot is developed using Spacy and Tensorflow + Flask. I tried Heroku but ran into the limits pretty quickly. And since I'm a broke student, don't really want to pay for it just yet. 

Any suggestions? Is it impossible to avoid paying for it?",t2_8vzdo03r,False,,0,False,Deploying chatbot on a website?,[],r/LanguageTechnology,False,6,,0,,False,t3_mk4a5z,False,dark,0.81,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1617597286.0,text,6,,,text,self.LanguageTechnology,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey,&lt;br/&gt;
I&amp;#39;m trying to create a chatbot website. The chatbot is developed using Spacy and Tensorflow + Flask. I tried Heroku but ran into the limits pretty quickly. And since I&amp;#39;m a broke student, don&amp;#39;t really want to pay for it just yet. &lt;/p&gt;

&lt;p&gt;Any suggestions? Is it impossible to avoid paying for it?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mk4a5z,True,,finance_and_kebabs,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mk4a5z/deploying_chatbot_on_a_website/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mk4a5z/deploying_chatbot_on_a_website/,30199,1617568486.0,0,,False,,,,,,296
409,,LanguageTechnology,"Is there such a thing as a real time language generator? 

Apologies if the question may seem silly, I'm a researcher from a completely different field. I only know of gpt3 but it has a significant delay in the responses. We would like to use it to give some semblance of life to virtual characters in a research project. 

What would be the next best thing? Should I come back in 5-10 years? Any pointers would be greatly appreciated.",t2_fp2pm,False,,0,False,Real-time language generation?,[],r/LanguageTechnology,False,6,,0,,False,t3_mjflif,False,dark,0.89,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1617507474.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there such a thing as a real time language generator? &lt;/p&gt;

&lt;p&gt;Apologies if the question may seem silly, I&amp;#39;m a researcher from a completely different field. I only know of gpt3 but it has a significant delay in the responses. We would like to use it to give some semblance of life to virtual characters in a research project. &lt;/p&gt;

&lt;p&gt;What would be the next best thing? Should I come back in 5-10 years? Any pointers would be greatly appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mjflif,True,,AvengerDr,,10,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mjflif/realtime_language_generation/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mjflif/realtime_language_generation/,30199,1617478674.0,0,,False,,,,,,435
410,,LanguageTechnology,,t2_apv7o2y8,False,,0,False,Kaggle Natural Language Processing with Disaster Tweets – Top 14% Solution with BERT (TensorFlow),[],r/LanguageTechnology,False,6,,0,,False,t3_mj4yaw,False,dark,0.88,,public,18,1,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/QrILL4Ch-aE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Kaggle Natural Language Processing with Disaster Tweets – Top 14% Solution with BERT (TensorFlow)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/QrILL4Ch-aE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech - NLP/AI/ML simplified', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/QrILL4Ch-aE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCPAfv1GeraR1WM0608He6ow'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/QrILL4Ch-aE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mj4yaw', 'height': 200}",,False,18,,False,False,,False,,[],{},,False,,1617469568.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mj4yaw,True,,rsree123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mj4yaw/kaggle_natural_language_processing_with_disaster/,all_ads,False,https://youtu.be/QrILL4Ch-aE,30199,1617440768.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Kaggle Natural Language Processing with Disaster Tweets – Top 14% Solution with BERT (TensorFlow)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/QrILL4Ch-aE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech - NLP/AI/ML simplified', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/QrILL4Ch-aE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCPAfv1GeraR1WM0608He6ow'}}",False,https://youtu.be/QrILL4Ch-aE,,,,,0
411,,LanguageTechnology,"I'm working on a project where I need to get the word embeddings of around 300,000 sentences so that I can compare them with other sentences using cosine similarity. Since I'm dealing with so many sentences, what would be the most efficient way to get the word embeddings? So far I've been using the [sentence-transformers](https://github.com/UKPLab/sentence-transformers) package (my model is initialized as SentenceTransformer('bert-base-nli-max-tokens')), but using this would take around 24 hours if not more to get the word embeddings of all sentences. If that's as fast as it can get I'm fine with that, but was wondering if there's a way to do this more efficiently?",t2_afu8glnj,False,,0,False,Most efficient method for converting ~300k sentences to word embeddings?,[],r/LanguageTechnology,False,6,,0,,False,t3_miyrcq,False,dark,1.0,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1617443057.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working on a project where I need to get the word embeddings of around 300,000 sentences so that I can compare them with other sentences using cosine similarity. Since I&amp;#39;m dealing with so many sentences, what would be the most efficient way to get the word embeddings? So far I&amp;#39;ve been using the &lt;a href=""https://github.com/UKPLab/sentence-transformers""&gt;sentence-transformers&lt;/a&gt; package (my model is initialized as SentenceTransformer(&amp;#39;bert-base-nli-max-tokens&amp;#39;)), but using this would take around 24 hours if not more to get the word embeddings of all sentences. If that&amp;#39;s as fast as it can get I&amp;#39;m fine with that, but was wondering if there&amp;#39;s a way to do this more efficiently?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,miyrcq,True,,cscqmastersthrowaway,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/miyrcq/most_efficient_method_for_converting_300k/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/miyrcq/most_efficient_method_for_converting_300k/,30199,1617414257.0,0,,False,,,,,,673
412,,LanguageTechnology,"I'm looking for resources/code which will allow me to generate text (Introduction/About Me) based on defined text inputs like Name, Place,  Occupation, etc.   


I was able to find a good relatable paper but there is no available code for [ToTTo: A Controlled Table-To-Text Generation Dataset](https://paperswithcode.com/paper/totto-a-controlled-table-to-text-generation) . If anyone have done something in generation or have some idea then it will be helpful.  
thanks in advance.",t2_3hprdu77,False,,0,False,Conditional Text Generation (About Me),[],r/LanguageTechnology,False,6,,0,,False,t3_mis4ye,False,dark,1.0,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1617421986.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m looking for resources/code which will allow me to generate text (Introduction/About Me) based on defined text inputs like Name, Place,  Occupation, etc.   &lt;/p&gt;

&lt;p&gt;I was able to find a good relatable paper but there is no available code for &lt;a href=""https://paperswithcode.com/paper/totto-a-controlled-table-to-text-generation""&gt;ToTTo: A Controlled Table-To-Text Generation Dataset&lt;/a&gt; . If anyone have done something in generation or have some idea then it will be helpful.&lt;br/&gt;
thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mis4ye,True,,prashants975,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mis4ye/conditional_text_generation_about_me/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mis4ye/conditional_text_generation_about_me/,30199,1617393186.0,0,,False,,,,,,481
413,,LanguageTechnology,"I can't find any good tutorial to learn all these strategies with my own csv dataset with labeled reviews. As I don't have the same data structure of imdb dataset, I can't follow any tutorial based on official documentation. Every f tutorial is based on the same example!

Anyway, I would appreciate any good resource!",t2_4df6a1hn,False,,0,False,"Any good tutorial for NLP using Tensorflow for classification, word embedding, ... WITHOUT imdb dataset.",[],r/LanguageTechnology,False,6,,0,,False,t3_mim9im,False,dark,0.85,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,1617378478.0,,[],{},,True,,1617404917.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I can&amp;#39;t find any good tutorial to learn all these strategies with my own csv dataset with labeled reviews. As I don&amp;#39;t have the same data structure of imdb dataset, I can&amp;#39;t follow any tutorial based on official documentation. Every f tutorial is based on the same example!&lt;/p&gt;

&lt;p&gt;Anyway, I would appreciate any good resource!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mim9im,True,,JotaPe-exe,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mim9im/any_good_tutorial_for_nlp_using_tensorflow_for/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mim9im/any_good_tutorial_for_nlp_using_tensorflow_for/,30199,1617376117.0,0,,False,,,,,,318
414,,LanguageTechnology,"The third webinar in the language tech series now has a date!

[https://us02web.zoom.us/webinar/register/8416174045220/WN\_DQnctbT7RD-Q7BzYzy2TsQ](https://us02web.zoom.us/webinar/register/8416174045220/WN_DQnctbT7RD-Q7BzYzy2TsQ)",t2_5w21fvbc,False,,0,False,"New date announced for language tech series, on Terminology Management Tools",[],r/LanguageTechnology,False,6,,0,,False,t3_miw0n9,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1617433516.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The third webinar in the language tech series now has a date!&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://us02web.zoom.us/webinar/register/8416174045220/WN_DQnctbT7RD-Q7BzYzy2TsQ""&gt;https://us02web.zoom.us/webinar/register/8416174045220/WN_DQnctbT7RD-Q7BzYzy2TsQ&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,miw0n9,True,,LanguageNurd,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/miw0n9/new_date_announced_for_language_tech_series_on/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/miw0n9/new_date_announced_for_language_tech_series_on/,30199,1617404716.0,0,,False,,,,,,228
415,,LanguageTechnology,"I have a problem in this vein, and was wondering what everyone thought:

**Problem:** Classify 2-line snippets of poems by their author.

**Data:** I have 9 poets from whom I've collected anywhere from 150-250 poems.  Assume that this dataset can't get any bigger.  

**Method:** Log. Reg./SVM/KNN as basline, ultimately implementing word-level and subword-level CNN (which is the point of the experiment, mining out the different between word-level vs. subword level models in this domain).

Now my question is, with such a small amount of data, should I split the poems into 2-line snippets for training, and thus have \~2500-5000 examples per author, or should I train with larger chunks that will have more semantic information (e.g. 4 lines, entire stanzas, entire poems)?  Additionally, assuming the answer is something along the line of ""it depends,"" what are some tools for exploring this tradeoff and deciding what to train with?

Thanks!",t2_9su0t3yo,False,,0,False,More examples or longer examples for small dataset classification problem,[],r/LanguageTechnology,False,6,,0,,False,t3_miiy76,False,dark,1.0,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1617393851.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a problem in this vein, and was wondering what everyone thought:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Classify 2-line snippets of poems by their author.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data:&lt;/strong&gt; I have 9 poets from whom I&amp;#39;ve collected anywhere from 150-250 poems.  Assume that this dataset can&amp;#39;t get any bigger.  &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Method:&lt;/strong&gt; Log. Reg./SVM/KNN as basline, ultimately implementing word-level and subword-level CNN (which is the point of the experiment, mining out the different between word-level vs. subword level models in this domain).&lt;/p&gt;

&lt;p&gt;Now my question is, with such a small amount of data, should I split the poems into 2-line snippets for training, and thus have ~2500-5000 examples per author, or should I train with larger chunks that will have more semantic information (e.g. 4 lines, entire stanzas, entire poems)?  Additionally, assuming the answer is something along the line of &amp;quot;it depends,&amp;quot; what are some tools for exploring this tradeoff and deciding what to train with?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,miiy76,True,,IglooAustralia88,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/miiy76/more_examples_or_longer_examples_for_small/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/miiy76/more_examples_or_longer_examples_for_small/,30199,1617365051.0,0,,False,,,,,,947
416,,LanguageTechnology," Looking to add relation extraction classifier to your NER?

Checkout our [new article](https://walidamamou.medium.com/how-to-train-a-joint-entities-and-relation-extraction-classifier-using-bert-transformer-with-spacy-49eb08d91b5c) on how to train a joint entities and relation extraction classifier using BERT Transformer with spaCy 3.",t2_32tnavmg,False,,0,False,How to Train a Joint Entities and Relation Extraction Classifier using BERT Transformer with spaCy 3,[],r/LanguageTechnology,False,6,,0,,False,t3_minujj,False,dark,0.75,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1617409727.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Looking to add relation extraction classifier to your NER?&lt;/p&gt;

&lt;p&gt;Checkout our &lt;a href=""https://walidamamou.medium.com/how-to-train-a-joint-entities-and-relation-extraction-classifier-using-bert-transformer-with-spacy-49eb08d91b5c""&gt;new article&lt;/a&gt; on how to train a joint entities and relation extraction classifier using BERT Transformer with spaCy 3.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,minujj,True,,UBIAI,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/minujj/how_to_train_a_joint_entities_and_relation/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/minujj/how_to_train_a_joint_entities_and_relation/,30199,1617380927.0,0,,False,,,,,,336
417,,LanguageTechnology,,t2_5cvo57qv,False,,0,False,Machine Translation in Argos Translate (2021),[],r/LanguageTechnology,False,6,,0,,False,t3_mirlz0,False,dark,0.5,,public,0,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/yn37-CpRzTc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Machine Translation in Argos Translate (2021)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/yn37-CpRzTc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Argos Open Tech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/yn37-CpRzTc/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCUhFxpTpgsu0rXKPrybCQSA'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/yn37-CpRzTc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mirlz0', 'height': 200}",,False,0,,False,False,,False,,[],{},,False,,1617420476.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mirlz0,True,,argosopentech,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mirlz0/machine_translation_in_argos_translate_2021/,all_ads,False,https://youtu.be/yn37-CpRzTc,30199,1617391676.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Machine Translation in Argos Translate (2021)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/yn37-CpRzTc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Argos Open Tech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/yn37-CpRzTc/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCUhFxpTpgsu0rXKPrybCQSA'}}",False,https://youtu.be/yn37-CpRzTc,,,,,0
418,,LanguageTechnology,"I spent \~50 hours looking for the best resources on various Transformer models and organized them into a website [backprop.org](https://backprop.org/).

The website is intended to help everyone learn faster by directly connecting you to the best resources without having to hunt everything down yourself.

Check it out! If people find this useful I'll add more pages on more NLP topics. Already planning on adding pages about RNNs, LSTMs, GRUs, etc.

Also, if you know any great resources that I missed send it my way!",t2_8tp141ey,False,,0,False,Made website with best resources I could find on Transformers,[],r/LanguageTechnology,False,6,,0,,False,t3_mhv7dw,False,dark,0.99,,public,55,1,{},,False,[],,False,False,,{},,False,55,,False,False,,False,,[],{},,True,,1617312177.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I spent ~50 hours looking for the best resources on various Transformer models and organized them into a website &lt;a href=""https://backprop.org/""&gt;backprop.org&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The website is intended to help everyone learn faster by directly connecting you to the best resources without having to hunt everything down yourself.&lt;/p&gt;

&lt;p&gt;Check it out! If people find this useful I&amp;#39;ll add more pages on more NLP topics. Already planning on adding pages about RNNs, LSTMs, GRUs, etc.&lt;/p&gt;

&lt;p&gt;Also, if you know any great resources that I missed send it my way!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 20, 'id': 'award_5eac457f-ebac-449b-93a7-eb17b557f03c', 'penny_donate': 0, 'award_sub_type': 'PREMIUM', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/j3azv69qjfn51_LOVE.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/j3azv69qjfn51_LOVE.png?width=16&amp;height=16&amp;auto=webp&amp;s=bc61b528b8d075c26a3d0f2ad3d9e42259c51cbe', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/j3azv69qjfn51_LOVE.png?width=32&amp;height=32&amp;auto=webp&amp;s=d576c9a19ed29ca5624333239dbde289a146930b', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/j3azv69qjfn51_LOVE.png?width=48&amp;height=48&amp;auto=webp&amp;s=da1e45654f5acfb6be44fa07c168ad6420796f56', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/j3azv69qjfn51_LOVE.png?width=64&amp;height=64&amp;auto=webp&amp;s=677455ac05c563b5585f76e52ee96354f1430799', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/j3azv69qjfn51_LOVE.png?width=128&amp;height=128&amp;auto=webp&amp;s=25a3b6021a92685b01883fb3d947d2959a75d8b3', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you follow your heart, love is the answer', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'LOVE!', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/j3azv69qjfn51_LOVE.png?width=16&amp;height=16&amp;auto=webp&amp;s=bc61b528b8d075c26a3d0f2ad3d9e42259c51cbe', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/j3azv69qjfn51_LOVE.png?width=32&amp;height=32&amp;auto=webp&amp;s=d576c9a19ed29ca5624333239dbde289a146930b', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/j3azv69qjfn51_LOVE.png?width=48&amp;height=48&amp;auto=webp&amp;s=da1e45654f5acfb6be44fa07c168ad6420796f56', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/j3azv69qjfn51_LOVE.png?width=64&amp;height=64&amp;auto=webp&amp;s=677455ac05c563b5585f76e52ee96354f1430799', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/j3azv69qjfn51_LOVE.png?width=128&amp;height=128&amp;auto=webp&amp;s=25a3b6021a92685b01883fb3d947d2959a75d8b3', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/j3azv69qjfn51_LOVE.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mhv7dw,True,,backpropsite,,17,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mhv7dw/made_website_with_best_resources_i_could_find_on/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mhv7dw/made_website_with_best_resources_i_could_find_on/,30199,1617283377.0,0,,False,,,,,,519
419,,LanguageTechnology,,t2_auwgbh53,False,,0,False,"Beginners in NLP, need your feedback. Made this video explaining building neural semantic search using open-source project Jina(think search like google). How good is this explanation?",[],r/LanguageTechnology,False,6,,0,,False,t3_mhyapm,False,dark,0.91,,public,9,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/zvXkQkqd2I8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Jina 101: Basic concepts in Jina', 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/zvXkQkqd2I8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'version': '1.0', 'author_name': 'Jina AI', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/zvXkQkqd2I8/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC1zsKWBPGWVM3IjAsp2IPdw'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/zvXkQkqd2I8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mhyapm', 'height': 200}",,False,9,,False,True,,False,,[],{},,False,,1617321321.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mhyapm,True,,opensourcecolumbus,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mhyapm/beginners_in_nlp_need_your_feedback_made_this/,all_ads,False,https://www.youtube.com/watch?v=zvXkQkqd2I8&amp;feature=youtu.be,30199,1617292521.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Jina 101: Basic concepts in Jina', 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/zvXkQkqd2I8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'version': '1.0', 'author_name': 'Jina AI', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/zvXkQkqd2I8/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC1zsKWBPGWVM3IjAsp2IPdw'}, 'type': 'youtube.com'}",False,https://www.youtube.com/watch?v=zvXkQkqd2I8&amp;feature=youtu.be,,,,,0
420,,LanguageTechnology,,t2_l6ugg,False,,0,False,"The Conversational AI and NLP Summit takes place later this month. See the speaker list and agenda below! Speaking companies include Deepmind, Facebook and more.",[],r/LanguageTechnology,False,6,,0,,False,t3_mhtihs,False,dark,0.82,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,False,,1617306287.0,text,6,,,text,re-work.co,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mhtihs,True,,nikitaljohnson,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mhtihs/the_conversational_ai_and_nlp_summit_takes_place/,all_ads,False,https://www.re-work.co/events/conversational-ai-nlp-summit-2021?utm_source=LK&amp;utm_medium=EB_Promo&amp;utm_campaign=LK_CONV_NLP_EB,30199,1617277487.0,0,,False,https://www.re-work.co/events/conversational-ai-nlp-summit-2021?utm_source=LK&amp;utm_medium=EB_Promo&amp;utm_campaign=LK_CONV_NLP_EB,,,,,0
421,,LanguageTechnology," Hi everyone, 

please excuse me if this is the wrong place to post! 

Myself and a small team are looking to train an existing AI tool to be able to identify corruption in organisations, and I am looking for historical examples to train it on. In particular, at this stage, I am looking for concrete text-based examples where evidence of corrupt practices is clearly visible (e.g. from meeting protocols/minutes, emails or other company communications, …) and should be from corruption scandals where text document-based leaks took place e.g. from Panama papers, Fifa scandal, Enron, 1MDB, Odebrecht, etc. 

At this stage of the project, we do not have the resources to go through the corpora manually to look for these concrete examples, therefore I have been looking for examples from the corpora which have been cited in academic papers or newspaper articles. At a later stage, the AI would be able to go through the corruption corpus itself. So far, my search to find such examples have been unsuccessful. 

I realise that corruption is a very clandestine topic, which is probably why I am having such difficulty. If anyone has ideas of how I can find articles etc where the data has already been interpreted and show concrete examples of corruption, then I would very much appreciate it!

Many thanks!",t2_6gsg13fy,False,,0,False,Training AI on anti-corruption,[],r/LanguageTechnology,False,6,,0,,False,t3_mhsk34,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1617302434.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;

&lt;p&gt;please excuse me if this is the wrong place to post! &lt;/p&gt;

&lt;p&gt;Myself and a small team are looking to train an existing AI tool to be able to identify corruption in organisations, and I am looking for historical examples to train it on. In particular, at this stage, I am looking for concrete text-based examples where evidence of corrupt practices is clearly visible (e.g. from meeting protocols/minutes, emails or other company communications, …) and should be from corruption scandals where text document-based leaks took place e.g. from Panama papers, Fifa scandal, Enron, 1MDB, Odebrecht, etc. &lt;/p&gt;

&lt;p&gt;At this stage of the project, we do not have the resources to go through the corpora manually to look for these concrete examples, therefore I have been looking for examples from the corpora which have been cited in academic papers or newspaper articles. At a later stage, the AI would be able to go through the corruption corpus itself. So far, my search to find such examples have been unsuccessful. &lt;/p&gt;

&lt;p&gt;I realise that corruption is a very clandestine topic, which is probably why I am having such difficulty. If anyone has ideas of how I can find articles etc where the data has already been interpreted and show concrete examples of corruption, then I would very much appreciate it!&lt;/p&gt;

&lt;p&gt;Many thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mhsk34,True,,ch0rapi,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mhsk34/training_ai_on_anticorruption/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mhsk34/training_ai_on_anticorruption/,30199,1617273634.0,0,,False,,,,,,1307
422,,LanguageTechnology,"Hi everyone,

We just released the version 1.0.0 for our Transformer-based Multilingual NLP toolkit named Trankit which outperforms the popular SOTA Stanford NLP (Stanza) in many tasks over 56 different languages.

### 💥 💥 💥 The new version v1.0.0 offers:

* **A trainable pipeline for fundamental NLP tasks over 100 languages**.
* **90 new pretrained transformer-based pipelines for 56 languages**. The new pipelines are trained with XLM-Roberta large, which further boosts the performance significantly over 90 treebanks of the Universal Dependencies v2.5 corpus. For **English**, Trankit is significantly better than Stanza on sentence segmentation (**+9.36%**) and dependency parsing (**+5.07%** for UAS and **+5.81%** for LAS). For **Arabic**, our toolkit substantially improves sentence segmentation performance by **16.36%** while **Chinese** observes **14.50%** and **15.0%** improvement of UAS and LAS for dependency parsing. Performance on other languages is also significantly improved. The detailed comparison between Trankit, Stanza, UDPipe, Spacy on other languages can be found  [here](https://trankit.readthedocs.io/en/latest/performance.html#universal-dependencies-v2-5) .
* **Auto Mode for multilingual pipelines**. In the Auto Mode, the language of the input will be automatically detected, enabling the multilingual pipelines to process the input without specifying its language.  Check out how to turn on the Auto Mode [here](https://trankit.readthedocs.io/en/latest/news.html#auto-mode-for-multilingual-pipelines). 
* **Command-line interface** is now available to use. This helps users who are not familiar with Python programming language can use Trankit more easily.  Check out the command-line tutorials on this [page](https://trankit.readthedocs.io/en/latest/commandline.html). 

**Trankit is written in Python** and can be easily installed via pip. Our code and pretrained models are publicly available at: [https://github.com/nlp-uoregon/trankit](https://github.com/nlp-uoregon/trankit)

We also created a documentation page and a demo website for Trankit.

Documentation page: [https://trankit.readthedocs.io/en/latest/index.html](https://trankit.readthedocs.io/en/latest/index.html)

Demo website: [http://nlp.uoregon.edu/trankit](http://nlp.uoregon.edu/trankit)

Technical details about Trankit can be found in our paper: [https://arxiv.org/pdf/2101.03289.pdf](https://arxiv.org/pdf/2101.03289.pdf)

Thank you for your time reading this post!

Hope you enjoy Trankit!",t2_7suf1otj,False,,0,False,Trankit v1.0.0 - An open-source Transformer-based Multilingual NLP Toolkit for 56 languages is out.,[],r/LanguageTechnology,False,6,,0,,False,t3_mhhs3a,False,dark,0.93,,public,33,0,{},,False,[],,False,False,,{},,False,33,,False,False,,False,,[],{},,True,,1617259733.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;We just released the version 1.0.0 for our Transformer-based Multilingual NLP toolkit named Trankit which outperforms the popular SOTA Stanford NLP (Stanza) in many tasks over 56 different languages.&lt;/p&gt;

&lt;h3&gt;💥 💥 💥 The new version v1.0.0 offers:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A trainable pipeline for fundamental NLP tasks over 100 languages&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;90 new pretrained transformer-based pipelines for 56 languages&lt;/strong&gt;. The new pipelines are trained with XLM-Roberta large, which further boosts the performance significantly over 90 treebanks of the Universal Dependencies v2.5 corpus. For &lt;strong&gt;English&lt;/strong&gt;, Trankit is significantly better than Stanza on sentence segmentation (&lt;strong&gt;+9.36%&lt;/strong&gt;) and dependency parsing (&lt;strong&gt;+5.07%&lt;/strong&gt; for UAS and &lt;strong&gt;+5.81%&lt;/strong&gt; for LAS). For &lt;strong&gt;Arabic&lt;/strong&gt;, our toolkit substantially improves sentence segmentation performance by &lt;strong&gt;16.36%&lt;/strong&gt; while &lt;strong&gt;Chinese&lt;/strong&gt; observes &lt;strong&gt;14.50%&lt;/strong&gt; and &lt;strong&gt;15.0%&lt;/strong&gt; improvement of UAS and LAS for dependency parsing. Performance on other languages is also significantly improved. The detailed comparison between Trankit, Stanza, UDPipe, Spacy on other languages can be found  &lt;a href=""https://trankit.readthedocs.io/en/latest/performance.html#universal-dependencies-v2-5""&gt;here&lt;/a&gt; .&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Auto Mode for multilingual pipelines&lt;/strong&gt;. In the Auto Mode, the language of the input will be automatically detected, enabling the multilingual pipelines to process the input without specifying its language.  Check out how to turn on the Auto Mode &lt;a href=""https://trankit.readthedocs.io/en/latest/news.html#auto-mode-for-multilingual-pipelines""&gt;here&lt;/a&gt;. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Command-line interface&lt;/strong&gt; is now available to use. This helps users who are not familiar with Python programming language can use Trankit more easily.  Check out the command-line tutorials on this &lt;a href=""https://trankit.readthedocs.io/en/latest/commandline.html""&gt;page&lt;/a&gt;. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Trankit is written in Python&lt;/strong&gt; and can be easily installed via pip. Our code and pretrained models are publicly available at: &lt;a href=""https://github.com/nlp-uoregon/trankit""&gt;https://github.com/nlp-uoregon/trankit&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We also created a documentation page and a demo website for Trankit.&lt;/p&gt;

&lt;p&gt;Documentation page: &lt;a href=""https://trankit.readthedocs.io/en/latest/index.html""&gt;https://trankit.readthedocs.io/en/latest/index.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Demo website: &lt;a href=""http://nlp.uoregon.edu/trankit""&gt;http://nlp.uoregon.edu/trankit&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Technical details about Trankit can be found in our paper: &lt;a href=""https://arxiv.org/pdf/2101.03289.pdf""&gt;https://arxiv.org/pdf/2101.03289.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thank you for your time reading this post!&lt;/p&gt;

&lt;p&gt;Hope you enjoy Trankit!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mhhs3a,True,,mgl96,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mhhs3a/trankit_v100_an_opensource_transformerbased/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mhhs3a/trankit_v100_an_opensource_transformerbased/,30199,1617230933.0,0,,False,,,,,,2499
423,,LanguageTechnology,,t2_hkv9s,False,,0,False,Reformulating Unsupervised Style Transfer as Paraphrase Generation | Research Paper Walkthrough,[],r/LanguageTechnology,False,6,,0,,False,t3_mhsxa9,False,dark,0.8,,public,3,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/cjnk3PJljDs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Reformulating Unsupervised Style Transfer as Paraphrase Generation | Research Paper Walkthrough', 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/cjnk3PJljDs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'version': '1.0', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/cjnk3PJljDs/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/cjnk3PJljDs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mhsxa9', 'height': 200}",,False,3,,False,False,,False,,[],{},,False,,1617303974.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mhsxa9,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mhsxa9/reformulating_unsupervised_style_transfer_as/,all_ads,False,https://youtu.be/cjnk3PJljDs,30199,1617275174.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Reformulating Unsupervised Style Transfer as Paraphrase Generation | Research Paper Walkthrough', 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/cjnk3PJljDs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'version': '1.0', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/cjnk3PJljDs/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}, 'type': 'youtube.com'}",False,https://youtu.be/cjnk3PJljDs,,,,,0
424,,LanguageTechnology,"I wanted to share this new library I've been working on and that I open-sourced!.

here are some links to the library:

💻 [GitHub Repository](https://github.com/Ki6an/fastT5)

🐍 [PyPi project](https://pypi.org/project/fastt5/)

as the title suggests, you can increase the inference speed of any pretrained T5 model and also decrease the models' size, in a single line of code.

The library can be installed with `pip install fastt5`. This code snippet from the repository's README gives a concise overview:

    from fastT5 import export_and_get_onnx_model
    from transformers import AutoTokenizer
    
    model_name = 't5-small'
    model = export_and_get_onnx_model(model_name)
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    t_input = ""translate English to French: The universe is a dark forest.""
    token = tokenizer(t_input, return_tensors='pt')
    
    tokens = model.generate(input_ids=token['input_ids'],
                   attention_mask=token['attention_mask'],
                   num_beams=2)
    
    output = tokenizer.decode(tokens.squeeze(), skip_special_tokens=True)
    print(output)

The fastT5 library exports the T5 model to onnx with `past_key_values,` then quantizes it and runs it on onnxruntime.

The exported onnx models support the `generate()` method of huggingface transformers for inferencing.

for more information on the project refer to the repository [here](https://github.com/Ki6an/fastT5#reduce-t5-model-size-by-3x-and-increase-the-inference-speed-up-to-5x).",t2_b4tks0mt,False,,0,False,A Python library to boost T5 models speed up to 5x &amp; reduce the model size by 3x.,[],r/LanguageTechnology,False,6,,0,,False,t3_mh6dtg,False,dark,0.99,,public,67,0,{},,False,[],,False,False,,{},,False,67,,False,False,,False,,[],{},,True,,1617226975.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I wanted to share this new library I&amp;#39;ve been working on and that I open-sourced!.&lt;/p&gt;

&lt;p&gt;here are some links to the library:&lt;/p&gt;

&lt;p&gt;💻 &lt;a href=""https://github.com/Ki6an/fastT5""&gt;GitHub Repository&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;🐍 &lt;a href=""https://pypi.org/project/fastt5/""&gt;PyPi project&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;as the title suggests, you can increase the inference speed of any pretrained T5 model and also decrease the models&amp;#39; size, in a single line of code.&lt;/p&gt;

&lt;p&gt;The library can be installed with &lt;code&gt;pip install fastt5&lt;/code&gt;. This code snippet from the repository&amp;#39;s README gives a concise overview:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from fastT5 import export_and_get_onnx_model
from transformers import AutoTokenizer

model_name = &amp;#39;t5-small&amp;#39;
model = export_and_get_onnx_model(model_name)

tokenizer = AutoTokenizer.from_pretrained(model_name)
t_input = &amp;quot;translate English to French: The universe is a dark forest.&amp;quot;
token = tokenizer(t_input, return_tensors=&amp;#39;pt&amp;#39;)

tokens = model.generate(input_ids=token[&amp;#39;input_ids&amp;#39;],
               attention_mask=token[&amp;#39;attention_mask&amp;#39;],
               num_beams=2)

output = tokenizer.decode(tokens.squeeze(), skip_special_tokens=True)
print(output)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The fastT5 library exports the T5 model to onnx with &lt;code&gt;past_key_values,&lt;/code&gt; then quantizes it and runs it on onnxruntime.&lt;/p&gt;

&lt;p&gt;The exported onnx models support the &lt;code&gt;generate()&lt;/code&gt; method of huggingface transformers for inferencing.&lt;/p&gt;

&lt;p&gt;for more information on the project refer to the repository &lt;a href=""https://github.com/Ki6an/fastT5#reduce-t5-model-size-by-3x-and-increase-the-inference-speed-up-to-5x""&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mh6dtg,True,,strngelet,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mh6dtg/a_python_library_to_boost_t5_models_speed_up_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mh6dtg/a_python_library_to_boost_t5_models_speed_up_to/,30199,1617198175.0,0,,False,,,,,,1517
425,,LanguageTechnology,,t2_apv7o2y8,False,,0,False,Sentence Transformers: Sentence-BERT - Sentence Embeddings using Siamese BERT-Networks | arXiv demo,[],r/LanguageTechnology,False,6,,0,,False,t3_mho2ia,False,dark,1.0,,public,3,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/4I3gS1cmqe4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Sentence Transformers:  Sentence-BERT - Sentence Embeddings using Siamese BERT-Networks | arXiv demo', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/4I3gS1cmqe4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech - NLP/AI/ML simplified', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/4I3gS1cmqe4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETechNLPAIMLsimplified'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/4I3gS1cmqe4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mho2ia', 'height': 200}",,False,3,,False,False,,False,,[],{},,False,,1617282225.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mho2ia,True,,rsree123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mho2ia/sentence_transformers_sentencebert_sentence/,all_ads,False,https://youtu.be/4I3gS1cmqe4,30199,1617253425.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Sentence Transformers:  Sentence-BERT - Sentence Embeddings using Siamese BERT-Networks | arXiv demo', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/4I3gS1cmqe4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech - NLP/AI/ML simplified', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/4I3gS1cmqe4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETechNLPAIMLsimplified'}}",False,https://youtu.be/4I3gS1cmqe4,,,,,0
426,,LanguageTechnology,"Hello,

I am a Computational Linguistics graduate student, and my peers and I are searching for corpora dealing with euphemisms/dysphemisms. We are interested in automatic detection of euphemisms (and other non-literal speech usage), but have yet to find any substantial collections. 

We are willing to create a corpus if none exists, but would like to have a comprehensive idea of previous work before embarking on creating our own. Any help would be appreciated!",t2_10vtid,False,,0,False,Euphemism Corpora?,[],r/LanguageTechnology,False,6,,0,,False,t3_mh9xlm,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1617237121.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am a Computational Linguistics graduate student, and my peers and I are searching for corpora dealing with euphemisms/dysphemisms. We are interested in automatic detection of euphemisms (and other non-literal speech usage), but have yet to find any substantial collections. &lt;/p&gt;

&lt;p&gt;We are willing to create a corpus if none exists, but would like to have a comprehensive idea of previous work before embarking on creating our own. Any help would be appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mh9xlm,True,,kookookachoo17,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mh9xlm/euphemism_corpora/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mh9xlm/euphemism_corpora/,30199,1617208321.0,1,,False,,,,,,465
427,,LanguageTechnology,"Most reinforcement learning algorithms work on a ‘reward’ function to teach the agents in an unknown environment. The reward is given if the action taken results in a good outcome. But it’s a difficult task to define rewards for situations lacking clear objectives. For example, whether a room is clean or if a door is sufficiently shut. In such scenarios, the user cannot describe the task in words or numbers; however, he can readily provide examples of how the world would look like if it were solved.

Thus, Google AI suggests an alternative, example-based control, which aims at teaching agents how to solve new tasks by providing examples of success. This is termed as **recursive classification of examples (RCE)**. It does not rely on formulated reward functions, distance functions, or features. It instead just uses the examples of success. RCE performs better than the prior approaches based on [imitation learning](https://arxiv.org/pdf/1811.06711.pdf) on simulated robotics tasks.

Summary: https://www.marktechpost.com/2021/03/31/google-ai-proposes-a-machine-learning-algorithm-for-teaching-agents-to-solve-new-tasks-by-providing-examples-of-success/

Summary: [https://www.marktechpost.com/2021/03/31/google-ai-proposes-a-machine-learning-algorithm-for-teaching-agents-to-solve-new-tasks-by-providing-examples-of-success/](https://www.marktechpost.com/2021/03/31/google-ai-proposes-a-machine-learning-algorithm-for-teaching-agents-to-solve-new-tasks-by-providing-examples-of-success/) 

Source: https://ai.googleblog.com/2021/03/recursive-classification-replacing.html

GitHub: https://github.com/google-research/google-research/tree/master/rce

Related videos: https://ben-eysenbach.github.io/rce/",t2_4wudjgid,False,,0,False,Google AI Proposes A Machine Learning Algorithm For Teaching Agents To Solve New Tasks By Providing Examples Of Success,[],r/LanguageTechnology,False,6,,0,,False,t3_mh8zao,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1617234439.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Most reinforcement learning algorithms work on a ‘reward’ function to teach the agents in an unknown environment. The reward is given if the action taken results in a good outcome. But it’s a difficult task to define rewards for situations lacking clear objectives. For example, whether a room is clean or if a door is sufficiently shut. In such scenarios, the user cannot describe the task in words or numbers; however, he can readily provide examples of how the world would look like if it were solved.&lt;/p&gt;

&lt;p&gt;Thus, Google AI suggests an alternative, example-based control, which aims at teaching agents how to solve new tasks by providing examples of success. This is termed as &lt;strong&gt;recursive classification of examples (RCE)&lt;/strong&gt;. It does not rely on formulated reward functions, distance functions, or features. It instead just uses the examples of success. RCE performs better than the prior approaches based on &lt;a href=""https://arxiv.org/pdf/1811.06711.pdf""&gt;imitation learning&lt;/a&gt; on simulated robotics tasks.&lt;/p&gt;

&lt;p&gt;Summary: &lt;a href=""https://www.marktechpost.com/2021/03/31/google-ai-proposes-a-machine-learning-algorithm-for-teaching-agents-to-solve-new-tasks-by-providing-examples-of-success/""&gt;https://www.marktechpost.com/2021/03/31/google-ai-proposes-a-machine-learning-algorithm-for-teaching-agents-to-solve-new-tasks-by-providing-examples-of-success/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Summary: &lt;a href=""https://www.marktechpost.com/2021/03/31/google-ai-proposes-a-machine-learning-algorithm-for-teaching-agents-to-solve-new-tasks-by-providing-examples-of-success/""&gt;https://www.marktechpost.com/2021/03/31/google-ai-proposes-a-machine-learning-algorithm-for-teaching-agents-to-solve-new-tasks-by-providing-examples-of-success/&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Source: &lt;a href=""https://ai.googleblog.com/2021/03/recursive-classification-replacing.html""&gt;https://ai.googleblog.com/2021/03/recursive-classification-replacing.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;GitHub: &lt;a href=""https://github.com/google-research/google-research/tree/master/rce""&gt;https://github.com/google-research/google-research/tree/master/rce&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Related videos: &lt;a href=""https://ben-eysenbach.github.io/rce/""&gt;https://ben-eysenbach.github.io/rce/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mh8zao,True,,techsucker,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mh8zao/google_ai_proposes_a_machine_learning_algorithm/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mh8zao/google_ai_proposes_a_machine_learning_algorithm/,30199,1617205639.0,0,,False,,,,,,1713
428,,LanguageTechnology,"This might be a little tricky, or even impossible without a previously tagged corpus but is there anyway to count the length between syntactic elements? 

For example, I may want to count the length between the matrix verb in a clause and up until the embedded verb as in 'I knew that last week I saw my friend' - so the count here would be 'knew that last week I' = 4. Is this possible at all using Python? Ideally I'd like to set more params but you get the idea.",t2_48eyl33r,False,,0,False,Counting the length between syntactic elements / clauses,[],r/LanguageTechnology,False,6,,0,,False,t3_mh29eb,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1617211287.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This might be a little tricky, or even impossible without a previously tagged corpus but is there anyway to count the length between syntactic elements? &lt;/p&gt;

&lt;p&gt;For example, I may want to count the length between the matrix verb in a clause and up until the embedded verb as in &amp;#39;I knew that last week I saw my friend&amp;#39; - so the count here would be &amp;#39;knew that last week I&amp;#39; = 4. Is this possible at all using Python? Ideally I&amp;#39;d like to set more params but you get the idea.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mh29eb,True,,crowpup783,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mh29eb/counting_the_length_between_syntactic_elements/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mh29eb/counting_the_length_between_syntactic_elements/,30199,1617182487.0,0,,False,,,,,,465
429,,LanguageTechnology,"It's a great pleasure to announce our Natural Language Date Time Parser using Stanford coreNLP in the backend is open-source now. Do Check and Let us Know the Feedback.  
Github: [https://github.com/zoho/hawking](https://github.com/zoho/hawking)

Blog : [https://www.zoho.com/blog/general/zias-nlp-based-hawking-date-time-parser-is-now-open-source.html](https://www.zoho.com/blog/general/zias-nlp-based-hawking-date-time-parser-is-now-open-source.html)  
Tweet from [Stanford University](https://twitter.com/stanfordnlp) :  [https://twitter.com/stanfordnlp/status/1376914683127492614?s=20](https://twitter.com/stanfordnlp/status/1376914683127492614?s=20)

\#nlp #stanfordnlp #datetimeparser",t2_65cn0xro,False,,0,False,Hawking Date Time Parser is Open-Source Now,[],r/LanguageTechnology,False,6,,0,,False,t3_mgmkuh,False,dark,1.0,,public,33,1,{},,False,[],,False,False,,{},,False,33,,False,False,,1617161020.0,,[],{},,True,,1617157691.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;It&amp;#39;s a great pleasure to announce our Natural Language Date Time Parser using Stanford coreNLP in the backend is open-source now. Do Check and Let us Know the Feedback.&lt;br/&gt;
Github: &lt;a href=""https://github.com/zoho/hawking""&gt;https://github.com/zoho/hawking&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Blog : &lt;a href=""https://www.zoho.com/blog/general/zias-nlp-based-hawking-date-time-parser-is-now-open-source.html""&gt;https://www.zoho.com/blog/general/zias-nlp-based-hawking-date-time-parser-is-now-open-source.html&lt;/a&gt;&lt;br/&gt;
Tweet from &lt;a href=""https://twitter.com/stanfordnlp""&gt;Stanford University&lt;/a&gt; :  &lt;a href=""https://twitter.com/stanfordnlp/status/1376914683127492614?s=20""&gt;https://twitter.com/stanfordnlp/status/1376914683127492614?s=20&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#nlp #stanfordnlp #datetimeparser&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mgmkuh,True,,ArulVendhan,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mgmkuh/hawking_date_time_parser_is_opensource_now/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mgmkuh/hawking_date_time_parser_is_opensource_now/,30199,1617128891.0,0,,False,,,,,,690
430,,LanguageTechnology," I have a dataset of names, full names which consist of three names, and they are labeled with their gender.

I am using Sklearn for this task. This is not a coding question I  guess, it theoretical. How I would represent or preprocess those names to predict their gender after the training of the model. I am aware of the many ways for this task such as vectorization and embedding but I am not sure if they are the best for data consists of names and gender.

I can't show any progress in my work since I am still in the data collection stage.

What I am trying to do is similar to what is going on in this paper: [https://arxiv.org/abs/2010.10852](https://arxiv.org/abs/2010.10852)

Appreciate the help, thank you",t2_3iof5qh8,False,,0,False,How would you preprocess human names?,[],r/LanguageTechnology,False,6,,0,,False,t3_mgqyda,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1617169905.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a dataset of names, full names which consist of three names, and they are labeled with their gender.&lt;/p&gt;

&lt;p&gt;I am using Sklearn for this task. This is not a coding question I  guess, it theoretical. How I would represent or preprocess those names to predict their gender after the training of the model. I am aware of the many ways for this task such as vectorization and embedding but I am not sure if they are the best for data consists of names and gender.&lt;/p&gt;

&lt;p&gt;I can&amp;#39;t show any progress in my work since I am still in the data collection stage.&lt;/p&gt;

&lt;p&gt;What I am trying to do is similar to what is going on in this paper: &lt;a href=""https://arxiv.org/abs/2010.10852""&gt;https://arxiv.org/abs/2010.10852&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Appreciate the help, thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mgqyda,True,,MooDev,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mgqyda/how_would_you_preprocess_human_names/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mgqyda/how_would_you_preprocess_human_names/,30199,1617141105.0,0,,False,,,,,,716
431,,LanguageTechnology,"I am sharing our [new interactive graph](https://crossminds.ai/graphlist/approximating-how-single-head-attention-learns-605e84761fb2cdbb7230ffcf/) project that maps out knowledge-paper connections for the paper ""Approximating How Single Head Attention Learns"" published earlier this month by Berkeley NLP. You can click the graph nodes to explore the most relevant papers with videos and key knowledge areas. This page also includes a list of related papers without videos. [You can try it here.](https://crossminds.ai/graphlist/approximating-how-single-head-attention-learns-605e84761fb2cdbb7230ffcf/)

I'd be curious to learn your thoughts. Do you find it helpful? How can we make it more useful for understanding new research papers/areas? Thank you!",t2_779ciqdy,False,,0,False,"Interactive Research Graph for New Paper ""Approximating How Single Head Attention Learns""",[],r/LanguageTechnology,False,6,,0,,False,t3_mgn2k0,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1617159032.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am sharing our &lt;a href=""https://crossminds.ai/graphlist/approximating-how-single-head-attention-learns-605e84761fb2cdbb7230ffcf/""&gt;new interactive graph&lt;/a&gt; project that maps out knowledge-paper connections for the paper &amp;quot;Approximating How Single Head Attention Learns&amp;quot; published earlier this month by Berkeley NLP. You can click the graph nodes to explore the most relevant papers with videos and key knowledge areas. This page also includes a list of related papers without videos. &lt;a href=""https://crossminds.ai/graphlist/approximating-how-single-head-attention-learns-605e84761fb2cdbb7230ffcf/""&gt;You can try it here.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;d be curious to learn your thoughts. Do you find it helpful? How can we make it more useful for understanding new research papers/areas? Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mgn2k0,True,,ccrbltscm,,0,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/mgn2k0/interactive_research_graph_for_new_paper/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mgn2k0/interactive_research_graph_for_new_paper/,30199,1617130232.0,0,,False,,,,,,753
432,,LanguageTechnology,,t2_apv7o2y8,False,,0,False,Med7: A Clinical Named Entity Recognition Model | Paper Explained | #NLproc | #spaCy,[],r/LanguageTechnology,False,6,,0,,False,t3_mgaxq1,False,dark,0.84,,public,17,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/PTu89JoxBTw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Med7:  A Clinical Named Entity Recognition Model | Paper Explained | #NLproc | #spaCy', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/PTu89JoxBTw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech - NLP/AI/ML simplified', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/PTu89JoxBTw/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETechNLPAIMLsimplified'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/PTu89JoxBTw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mgaxq1', 'height': 200}",,False,17,,False,False,,False,,[],{},,False,,1617120366.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mgaxq1,True,,rsree123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mgaxq1/med7_a_clinical_named_entity_recognition_model/,all_ads,False,https://youtu.be/PTu89JoxBTw,30199,1617091566.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Med7:  A Clinical Named Entity Recognition Model | Paper Explained | #NLproc | #spaCy', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/PTu89JoxBTw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech - NLP/AI/ML simplified', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/PTu89JoxBTw/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETechNLPAIMLsimplified'}}",False,https://youtu.be/PTu89JoxBTw,,,,,0
433,,LanguageTechnology,"At the moment, I am having to manually correct for tokens in order to force limit them to 512 tokens -- the limit for distilBERT

Can some please guide me to a solution which works with Feature Extraction Pipeline?

`tokenizer = AutoTokenizer.from_pretrained(""distilbert-base-uncased"")`

`model = AutoModel.from_pretrained(""distilbert-base-uncased"")`

`model_use = pipeline('feature-extraction', model=model, tokenizer=tokenizer) `

`embedding = model_use( text )`



Thanks",t2_6wt7b8ir,False,,0,False,[Q] How to truncate text to max. permissible tokens within Huggingface Pipeline?,[],r/LanguageTechnology,False,6,,0,,False,t3_mgcbfj,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1617126600.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;At the moment, I am having to manually correct for tokens in order to force limit them to 512 tokens -- the limit for distilBERT&lt;/p&gt;

&lt;p&gt;Can some please guide me to a solution which works with Feature Extraction Pipeline?&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tokenizer = AutoTokenizer.from_pretrained(&amp;quot;distilbert-base-uncased&amp;quot;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;model = AutoModel.from_pretrained(&amp;quot;distilbert-base-uncased&amp;quot;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;model_use = pipeline(&amp;#39;feature-extraction&amp;#39;, model=model, tokenizer=tokenizer)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;embedding = model_use( text )&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mgcbfj,True,,socialmedia_research,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mgcbfj/q_how_to_truncate_text_to_max_permissible_tokens/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mgcbfj/q_how_to_truncate_text_to_max_permissible_tokens/,30199,1617097800.0,0,,False,,,,,,474
434,,LanguageTechnology,"Hei mates, I’m currently studying a BA in Linguistics and planning doing an international exchange program offered by my local faculty. I’m quite interested in the biolinguistics framework (yes, chomskyan), evolution of language experimental linguistics and language technologies. Since these fields are not deeply investigated at my university, I am looking for institutions whose programs include the topics mentioned above. Hope I can get a review/advise from any of you!! These are my options:

1. University College London - UK (not my best bc london is $$$$ but if u got any info i will rlly appreciate it)
2. Newcastle University - UK
3. Lund University - Sweden
4. Copenhagen University - Denmark
5. University of Bergen - Norway
6. Tübingen Universität - Germany 
7. University of Helsinki - Finland

Txs &lt;3",t2_8yok0ng0,False,,0,False,BA Ling - Exchange Program,[],r/LanguageTechnology,False,6,,0,,False,t3_mg7q37,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1617106759.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hei mates, I’m currently studying a BA in Linguistics and planning doing an international exchange program offered by my local faculty. I’m quite interested in the biolinguistics framework (yes, chomskyan), evolution of language experimental linguistics and language technologies. Since these fields are not deeply investigated at my university, I am looking for institutions whose programs include the topics mentioned above. Hope I can get a review/advise from any of you!! These are my options:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;University College London - UK (not my best bc london is $$$$ but if u got any info i will rlly appreciate it)&lt;/li&gt;
&lt;li&gt;Newcastle University - UK&lt;/li&gt;
&lt;li&gt;Lund University - Sweden&lt;/li&gt;
&lt;li&gt;Copenhagen University - Denmark&lt;/li&gt;
&lt;li&gt;University of Bergen - Norway&lt;/li&gt;
&lt;li&gt;Tübingen Universität - Germany &lt;/li&gt;
&lt;li&gt;University of Helsinki - Finland&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Txs &amp;lt;3&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mg7q37,True,,xechrafna,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mg7q37/ba_ling_exchange_program/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mg7q37/ba_ling_exchange_program/,30199,1617077959.0,0,,False,,,,,,819
435,,LanguageTechnology,"Hi guys. I’m playing about with XLM Roberta Large XNLI and Python and I’m conscious it has a maximum file size of 512 tokens. 

Can anybody advice me how they are handling inputs that are larger than this? Any suggestions on code edit to only query 512 tokens to the model and ignore any more? Thanks all",t2_2z99aubf,False,,0,False,XLM Roberta - Maximum File Size,[],r/LanguageTechnology,False,6,,0,,False,t3_mfwnl5,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1617072935.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys. I’m playing about with XLM Roberta Large XNLI and Python and I’m conscious it has a maximum file size of 512 tokens. &lt;/p&gt;

&lt;p&gt;Can anybody advice me how they are handling inputs that are larger than this? Any suggestions on code edit to only query 512 tokens to the model and ignore any more? Thanks all&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mfwnl5,True,,w3aryb0arpig,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mfwnl5/xlm_roberta_maximum_file_size/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mfwnl5/xlm_roberta_maximum_file_size/,30199,1617044135.0,0,,False,,,,,,304
436,,LanguageTechnology,"I am sorry if my post doesn't sound like an innovation to you, but would like you to take a look as it evolved out of a research project! I thought people in this subreddit might be interested :) Oh and yes! Anyone can use it!

The model has been trained on bug fixes in open source Github projects, and the tool itself is largely written in Python and hoping to help python coders!

The repository I visualized as an example is: [https://metabob.com/gh/galt2x/fastapi](https://metabob.com/gh/galt2x/fastapi?utm_source=Reddit&amp;utm_medium=Reddit%20Post&amp;utm_campaign=Reddit%20Post%20(r%2FLanguageTechnology)%203-29)

The program works best on Google Chrome, If you would like to check out the website, I linked it [here](https://www.metabob.com/?utm_source=Reddit&amp;utm_medium=Reddit%20Post&amp;utm_campaign=Reddit%20Post%20(r%2FLanguageTechnology)%203-29).",t2_83ex9eul,False,,0,False,Update on my project to debug and visualize Python code by using a combination of conventional static analysis tools and the attention based AI model. - Please ask me any questions!,[],r/LanguageTechnology,False,6,,0,,False,t3_mfu7jr,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1617066399.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am sorry if my post doesn&amp;#39;t sound like an innovation to you, but would like you to take a look as it evolved out of a research project! I thought people in this subreddit might be interested :) Oh and yes! Anyone can use it!&lt;/p&gt;

&lt;p&gt;The model has been trained on bug fixes in open source Github projects, and the tool itself is largely written in Python and hoping to help python coders!&lt;/p&gt;

&lt;p&gt;The repository I visualized as an example is: &lt;a href=""https://metabob.com/gh/galt2x/fastapi?utm_source=Reddit&amp;amp;utm_medium=Reddit%20Post&amp;amp;utm_campaign=Reddit%20Post%20(r%2FLanguageTechnology""&gt;https://metabob.com/gh/galt2x/fastapi&lt;/a&gt;%203-29)&lt;/p&gt;

&lt;p&gt;The program works best on Google Chrome, If you would like to check out the website, I linked it &lt;a href=""https://www.metabob.com/?utm_source=Reddit&amp;amp;utm_medium=Reddit%20Post&amp;amp;utm_campaign=Reddit%20Post%20(r%2FLanguageTechnology""&gt;here&lt;/a&gt;%203-29).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mfu7jr,True,,bobcodes247365,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mfu7jr/update_on_my_project_to_debug_and_visualize/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mfu7jr/update_on_my_project_to_debug_and_visualize/,30199,1617037599.0,0,,False,,,,,,864
437,,LanguageTechnology,,t2_entq9,False,,0,False,deep-significance: Easy and Better Significance Testing for Deep Neural Networks,[],r/LanguageTechnology,False,6,,0,,False,t3_mfq8xn,False,dark,1.0,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,False,,1617055426.0,text,6,,,text,github.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mfq8xn,True,,Kaleidophon,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mfq8xn/deepsignificance_easy_and_better_significance/,all_ads,False,https://github.com/Kaleidophon/deep-significance,30199,1617026626.0,0,,False,https://github.com/Kaleidophon/deep-significance,,,,,0
438,,LanguageTechnology,,t2_hkv9s,False,,0,False,Unit Test Case Generation with Transformers (Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_mfi5c4,False,dark,0.96,,public,22,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3tMqXWnHlfs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Unit Test Case Generation with Transformers (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3tMqXWnHlfs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3tMqXWnHlfs/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3tMqXWnHlfs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mfi5c4', 'height': 200}",,False,22,,False,False,,False,,[],{},,False,,1617022949.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mfi5c4,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mfi5c4/unit_test_case_generation_with_transformers/,all_ads,False,https://youtu.be/3tMqXWnHlfs,30199,1616994149.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Unit Test Case Generation with Transformers (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3tMqXWnHlfs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3tMqXWnHlfs/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/3tMqXWnHlfs,,,,,0
439,,LanguageTechnology,"Hey all! Hope everyone is having a great Monday, I know I am. I want to let everyone know about a NVIDIA AI platform piece I use daily, NeMo.

[NVIDIA NeMo](https://github.com/NVIDIA/NeMo) is a toolkit for building new State-of-the-Art Conversational AI models. NeMo has separate collections for Automatic Speech Recognition (ASR), Natural Language Processing (NLP), and Text-to-Speech (TTS) models. Each collection consists of prebuilt modules that include everything needed to train on your data. Every module can easily be customized, extended, and composed to create new Conversational AI model architectures.

Conversational AI architectures are typically large and require a lot of data and compute for training. NeMo uses PyTorch Lightning for easy and performant multi-GPU/multi-node mixed-precision training.

[Cool collab notebook for getting started with NeMo](https://colab.research.google.com/github/NVIDIA/NeMo/blob/r1.0.0rc1/tutorials/NeMo_Getting_Started.ipynb) shows how we can construct a toy demo showing the ability to translate Russian audio files into English. That's what is so neat about NeMo is the pipeline is so diverse; we can translate these audio files into English, then if we wanted to, use BERT for the NLP task for the translated English as well. And so on and so on.

Anyways, I hope you enjoy this collab book, and to have a poke around NeMo afterwards.",,False,,0,False,NeMo Getting Started. Prototyping Conversational AI Application,[],r/LanguageTechnology,False,6,,0,,False,t3_mfsdnx,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,,,False,,,{},,True,,1617061464.0,text,6,,,,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all! Hope everyone is having a great Monday, I know I am. I want to let everyone know about a NVIDIA AI platform piece I use daily, NeMo.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/NVIDIA/NeMo""&gt;NVIDIA NeMo&lt;/a&gt; is a toolkit for building new State-of-the-Art Conversational AI models. NeMo has separate collections for Automatic Speech Recognition (ASR), Natural Language Processing (NLP), and Text-to-Speech (TTS) models. Each collection consists of prebuilt modules that include everything needed to train on your data. Every module can easily be customized, extended, and composed to create new Conversational AI model architectures.&lt;/p&gt;

&lt;p&gt;Conversational AI architectures are typically large and require a lot of data and compute for training. NeMo uses PyTorch Lightning for easy and performant multi-GPU/multi-node mixed-precision training.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://colab.research.google.com/github/NVIDIA/NeMo/blob/r1.0.0rc1/tutorials/NeMo_Getting_Started.ipynb""&gt;Cool collab notebook for getting started with NeMo&lt;/a&gt; shows how we can construct a toy demo showing the ability to translate Russian audio files into English. That&amp;#39;s what is so neat about NeMo is the pipeline is so diverse; we can translate these audio files into English, then if we wanted to, use BERT for the NLP task for the translated English as well. And so on and so on.&lt;/p&gt;

&lt;p&gt;Anyways, I hope you enjoy this collab book, and to have a poke around NeMo afterwards.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mfsdnx,True,,[deleted],,0,True,all_ads,False,[],,dark,/r/LanguageTechnology/comments/mfsdnx/nemo_getting_started_prototyping_conversational/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mfsdnx/nemo_getting_started_prototyping_conversational/,30199,1617032664.0,0,,False,,,,,,1389
440,,LanguageTechnology,"Hello,

After adding embedding layer to my ANN I'm facing this issue. 

InvalidArgumentError:  indices[6,1] = -11 is not in [0, 6505)
	 [[node model_10/embedding_15/embedding_lookup (defined at &lt;ipython-input-125-fcde7e47b9e9&gt;:3) ]] [Op:__inference_train_function_11619]

Errors may have originated from an input operation.
Input Source operations connected to node model_10/embedding_15/embedding_lookup:
 model_10/embedding_15/embedding_lookup/10457 (defined at C:\Users\erdiv\.conda\envs\python-tf2.0\lib\contextlib.py:81)

Function call stack:
train_function",t2_3sm3bnat,False,,0,False,Error after adding Embedding Layer to my ANN,[],r/LanguageTechnology,False,6,,0,,False,t3_mfpn8i,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1617053510.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;After adding embedding layer to my ANN I&amp;#39;m facing this issue. &lt;/p&gt;

&lt;p&gt;InvalidArgumentError:  indices[6,1] = -11 is not in [0, 6505)
     [[node model&lt;em&gt;10/embedding_15/embedding_lookup (defined at &amp;lt;ipython-input-125-fcde7e47b9e9&amp;gt;:3) ]] [Op:&lt;/em&gt;_inference_train_function_11619]&lt;/p&gt;

&lt;p&gt;Errors may have originated from an input operation.
Input Source operations connected to node model_10/embedding_15/embedding_lookup:
 model_10/embedding_15/embedding_lookup/10457 (defined at C:\Users\erdiv.conda\envs\python-tf2.0\lib\contextlib.py:81)&lt;/p&gt;

&lt;p&gt;Function call stack:
train_function&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mfpn8i,True,,Lannister07,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mfpn8i/error_after_adding_embedding_layer_to_my_ann/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mfpn8i/error_after_adding_embedding_layer_to_my_ann/,30199,1617024710.0,0,,False,,,,,,568
441,,LanguageTechnology,"I have an Embedding Matrix resultant from the Word2Vec model. How can I use that in my XGBClassifier? How can I add an Embedding Layer to my XGBClassifier? 
(I'm new to NLP, Plese correct me if I'm wrong)",t2_3sm3bnat,False,,0,False,Embedding Layer to XGBClassifier,[],r/LanguageTechnology,False,6,,0,,False,t3_mfp7qk,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1617052127.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have an Embedding Matrix resultant from the Word2Vec model. How can I use that in my XGBClassifier? How can I add an Embedding Layer to my XGBClassifier? 
(I&amp;#39;m new to NLP, Plese correct me if I&amp;#39;m wrong)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mfp7qk,True,,Lannister07,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mfp7qk/embedding_layer_to_xgbclassifier/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mfp7qk/embedding_layer_to_xgbclassifier/,30199,1617023327.0,0,,False,,,,,,204
442,,LanguageTechnology,"I have a data which is already labeled different type of emotions based on text context:

Some of the label types: 

    funny, anger, boredom, empty, fun relief, sadness, happines

when I use TFIDF alongside Logistic Regression to predict it gives me a shitty result.

Here is the code:

    ### HERE GOES PREPROCESSING FIRST
    and then
    
    #Encoding output labels 'sadness' as '1' &amp; 'happiness' as '0'
    lbl_enc = preprocessing.LabelEncoder()
    y = lbl_enc.fit_transform(data.sentiment.values)
    
    # Splitting into training and testing data in 90:10 ratio
    X_train, X_val, y_train, y_val = train_test_split(data.content.values, y, stratify=y, random_state=42, test_size=0.1, shuffle=True)
    
    # Extracting TF-IDF parameters
    tfidf = TfidfVectorizer(max_features=1000, analyzer='word',ngram_range=(1,3))
    X_train_tfidf = tfidf.fit_transform(X_train)
    X_val_tfidf = tfidf.fit_transform(X_val)
    
    # Model 3: logistic regression
    logreg = LogisticRegression(C=1,solver='lbfgs', max_iter=3000)
    logreg.fit(X_train_tfidf, y_train)
    y_pred = logreg.predict(X_val_tfidf)
    print('log reg tfidf accuracy %s' % accuracy_score(y_pred, y_val))

this is giving me a very bad result something like 0.2342355%

however, if I remove all types of emotions but leave only two. For example Happiness and Sadness,

it gives me a better result of 0.782342342%

&amp;#x200B;

Why is this?

How can i make it so that the model can predict not only 2 types of emotions but also other types f emotions such as *""Excitement, Fun, Relief""* and so on?

'",t2_6c0lef9b,False,,0,False,Emotion detection question,[],r/LanguageTechnology,False,6,,0,,False,t3_mfc5zo,False,dark,1.0,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1617001082.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a data which is already labeled different type of emotions based on text context:&lt;/p&gt;

&lt;p&gt;Some of the label types: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;funny, anger, boredom, empty, fun relief, sadness, happines
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;when I use TFIDF alongside Logistic Regression to predict it gives me a shitty result.&lt;/p&gt;

&lt;p&gt;Here is the code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;### HERE GOES PREPROCESSING FIRST
and then

#Encoding output labels &amp;#39;sadness&amp;#39; as &amp;#39;1&amp;#39; &amp;amp; &amp;#39;happiness&amp;#39; as &amp;#39;0&amp;#39;
lbl_enc = preprocessing.LabelEncoder()
y = lbl_enc.fit_transform(data.sentiment.values)

# Splitting into training and testing data in 90:10 ratio
X_train, X_val, y_train, y_val = train_test_split(data.content.values, y, stratify=y, random_state=42, test_size=0.1, shuffle=True)

# Extracting TF-IDF parameters
tfidf = TfidfVectorizer(max_features=1000, analyzer=&amp;#39;word&amp;#39;,ngram_range=(1,3))
X_train_tfidf = tfidf.fit_transform(X_train)
X_val_tfidf = tfidf.fit_transform(X_val)

# Model 3: logistic regression
logreg = LogisticRegression(C=1,solver=&amp;#39;lbfgs&amp;#39;, max_iter=3000)
logreg.fit(X_train_tfidf, y_train)
y_pred = logreg.predict(X_val_tfidf)
print(&amp;#39;log reg tfidf accuracy %s&amp;#39; % accuracy_score(y_pred, y_val))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;this is giving me a very bad result something like 0.2342355%&lt;/p&gt;

&lt;p&gt;however, if I remove all types of emotions but leave only two. For example Happiness and Sadness,&lt;/p&gt;

&lt;p&gt;it gives me a better result of 0.782342342%&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Why is this?&lt;/p&gt;

&lt;p&gt;How can i make it so that the model can predict not only 2 types of emotions but also other types f emotions such as &lt;em&gt;&amp;quot;Excitement, Fun, Relief&amp;quot;&lt;/em&gt; and so on?&lt;/p&gt;

&lt;p&gt;&amp;#39;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mfc5zo,True,,strangeguy111,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mfc5zo/emotion_detection_question/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mfc5zo/emotion_detection_question/,30199,1616972282.0,0,,False,,,,,,1582
443,,LanguageTechnology,"I have a specific task which i would like to fine tune a pre-trained GPT2 model. I know those models require a lot of data to train, but what about fine tuning? And how can i measure if my model is actually working for this specific task? I’ve tried to fine tune with about 1600 samples, but after the third epoch the model starts to overfit",t2_59fb4qmk,False,,0,False,Tunning GPT2,[],r/LanguageTechnology,False,6,,0,,False,t3_mfbz76,False,dark,0.88,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1617000459.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a specific task which i would like to fine tune a pre-trained GPT2 model. I know those models require a lot of data to train, but what about fine tuning? And how can i measure if my model is actually working for this specific task? I’ve tried to fine tune with about 1600 samples, but after the third epoch the model starts to overfit&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mfbz76,True,,Felipehonorato,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mfbz76/tunning_gpt2/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mfbz76/tunning_gpt2/,30199,1616971659.0,0,,False,,,,,,341
444,,LanguageTechnology,"Obviously AI based search engine is the next step . Because of too much data available today on internet . We need this tool . I think it will be more interactive version of google . Where you can search based on size or similarity to other website , article or post . Or specify some part of text and find similar parts from other data sources .  To rate the quality of the search after search query . And there could be a lot more features . 

Idea is quite simple , it's all about execution . That's why I am searching for cofounders . Feel free to DM me  .  Potential investors are welcome too . Thank you for reading this ! Stay hungry , stay foolish )",t2_4hg6h8aj,False,,0,False,AI based search engine { Startup idea },[],r/LanguageTechnology,False,6,,0,,False,t3_mfovwn,False,dark,0.22,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1617051056.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Obviously AI based search engine is the next step . Because of too much data available today on internet . We need this tool . I think it will be more interactive version of google . Where you can search based on size or similarity to other website , article or post . Or specify some part of text and find similar parts from other data sources .  To rate the quality of the search after search query . And there could be a lot more features . &lt;/p&gt;

&lt;p&gt;Idea is quite simple , it&amp;#39;s all about execution . That&amp;#39;s why I am searching for cofounders . Feel free to DM me  .  Potential investors are welcome too . Thank you for reading this ! Stay hungry , stay foolish )&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mfovwn,True,,habibTheCoel,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mfovwn/ai_based_search_engine_startup_idea/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mfovwn/ai_based_search_engine_startup_idea/,30199,1617022256.0,0,,False,,,,,,657
445,,LanguageTechnology,"Open-domain long-on answering (LFQA) form questions a fundamental challenge in natural language processing (NLP) that involves retrieving documents relevant to a given query and using them to generate a detailed paragraph-length answer. 

Recently, there has been significant progress in factoid open-domain question answering (QA). In this technique, a short phrase or entity is enough to answer a question, but significantly less work has been done in long-form question answering (LFQA). LFQA is an important task, primarily because it provides a testbed to measure generative text models’ factuality. But, the current benchmarks and evaluation metrics aren’t quite suitable for making progress on LFQA.

In a recent paper, [“Hurdles to Progress in Long-form Question Answering”,](https://arxiv.org/abs/2103.06332) that is set to appear at NAACL 2021, Google.ai present a new system for open-domain long-form question answering that utilizes two recent advances in NLP: One is the state-of-the-art sparse attention models, such as Routing Transformer (RT), which allows attention-based models to scale to long sequences, and other is the retrieval-based models, like REALM, that can facilitate retrievals of Wikipedia articles related to a given query.

Short Summary: [https://www.marktechpost.com/2021/03/27/google-ai-introduces-a-new-system-for-open-domain-long-form-question-answering-lfqa/](https://www.marktechpost.com/2021/03/27/google-ai-introduces-a-new-system-for-open-domain-long-form-question-answering-lfqa/)

Google blog: [https://ai.googleblog.com/2021/03/progress-and-challenges-in-long-form.html](https://ai.googleblog.com/2021/03/progress-and-challenges-in-long-form.html)

Paper: [https://arxiv.org/abs/2103.06332](https://arxiv.org/abs/2103.06332)",t2_4wudjgid,False,,0,False,Google AI Introduces a New System for Open-Domain Long-Form Question Answering (LFQA),[],r/LanguageTechnology,False,6,,0,,False,t3_mehj2t,False,dark,0.94,,public,36,0,{},,False,[],,False,False,,{},,False,36,,False,False,,False,,[],{},,True,,1616892381.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Open-domain long-on answering (LFQA) form questions a fundamental challenge in natural language processing (NLP) that involves retrieving documents relevant to a given query and using them to generate a detailed paragraph-length answer. &lt;/p&gt;

&lt;p&gt;Recently, there has been significant progress in factoid open-domain question answering (QA). In this technique, a short phrase or entity is enough to answer a question, but significantly less work has been done in long-form question answering (LFQA). LFQA is an important task, primarily because it provides a testbed to measure generative text models’ factuality. But, the current benchmarks and evaluation metrics aren’t quite suitable for making progress on LFQA.&lt;/p&gt;

&lt;p&gt;In a recent paper, &lt;a href=""https://arxiv.org/abs/2103.06332""&gt;“Hurdles to Progress in Long-form Question Answering”,&lt;/a&gt; that is set to appear at NAACL 2021, Google.ai present a new system for open-domain long-form question answering that utilizes two recent advances in NLP: One is the state-of-the-art sparse attention models, such as Routing Transformer (RT), which allows attention-based models to scale to long sequences, and other is the retrieval-based models, like REALM, that can facilitate retrievals of Wikipedia articles related to a given query.&lt;/p&gt;

&lt;p&gt;Short Summary: &lt;a href=""https://www.marktechpost.com/2021/03/27/google-ai-introduces-a-new-system-for-open-domain-long-form-question-answering-lfqa/""&gt;https://www.marktechpost.com/2021/03/27/google-ai-introduces-a-new-system-for-open-domain-long-form-question-answering-lfqa/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Google blog: &lt;a href=""https://ai.googleblog.com/2021/03/progress-and-challenges-in-long-form.html""&gt;https://ai.googleblog.com/2021/03/progress-and-challenges-in-long-form.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/abs/2103.06332""&gt;https://arxiv.org/abs/2103.06332&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mehj2t,True,,techsucker,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mehj2t/google_ai_introduces_a_new_system_for_opendomain/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mehj2t/google_ai_introduces_a_new_system_for_opendomain/,30199,1616863581.0,0,,False,,,,,,1770
446,,LanguageTechnology,"Where can I find the best description of the attention mechanism for NLP?  Blog post or video is good, but both are great!",t2_gnq642k,False,,0,False,Best description of attention?,[],r/LanguageTechnology,False,6,,0,,False,t3_menliz,False,dark,0.8,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1616910771.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Where can I find the best description of the attention mechanism for NLP?  Blog post or video is good, but both are great!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,menliz,True,,ugeb318,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/menliz/best_description_of_attention/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/menliz/best_description_of_attention/,30199,1616881971.0,0,,False,,,,,,122
447,,LanguageTechnology,"Is there anyone who has any information about this program? I could not find much info online. I have a linguistics background with little computer knowledge, what is the state of the work prospects I might get after the completion of this program? What languages are prominently being worked upon here? Especially from the point of view of an international (non EU) student, is the university nice? Are the job prospects there for international applicants? 

Also, anything you could tell me about the place, accommodation and part time jobs that I might take to sustain myself while studying? I've heard Nordic countries are very expensive to live in. Any info would be helpful at this point.",t2_65bnnyzy,False,,0,False,"Linguistic Data Science at University of Eastern Finland, how is it?",[],r/LanguageTechnology,False,6,,0,,False,t3_meix62,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1616896575.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there anyone who has any information about this program? I could not find much info online. I have a linguistics background with little computer knowledge, what is the state of the work prospects I might get after the completion of this program? What languages are prominently being worked upon here? Especially from the point of view of an international (non EU) student, is the university nice? Are the job prospects there for international applicants? &lt;/p&gt;

&lt;p&gt;Also, anything you could tell me about the place, accommodation and part time jobs that I might take to sustain myself while studying? I&amp;#39;ve heard Nordic countries are very expensive to live in. Any info would be helpful at this point.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,meix62,True,,ipissalone,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/meix62/linguistic_data_science_at_university_of_eastern/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/meix62/linguistic_data_science_at_university_of_eastern/,30199,1616867775.0,1,,False,,,,,,694
448,,LanguageTechnology,"I have just a list of 43 texts. I have encoded them via [Bert Base](https://huggingface.co/bert-base-uncased) model from Huggingface, limited to 512 tokens.

```py
tokenized = df.text.apply(lambda text: tokenizer.tokenize(text)).to_list()
inputs = tokenizer(tokenized, is_split_into_words=True, truncation=True, max_length=512, padding='max_length', return_tensors='pt')
outputs = model(**inputs)  # this line fills my ram
```

I have 16 GB of RAM and another 16 GB of swap. When the third line is being run, the RAM usage goes beyond my machine's limitations. I have tried many different ways but didn't helped.

What could be done? My texts have long sentences, some of them may be consist of no sentences at all; could this be the problem?",t2_3kwtnien,False,,0,False,Running Transformer model nearly kills my machine,[],r/LanguageTechnology,False,6,,0,,False,t3_med9c2,False,dark,0.81,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1616878567.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have just a list of 43 texts. I have encoded them via &lt;a href=""https://huggingface.co/bert-base-uncased""&gt;Bert Base&lt;/a&gt; model from Huggingface, limited to 512 tokens.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;py
tokenized = df.text.apply(lambda text: tokenizer.tokenize(text)).to_list()
inputs = tokenizer(tokenized, is_split_into_words=True, truncation=True, max_length=512, padding=&amp;#39;max_length&amp;#39;, return_tensors=&amp;#39;pt&amp;#39;)
outputs = model(**inputs)  # this line fills my ram
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I have 16 GB of RAM and another 16 GB of swap. When the third line is being run, the RAM usage goes beyond my machine&amp;#39;s limitations. I have tried many different ways but didn&amp;#39;t helped.&lt;/p&gt;

&lt;p&gt;What could be done? My texts have long sentences, some of them may be consist of no sentences at all; could this be the problem?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,med9c2,True,,emremrah,,24,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/med9c2/running_transformer_model_nearly_kills_my_machine/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/med9c2/running_transformer_model_nearly_kills_my_machine/,30199,1616849767.0,0,,False,,,,,,742
449,,LanguageTechnology,"how to measure emotions while retrieving data from the text?

I know about the keyword-based methods, now i am more interested in the **Vector space model ( TF-IDF )** method. However, I can't find a working example of it, only research papers without any code.",t2_6c0lef9b,False,,0,False,Emotions extraction,[],r/LanguageTechnology,False,6,,0,,False,t3_me98xd,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1616859866.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;how to measure emotions while retrieving data from the text?&lt;/p&gt;

&lt;p&gt;I know about the keyword-based methods, now i am more interested in the &lt;strong&gt;Vector space model ( TF-IDF )&lt;/strong&gt; method. However, I can&amp;#39;t find a working example of it, only research papers without any code.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,me98xd,True,,strangeguy111,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/me98xd/emotions_extraction/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/me98xd/emotions_extraction/,30199,1616831066.0,0,,False,,,,,,261
450,,LanguageTechnology,"I was recently accepted to the program and I was wondering if anyone had any comments on it. Also, is it really free? Even for international students?
I appreciate any info you can provide. There’s not much info on their website. Thanks!",,False,,0,False,Thoughts on MSc Language Science &amp; Tech at Saarland University?,[],r/LanguageTechnology,False,6,,0,,False,t3_mdy7tf,False,dark,0.86,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,,,False,,,{},,True,,1616820870.0,text,6,,,,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was recently accepted to the program and I was wondering if anyone had any comments on it. Also, is it really free? Even for international students?
I appreciate any info you can provide. There’s not much info on their website. Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mdy7tf,True,,[deleted],,19,True,all_ads,False,[],,dark,/r/LanguageTechnology/comments/mdy7tf/thoughts_on_msc_language_science_tech_at_saarland/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mdy7tf/thoughts_on_msc_language_science_tech_at_saarland/,30199,1616792070.0,0,,False,,,,,,237
451,,LanguageTechnology,"I've got a question - I'm trying to evaluate some sets of ngrams using a training set and a data set from a corpus and write some statistics about it. I have to include two measures: accuracy (which is easy using the "".evaluate"" module in NLTK. But I also have to find out the ""words/error"" rate of it. So for example I'm:

- using the Brown corpus' subcorpus ""news""
- this supcorpus has 100,554 words in total 
- I've split the corpus in to a training set of 500 sentences, and the rest are the testing set (the suborpus has 4,623 phrases in all). 
- I have a partially filled chart with an example:
 -  The default ngram tagger has an accuracy of 30.41% (when comparing the training and testing sets) and a error rate of **1.4 words/error**

But I can't figure out how this 1.4 words/error was calculated. Anyone have any ideas? I think, and I may be wrong here, that it's calculated as a function of the rest of the corpus that is NOT accurate - that is, 100%-30.41% = **69.95%** of the corpus. That number is then related to the total number of words somehow I think?",t2_4un59,False,,0,False,Computing Words per Error of an N-Gram tagger using NLTK in Python?,[],r/LanguageTechnology,False,6,,0,,False,t3_me3l1w,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1616837325.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve got a question - I&amp;#39;m trying to evaluate some sets of ngrams using a training set and a data set from a corpus and write some statistics about it. I have to include two measures: accuracy (which is easy using the &amp;quot;.evaluate&amp;quot; module in NLTK. But I also have to find out the &amp;quot;words/error&amp;quot; rate of it. So for example I&amp;#39;m:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;using the Brown corpus&amp;#39; subcorpus &amp;quot;news&amp;quot;&lt;/li&gt;
&lt;li&gt;this supcorpus has 100,554 words in total &lt;/li&gt;
&lt;li&gt;I&amp;#39;ve split the corpus in to a training set of 500 sentences, and the rest are the testing set (the suborpus has 4,623 phrases in all). &lt;/li&gt;
&lt;li&gt;I have a partially filled chart with an example:

&lt;ul&gt;
&lt;li&gt; The default ngram tagger has an accuracy of 30.41% (when comparing the training and testing sets) and a error rate of &lt;strong&gt;1.4 words/error&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But I can&amp;#39;t figure out how this 1.4 words/error was calculated. Anyone have any ideas? I think, and I may be wrong here, that it&amp;#39;s calculated as a function of the rest of the corpus that is NOT accurate - that is, 100%-30.41% = &lt;strong&gt;69.95%&lt;/strong&gt; of the corpus. That number is then related to the total number of words somehow I think?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,me3l1w,True,,MattyXarope,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/me3l1w/computing_words_per_error_of_an_ngram_tagger/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/me3l1w/computing_words_per_error_of_an_ngram_tagger/,30199,1616808525.0,1,,False,,,,,,1071
452,,LanguageTechnology,"I wrote an overview of QA systems that covers early heuristic methods to modern neural net methods. I would appreciate any feedback you give! Thank you :)

[https://www.wittwise.com/blog/reading\_comprehension/](https://www.wittwise.com/blog/reading_comprehension/)",t2_8py9vlu0,False,,0,False,Feedback on post - Overview of reading comprehension systems,[],r/LanguageTechnology,False,6,,0,,False,t3_me1jpc,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1616830790.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I wrote an overview of QA systems that covers early heuristic methods to modern neural net methods. I would appreciate any feedback you give! Thank you :)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.wittwise.com/blog/reading_comprehension/""&gt;https://www.wittwise.com/blog/reading_comprehension/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,me1jpc,True,,nuvicc,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/me1jpc/feedback_on_post_overview_of_reading/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/me1jpc/feedback_on_post_overview_of_reading/,30199,1616801990.0,0,,False,,,,,,265
453,,LanguageTechnology,"Hello! Hope this question makes sense. I'm new to NLP and Python in general. I am using NLTK. I'm getting a Masters in Strategic Communications, and want to do some NLP for a couple of my projects. Specifically, I've been researching Political-based speech.

I want to evaluate pieces of text based on keywords in the text, and match those keywords to broader concepts. As a very rough example, ""Heartland"", ""Farmer"", ""Towns"" might all be classed under ""Small City"" while ""Army"", ""War"", ""Defense"", might all be classed under ""Strength"". I'm pretty open to what style of concept groups are being used (issue based ideas vs. economic based ideas, etc...)- I really just want to see if databases like this already exist so I don't have to start from square one. I'm gathering a lot of ideas based off of the Diction software program, if anyone is familiar. Ideally the concept groups would be housed in a dictionary, with the specific terms being listed as keys, but I'm not picky and if I have to re-format existing work, so be it. 

Also if you know of databases that exist that are not Political-based, I would be happy to see those as well. 

Any ideas or directions? I appreciate it!",t2_yqo4l,False,,0,False,Existing Datasets/ Corpus of Word Families,[],r/LanguageTechnology,False,6,,0,,False,t3_mdvwx3,False,dark,0.72,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1616814434.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! Hope this question makes sense. I&amp;#39;m new to NLP and Python in general. I am using NLTK. I&amp;#39;m getting a Masters in Strategic Communications, and want to do some NLP for a couple of my projects. Specifically, I&amp;#39;ve been researching Political-based speech.&lt;/p&gt;

&lt;p&gt;I want to evaluate pieces of text based on keywords in the text, and match those keywords to broader concepts. As a very rough example, &amp;quot;Heartland&amp;quot;, &amp;quot;Farmer&amp;quot;, &amp;quot;Towns&amp;quot; might all be classed under &amp;quot;Small City&amp;quot; while &amp;quot;Army&amp;quot;, &amp;quot;War&amp;quot;, &amp;quot;Defense&amp;quot;, might all be classed under &amp;quot;Strength&amp;quot;. I&amp;#39;m pretty open to what style of concept groups are being used (issue based ideas vs. economic based ideas, etc...)- I really just want to see if databases like this already exist so I don&amp;#39;t have to start from square one. I&amp;#39;m gathering a lot of ideas based off of the Diction software program, if anyone is familiar. Ideally the concept groups would be housed in a dictionary, with the specific terms being listed as keys, but I&amp;#39;m not picky and if I have to re-format existing work, so be it. &lt;/p&gt;

&lt;p&gt;Also if you know of databases that exist that are not Political-based, I would be happy to see those as well. &lt;/p&gt;

&lt;p&gt;Any ideas or directions? I appreciate it!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mdvwx3,True,,teamlie,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mdvwx3/existing_datasets_corpus_of_word_families/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mdvwx3/existing_datasets_corpus_of_word_families/,30199,1616785634.0,0,,False,,,,,True,1185
454,,LanguageTechnology,"Hello everyone, I've been looking for an article for some time now that I can't for the life of me find anymore...

It was about an NLP dataset (SQuAD, SNLI, or something like that). I mean, you had a text and three theses and the system was supposed to recognise which thesis was covered. One was surprised how well systems performed on this. Then they found out that a large part of the theses was recognised by the wording alone, without the system even knowing the background text. Does anyone know the corresponding article on this?",t2_973rli8j,False,,0,False,Looking for a Dataset study,[],r/LanguageTechnology,False,6,,0,,False,t3_mdkwx7,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1616780096.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone, I&amp;#39;ve been looking for an article for some time now that I can&amp;#39;t for the life of me find anymore...&lt;/p&gt;

&lt;p&gt;It was about an NLP dataset (SQuAD, SNLI, or something like that). I mean, you had a text and three theses and the system was supposed to recognise which thesis was covered. One was surprised how well systems performed on this. Then they found out that a large part of the theses was recognised by the wording alone, without the system even knowing the background text. Does anyone know the corresponding article on this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mdkwx7,True,,Ghosttraider,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mdkwx7/looking_for_a_dataset_study/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mdkwx7/looking_for_a_dataset_study/,30199,1616751296.0,0,,False,,,,,,537
455,,LanguageTechnology,"For language models, perplexity is defined as the inverse probability of the test set, normalized by the number of words. So far, so clear.  
What confuses me is the following section:  


""What we generally use for word sequence (...) is the entire sequence of words in some test set. Since this sequence will cross many sentence boundaries, we need to include the begin- and end-sentence markers &lt;s&gt; and &lt;/s&gt; in the probability computation. We also need to include the end-of-sentence marker &lt;/s&gt; (but not the beginning-of-sentence marker &lt;s&gt;) in the total count of word tokens *N*."" 

\- Speech and Language Processing. Daniel Jurafsky &amp; James H. Martin.   


Why do we not include the beginning-of-sentence marker?",t2_4u537ww6,False,,0,False,Computing perplexity,[],r/LanguageTechnology,False,6,,0,,False,t3_mdqyrq,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1616800847.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For language models, perplexity is defined as the inverse probability of the test set, normalized by the number of words. So far, so clear.&lt;br/&gt;
What confuses me is the following section:  &lt;/p&gt;

&lt;p&gt;&amp;quot;What we generally use for word sequence (...) is the entire sequence of words in some test set. Since this sequence will cross many sentence boundaries, we need to include the begin- and end-sentence markers &amp;lt;s&amp;gt; and &amp;lt;/s&amp;gt; in the probability computation. We also need to include the end-of-sentence marker &amp;lt;/s&amp;gt; (but not the beginning-of-sentence marker &amp;lt;s&amp;gt;) in the total count of word tokens &lt;em&gt;N&lt;/em&gt;.&amp;quot; &lt;/p&gt;

&lt;p&gt;- Speech and Language Processing. Daniel Jurafsky &amp;amp; James H. Martin.   &lt;/p&gt;

&lt;p&gt;Why do we not include the beginning-of-sentence marker?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mdqyrq,True,,Blutorangensaft,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mdqyrq/computing_perplexity/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mdqyrq/computing_perplexity/,30199,1616772047.0,0,,False,,,,,,746
456,,LanguageTechnology,"If you're doing an ML project and you think others would benefit from what you're doing, please consider sharing in Learn Data Science with Expert Guidance: [https://discord.gg/Gg3EJrn3](https://discord.gg/Gg3EJrn3) I created a place to share my project publicly for others and I came to realize that people entering data science really need expert guidance, so this server has a focus on bringing together experts and novices so that new entrants can build relationships with more experienced data scientists and feel confident in what they're studying and working on. Consider joining if you feel you have something to teach",t2_7qjxurpd,False,,0,False,Doing an ML project and feel others would benefit from what you're doing?,[],r/LanguageTechnology,False,6,,0,,False,t3_mdpytq,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1616797995.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If you&amp;#39;re doing an ML project and you think others would benefit from what you&amp;#39;re doing, please consider sharing in Learn Data Science with Expert Guidance: &lt;a href=""https://discord.gg/Gg3EJrn3""&gt;https://discord.gg/Gg3EJrn3&lt;/a&gt; I created a place to share my project publicly for others and I came to realize that people entering data science really need expert guidance, so this server has a focus on bringing together experts and novices so that new entrants can build relationships with more experienced data scientists and feel confident in what they&amp;#39;re studying and working on. Consider joining if you feel you have something to teach&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mdpytq,True,,Necessary-Stage2206,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mdpytq/doing_an_ml_project_and_feel_others_would_benefit/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mdpytq/doing_an_ml_project_and_feel_others_would_benefit/,30199,1616769195.0,0,,False,,,,,,626
457,,LanguageTechnology,"I have a microservice that receives plain-text messages from an external API like:

""... You have received 1254.56 from Thomas Paine. Transaction id FGR412512 at 11:02 ...""

Using Regex I can get:

`double amount = 1254.56;`

`String transactionId = FGR412512;`

`String time = 11.02;`

However, I would like to make my code more resilient to possible changes in text to, say, ""... at 11.02, Thomas Pain sent you 1254.56 ... "" in the future.

Is there a Natural Language Processing library or framework that I can use to extract these relationships and convert them to variables like I do with Regex?",t2_84iq5o3s,False,,0,False,Extract name:value relationships from plain text,[],r/LanguageTechnology,False,6,,0,,False,t3_mdlq1y,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,1616757105.0,,[],{},,True,,1616783635.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a microservice that receives plain-text messages from an external API like:&lt;/p&gt;

&lt;p&gt;&amp;quot;... You have received 1254.56 from Thomas Paine. Transaction id FGR412512 at 11:02 ...&amp;quot;&lt;/p&gt;

&lt;p&gt;Using Regex I can get:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;double amount = 1254.56;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;String transactionId = FGR412512;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;String time = 11.02;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;However, I would like to make my code more resilient to possible changes in text to, say, &amp;quot;... at 11.02, Thomas Pain sent you 1254.56 ... &amp;quot; in the future.&lt;/p&gt;

&lt;p&gt;Is there a Natural Language Processing library or framework that I can use to extract these relationships and convert them to variables like I do with Regex?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mdlq1y,True,,brownnakedape,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mdlq1y/extract_namevalue_relationships_from_plain_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mdlq1y/extract_namevalue_relationships_from_plain_text/,30199,1616754835.0,0,,False,,,,,,600
458,,LanguageTechnology,"Does anyone know where i can find data on the best performing multiclass text classifiers? [This](https://github.com/sebastianruder/NLP-progress/blob/master/english/text_classification.md) is the only info i could find and it seems it hasn't been updated since 2019.

I'm looking to use one of these for 3 class sentiment classification, negative, neutral, positive. Looking for data comparing the likes of:  

Mpnet

Electra

RoBERTa

BERT

ALBERT

Or any other better models i haven't heard of. 

&amp;#x200B;

On a side note i see a lot of benchmarks such as SQUAD have ensembles of 2 or more models. How is this done? Do they get predictions from both and then take the highest output vector score between the two of them as the prediction?",t2_aao51,False,,0,False,Current Sota for Multiclass Text Classification?,[],r/LanguageTechnology,False,6,,0,,False,t3_mdgac2,False,dark,0.72,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1616760223.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Does anyone know where i can find data on the best performing multiclass text classifiers? &lt;a href=""https://github.com/sebastianruder/NLP-progress/blob/master/english/text_classification.md""&gt;This&lt;/a&gt; is the only info i could find and it seems it hasn&amp;#39;t been updated since 2019.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m looking to use one of these for 3 class sentiment classification, negative, neutral, positive. Looking for data comparing the likes of:  &lt;/p&gt;

&lt;p&gt;Mpnet&lt;/p&gt;

&lt;p&gt;Electra&lt;/p&gt;

&lt;p&gt;RoBERTa&lt;/p&gt;

&lt;p&gt;BERT&lt;/p&gt;

&lt;p&gt;ALBERT&lt;/p&gt;

&lt;p&gt;Or any other better models i haven&amp;#39;t heard of. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;On a side note i see a lot of benchmarks such as SQUAD have ensembles of 2 or more models. How is this done? Do they get predictions from both and then take the highest output vector score between the two of them as the prediction?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mdgac2,True,,rpatel9,,10,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mdgac2/current_sota_for_multiclass_text_classification/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mdgac2/current_sota_for_multiclass_text_classification/,30199,1616731423.0,0,,False,,,,,,744
459,,LanguageTechnology,"Hi everyone,

Does anyone know what is the state of the art for Topic Modeling and what kind of models do they use for production at Facebook, Amazon, Google etc.? I found a couple of recent papers on NTMs, but not sure how well they work and how well they would scale.

There are many approaches that are quite popular (like LDA, Neural Topic Models, topic models + BERT etc), but I was quite interested in something that is quite scalable for large datasets and is easy to use for production. Also, majority of the models are unsupervised and used on ""exploratory"" basis, is there something that you would recommend when looking for a particular narrative e.g. ""PC Gaming"" or something that will give you ""pure"" topics?

Any comments appreciated, thanks!",t2_8zryq0nl,False,,0,False,SOTA for Topic Modeling,[],r/LanguageTechnology,False,6,,0,,False,t3_mcwv5n,False,dark,0.93,,public,24,0,{},,False,[],,False,False,,{},,False,24,,False,False,,False,,[],{},,True,,1616703382.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;Does anyone know what is the state of the art for Topic Modeling and what kind of models do they use for production at Facebook, Amazon, Google etc.? I found a couple of recent papers on NTMs, but not sure how well they work and how well they would scale.&lt;/p&gt;

&lt;p&gt;There are many approaches that are quite popular (like LDA, Neural Topic Models, topic models + BERT etc), but I was quite interested in something that is quite scalable for large datasets and is easy to use for production. Also, majority of the models are unsupervised and used on &amp;quot;exploratory&amp;quot; basis, is there something that you would recommend when looking for a particular narrative e.g. &amp;quot;PC Gaming&amp;quot; or something that will give you &amp;quot;pure&amp;quot; topics?&lt;/p&gt;

&lt;p&gt;Any comments appreciated, thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mcwv5n,True,,Hanami89,,11,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mcwv5n/sota_for_topic_modeling/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mcwv5n/sota_for_topic_modeling/,30199,1616674582.0,0,,False,,,,,,756
460,,LanguageTechnology,"I'm fairly new to OSINT and would like to try and detect similarities in two different short-paragraph texts that I think are coming from the same author.

For example, a person or bot sends an English-language text message with about 100 words. Then I get another text message with about 150 words, but the words used and the abbreviations are common (e.g. ""St."" with a period used for street).

Is there an NLP library/tool that you can recommend that I can use to analyze 2 text corpus to get a percentage confidence that the two texts were created by the same author (authorship identification?)? Sort of like a plagiarism detector or a library/tool that can check for semantical identicalities (not sure if the manner of use of period, commas, etc. are also detected).

I would love to use a library/environment that uses python but if not, please suggest any other (even if its a hosted website that allows you to paste in texts to compare).

Thanks!",t2_a48goybs,False,,0,False,Libraries/tools to determine if same authorship of short text,[],r/LanguageTechnology,False,6,,0,,False,t3_md9j4k,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1616738289.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m fairly new to OSINT and would like to try and detect similarities in two different short-paragraph texts that I think are coming from the same author.&lt;/p&gt;

&lt;p&gt;For example, a person or bot sends an English-language text message with about 100 words. Then I get another text message with about 150 words, but the words used and the abbreviations are common (e.g. &amp;quot;St.&amp;quot; with a period used for street).&lt;/p&gt;

&lt;p&gt;Is there an NLP library/tool that you can recommend that I can use to analyze 2 text corpus to get a percentage confidence that the two texts were created by the same author (authorship identification?)? Sort of like a plagiarism detector or a library/tool that can check for semantical identicalities (not sure if the manner of use of period, commas, etc. are also detected).&lt;/p&gt;

&lt;p&gt;I would love to use a library/environment that uses python but if not, please suggest any other (even if its a hosted website that allows you to paste in texts to compare).&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,md9j4k,True,,Bikerrrrrrr,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/md9j4k/librariestools_to_determine_if_same_authorship_of/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/md9j4k/librariestools_to_determine_if_same_authorship_of/,30199,1616709489.0,0,,False,,,,,,956
461,,LanguageTechnology,"Hi All,

I came across this exciting data set, follow the link: [https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home?authuser=0](https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home?authuser=0)

I want to try my hand at user interaction data for building a book recommendation system, but its too big for my machine, even Google Colab and Kaggle kernels. Anyone with ideas how to go about it?

&amp;#x200B;

cheers",t2_4inu94cw,False,,0,False,Goodreads user interaction dataset,[],r/LanguageTechnology,False,6,,0,,False,t3_md0xqr,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1616715105.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All,&lt;/p&gt;

&lt;p&gt;I came across this exciting data set, follow the link: &lt;a href=""https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home?authuser=0""&gt;https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home?authuser=0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I want to try my hand at user interaction data for building a book recommendation system, but its too big for my machine, even Google Colab and Kaggle kernels. Anyone with ideas how to go about it?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;cheers&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,md0xqr,True,,gubberex,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/md0xqr/goodreads_user_interaction_dataset/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/md0xqr/goodreads_user_interaction_dataset/,30199,1616686305.0,0,,False,,,,,,423
462,,LanguageTechnology,"(Novice here)
What's the best method for NER? What do big companies like Google use? Can those methods used by big tech be used in a small research project?

 I am thinking about taking the most common /or the most efficient NER system used for English NLP and applying that to my native language (with modifications, off course). To see how it well it performs on my language.

And then maybe further improve on it... :)",t2_33dp606u,False,,0,False,Named Entity Recognition,[],r/LanguageTechnology,False,6,,0,,False,t3_mcwmhg,False,dark,0.99,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,1616677529.0,,[],{},,True,,1616702571.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;(Novice here)
What&amp;#39;s the best method for NER? What do big companies like Google use? Can those methods used by big tech be used in a small research project?&lt;/p&gt;

&lt;p&gt;I am thinking about taking the most common /or the most efficient NER system used for English NLP and applying that to my native language (with modifications, off course). To see how it well it performs on my language.&lt;/p&gt;

&lt;p&gt;And then maybe further improve on it... :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mcwmhg,True,,AizazKhan97,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mcwmhg/named_entity_recognition/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mcwmhg/named_entity_recognition/,30199,1616673771.0,0,,False,,,,,,421
463,,LanguageTechnology,"Hello everyone,

My intention is to pursue a MA in computational linguistics/language technology in one of the two universities mentioned in the tittle.

Is there anybody who studies or have studied in one of the two programmes mentioned above? What are the pros and cons of each choice in terms of academic quality, international student's life etc?

Thank you in advance",t2_9q1deko2,False,,0,False,Uppsala University VS University of Tübingen,[],r/LanguageTechnology,False,6,,0,,False,t3_mcw84m,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,1616672740.0,,[],{},,True,,1616701174.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;

&lt;p&gt;My intention is to pursue a MA in computational linguistics/language technology in one of the two universities mentioned in the tittle.&lt;/p&gt;

&lt;p&gt;Is there anybody who studies or have studied in one of the two programmes mentioned above? What are the pros and cons of each choice in terms of academic quality, international student&amp;#39;s life etc?&lt;/p&gt;

&lt;p&gt;Thank you in advance&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mcw84m,True,,Suspicious_Grocery64,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mcw84m/uppsala_university_vs_university_of_tübingen/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mcw84m/uppsala_university_vs_university_of_tübingen/,30199,1616672374.0,0,,False,,,,,,372
464,,LanguageTechnology,"Hey,

FastAPI has been a nice addition to the Python ecosystem. In my opinion it makes API creation easier, and less error-prone. It also comes with great performances that make it perfectly suited for machine learning APIs.

The [NLPCloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=j80a8332-aaaf-11eb-bcbc-0242ac130002) API has been developed using FastAPI, so I thought it would be interesting to write a concrete article about how to set up an NLP API with FastAPI that is serving spaCy models for NER:

[https://juliensalinas.com/en/machine-learning-nlp-api-production-fastapi-nlpcloud/](https://juliensalinas.com/en/machine-learning-nlp-api-production-fastapi-nlpcloud/)

I'd love to have your feedback on this guys. Are you also FastAPI users? Did you notice caveats I'm not aware of? Or can you think of better tools for machine learning APIs?

Thanks!",t2_4z4m2qcs,False,,0,False,Production-Ready Machine Learning NLP API with FastAPI and spaCy,[],r/LanguageTechnology,False,6,,0,,False,t3_mc6qta,False,dark,0.96,,public,21,0,{},,False,[],,False,False,,{},,False,21,,False,False,,1619899083.0,,[],{},,True,,1616624073.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey,&lt;/p&gt;

&lt;p&gt;FastAPI has been a nice addition to the Python ecosystem. In my opinion it makes API creation easier, and less error-prone. It also comes with great performances that make it perfectly suited for machine learning APIs.&lt;/p&gt;

&lt;p&gt;The &lt;a href=""https://nlpcloud.io/?utm_source=reddit&amp;amp;utm_campaign=j80a8332-aaaf-11eb-bcbc-0242ac130002""&gt;NLPCloud.io&lt;/a&gt; API has been developed using FastAPI, so I thought it would be interesting to write a concrete article about how to set up an NLP API with FastAPI that is serving spaCy models for NER:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://juliensalinas.com/en/machine-learning-nlp-api-production-fastapi-nlpcloud/""&gt;https://juliensalinas.com/en/machine-learning-nlp-api-production-fastapi-nlpcloud/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;d love to have your feedback on this guys. Are you also FastAPI users? Did you notice caveats I&amp;#39;m not aware of? Or can you think of better tools for machine learning APIs?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mc6qta,True,,juliensalinas,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mc6qta/productionready_machine_learning_nlp_api_with/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mc6qta/productionready_machine_learning_nlp_api_with/,30199,1616595273.0,0,,False,,,,,,876
465,,LanguageTechnology,"I am going to build some models over wiki-dump dataset and then try to compare the results to WS353 (for word similarity). So, I need to **check** whether my understanding is correct or not. Firstly, I need to read the text from wiki file and tokenize it, so that I am able to build the co-occurrence matrix. I am going to build the co-occurrence matrix in 3 ways : based on count, based on PMI and based on SPPMI. Then, I am going to build the embedding matrix using SVD. So, after that, I can have the word embedding matrix and I can compare the results to WS353.

So, the way is correct ? Thanks",t2_58jdxlet,False,,0,False,"Compare my word embedding models (Count based, PMI, SPPMI)",[],r/LanguageTechnology,False,6,,0,,False,t3_mccqk1,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1616638932.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am going to build some models over wiki-dump dataset and then try to compare the results to WS353 (for word similarity). So, I need to &lt;strong&gt;check&lt;/strong&gt; whether my understanding is correct or not. Firstly, I need to read the text from wiki file and tokenize it, so that I am able to build the co-occurrence matrix. I am going to build the co-occurrence matrix in 3 ways : based on count, based on PMI and based on SPPMI. Then, I am going to build the embedding matrix using SVD. So, after that, I can have the word embedding matrix and I can compare the results to WS353.&lt;/p&gt;

&lt;p&gt;So, the way is correct ? Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mccqk1,True,,mamrollahi,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mccqk1/compare_my_word_embedding_models_count_based_pmi/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mccqk1/compare_my_word_embedding_models_count_based_pmi/,30199,1616610132.0,0,,False,,,,,,598
466,,LanguageTechnology,"I want to create a database of written works that parses each work individually to determine subjects, authoritativeness, accuracy, intention and overall reliability.  I know this is a massive challenge and I'm looking for collaborators.  I think having a linguist or three onboard early is mandatory.  I can parse text all day long, but understanding how to classify words, phrases, sentences, etc is beyond my pay grade.",t2_7ij6klun,False,,0,False,Computer Scientist wants to Parse Meaning from Language,[],r/LanguageTechnology,False,6,,0,,False,t3_mc41ia,False,dark,0.65,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,True,,False,,[],{},,True,,1616616491.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to create a database of written works that parses each work individually to determine subjects, authoritativeness, accuracy, intention and overall reliability.  I know this is a massive challenge and I&amp;#39;m looking for collaborators.  I think having a linguist or three onboard early is mandatory.  I can parse text all day long, but understanding how to classify words, phrases, sentences, etc is beyond my pay grade.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mc41ia,True,,TheSharpestNinja,,26,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mc41ia/computer_scientist_wants_to_parse_meaning_from/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mc41ia/computer_scientist_wants_to_parse_meaning_from/,30199,1616587691.0,0,,False,,,,,,422
467,,LanguageTechnology,"Hi I am new on nlp and need little bit help. I would like to train a model for filtering comments

I want to filter: Spam, offensive and toxic political comments. Also I have three dataset: spam messages, offansive words and comments approval status (all website comment data ).

How can I train a model for filtering comments?

Can I use three models for for each separate transaction ?Would this be efficient?

Or can I solve this problem in a one model?

&amp;#x200B;

Thank you.",t2_59iti4cv,False,,0,False,NLP model with more than one dataset,[],r/LanguageTechnology,False,6,,0,,False,t3_mcgp6v,False,dark,0.6,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1616648872.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi I am new on nlp and need little bit help. I would like to train a model for filtering comments&lt;/p&gt;

&lt;p&gt;I want to filter: Spam, offensive and toxic political comments. Also I have three dataset: spam messages, offansive words and comments approval status (all website comment data ).&lt;/p&gt;

&lt;p&gt;How can I train a model for filtering comments?&lt;/p&gt;

&lt;p&gt;Can I use three models for for each separate transaction ?Would this be efficient?&lt;/p&gt;

&lt;p&gt;Or can I solve this problem in a one model?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mcgp6v,True,,mrfalconx,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mcgp6v/nlp_model_with_more_than_one_dataset/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mcgp6v/nlp_model_with_more_than_one_dataset/,30199,1616620072.0,0,,False,,,,,,482
468,,LanguageTechnology,"Hi everyone,

I was wondering about the masked word prediction task of BERT and how exactly it is carried out.

In the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf) the authors wrote that the masked tokens are used to predict the real tokens using a cross entropy loss. 

Furthermore they wrote ""\[...\]  the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM.""

So from my understanding I would just use a fully connected layer with a softmax at the end of the BERT model to obtain a output matrix of shape `[vocab_size, input_length]` and then use the actual position of the masked words (15% words masked) to obtain the output matrix of shape `[vocab_size, input_length*0.15]`. This matrix can then be used for the (categorical) cross entropy loss.

Is my way of reasoning correct or am I missing something important here?

&amp;#x200B;

Thank you in advance!",t2_4sv7ihod,False,,0,False,BERT MLM,[],r/LanguageTechnology,False,6,,0,,False,t3_mc37gf,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1616613507.0,text,6,,,text,self.LanguageTechnology,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;I was wondering about the masked word prediction task of BERT and how exactly it is carried out.&lt;/p&gt;

&lt;p&gt;In the &lt;a href=""https://arxiv.org/pdf/1810.04805.pdf""&gt;BERT paper&lt;/a&gt; the authors wrote that the masked tokens are used to predict the real tokens using a cross entropy loss. &lt;/p&gt;

&lt;p&gt;Furthermore they wrote &amp;quot;[...]  the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM.&amp;quot;&lt;/p&gt;

&lt;p&gt;So from my understanding I would just use a fully connected layer with a softmax at the end of the BERT model to obtain a output matrix of shape &lt;code&gt;[vocab_size, input_length]&lt;/code&gt; and then use the actual position of the masked words (15% words masked) to obtain the output matrix of shape &lt;code&gt;[vocab_size, input_length*0.15]&lt;/code&gt;. This matrix can then be used for the (categorical) cross entropy loss.&lt;/p&gt;

&lt;p&gt;Is my way of reasoning correct or am I missing something important here?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thank you in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mc37gf,True,,kengrewlong,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mc37gf/bert_mlm/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mc37gf/bert_mlm/,30199,1616584707.0,0,,False,,,,,,942
469,,LanguageTechnology,"I'm trying to take Arabic text (e-mail messages, each of which are a few sentences long) and segment it all into their individual sentences.

It's not working. Most of the time I'm getting the entire e-mail message as my output, meaning it thinks the entire thing is one sentence, but really there are 3-5 different sentences in there.

Why is this not working? The stanza language models are working properly for like 7 other languages I've tried. It's not working for Arabic. Occasionally it does separate real sentences, but most of the time it just prints out 3-5 sentences as if it's one tokenized sentence. Does anyone know why the Arabic language model isn't tokenizing these e-mail messages properly?",t2_b3gyryew,False,,0,False,stanza's Arabic language model doesn't tokenize sentences properly,[],r/LanguageTechnology,False,6,,0,,False,t3_mccpii,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1616638862.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to take Arabic text (e-mail messages, each of which are a few sentences long) and segment it all into their individual sentences.&lt;/p&gt;

&lt;p&gt;It&amp;#39;s not working. Most of the time I&amp;#39;m getting the entire e-mail message as my output, meaning it thinks the entire thing is one sentence, but really there are 3-5 different sentences in there.&lt;/p&gt;

&lt;p&gt;Why is this not working? The stanza language models are working properly for like 7 other languages I&amp;#39;ve tried. It&amp;#39;s not working for Arabic. Occasionally it does separate real sentences, but most of the time it just prints out 3-5 sentences as if it&amp;#39;s one tokenized sentence. Does anyone know why the Arabic language model isn&amp;#39;t tokenizing these e-mail messages properly?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mccpii,True,,gehith,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mccpii/stanzas_arabic_language_model_doesnt_tokenize/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mccpii/stanzas_arabic_language_model_doesnt_tokenize/,30199,1616610062.0,0,,False,,,,,,708
470,,LanguageTechnology,"I’m a data scientist with a background in image and natural language processing, and I’m stoked to get my feet wet with AI for audio. I’ll be taking on a paid audio processing project in the next couple months, so I decided to be very public about my learning process to allow others to gain the same knowledge and also observe my learning process. Plus we can all add a python project to our portfolios.

I've never tried sharing this process publicly before, so I'm going to start by sharing my journey on a few different mediums and I'll see what sticks. Please let me know if there's a platform you'd prefer I share this on. Here's how to follow along:

Twitter: ""@yacov\_lewis""

Whatsapp: [https://chat.whatsapp.com/HN3p0llOF9nJY0mQPX5WLe](https://chat.whatsapp.com/HN3p0llOF9nJY0mQPX5WLe)

Facebook group: [https://www.facebook.com/groups/dswithexperts](https://www.facebook.com/groups/dswithexperts)

Excited to share this journey!",t2_7qjxurpd,False,,0,False,Join an experienced data scientist in learning a new specialization,[],r/LanguageTechnology,False,6,,0,,False,t3_mcakz9,False,dark,0.56,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1616633549.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m a data scientist with a background in image and natural language processing, and I’m stoked to get my feet wet with AI for audio. I’ll be taking on a paid audio processing project in the next couple months, so I decided to be very public about my learning process to allow others to gain the same knowledge and also observe my learning process. Plus we can all add a python project to our portfolios.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve never tried sharing this process publicly before, so I&amp;#39;m going to start by sharing my journey on a few different mediums and I&amp;#39;ll see what sticks. Please let me know if there&amp;#39;s a platform you&amp;#39;d prefer I share this on. Here&amp;#39;s how to follow along:&lt;/p&gt;

&lt;p&gt;Twitter: &amp;quot;@yacov_lewis&amp;quot;&lt;/p&gt;

&lt;p&gt;Whatsapp: &lt;a href=""https://chat.whatsapp.com/HN3p0llOF9nJY0mQPX5WLe""&gt;https://chat.whatsapp.com/HN3p0llOF9nJY0mQPX5WLe&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Facebook group: &lt;a href=""https://www.facebook.com/groups/dswithexperts""&gt;https://www.facebook.com/groups/dswithexperts&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Excited to share this journey!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mcakz9,True,,Necessary-Stage2206,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mcakz9/join_an_experienced_data_scientist_in_learning_a/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mcakz9/join_an_experienced_data_scientist_in_learning_a/,30199,1616604749.0,0,,False,,,,,,938
471,,LanguageTechnology,"Hi all, does anyone knows what is the best algorithm to do something like that:

I have a template sentence: 

**""declare a {memType:constant|variable} called {name:term} with value {value:term}""**

That when compared with real life sentences such as: 

1. declare a variable called A with value 32
2. declare variable called XB with value ZA
3. declare a constant called C with **the** value 42

Should all return true.

I tried using regex and it works fine until it does not, because of small variations in the real life sentence such as articles (see the article ""**the""** in sentence number 3).

My question is: is there a better way to do that comparison ?",t2_7kgrc4,False,,0,False,How to compare a template sentence with a real sentence ?,[],r/LanguageTechnology,False,6,,0,,False,t3_mc3sad,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1616615620.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, does anyone knows what is the best algorithm to do something like that:&lt;/p&gt;

&lt;p&gt;I have a template sentence: &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&amp;quot;declare a {memType:constant|variable} called {name:term} with value {value:term}&amp;quot;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;That when compared with real life sentences such as: &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;declare a variable called A with value 32&lt;/li&gt;
&lt;li&gt;declare variable called XB with value ZA&lt;/li&gt;
&lt;li&gt;declare a constant called C with &lt;strong&gt;the&lt;/strong&gt; value 42&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Should all return true.&lt;/p&gt;

&lt;p&gt;I tried using regex and it works fine until it does not, because of small variations in the real life sentence such as articles (see the article &amp;quot;&lt;strong&gt;the&amp;quot;&lt;/strong&gt; in sentence number 3).&lt;/p&gt;

&lt;p&gt;My question is: is there a better way to do that comparison ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mc3sad,True,,the42thdoctor,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mc3sad/how_to_compare_a_template_sentence_with_a_real/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mc3sad/how_to_compare_a_template_sentence_with_a_real/,30199,1616586820.0,0,,False,,,,,,662
472,,LanguageTechnology,"Hi everyone,

in the Roberta paper ([https://arxiv.org/pdf/1907.11692.pdf](https://arxiv.org/pdf/1907.11692.pdf) page 5) they say they use doc-sentences formatted inputs (512 tokens taken contiguously across multiple documents, documents are separated by a sep token). 

e.g. if I understand well: tokenized-doc 1 : 220 tokens tokenized-doc 2: 350 tokens

\-&gt; corresponding Roberta pre-training inputs: 220 tokens from doc 1 + sep token + 291 first tokens of doc 2 

my question is as follows:

Is there a way to obtain this format of input from a corpus of documents in a convenient way using pytorch or hugging face and a custom BPE tokenizer?",t2_7o08xo35,False,,0,False,question RoBERTa doc sentances.,[],r/LanguageTechnology,False,6,,0,,False,t3_mc6k6m,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1616623599.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;in the Roberta paper (&lt;a href=""https://arxiv.org/pdf/1907.11692.pdf""&gt;https://arxiv.org/pdf/1907.11692.pdf&lt;/a&gt; page 5) they say they use doc-sentences formatted inputs (512 tokens taken contiguously across multiple documents, documents are separated by a sep token). &lt;/p&gt;

&lt;p&gt;e.g. if I understand well: tokenized-doc 1 : 220 tokens tokenized-doc 2: 350 tokens&lt;/p&gt;

&lt;p&gt;-&amp;gt; corresponding Roberta pre-training inputs: 220 tokens from doc 1 + sep token + 291 first tokens of doc 2 &lt;/p&gt;

&lt;p&gt;my question is as follows:&lt;/p&gt;

&lt;p&gt;Is there a way to obtain this format of input from a corpus of documents in a convenient way using pytorch or hugging face and a custom BPE tokenizer?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mc6k6m,True,,Some-Potential3341,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mc6k6m/question_roberta_doc_sentances/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mc6k6m/question_roberta_doc_sentances/,30199,1616594799.0,0,,False,,,,,,648
473,,LanguageTechnology,"Hi!

I hold a BA and an MA in English language and literature.
Since I finished BA in 2008, I worked in translation, interpreting, NGOs and Arabic language lecturing. 
Even though I worked with big comlanies, I feel I need to improve my career and focus on a specific path, after I recently decided to not pursue PHD and academa. 
TBH, I am astonished at how IT and tech industry is defining the future with tech careers growing steadily, as other careers are dying. Unfortunately, my math skills suck and programming is not my thing.

So I noticed jobs posted as data linguist, QA linguist, linguist engineer and linguist posted in tech companies such as Apple and Amazon, where some of Arabic language major firends I know landed similar jobs.

I think these positions are in high demand, where I can be paid well, and work in something I enjoy! 
Esp. that I am fluent in English and a native Arabic speaker. 

Do I need to take courses or internships to imorove my chances to get these jobs? What sort of courses? Provided by which platforms?
Any advice on writing cv and preparing for interviews? 

And is it late too late make such a change in my mid career with no experience in tech industry? 

Thank you so much for for your advice, it means a lot to me!",t2_4i94kuv9,False,,0,False,A linguist pursuing career im tech industry!,[],r/LanguageTechnology,False,6,,0,,False,t3_mbv3pc,False,dark,1.0,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,1616553048.0,,[],{},,True,,1616580934.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;I hold a BA and an MA in English language and literature.
Since I finished BA in 2008, I worked in translation, interpreting, NGOs and Arabic language lecturing. 
Even though I worked with big comlanies, I feel I need to improve my career and focus on a specific path, after I recently decided to not pursue PHD and academa. 
TBH, I am astonished at how IT and tech industry is defining the future with tech careers growing steadily, as other careers are dying. Unfortunately, my math skills suck and programming is not my thing.&lt;/p&gt;

&lt;p&gt;So I noticed jobs posted as data linguist, QA linguist, linguist engineer and linguist posted in tech companies such as Apple and Amazon, where some of Arabic language major firends I know landed similar jobs.&lt;/p&gt;

&lt;p&gt;I think these positions are in high demand, where I can be paid well, and work in something I enjoy! 
Esp. that I am fluent in English and a native Arabic speaker. &lt;/p&gt;

&lt;p&gt;Do I need to take courses or internships to imorove my chances to get these jobs? What sort of courses? Provided by which platforms?
Any advice on writing cv and preparing for interviews? &lt;/p&gt;

&lt;p&gt;And is it late too late make such a change in my mid career with no experience in tech industry? &lt;/p&gt;

&lt;p&gt;Thank you so much for for your advice, it means a lot to me!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mbv3pc,True,,sallyholmes86,,12,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mbv3pc/a_linguist_pursuing_career_im_tech_industry/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mbv3pc/a_linguist_pursuing_career_im_tech_industry/,30199,1616552134.0,0,,False,,,,,,1262
474,,LanguageTechnology,"I've come across the idea of pre-trained tokenizers, but I'm struggling to understand why and when these would be useful. Isn't it better if you train a tokenizer on your own training data? What if the pretrained tokenizer has only a small vocabulary or is domain-specific? Wouldn't that be more detrimental to your model?",t2_4m6jgrsb,False,,0,False,What's the point of pre-trained tokenizers?,[],r/LanguageTechnology,False,6,,0,,False,t3_mc2zzv,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1616612727.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve come across the idea of pre-trained tokenizers, but I&amp;#39;m struggling to understand why and when these would be useful. Isn&amp;#39;t it better if you train a tokenizer on your own training data? What if the pretrained tokenizer has only a small vocabulary or is domain-specific? Wouldn&amp;#39;t that be more detrimental to your model?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mc2zzv,True,,permadressed,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mc2zzv/whats_the_point_of_pretrained_tokenizers/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mc2zzv/whats_the_point_of_pretrained_tokenizers/,30199,1616583927.0,0,,False,,,,,,322
475,,LanguageTechnology,I've been training my model on a huge corpus. Loss right now is pretty high (100000ish) and I noticed that it does drop when I increase the number of iterations. I wanted to know is there something I'm missing out on or if there's any way to find out the optimal number of iterations without overtraining.,t2_1cmvtbvz,False,,0,False,SpaCy Loss Reduction,[],r/LanguageTechnology,False,6,,0,,False,t3_mc0s24,False,dark,0.66,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1616603383.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been training my model on a huge corpus. Loss right now is pretty high (100000ish) and I noticed that it does drop when I increase the number of iterations. I wanted to know is there something I&amp;#39;m missing out on or if there&amp;#39;s any way to find out the optimal number of iterations without overtraining.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mc0s24,True,,iamrahil,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mc0s24/spacy_loss_reduction/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mc0s24/spacy_loss_reduction/,30199,1616574583.0,0,,False,,,,,,305
476,,LanguageTechnology,,t2_852buvzd,False,,0,False,"EleutherAI releases gpt-neo models trained on GPT3, GPT2",[],r/LanguageTechnology,False,6,,0,,False,t3_mbm5kq,False,dark,0.64,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,False,,1616555215.0,text,6,,,text,github.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mbm5kq,True,,Attention-Spa,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mbm5kq/eleutherai_releases_gptneo_models_trained_on_gpt3/,all_ads,False,https://github.com/EleutherAI/gpt-neo,30199,1616526415.0,0,,False,https://github.com/EleutherAI/gpt-neo,,,,,0
477,,LanguageTechnology,,t2_4fyjv,False,,0,False,University of Helsinki language technology professor Jörg Tiedemann has released a dataset with over 500 million translated sentences in 188 languages,[],r/LanguageTechnology,False,6,,0,,False,t3_mb12gd,False,dark,0.99,,public,103,0,{},,False,[],,False,False,,{},,False,103,,False,False,,False,,[],{},,False,,1616484840.0,text,6,,,text,github.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mb12gd,True,,Caesarr,,0,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/mb12gd/university_of_helsinki_language_technology/,all_ads,False,https://github.com/Helsinki-NLP/Tatoeba-Challenge/blob/master/Backtranslations.md,30199,1616456040.0,0,,False,https://github.com/Helsinki-NLP/Tatoeba-Challenge/blob/master/Backtranslations.md,"[{'approved_at_utc': None, 'subreddit': 'programming', 'selftext': '[deleted]', 'user_reports': [], 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'University of Helsinki language technology professor Jörg Tiedemann has released a dataset with over 500 million translated sentences in 188 languages', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/programming', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_mao82o', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.99, 'author_flair_background_color': '', 'subreddit_type': 'public', 'ups': 3191, 'total_awards_received': 9, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 3191, 'approved_by': None, 'is_created_from_ads_ui': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'gildings': {'gid_1': 2}, 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1616450336.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': 'deleted', 'banned_by': None, 'domain': 'github.com', 'allow_live_comments': True, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;[deleted]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://github.com/Helsinki-NLP/Tatoeba-Challenge/blob/master/Backtranslations.md', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 4, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 50, 'id': 'award_02d9ab2c-162e-4c01-8438-317a016ed3d9', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=16&amp;height=16&amp;auto=webp&amp;s=10034f3fdf8214c8377134bb60c5b832d4bbf588', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=32&amp;height=32&amp;auto=webp&amp;s=100f785bf261fa9452a5d82ee0ef0793369dbfa5', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=48&amp;height=48&amp;auto=webp&amp;s=b15d030fdfbbe4af4a5b34ab9dc90a174df40a23', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=64&amp;height=64&amp;auto=webp&amp;s=601c75be6ee30dc4b47a5c65d64dea9a185502a1', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=128&amp;height=128&amp;auto=webp&amp;s=540f36e65c0e2f1347fe32020e4a1565e3680437', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""I'm in this with you."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Take My Energy', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=16&amp;height=16&amp;auto=webp&amp;s=045db73f47a9513c44823d132b4c393ab9241b6a', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=32&amp;height=32&amp;auto=webp&amp;s=298a02e0edbb5b5e293087eeede63802cbe1d2c7', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=48&amp;height=48&amp;auto=webp&amp;s=7d06d606eb23dbcd6dbe39ee0e60588c5eb89065', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=64&amp;height=64&amp;auto=webp&amp;s=ecd9854b14104a36a210028c43420f0dababd96b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=128&amp;height=128&amp;auto=webp&amp;s=0d5d7b92c1d66aff435f2ad32e6330ca2b971f6d', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png'}], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2fwo', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'mao82o', 'is_robot_indexable': False, 'report_reasons': None, 'author': '[deleted]', 'discussion_type': None, 'num_comments': 117, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_flair_text_color': 'dark', 'permalink': '/r/programming/comments/mao82o/university_of_helsinki_language_technology/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://github.com/Helsinki-NLP/Tatoeba-Challenge/blob/master/Backtranslations.md', 'subreddit_subscribers': 3435474, 'created_utc': 1616421536.0, 'num_crossposts': 7, 'media': None, 'is_video': False}]",t3_mao82o,,,0
478,,LanguageTechnology,,t2_hnib9,False,,0,False,Be patient,[],r/LanguageTechnology,False,6,,0,,False,t3_mbojsq,False,dark,0.57,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,False,,1616561624.0,text,6,,,text,terrytao.wordpress.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mbojsq,True,,boilerup800,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mbojsq/be_patient/,all_ads,False,https://terrytao.wordpress.com/career-advice/be-patient/,30199,1616532824.0,0,,False,https://terrytao.wordpress.com/career-advice/be-patient/,,,,,0
479,,LanguageTechnology," 

This may be a simple question for those with a better understanding. I’m trying to figure out if it is possible to use NLP to describe the topic of conversation in a group chat (text). For example, people in a chat are giving advice on surfing for beginners, and using machine learning we would be able to generate something like ‘learning to surf’ or ‘beginner tips for surfing’. My question for you all is (1) is this possible in real-time for an ongoing chat where the topic of conversation evolves/changes and (2) if possible, how specific can the generated topic be? Would it just be ‘surfing’ or can it be more specific such as ‘advice on learning to surf for beginners?

Any help would be much appreciated! Even a resource or guidance would be great if this is too simple of a question. Just haven’t been able to find the answer on google searches.",t2_150dxmro,False,,0,False,Interpreting the topic of a streaming text-based group chat in real-time. Is this possible?,[],r/LanguageTechnology,False,6,,0,,False,t3_mbew6j,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1616535431.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This may be a simple question for those with a better understanding. I’m trying to figure out if it is possible to use NLP to describe the topic of conversation in a group chat (text). For example, people in a chat are giving advice on surfing for beginners, and using machine learning we would be able to generate something like ‘learning to surf’ or ‘beginner tips for surfing’. My question for you all is (1) is this possible in real-time for an ongoing chat where the topic of conversation evolves/changes and (2) if possible, how specific can the generated topic be? Would it just be ‘surfing’ or can it be more specific such as ‘advice on learning to surf for beginners?&lt;/p&gt;

&lt;p&gt;Any help would be much appreciated! Even a resource or guidance would be great if this is too simple of a question. Just haven’t been able to find the answer on google searches.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mbew6j,True,,armaboi,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mbew6j/interpreting_the_topic_of_a_streaming_textbased/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mbew6j/interpreting_the_topic_of_a_streaming_textbased/,30199,1616506631.0,1,,False,,,,,,858
480,,LanguageTechnology,"The trending Today topics show up when you click search bar, and below the recent searches, appear the Trending topics. I would like to know how reddit clusters these sub-reddits, how it ranks various topics, and how it identifies various topics in the first place. The names of the algorithms used would be helpful. Thanks in advance!",t2_8n5d56qx,False,,0,False,"How does Reddit show ""Trending Today"" topics? What algorithm does it use?",[],r/LanguageTechnology,False,6,,0,,False,t3_mar9z8,False,dark,0.91,,public,25,0,{},,False,[],,False,False,,{},,False,25,,False,False,,False,,[],{},,True,,1616458275.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The trending Today topics show up when you click search bar, and below the recent searches, appear the Trending topics. I would like to know how reddit clusters these sub-reddits, how it ranks various topics, and how it identifies various topics in the first place. The names of the algorithms used would be helpful. Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mar9z8,True,,ChandlerBingggggggg,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mar9z8/how_does_reddit_show_trending_today_topics_what/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mar9z8/how_does_reddit_show_trending_today_topics_what/,30199,1616429475.0,0,,False,,,,,,335
481,,LanguageTechnology,,t2_v7nu2,False,,0,False,"John Snow Labs Spark-NLP 3.0.0: Supporting Spark 3.x, Scala 2.12, more Databricks runtimes, more EMR versions, performance improvements &amp; lots more",[],r/LanguageTechnology,False,6,,0,,False,t3_mav0s8,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1616468037.0,text,6,,,text,github.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mav0s8,True,,dark-night-rises,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mav0s8/john_snow_labs_sparknlp_300_supporting_spark_3x/,all_ads,False,https://github.com/JohnSnowLabs/spark-nlp/releases/tag/3.0.0,30199,1616439237.0,0,,False,https://github.com/JohnSnowLabs/spark-nlp/releases/tag/3.0.0,,,,,0
482,,LanguageTechnology,"There’s also a Voice Tech MSc at the University of Groningen (in the Netherlands) which could be interesting for you

The basics:

- It is only one year
- There is no NLP/NLU — only speech synthesis and speech recognition
- It is  very interdisciplinary, students from Linguistics, CS, AI, digital humanities, etc. are welcome, so long as they are not scared to learn programming!
- It is an English language program


Check it out here 

https://www.rug.nl/masters/voice-technology/

There are two (free!) online webinars next week, if you want to lean more! Register at the links below to get the link to the event. 

- March 23rd: https://www.rug.nl/cf/campus-fryslan/cf-masters/events/online-master-week-voice-technology
- March 24th: https://www.rug.nl/cf/campus-fryslan/cf-masters/events/online-master-week-voice-technology",t2_8wbdz21a,False,,0,False,Masters in Voice Technology (Europe) online webinar this Wed/Thursday,[],r/LanguageTechnology,False,6,,0,,False,t3_mah7v7,False,dark,0.92,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,True,,1616425027.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;There’s also a Voice Tech MSc at the University of Groningen (in the Netherlands) which could be interesting for you&lt;/p&gt;

&lt;p&gt;The basics:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It is only one year&lt;/li&gt;
&lt;li&gt;There is no NLP/NLU — only speech synthesis and speech recognition&lt;/li&gt;
&lt;li&gt;It is  very interdisciplinary, students from Linguistics, CS, AI, digital humanities, etc. are welcome, so long as they are not scared to learn programming!&lt;/li&gt;
&lt;li&gt;It is an English language program&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Check it out here &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.rug.nl/masters/voice-technology/""&gt;https://www.rug.nl/masters/voice-technology/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;There are two (free!) online webinars next week, if you want to lean more! Register at the links below to get the link to the event. &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;March 23rd: &lt;a href=""https://www.rug.nl/cf/campus-fryslan/cf-masters/events/online-master-week-voice-technology""&gt;https://www.rug.nl/cf/campus-fryslan/cf-masters/events/online-master-week-voice-technology&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;March 24th: &lt;a href=""https://www.rug.nl/cf/campus-fryslan/cf-masters/events/online-master-week-voice-technology""&gt;https://www.rug.nl/cf/campus-fryslan/cf-masters/events/online-master-week-voice-technology&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mah7v7,True,,VoiceTech,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mah7v7/masters_in_voice_technology_europe_online_webinar/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mah7v7/masters_in_voice_technology_europe_online_webinar/,30199,1616396227.0,0,,False,,,,,,829
483,,LanguageTechnology,,t2_q1dia,False,,0,False,A Directory of Online Newspaper Sources for 70+ Languages,[],r/LanguageTechnology,False,6,,0,,False,t3_maum7o,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,False,,1616466966.0,text,6,,,text,github.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,maum7o,True,,divkakwani,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/maum7o/a_directory_of_online_newspaper_sources_for_70/,all_ads,False,https://github.com/divkakwani/awesome-newspapers,30199,1616438166.0,0,,False,https://github.com/divkakwani/awesome-newspapers,,,,,0
484,,LanguageTechnology,"A task that has grasped the attention of the AI community recently is that of visual question answering.

Visual question answering involves answering questions about images in natural language. It is an interesting problem, as it combines aspects of both computer vision and natural language processing.

This article explores the problem of visual question answering, different approaches to solve it, associated challenges, datasets, and evaluation methods. Topics such as image featurization, question featurization, joint feature representation, and answer generation will be explained, along with a survey of recent research efforts tackling each of these problems.

Article link: [https://blog.paperspace.com/introduction-to-visual-question-answering/](https://blog.paperspace.com/introduction-to-visual-question-answering/)",t2_15en0l,False,,0,False,[Article] Survey of Visual Question Answering,[],r/LanguageTechnology,False,6,,0,,False,t3_masglx,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1616461374.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;A task that has grasped the attention of the AI community recently is that of visual question answering.&lt;/p&gt;

&lt;p&gt;Visual question answering involves answering questions about images in natural language. It is an interesting problem, as it combines aspects of both computer vision and natural language processing.&lt;/p&gt;

&lt;p&gt;This article explores the problem of visual question answering, different approaches to solve it, associated challenges, datasets, and evaluation methods. Topics such as image featurization, question featurization, joint feature representation, and answer generation will be explained, along with a survey of recent research efforts tackling each of these problems.&lt;/p&gt;

&lt;p&gt;Article link: &lt;a href=""https://blog.paperspace.com/introduction-to-visual-question-answering/""&gt;https://blog.paperspace.com/introduction-to-visual-question-answering/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,masglx,True,,hellopaperspace,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/masglx/article_survey_of_visual_question_answering/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/masglx/article_survey_of_visual_question_answering/,30199,1616432574.0,0,,False,,,,,,831
485,,LanguageTechnology,,t2_zkob0fz,False,,0,False,Handwritten Recognition and Analysis,[],r/LanguageTechnology,False,6,,0,,False,t3_maqcol,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,False,,1616455962.0,text,6,,,text,community.wolfram.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,maqcol,True,,CuttingWithScissors,,0,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/maqcol/handwritten_recognition_and_analysis/,all_ads,False,https://community.wolfram.com/groups/-/m/t/2091085,30199,1616427162.0,0,,False,https://community.wolfram.com/groups/-/m/t/2091085,,,,,0
486,,LanguageTechnology,,t2_ohi7h,False,,0,False,Help understanding a NLP methodology in a research paper,[],r/LanguageTechnology,False,6,,0,,False,t3_mapdqs,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,False,,1616453444.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mapdqs,True,,imalkrulez,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mapdqs/help_understanding_a_nlp_methodology_in_a/,all_ads,False,/r/learnmachinelearning/comments/mapc3c/help_understanding_a_nlp_methodology_in_a/,30199,1616424644.0,0,,False,/r/learnmachinelearning/comments/mapc3c/help_understanding_a_nlp_methodology_in_a/,"[{'approved_at_utc': None, 'subreddit': 'learnmachinelearning', 'selftext': "" ([https://imgur.com/Zjx0KGY.jpg](https://imgur.com/Zjx0KGY.jpg)) Hi might seem a silly question but can anyone help me breakdown parts 2.2.1 and 2.2.2,. I'm an NLP noob and can't make heads or tails of what's been done here."", 'author_fullname': 't2_ohi7h', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Help understanding a NLP methodology in a research paper', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/learnmachinelearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_mapc3c', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1616453325.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.learnmachinelearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;(&lt;a href=""https://imgur.com/Zjx0KGY.jpg""&gt;https://imgur.com/Zjx0KGY.jpg&lt;/a&gt;) Hi might seem a silly question but can anyone help me breakdown parts 2.2.1 and 2.2.2,. I&amp;#39;m an NLP noob and can&amp;#39;t make heads or tails of what&amp;#39;s been done here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_3cqa1', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'mapc3c', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'imalkrulez', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/learnmachinelearning/comments/mapc3c/help_understanding_a_nlp_methodology_in_a/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/learnmachinelearning/comments/mapc3c/help_understanding_a_nlp_methodology_in_a/', 'subreddit_subscribers': 232286, 'created_utc': 1616424525.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_mapc3c,,,0
487,,LanguageTechnology,"Hi Guys,

I am getting started with my blogging journey in NLP and written my first blog post. Would really appreciate it if you guys give constructive feedback on content /quality-wise so that I can improve and provide more useful content to the NLP community. Thanks!!

[https://raghavakotala.medium.com/how-to-choose-embedding-effectively-for-task-specific-use-cases-a-developer-notes-f60f707ac162](https://raghavakotala.medium.com/how-to-choose-embedding-effectively-for-task-specific-use-cases-a-developer-notes-f60f707ac162)",t2_13cnmn,False,,0,False,Kindly give constructive feedback on my first NLP blog post.,[],r/LanguageTechnology,False,6,,0,,False,t3_mafc1y,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1616417755.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi Guys,&lt;/p&gt;

&lt;p&gt;I am getting started with my blogging journey in NLP and written my first blog post. Would really appreciate it if you guys give constructive feedback on content /quality-wise so that I can improve and provide more useful content to the NLP community. Thanks!!&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://raghavakotala.medium.com/how-to-choose-embedding-effectively-for-task-specific-use-cases-a-developer-notes-f60f707ac162""&gt;https://raghavakotala.medium.com/how-to-choose-embedding-effectively-for-task-specific-use-cases-a-developer-notes-f60f707ac162&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mafc1y,True,,raghavakotala,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mafc1y/kindly_give_constructive_feedback_on_my_first_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mafc1y/kindly_give_constructive_feedback_on_my_first_nlp/,30199,1616388955.0,0,,False,,,,,,530
488,,LanguageTechnology,,t2_hkv9s,False,,0,False,Ex2: Neural Data Augmentation via Example Extrapolation (Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_m9xrxl,False,dark,0.75,,public,4,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/d0KUF_aNS_s?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Ex2: Neural Data Augmentation via Example Extrapolation (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/d0KUF_aNS_s?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/d0KUF_aNS_s/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/d0KUF_aNS_s?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/m9xrxl', 'height': 200}",,False,4,,False,False,,False,,[],{},,False,,1616365613.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m9xrxl,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m9xrxl/ex2_neural_data_augmentation_via_example/,all_ads,False,https://youtu.be/d0KUF_aNS_s,30199,1616336813.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Ex2: Neural Data Augmentation via Example Extrapolation (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/d0KUF_aNS_s?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/d0KUF_aNS_s/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/d0KUF_aNS_s,,,,,0
489,,LanguageTechnology,,t2_732g5bga,False,,0,False,"Integrating Ray Tune, Hugging Face Transformers and W&amp;B",[],r/LanguageTechnology,False,6,,0,,False,t3_m9ffg3,False,dark,0.81,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,False,,1616300766.0,text,6,,,text,ruanchaves.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m9ffg3,True,,ruanchaves93,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m9ffg3/integrating_ray_tune_hugging_face_transformers/,all_ads,False,https://ruanchaves.com/blog/ray-wandb-transformers-integration/,30199,1616271966.0,0,,False,https://ruanchaves.com/blog/ray-wandb-transformers-integration/,,,,,0
490,,LanguageTechnology,"I am currently working on an NLP project that deals with dialectal Arabic. Resources are scarce for most dialects, so I want a dataset that has annotated words from Arabizi to Arabic in whichever Arabic dialect. Problem is I don't speak Arabic, so I can't create the dataset myself.

Ps: Arabizi is Arabic written in latin characters.",t2_168yi3j8,False,,0,False,Where can I find a dataset of annotated Arabic to Arabizi dataset?,[],r/LanguageTechnology,False,6,,0,,False,t3_m9e2rn,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1616296752.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am currently working on an NLP project that deals with dialectal Arabic. Resources are scarce for most dialects, so I want a dataset that has annotated words from Arabizi to Arabic in whichever Arabic dialect. Problem is I don&amp;#39;t speak Arabic, so I can&amp;#39;t create the dataset myself.&lt;/p&gt;

&lt;p&gt;Ps: Arabizi is Arabic written in latin characters.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m9e2rn,True,,maroxtn13,,12,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m9e2rn/where_can_i_find_a_dataset_of_annotated_arabic_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m9e2rn/where_can_i_find_a_dataset_of_annotated_arabic_to/,30199,1616267952.0,0,,False,,,,,,334
491,,LanguageTechnology,"We  have a project that with a given of arbitrary number of phrases, we  want to find to best fitting document in a bunch of documents.

Consider this as, with a given set of phrases of skillsets, we want to find the best candidate for a job.

So Gensim's Similarity module seems like a good fit for this problem, especially [soft cosine similarity](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb) checking. But inside I can't get comfortable, because transformers are very popular lately.

I  can't be sure if Gensim's similarity is the best fit but I'm not an  expert on transformers either to tell how it can be done with  transformers or do I even need them. Any suggestions on this? Thanks.",t2_3kwtnien,False,,0,False,Superior tools than Gensim's Similarity module,[],r/LanguageTechnology,False,6,,0,,False,t3_m9eynv,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1616299371.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We  have a project that with a given of arbitrary number of phrases, we  want to find to best fitting document in a bunch of documents.&lt;/p&gt;

&lt;p&gt;Consider this as, with a given set of phrases of skillsets, we want to find the best candidate for a job.&lt;/p&gt;

&lt;p&gt;So Gensim&amp;#39;s Similarity module seems like a good fit for this problem, especially &lt;a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb""&gt;soft cosine similarity&lt;/a&gt; checking. But inside I can&amp;#39;t get comfortable, because transformers are very popular lately.&lt;/p&gt;

&lt;p&gt;I  can&amp;#39;t be sure if Gensim&amp;#39;s similarity is the best fit but I&amp;#39;m not an  expert on transformers either to tell how it can be done with  transformers or do I even need them. Any suggestions on this? Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m9eynv,True,,emremrah,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m9eynv/superior_tools_than_gensims_similarity_module/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m9eynv/superior_tools_than_gensims_similarity_module/,30199,1616270571.0,0,,False,,,,,,747
492,,LanguageTechnology,,t2_7qjxurpd,False,,0,False,A Transformers Tutorial in Plain English To Get You Thinking,[],r/LanguageTechnology,False,6,,0,,False,t3_m8t7sd,False,dark,0.9,,public,17,0,{},,False,[],,False,False,,{},,False,17,,False,False,,False,,[],{},,False,,1616222575.0,text,6,,,text,towardsdatascience.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m8t7sd,True,,Necessary-Stage2206,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m8t7sd/a_transformers_tutorial_in_plain_english_to_get/,all_ads,False,https://towardsdatascience.com/everything-product-people-need-to-know-about-transformers-part-3-bert-a1227cead488,30199,1616193775.0,0,,False,https://towardsdatascience.com/everything-product-people-need-to-know-about-transformers-part-3-bert-a1227cead488,,,,,0
493,,LanguageTechnology,"Hey all, I had to take a day off from posting our DSotD. I think sometimes the amount of information we through daily at everyone is exhausting, and it's better to not overload and instead digest.

So today we have an [awesome blog post](https://forums.developer.nvidia.com/t/nlp-is-not-just-a-domain-solved-by-deep-learning-this-is-also-in-the-ml-wheelhouse/165337?u=kasmith), which explains how we can do an entire NLP pipeline on a GPU... creating incredible fast lightening speeds for NLP solutions. Now, this is not diving into the transformer craze like BERT or GPT, so don't get your hopes up (though you can see easily where those models can be placed in the pipeline).

What's interesting is the majority feel that Deep Transformers are the way to solve all NLP, but it's not always the case. [Here](https://www.kaggle.com/cdeotte/rapids-cuml-tfidfvectorizer-and-knn) is an example of a Nvidia Kaggle Grandmaster (and absolute Kaggle tank!) using the described pipeline above to Kaggle's Shopee - Price Match Guarantee, determining if two products are the same by their images and product description. Also, a even more streamlined Kaggle memory efficient book can be seen [here](https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700) .  


Let me know what you think, and we can get a great discussion going. Anyone want to vent about transformers being the solution to all NLP? *(laughing face emoji, I think of Walid Saba and his many posts on LinkedIn. Give him a follow, it's amazing pure linguist information).* ",,False,,0,False,NLP Pipeline Completely on the GPU,[],r/LanguageTechnology,False,6,,0,,False,t3_m8gsto,False,dark,0.97,,public,21,0,{},,False,[],,False,False,,{},,False,21,,False,,,1616159459.0,,,{},,True,,1616187871.0,text,6,,,,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all, I had to take a day off from posting our DSotD. I think sometimes the amount of information we through daily at everyone is exhausting, and it&amp;#39;s better to not overload and instead digest.&lt;/p&gt;

&lt;p&gt;So today we have an &lt;a href=""https://forums.developer.nvidia.com/t/nlp-is-not-just-a-domain-solved-by-deep-learning-this-is-also-in-the-ml-wheelhouse/165337?u=kasmith""&gt;awesome blog post&lt;/a&gt;, which explains how we can do an entire NLP pipeline on a GPU... creating incredible fast lightening speeds for NLP solutions. Now, this is not diving into the transformer craze like BERT or GPT, so don&amp;#39;t get your hopes up (though you can see easily where those models can be placed in the pipeline).&lt;/p&gt;

&lt;p&gt;What&amp;#39;s interesting is the majority feel that Deep Transformers are the way to solve all NLP, but it&amp;#39;s not always the case. &lt;a href=""https://www.kaggle.com/cdeotte/rapids-cuml-tfidfvectorizer-and-knn""&gt;Here&lt;/a&gt; is an example of a Nvidia Kaggle Grandmaster (and absolute Kaggle tank!) using the described pipeline above to Kaggle&amp;#39;s Shopee - Price Match Guarantee, determining if two products are the same by their images and product description. Also, a even more streamlined Kaggle memory efficient book can be seen &lt;a href=""https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700""&gt;here&lt;/a&gt; .  &lt;/p&gt;

&lt;p&gt;Let me know what you think, and we can get a great discussion going. Anyone want to vent about transformers being the solution to all NLP? &lt;em&gt;(laughing face emoji, I think of Walid Saba and his many posts on LinkedIn. Give him a follow, it&amp;#39;s amazing pure linguist information).&lt;/em&gt; &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m8gsto,True,,[deleted],,2,True,all_ads,False,[],,dark,/r/LanguageTechnology/comments/m8gsto/nlp_pipeline_completely_on_the_gpu/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m8gsto/nlp_pipeline_completely_on_the_gpu/,30199,1616159071.0,0,,False,,,,,,1543
494,,LanguageTechnology,"
Hello everyone! We are studying NLP in the university, and I have been given a task to implement a typo correction using NLP. Are there guides and examples that could help me in my work?

Also wonder about NLP-based punctuation and grammar correction.",t2_26qth9a3,False,,0,False,Typo correction using NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_m8nrgt,False,dark,1.0,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1616207504.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone! We are studying NLP in the university, and I have been given a task to implement a typo correction using NLP. Are there guides and examples that could help me in my work?&lt;/p&gt;

&lt;p&gt;Also wonder about NLP-based punctuation and grammar correction.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m8nrgt,True,,ChrysoliteAzalea,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m8nrgt/typo_correction_using_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m8nrgt/typo_correction_using_nlp/,30199,1616178704.0,0,,False,,,,,,252
495,,LanguageTechnology,"I am trying to build a QA pipeline. My end goal is that :  
There are total X documents in database. given a query fetch top Y documents using some fast model (tfidf/bm25 etc) where Y is a subset of X. Then run a deep learning model (bert, longformer etc) to fetch and rank Z number of documents where Z is a subset of Y. 

I am using haystack and although I heard good things about it but for some reason its not working for me. its slower than my own pipeline, its retriever (the first/recall module which runs keyword model) is buggy (returning duplicate documents even though my data did not have any duplicate). 

So I was wondering if there are any other tried and tested frameworks that you guys know of. 

Thank you in advance.",t2_5pmdgm1l,False,,0,False,What Are Some Open Source NLP Framework Pipelines For QA Task,[],r/LanguageTechnology,False,6,,0,,False,t3_m8rpaz,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1616218447.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to build a QA pipeline. My end goal is that :&lt;br/&gt;
There are total X documents in database. given a query fetch top Y documents using some fast model (tfidf/bm25 etc) where Y is a subset of X. Then run a deep learning model (bert, longformer etc) to fetch and rank Z number of documents where Z is a subset of Y. &lt;/p&gt;

&lt;p&gt;I am using haystack and although I heard good things about it but for some reason its not working for me. its slower than my own pipeline, its retriever (the first/recall module which runs keyword model) is buggy (returning duplicate documents even though my data did not have any duplicate). &lt;/p&gt;

&lt;p&gt;So I was wondering if there are any other tried and tested frameworks that you guys know of. &lt;/p&gt;

&lt;p&gt;Thank you in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m8rpaz,True,,inopico3,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m8rpaz/what_are_some_open_source_nlp_framework_pipelines/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m8rpaz/what_are_some_open_source_nlp_framework_pipelines/,30199,1616189647.0,0,,False,,,,,,735
496,,LanguageTechnology,,t2_21qaqh1p,False,,0,False,Build ASR for Any Language with Hugginface Transformers,[],r/LanguageTechnology,False,6,,0,,False,t3_m8hzci,False,dark,1.0,,public,5,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/dAbtQRDaoro?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Build Speech Recognition for any Language with 🤗  Transformers - Finetune XLSR-Wav2Vec2 (Hindi)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/dAbtQRDaoro?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/dAbtQRDaoro/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCpV_X0VrL8-jg3t6wYGS-1g'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/dAbtQRDaoro?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/m8hzci', 'height': 200}",,False,5,,False,False,,False,,[],{},,False,,1616191532.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m8hzci,True,,dulldata,,2,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/m8hzci/build_asr_for_any_language_with_hugginface/,all_ads,False,https://www.youtube.com/watch?v=dAbtQRDaoro,30199,1616162732.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Build Speech Recognition for any Language with 🤗  Transformers - Finetune XLSR-Wav2Vec2 (Hindi)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/dAbtQRDaoro?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/dAbtQRDaoro/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCpV_X0VrL8-jg3t6wYGS-1g'}}",False,https://www.youtube.com/watch?v=dAbtQRDaoro,,,,,0
497,,LanguageTechnology,"Hey guys, I have been following this sub for quite some time now and I am always excited to learn about the new technologies that people post here. I wanted to some advice/guidance on what topics are there in NLP that are related to Finance. I obviously know about stock prediction and sentiment analysis of financial documents but I wanted to know if there are any other ideas/tasks that I could pursue for my undergraduate thesis. I have been reading papers on Finance related NLP tasks but I have been unable to narrow down a topic for which I can start collecting data and formulating potential models. Could you guys please help me out?",t2_7nt9jby8,False,,0,False,NLP in Finance,[],r/LanguageTechnology,False,6,,0,,False,t3_m8iz81,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1616194428.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys, I have been following this sub for quite some time now and I am always excited to learn about the new technologies that people post here. I wanted to some advice/guidance on what topics are there in NLP that are related to Finance. I obviously know about stock prediction and sentiment analysis of financial documents but I wanted to know if there are any other ideas/tasks that I could pursue for my undergraduate thesis. I have been reading papers on Finance related NLP tasks but I have been unable to narrow down a topic for which I can start collecting data and formulating potential models. Could you guys please help me out?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m8iz81,True,,cs_deep_learning_umd,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m8iz81/nlp_in_finance/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m8iz81/nlp_in_finance/,30199,1616165628.0,0,,False,,,,,,641
498,,LanguageTechnology,"I’ve been searching around but haven’t had much luck on kaggle, anywhere else i could look?",t2_7wo3mdn,False,,0,False,Anyone know where I could find a dataset that contains song lyrics and genre tags?,[],r/LanguageTechnology,False,6,,0,,False,t3_m8qm05,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1616215432.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’ve been searching around but haven’t had much luck on kaggle, anywhere else i could look?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m8qm05,True,,edwardsrk,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m8qm05/anyone_know_where_i_could_find_a_dataset_that/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m8qm05/anyone_know_where_i_could_find_a_dataset_that/,30199,1616186632.0,0,,False,,,,,,91
499,,LanguageTechnology,,t2_trjf2,False,,0,False,Open-source libraries for machine translation,[],r/LanguageTechnology,False,6,,0,,False,t3_m8csmr,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,False,,1616171818.0,text,6,,,text,modelfront.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m8csmr,True,,adammathias,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m8csmr/opensource_libraries_for_machine_translation/,all_ads,False,https://modelfront.com/compare#machine-translation-libraries,30199,1616143018.0,0,,False,https://modelfront.com/compare#machine-translation-libraries,"[{'approved_at_utc': None, 'subreddit': 'machinetranslation', 'selftext': '', 'author_fullname': 't2_trjf2', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Open-source libraries for machine translation', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/machinetranslation', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_m8csek', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'engineering', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1616171791.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'modelfront.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://modelfront.com/compare#machine-translation-libraries', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'b9c8a4a6-aab1-11e8-a149-0e15ac70a998', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_343gn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#dadada', 'id': 'm8csek', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'adammathias', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/machinetranslation/comments/m8csek/opensource_libraries_for_machine_translation/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://modelfront.com/compare#machine-translation-libraries', 'subreddit_subscribers': 464, 'created_utc': 1616142991.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_m8csek,,,0
500,,LanguageTechnology,"Hey!

I made a quick article about Named Entity Recognition (NER). What is it and why is it a useful NLP technique?

[https://nlpcloud.io/nlp-named-entity-recognition-ner-api.html](https://nlpcloud.io/nlp-named-entity-recognition-ner-api.html)

Hope some of you will find it useful.

If you have any comment, please feel free!",t2_4z4m2qcs,False,,0,False,Introduction to Named Entity Recognition (NER),[],r/LanguageTechnology,False,6,,0,,False,t3_m8lsgd,False,dark,0.4,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,1619936177.0,,[],{},,True,,1616202141.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey!&lt;/p&gt;

&lt;p&gt;I made a quick article about Named Entity Recognition (NER). What is it and why is it a useful NLP technique?&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://nlpcloud.io/nlp-named-entity-recognition-ner-api.html""&gt;https://nlpcloud.io/nlp-named-entity-recognition-ner-api.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Hope some of you will find it useful.&lt;/p&gt;

&lt;p&gt;If you have any comment, please feel free!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m8lsgd,True,,juliensalinas,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m8lsgd/introduction_to_named_entity_recognition_ner/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m8lsgd/introduction_to_named_entity_recognition_ner/,30199,1616173341.0,0,,False,,,,,,326
501,,LanguageTechnology,"I want to create BeRT embeddings for a corpus representing a specific subject or topic. I know how to extract embeddings from the general BeRT model and how to work with those embeddings, but I'm curious to see if I can finetune BeRT on a corpus and then extract embeddings that are more attuned to the subject that corpus represents.

Would this involve creating my own language model by applying transfer learning to BeRT?

Basically, I want to know if I can use BeRT to create/fine-tune custom embeddings similar to how this [tutorial shows how to create custom W2V embeddings using gensim](https://machinelearningmastery.com/develop-word-embeddings-python-gensim/).

Thanks.",t2_8iobl,False,,0,False,Corpus Specific Vectors with BeRT,[],r/LanguageTechnology,False,6,,0,,False,t3_m83qqc,False,dark,0.91,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1616138848.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to create BeRT embeddings for a corpus representing a specific subject or topic. I know how to extract embeddings from the general BeRT model and how to work with those embeddings, but I&amp;#39;m curious to see if I can finetune BeRT on a corpus and then extract embeddings that are more attuned to the subject that corpus represents.&lt;/p&gt;

&lt;p&gt;Would this involve creating my own language model by applying transfer learning to BeRT?&lt;/p&gt;

&lt;p&gt;Basically, I want to know if I can use BeRT to create/fine-tune custom embeddings similar to how this &lt;a href=""https://machinelearningmastery.com/develop-word-embeddings-python-gensim/""&gt;tutorial shows how to create custom W2V embeddings using gensim&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m83qqc,True,,Cheesebro69,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m83qqc/corpus_specific_vectors_with_bert/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m83qqc/corpus_specific_vectors_with_bert/,30199,1616110048.0,0,,False,,,,,,678
502,,LanguageTechnology,"So I am trying to make a rules based bot using AIML + Python that can perform task like fill in forms, execute queries, and do a couple simple things like look up the time.

I have trouble finding the docs to the Python-AIML library but the best tutorial is [here](https://www.devdungeon.com/content/ai-chat-bot-python-aiml) 

To execute code from python you put it in as conditional statements inside a while loop, you can see the example for quitting and saving the brn file in the example. I have two questions on how I should organize and structure my code:

1. What would be the best way to structure the following task: 
Match the correct user inputs to run specific task? I can do this by using regex to match words or spacy's matching system for matching intents using cosine similarities. 

2. Create dialog trees for filling in forms? How do I structure file and code for a dialog tree where a user has to answer questions for a specific input like a query or filling a form? Adding a bunch of code inside the while loop can get messy.",t2_erday,False,,0,False,Recommendations for structure for AIML + Python bot,[],r/LanguageTechnology,False,6,,0,,False,t3_m8bi4r,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1616166106.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I am trying to make a rules based bot using AIML + Python that can perform task like fill in forms, execute queries, and do a couple simple things like look up the time.&lt;/p&gt;

&lt;p&gt;I have trouble finding the docs to the Python-AIML library but the best tutorial is &lt;a href=""https://www.devdungeon.com/content/ai-chat-bot-python-aiml""&gt;here&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;To execute code from python you put it in as conditional statements inside a while loop, you can see the example for quitting and saving the brn file in the example. I have two questions on how I should organize and structure my code:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;What would be the best way to structure the following task: 
Match the correct user inputs to run specific task? I can do this by using regex to match words or spacy&amp;#39;s matching system for matching intents using cosine similarities. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create dialog trees for filling in forms? How do I structure file and code for a dialog tree where a user has to answer questions for a specific input like a query or filling a form? Adding a bunch of code inside the while loop can get messy.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m8bi4r,True,,chchan,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m8bi4r/recommendations_for_structure_for_aiml_python_bot/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m8bi4r/recommendations_for_structure_for_aiml_python_bot/,30199,1616137306.0,0,,False,,,,,,1045
503,,LanguageTechnology,Word co-occurrencies? What are these hundreds of dimensions?,t2_4est30d3,False,,0,False,"When I vectorize words, what are vector dimensions actually?",[],r/LanguageTechnology,False,6,,0,,False,t3_m7nwcw,False,dark,0.94,,public,15,0,{},,False,[],,False,False,,{},,False,15,,False,False,,False,,[],{},,True,,1616092070.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Word co-occurrencies? What are these hundreds of dimensions?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m7nwcw,True,,meanwolfpolis,,15,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m7nwcw/when_i_vectorize_words_what_are_vector_dimensions/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m7nwcw/when_i_vectorize_words_what_are_vector_dimensions/,30199,1616063270.0,0,,False,,,,,,60
504,,LanguageTechnology,"Hello ! Spacy isn't that good for that, nltk works but it's quite old. What do you use for sentence tokenization in english ?",t2_wvgmf,False,,0,False,[D] What is currently the best SENTENCE level tokenizer ?,[],r/LanguageTechnology,False,6,,0,,False,t3_m7qal4,False,dark,1.0,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1616101265.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello ! Spacy isn&amp;#39;t that good for that, nltk works but it&amp;#39;s quite old. What do you use for sentence tokenization in english ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m7qal4,True,,Jean-Porte,,13,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m7qal4/d_what_is_currently_the_best_sentence_level/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m7qal4/d_what_is_currently_the_best_sentence_level/,30199,1616072465.0,0,,False,,,,,,125
505,,LanguageTechnology,,t2_2ztuwy1l,False,,0,False,Do ASIC and FPGA have any use in NLP?,[],r/LanguageTechnology,False,6,,0,,False,t3_m7y1wd,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1616122664.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m7y1wd,True,,MoonLaughter,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m7y1wd/do_asic_and_fpga_have_any_use_in_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m7y1wd/do_asic_and_fpga_have_any_use_in_nlp/,30199,1616093864.0,0,,False,,,,,,0
506,,LanguageTechnology,"Hi,

I would like to generate questions out of a text corpus. However, I can only find models (based on T5) that generate exam like questions.

Does anybody know question generation models that create opionion questions that are useable for polls? e.g. ""How do you feel about NLP?""  


Thanks in advance!",t2_sdfge43,False,,0,False,Automatic poll generation with transformers similar to guestion generation?,[],r/LanguageTechnology,False,6,,0,,False,t3_m7slmu,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1616108202.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I would like to generate questions out of a text corpus. However, I can only find models (based on T5) that generate exam like questions.&lt;/p&gt;

&lt;p&gt;Does anybody know question generation models that create opionion questions that are useable for polls? e.g. &amp;quot;How do you feel about NLP?&amp;quot;  &lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m7slmu,True,,ProKellaSK,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m7slmu/automatic_poll_generation_with_transformers/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m7slmu/automatic_poll_generation_with_transformers/,30199,1616079402.0,0,,False,,,,,,304
507,,LanguageTechnology,"Greetings folks,

4 months ago, I started working on a ***machine learning based website which helps readers***

***find exiting content from a book*** or a pdf file without reading all of it.

And now after working hard on it for the past few months, I have come up with a

prototype version. You can upload any pdf and ask questions to get to the interesting part quickly.

Here is the link :

[https://read-what-you-need.firebaseapp.com/](https://read-what-you-need.firebaseapp.com/)

I hope the app saves your time, and fuels your love for reading.

Have a great day ahead!

**ps: here's an account to try the app quickly, set up just for our reddit users**

username: reddit

password: reddit2021

Enjoy !",t2_1lyv6xbt,False,,0,False,AI app to help you read your book smartly,[],r/LanguageTechnology,False,6,,0,,False,t3_m7xpqg,False,dark,0.54,,public,1,1,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1616121739.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Greetings folks,&lt;/p&gt;

&lt;p&gt;4 months ago, I started working on a &lt;strong&gt;&lt;em&gt;machine learning based website which helps readers&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;find exiting content from a book&lt;/em&gt;&lt;/strong&gt; or a pdf file without reading all of it.&lt;/p&gt;

&lt;p&gt;And now after working hard on it for the past few months, I have come up with a&lt;/p&gt;

&lt;p&gt;prototype version. You can upload any pdf and ask questions to get to the interesting part quickly.&lt;/p&gt;

&lt;p&gt;Here is the link :&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://read-what-you-need.firebaseapp.com/""&gt;https://read-what-you-need.firebaseapp.com/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I hope the app saves your time, and fuels your love for reading.&lt;/p&gt;

&lt;p&gt;Have a great day ahead!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ps: here&amp;#39;s an account to try the app quickly, set up just for our reddit users&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;username: reddit&lt;/p&gt;

&lt;p&gt;password: reddit2021&lt;/p&gt;

&lt;p&gt;Enjoy !&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m7xpqg,True,,deep_ak,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m7xpqg/ai_app_to_help_you_read_your_book_smartly/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m7xpqg/ai_app_to_help_you_read_your_book_smartly/,30199,1616092939.0,0,,False,,,,,,710
508,,LanguageTechnology,"Those in the NLP community interested in building language understanding applications used in the healthcare industry should check out this NLP summit event. Learn from and engage with leading experts and a growing community in the space. 

The tickets are free.",t2_az2lu6ev,False,,0,False,"Healthcare NLP Summit, April 8-9",[],r/LanguageTechnology,False,6,,0,,False,t3_m7trsz,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1616111419.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Those in the NLP community interested in building language understanding applications used in the healthcare industry should check out this NLP summit event. Learn from and engage with leading experts and a growing community in the space. &lt;/p&gt;

&lt;p&gt;The tickets are free.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m7trsz,True,,Lopsided-Yam-5361,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m7trsz/healthcare_nlp_summit_april_89/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m7trsz/healthcare_nlp_summit_april_89/,30199,1616082619.0,0,,False,,,,,,262
509,,LanguageTechnology,"Hi everyone, 

i am new to the world of NLP. For an upcoming project I would like to use BERT in order to do sentiment analysis with just the headlines of news articles. But BERT has two training tasks 1. Predict masked words and 2. Next sentence prediction. But since I will use only the headlines of an article the second training task becomes obsolet. 

My question : Is it possible to just train BERT with the 'predict the masked words' task ?

&amp;#x200B;

And maybe an off-topic question: Do you know how difficult it is to make a tokenizer from scratch ?

&amp;#x200B;

Thank you in advance!",t2_4sv7ihod,False,,0,False,BERT pre-training with only masked words,[],r/LanguageTechnology,False,6,,0,,False,t3_m7q6ys,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1616100937.0,text,6,,,text,self.LanguageTechnology,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;

&lt;p&gt;i am new to the world of NLP. For an upcoming project I would like to use BERT in order to do sentiment analysis with just the headlines of news articles. But BERT has two training tasks 1. Predict masked words and 2. Next sentence prediction. But since I will use only the headlines of an article the second training task becomes obsolet. &lt;/p&gt;

&lt;p&gt;My question : Is it possible to just train BERT with the &amp;#39;predict the masked words&amp;#39; task ?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;And maybe an off-topic question: Do you know how difficult it is to make a tokenizer from scratch ?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thank you in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m7q6ys,True,,kengrewlong,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m7q6ys/bert_pretraining_with_only_masked_words/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m7q6ys/bert_pretraining_with_only_masked_words/,30199,1616072137.0,0,,False,,,,,,599
510,,LanguageTechnology,"\[cross posting from /r/MachineLearning\]

I’ve just finished building a little side project of mine - [https://gpu.land/](https://gpu.land/).

**What is it?** Cheap GPU instances in the cloud.

**Why is it awesome?**

* It’s dirt-cheap. You get a Tesla V100 for $0.99/hr, which is 1/3 the cost of AWS/GCP/Azure/\[insert big cloud name\].
* It’s dead simple. It takes 2mins from registration to a launched instance. Instances come pre-installed with everything you need for Deep Learning, including a 1-click Jupyter server.
* It sports a retro, MS-DOS-like look. Because why not:)

I’m a self-taught ML engineer. I built this because when I was starting my ML journey I was totally lost and frustrated by AWS. Hope this saves some of you some nerve cells (and some pennies)!

The most common question I get is - how is this so cheap? The answer is because AWS/GCP are charging you a huge markup and I’m not. In fact I’m charging just enough to break even, and built this project really to give back to community (and to learn some of the tech in the process). 

AMA!",t2_ywn1tz,False,,0,False,My side project: Cloud GPUs for 1/3 the cost of AWS/GCP,[],r/LanguageTechnology,False,6,,0,,False,t3_m74aj5,False,dark,0.97,,public,39,0,{},,False,[],,False,False,,{},,False,39,,False,False,,False,,[],{},,True,,1616027306.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;[cross posting from &lt;a href=""/r/MachineLearning""&gt;/r/MachineLearning&lt;/a&gt;]&lt;/p&gt;

&lt;p&gt;I’ve just finished building a little side project of mine - &lt;a href=""https://gpu.land/""&gt;https://gpu.land/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What is it?&lt;/strong&gt; Cheap GPU instances in the cloud.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why is it awesome?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It’s dirt-cheap. You get a Tesla V100 for $0.99/hr, which is 1/3 the cost of AWS/GCP/Azure/[insert big cloud name].&lt;/li&gt;
&lt;li&gt;It’s dead simple. It takes 2mins from registration to a launched instance. Instances come pre-installed with everything you need for Deep Learning, including a 1-click Jupyter server.&lt;/li&gt;
&lt;li&gt;It sports a retro, MS-DOS-like look. Because why not:)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I’m a self-taught ML engineer. I built this because when I was starting my ML journey I was totally lost and frustrated by AWS. Hope this saves some of you some nerve cells (and some pennies)!&lt;/p&gt;

&lt;p&gt;The most common question I get is - how is this so cheap? The answer is because AWS/GCP are charging you a huge markup and I’m not. In fact I’m charging just enough to break even, and built this project really to give back to community (and to learn some of the tech in the process). &lt;/p&gt;

&lt;p&gt;AMA!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m74aj5,True,,xepo3abp,,15,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m74aj5/my_side_project_cloud_gpus_for_13_the_cost_of/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m74aj5/my_side_project_cloud_gpus_for_13_the_cost_of/,30199,1615998506.0,0,,False,,,,,,1067
511,,LanguageTechnology,"Hi there, 

I am preprocessing some reddit submission downloaded from Pushshift, which I'll later feed to a Bert sentiment encoder.

Is there any predefined pipeline (possibly in Python) for which preprocessing steps to perform (e.g., stripping html tags, etc.)?

Thank you in advance!",t2_ahn5elmr,False,,0,False,preprocessing reddit submissions in python (html tag stripping etc...),[],r/LanguageTechnology,False,6,,0,,False,t3_m7pmtx,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1616098994.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there, &lt;/p&gt;

&lt;p&gt;I am preprocessing some reddit submission downloaded from Pushshift, which I&amp;#39;ll later feed to a Bert sentiment encoder.&lt;/p&gt;

&lt;p&gt;Is there any predefined pipeline (possibly in Python) for which preprocessing steps to perform (e.g., stripping html tags, etc.)?&lt;/p&gt;

&lt;p&gt;Thank you in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m7pmtx,True,,Dazzling-Schedule872,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m7pmtx/preprocessing_reddit_submissions_in_python_html/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m7pmtx/preprocessing_reddit_submissions_in_python_html/,30199,1616070194.0,0,,False,,,,,,285
512,,LanguageTechnology,"Hey, I've created a tutorial on how to create categorical variables based on integers and numerical ranges using the R programming language: [https://statisticsglobe.com/create-categories-based-on-integer-and-numeric-range-r](https://statisticsglobe.com/create-categories-based-on-integer-and-numeric-range-r)",t2_77cigax1,False,,0,False,Tutorial on how to create categorical variables based on integers and numerical ranges,[],r/LanguageTechnology,False,6,,0,,False,t3_m7lc3s,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1616080252.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, I&amp;#39;ve created a tutorial on how to create categorical variables based on integers and numerical ranges using the R programming language: &lt;a href=""https://statisticsglobe.com/create-categories-based-on-integer-and-numeric-range-r""&gt;https://statisticsglobe.com/create-categories-based-on-integer-and-numeric-range-r&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m7lc3s,True,,JoachimSchork,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m7lc3s/tutorial_on_how_to_create_categorical_variables/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m7lc3s/tutorial_on_how_to_create_categorical_variables/,30199,1616051452.0,0,,False,,,,,,309
513,,LanguageTechnology,"Hi all,

With only a little experience on NLP, I was wondering if you guys could help me out.  
From a list of keywords/subclasses/objects, I have to extract all relevant elements that are closely related to a certain topic/class.

So, let's say we have the following topic/class as input: ""Human"".  
Then, ideally, I would like to extract the following keywords:

1. Synonyms, i.e. {Person, Individual, Child} (This should be relatively simple with NLTK/WordNet)
2. Subclass and/or profession, i.e. {Man, Girl, Boy, Grandma, Cousin, Chef, Professor}
3. Body parts, i.e. {Arm, Leg, Knee, Eyes}

One way to deal with this, is to hard code each different subclass or body part and obtain their synonyms using synset. But for obvious reasons, this is not desireable.

Therefore, I was wondering if there are any tips/sources how to effectively approach this operation (preferably in Python)?

Thanks in advance.",t2_jr1qy,False,,0,False,Extracting related keywords from a specific topic/class,[],r/LanguageTechnology,False,6,,0,,False,t3_m7exk1,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,1616028921.0,,[],{},,True,,1616056448.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;With only a little experience on NLP, I was wondering if you guys could help me out.&lt;br/&gt;
From a list of keywords/subclasses/objects, I have to extract all relevant elements that are closely related to a certain topic/class.&lt;/p&gt;

&lt;p&gt;So, let&amp;#39;s say we have the following topic/class as input: &amp;quot;Human&amp;quot;.&lt;br/&gt;
Then, ideally, I would like to extract the following keywords:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Synonyms, i.e. {Person, Individual, Child} (This should be relatively simple with NLTK/WordNet)&lt;/li&gt;
&lt;li&gt;Subclass and/or profession, i.e. {Man, Girl, Boy, Grandma, Cousin, Chef, Professor}&lt;/li&gt;
&lt;li&gt;Body parts, i.e. {Arm, Leg, Knee, Eyes}&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;One way to deal with this, is to hard code each different subclass or body part and obtain their synonyms using synset. But for obvious reasons, this is not desireable.&lt;/p&gt;

&lt;p&gt;Therefore, I was wondering if there are any tips/sources how to effectively approach this operation (preferably in Python)?&lt;/p&gt;

&lt;p&gt;Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m7exk1,True,,banmyhit,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m7exk1/extracting_related_keywords_from_a_specific/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m7exk1/extracting_related_keywords_from_a_specific/,30199,1616027648.0,0,,False,,,,,,908
514,,LanguageTechnology,"Since I am new to NLP, I would like to run something by this community. I appreciate in advance any pointers you may provide.

I am using Stanza to compute the sentiment of a tweet. As far as I can tell, Stanza can only compute the sentiment of one sentence at a time. So I devised the following simple computation to aggregate the sentiment of a whole tweet:

1. Get list of sentiments for each sentence as -1 (negative), 0 (neutral), and 1 (positive).

2. Sum up the sentiments.

3. If it’s &lt;= -1, the overall sentiment is negative, if it’s &gt;= 1, it’s positive, otherwise it’s neutral.

It’s a straightforward calculation where neutral sentences don’t affect the outcome, and whichever sentiment appears on the largest number of sentences will determine the overall sentiment of a tweet. Otherwise it’s a tie and the tweet is neutral. Does this make sense?

In case this method is not suitable, I was wondering if anyone had any suggestions on tried-and-tested methods to aggregate the sentiment of a text from its sentences.",t2_ihpak,False,,0,False,How to aggregate overall text sentiment from sentences?,[],r/LanguageTechnology,False,6,,0,,False,t3_m79wow,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1616041952.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Since I am new to NLP, I would like to run something by this community. I appreciate in advance any pointers you may provide.&lt;/p&gt;

&lt;p&gt;I am using Stanza to compute the sentiment of a tweet. As far as I can tell, Stanza can only compute the sentiment of one sentence at a time. So I devised the following simple computation to aggregate the sentiment of a whole tweet:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Get list of sentiments for each sentence as -1 (negative), 0 (neutral), and 1 (positive).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sum up the sentiments.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If it’s &amp;lt;= -1, the overall sentiment is negative, if it’s &amp;gt;= 1, it’s positive, otherwise it’s neutral.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It’s a straightforward calculation where neutral sentences don’t affect the outcome, and whichever sentiment appears on the largest number of sentences will determine the overall sentiment of a tweet. Otherwise it’s a tie and the tweet is neutral. Does this make sense?&lt;/p&gt;

&lt;p&gt;In case this method is not suitable, I was wondering if anyone had any suggestions on tried-and-tested methods to aggregate the sentiment of a text from its sentences.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m79wow,True,,vitorsb,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m79wow/how_to_aggregate_overall_text_sentiment_from/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m79wow/how_to_aggregate_overall_text_sentiment_from/,30199,1616013152.0,0,,False,,,,,,1033
515,,LanguageTechnology,"Hi NLP folks,

I have create a new model focused mainly on smaller size with decent performance. And my experiments with GLUE proved that Joint Loss is all we need. With just  **14 million parameters and half the computation ( 6 layers instead of 12 layers** , we got a **GLUE score of 81.0**, which is **4 points** ahead of **DistillBERT** which has **60 million parameters** and **requires more training ( may be few days in a single GPU )**. The **Albert-Joint** model is even better than **MobileBERT.**

You can read more about that 

[https://www.reddit.com/r/LanguageTechnology/comments/m5j2hi/tftransformers\_state\_of\_the\_art\_faster\_nlp\_in/](https://www.reddit.com/r/LanguageTechnology/comments/m5j2hi/tftransformers_state_of_the_art_faster_nlp_in/)

For Code and Models

[https://github.com/legacyai/tf-transformers](https://github.com/legacyai/tf-transformers)

[Benchmarks](https://github.com/legacyai/tf-transformers/blob/main/src/tf_transformers/notebooks/tutorials/joint_loss_experiments/glue/glue_benchmark.png)

&amp;#x200B;

Please share feedback, comments, and raise if any issues in Github

                                                    glue_score
    Abert-Joint-layer_0	                        0.504815
    Abert-Joint-layer_1	                        0.682751
    Abert-Joint-layer_2	                        0.743739
    Abert-Joint-layer_3	                        0.773670
    Abert-Joint-layer_4	                        0.798181
    Abert-Joint-layer_5	                        0.810039
    Abert-Joint-layer_6	                        0.813973
    Abert-Joint-layer_7	                        0.822181
    Abert-Joint-layer_8	                        0.823916
    Abert-Joint-layer_9	                        0.823932
    Abert-Joint-layer_10	                        0.827925
    Abert-Joint-layer_11	                        0.821628
    DistillBert	                                0.776625",t2_87tdfx60,False,,0,False,Do we really need to Dstill Language Models? Joint loss is all we need - Albert-Joint .,[],r/LanguageTechnology,False,6,,0,,False,t3_m6svod,False,dark,0.93,,public,25,0,{},,False,[],,False,False,,{},,False,25,,False,False,,False,,[],{},,True,,1615986792.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi NLP folks,&lt;/p&gt;

&lt;p&gt;I have create a new model focused mainly on smaller size with decent performance. And my experiments with GLUE proved that Joint Loss is all we need. With just  &lt;strong&gt;14 million parameters and half the computation ( 6 layers instead of 12 layers&lt;/strong&gt; , we got a &lt;strong&gt;GLUE score of 81.0&lt;/strong&gt;, which is &lt;strong&gt;4 points&lt;/strong&gt; ahead of &lt;strong&gt;DistillBERT&lt;/strong&gt; which has &lt;strong&gt;60 million parameters&lt;/strong&gt; and &lt;strong&gt;requires more training ( may be few days in a single GPU )&lt;/strong&gt;. The &lt;strong&gt;Albert-Joint&lt;/strong&gt; model is even better than &lt;strong&gt;MobileBERT.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You can read more about that &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.reddit.com/r/LanguageTechnology/comments/m5j2hi/tftransformers_state_of_the_art_faster_nlp_in/""&gt;https://www.reddit.com/r/LanguageTechnology/comments/m5j2hi/tftransformers_state_of_the_art_faster_nlp_in/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For Code and Models&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/legacyai/tf-transformers""&gt;https://github.com/legacyai/tf-transformers&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/legacyai/tf-transformers/blob/main/src/tf_transformers/notebooks/tutorials/joint_loss_experiments/glue/glue_benchmark.png""&gt;Benchmarks&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Please share feedback, comments, and raise if any issues in Github&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                                                glue_score
Abert-Joint-layer_0                         0.504815
Abert-Joint-layer_1                         0.682751
Abert-Joint-layer_2                         0.743739
Abert-Joint-layer_3                         0.773670
Abert-Joint-layer_4                         0.798181
Abert-Joint-layer_5                         0.810039
Abert-Joint-layer_6                         0.813973
Abert-Joint-layer_7                         0.822181
Abert-Joint-layer_8                         0.823916
Abert-Joint-layer_9                         0.823932
Abert-Joint-layer_10                            0.827925
Abert-Joint-layer_11                            0.821628
DistillBert                                 0.776625
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m6svod,True,,aintnosunshine2,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m6svod/do_we_really_need_to_dstill_language_models_joint/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m6svod/do_we_really_need_to_dstill_language_models_joint/,30199,1615957992.0,0,,False,,,,,,1921
516,,LanguageTechnology,"Hi everyone, 

I'm considering applying for this masters for September entry this year. I've had it on my mind for a few months, and really can't decide if I should focus on studying for my undergrad finals right now and wait to apply next year, or apply now so I can start asap. 

My questions: how long did it take you to write your personal statement? What did it contain? I would really appreciate any guidance! Were you interviewed?

And do you think this course will be much harder coming from a linguistics background rather than STEM? I haven't done any programming beyond tinkering in my spare time.

thank you - any help is really appreciated",t2_8lh64x5u,False,,0,False,Edinburgh MSc Speech &amp; Language Processing,[],r/LanguageTechnology,False,6,,0,,False,t3_m6yih7,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1616010068.0,text,6,,,text,self.LanguageTechnology,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;

&lt;p&gt;I&amp;#39;m considering applying for this masters for September entry this year. I&amp;#39;ve had it on my mind for a few months, and really can&amp;#39;t decide if I should focus on studying for my undergrad finals right now and wait to apply next year, or apply now so I can start asap. &lt;/p&gt;

&lt;p&gt;My questions: how long did it take you to write your personal statement? What did it contain? I would really appreciate any guidance! Were you interviewed?&lt;/p&gt;

&lt;p&gt;And do you think this course will be much harder coming from a linguistics background rather than STEM? I haven&amp;#39;t done any programming beyond tinkering in my spare time.&lt;/p&gt;

&lt;p&gt;thank you - any help is really appreciated&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m6yih7,True,,Affectionate_Mood921,,12,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m6yih7/edinburgh_msc_speech_language_processing/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m6yih7/edinburgh_msc_speech_language_processing/,30199,1615981268.0,0,,False,,,,,,652
517,,LanguageTechnology,"An open-source project, where you can learn more about NLP pipelines while looking at an impactful use case.  The project was done in collaboration with a group at Stanford University, Code for Africa, WRI, and 40+ individual collaborators. 

From collecting and preparing more than 32.000 notices, legal entities, and court documents to build a web-based dashboard displaying land ownership in Kenya. The purpose of this project is to boost Kenya’s efforts to restore degraded land in an equitable way.  
https://omdena.com/blog/identifying-land-ownership/",t2_69ezr99,False,,0,False,An end-to-end NLP Pipeline Tutorial Applied to Landscape Restoration in Kenya,[],r/LanguageTechnology,False,6,,0,,False,t3_m72w2q,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1616023554.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;An open-source project, where you can learn more about NLP pipelines while looking at an impactful use case.  The project was done in collaboration with a group at Stanford University, Code for Africa, WRI, and 40+ individual collaborators. &lt;/p&gt;

&lt;p&gt;From collecting and preparing more than 32.000 notices, legal entities, and court documents to build a web-based dashboard displaying land ownership in Kenya. The purpose of this project is to boost Kenya’s efforts to restore degraded land in an equitable way.&lt;br/&gt;
&lt;a href=""https://omdena.com/blog/identifying-land-ownership/""&gt;https://omdena.com/blog/identifying-land-ownership/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m72w2q,True,,Lordobba,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m72w2q/an_endtoend_nlp_pipeline_tutorial_applied_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m72w2q/an_endtoend_nlp_pipeline_tutorial_applied_to/,30199,1615994754.0,0,,False,,,,,,557
518,,LanguageTechnology,"I was wondering if there are any solutions to using coreference resolution in spaCy 3.0 currently as neuralcoref has not been updated for it yet. I unfortunately can’t wait as I it is for a class project.

It is just a small part of the pipeline and I am using it to help with with a question generation system. So if there is a paragraph like “John went to the cleaners. He also ate dinner.” I could link He to John to create the question “Did John eat dinner”. I want to use 3.0 as it sounds like it has improved dependency parsing in English which is essential. Any help is appreciated thanks!",t2_r6pwgyr,False,,0,False,Coreference resolution with spaCy 3.0,[],r/LanguageTechnology,False,6,,0,,False,t3_m77loy,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1616035858.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was wondering if there are any solutions to using coreference resolution in spaCy 3.0 currently as neuralcoref has not been updated for it yet. I unfortunately can’t wait as I it is for a class project.&lt;/p&gt;

&lt;p&gt;It is just a small part of the pipeline and I am using it to help with with a question generation system. So if there is a paragraph like “John went to the cleaners. He also ate dinner.” I could link He to John to create the question “Did John eat dinner”. I want to use 3.0 as it sounds like it has improved dependency parsing in English which is essential. Any help is appreciated thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m77loy,True,,jei987,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m77loy/coreference_resolution_with_spacy_30/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m77loy/coreference_resolution_with_spacy_30/,30199,1616007058.0,0,,False,,,,,,596
519,,LanguageTechnology,"Google Colab is an undeniably popular choice for free GPU-backed Jupyter notebooks for deep learning projects, but it's not without its drawbacks.

This article discusses alternate sources of free GPUs in cloud-hosted Jupyter environments:

[https://blog.paperspace.com/best-google-colab-alternatives/](https://blog.paperspace.com/best-google-colab-alternatives/)

Really curious what others' experiences with Google Colab are like, and if any of these issues resonate. Also interested in what the community has to say about the alternates presented here!",t2_15en0l,False,,0,False,Free GPU alternatives to Google Colab for ML/DL,[],r/LanguageTechnology,False,6,,0,,False,t3_m74blu,False,dark,0.43,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1616027388.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Google Colab is an undeniably popular choice for free GPU-backed Jupyter notebooks for deep learning projects, but it&amp;#39;s not without its drawbacks.&lt;/p&gt;

&lt;p&gt;This article discusses alternate sources of free GPUs in cloud-hosted Jupyter environments:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://blog.paperspace.com/best-google-colab-alternatives/""&gt;https://blog.paperspace.com/best-google-colab-alternatives/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Really curious what others&amp;#39; experiences with Google Colab are like, and if any of these issues resonate. Also interested in what the community has to say about the alternates presented here!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m74blu,True,,hellopaperspace,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m74blu/free_gpu_alternatives_to_google_colab_for_mldl/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m74blu/free_gpu_alternatives_to_google_colab_for_mldl/,30199,1615998588.0,0,,False,,,,,,555
520,,LanguageTechnology,"I'm new to the field of natural language processing so please forgive me if I say something incorrectly. I've been reading information and running some basic experiments on the task of Named Entity Recognition. I've seen a wide range of definitions on what constitutes a ""named entity"" including the following list: [https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119268567.app5#:\~:text=BUSINESS%20ANALYTICS%3A%20%E2%80%9CIn%20data%20mining,phone%20numbers%2C%20companies%20and%20addresses](https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119268567.app5#:~:text=BUSINESS%20ANALYTICS%3A%20%E2%80%9CIn%20data%20mining,phone%20numbers%2C%20companies%20and%20addresses).

Since there is not exact agreement on what constitutes a named entity, have there been any studies on optional entities? 

In my basic experiments I wanted to build familiarity with some of the different tools that are available so I tagged some text data that I have and compared the results of the different tools. But as I was tagging I had some difficulty deciding whether certain noun phrases should be considered named entities or not. For the time being, I created an OPTIONAL tag and used that for entities where they fit some definitions of named entity but not others. I wrote a simple script that would score the list of named entities returned by each tool against my taggings where OPTIONAL entities are skipped - neither counted as correct or incorrect. And I can imagine real scenarios where I might not care whether a tool counts certain noun phrases as entities or non-entities. 

I was wondering if there are more sophisticated analyses of these kinds of scenarios? Has this been explored? If so, can anyone point me to resources where I can learn more about it?

Thanks!",t2_7nbosy83,False,,0,False,Optional Entities in Named Entity Recognition?,[],r/LanguageTechnology,False,6,,0,,False,t3_m6z3bo,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1616012191.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m new to the field of natural language processing so please forgive me if I say something incorrectly. I&amp;#39;ve been reading information and running some basic experiments on the task of Named Entity Recognition. I&amp;#39;ve seen a wide range of definitions on what constitutes a &amp;quot;named entity&amp;quot; including the following list: &lt;a href=""https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119268567.app5#:%7E:text=BUSINESS%20ANALYTICS%3A%20%E2%80%9CIn%20data%20mining,phone%20numbers%2C%20companies%20and%20addresses""&gt;https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119268567.app5#:~:text=BUSINESS%20ANALYTICS%3A%20%E2%80%9CIn%20data%20mining,phone%20numbers%2C%20companies%20and%20addresses&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Since there is not exact agreement on what constitutes a named entity, have there been any studies on optional entities? &lt;/p&gt;

&lt;p&gt;In my basic experiments I wanted to build familiarity with some of the different tools that are available so I tagged some text data that I have and compared the results of the different tools. But as I was tagging I had some difficulty deciding whether certain noun phrases should be considered named entities or not. For the time being, I created an OPTIONAL tag and used that for entities where they fit some definitions of named entity but not others. I wrote a simple script that would score the list of named entities returned by each tool against my taggings where OPTIONAL entities are skipped - neither counted as correct or incorrect. And I can imagine real scenarios where I might not care whether a tool counts certain noun phrases as entities or non-entities. &lt;/p&gt;

&lt;p&gt;I was wondering if there are more sophisticated analyses of these kinds of scenarios? Has this been explored? If so, can anyone point me to resources where I can learn more about it?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m6z3bo,True,,Turing-test-failure,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m6z3bo/optional_entities_in_named_entity_recognition/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m6z3bo/optional_entities_in_named_entity_recognition/,30199,1615983391.0,0,,False,,,,,,1767
521,,LanguageTechnology,"Hi, I'm very interested in Variational Autoencoders (VAEs) for text generation but struggle to find the current state of the art method. Does someone know a reference?

At the moment all I can find for basically any nlp task are these huge uninterpretable models from Google. Is any research towards anything else dead?

Is there an example of transformers being used for the encoder-decoder parts of a VAE?",t2_gs5nviq,False,,0,False,VAEs for text generation,[],r/LanguageTechnology,False,6,,0,,False,t3_m6vsl5,False,dark,0.83,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1615998996.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m very interested in Variational Autoencoders (VAEs) for text generation but struggle to find the current state of the art method. Does someone know a reference?&lt;/p&gt;

&lt;p&gt;At the moment all I can find for basically any nlp task are these huge uninterpretable models from Google. Is any research towards anything else dead?&lt;/p&gt;

&lt;p&gt;Is there an example of transformers being used for the encoder-decoder parts of a VAE?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m6vsl5,True,,Doodah249,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m6vsl5/vaes_for_text_generation/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m6vsl5/vaes_for_text_generation/,30199,1615970196.0,0,,False,,,,,,407
522,,LanguageTechnology,"Good morning all! I'm going to try and keep up these Data Science of the Day posts. I think they are fun to share and the feedback has been great so far.

Yesterday I talked to [u/drekalo](https://www.reddit.com/u/drekalo/) in my [r/datascience](https://www.reddit.com/r/datascience/) [post](https://www.reddit.com/r/datascience/comments/m6axq4/embed_your_sql_query_into_your_python_code_and/)(I think might have gotten removed) but I posted it in a few other subs too, here is the [post](https://www.reddit.com/r/MachineLearning/comments/m6b047/p_embed_your_sql_query_into_your_python_code_and/) in [r/MachineLearning](https://www.reddit.com/r/MachineLearning/). We talked a bit about RAPIDS, which is an Nvidia library for accelerated data engineering/science (it's quit awesome). Today I want to shed light on another [RAPIDS](https://rapids.ai/) ability and that's doing classic ML algorithms **FAST**. This article talks about K-Nearest Neighbors (KNN) probably the most well known ML algorithm there is. One of Nvidia's Kaggle Grandmasters wrote this great [article](https://forums.developer.nvidia.com/t/with-gpus-k-nearest-neighbor-algorithm-crosses-the-finish-line-when-others-are-in-the-starting-blocks/168785?u=kasmith) on how RAPIDS accelerates KNN **600x** versus CPU. This speed up might be what many of you need to help get a benchmark on large or complicated data sets, OR get a medal in Kaggle! It's rudimentary, and not as attractive as a DL Algo like BERT, but you can use KNN for text classification to get that first step.

&amp;#x200B;",,False,,0,False,"With GPUs, K-nearest Neighbor Algorithm Crosses the Finish Line When Others are in the Starting Blocks",[],r/LanguageTechnology,False,6,,0,,False,t3_m6zqym,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,,,1615990498.0,,,{},,True,,1616014484.0,text,6,,,,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Good morning all! I&amp;#39;m going to try and keep up these Data Science of the Day posts. I think they are fun to share and the feedback has been great so far.&lt;/p&gt;

&lt;p&gt;Yesterday I talked to &lt;a href=""https://www.reddit.com/u/drekalo/""&gt;u/drekalo&lt;/a&gt; in my &lt;a href=""https://www.reddit.com/r/datascience/""&gt;r/datascience&lt;/a&gt; &lt;a href=""https://www.reddit.com/r/datascience/comments/m6axq4/embed_your_sql_query_into_your_python_code_and/""&gt;post&lt;/a&gt;(I think might have gotten removed) but I posted it in a few other subs too, here is the &lt;a href=""https://www.reddit.com/r/MachineLearning/comments/m6b047/p_embed_your_sql_query_into_your_python_code_and/""&gt;post&lt;/a&gt; in &lt;a href=""https://www.reddit.com/r/MachineLearning/""&gt;r/MachineLearning&lt;/a&gt;. We talked a bit about RAPIDS, which is an Nvidia library for accelerated data engineering/science (it&amp;#39;s quit awesome). Today I want to shed light on another &lt;a href=""https://rapids.ai/""&gt;RAPIDS&lt;/a&gt; ability and that&amp;#39;s doing classic ML algorithms &lt;strong&gt;FAST&lt;/strong&gt;. This article talks about K-Nearest Neighbors (KNN) probably the most well known ML algorithm there is. One of Nvidia&amp;#39;s Kaggle Grandmasters wrote this great &lt;a href=""https://forums.developer.nvidia.com/t/with-gpus-k-nearest-neighbor-algorithm-crosses-the-finish-line-when-others-are-in-the-starting-blocks/168785?u=kasmith""&gt;article&lt;/a&gt; on how RAPIDS accelerates KNN &lt;strong&gt;600x&lt;/strong&gt; versus CPU. This speed up might be what many of you need to help get a benchmark on large or complicated data sets, OR get a medal in Kaggle! It&amp;#39;s rudimentary, and not as attractive as a DL Algo like BERT, but you can use KNN for text classification to get that first step.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m6zqym,True,,[deleted],,0,True,all_ads,False,[],,dark,/r/LanguageTechnology/comments/m6zqym/with_gpus_knearest_neighbor_algorithm_crosses_the/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m6zqym/with_gpus_knearest_neighbor_algorithm_crosses_the/,30199,1615985684.0,0,,False,,,,,,1557
523,,LanguageTechnology,,t2_pc6ycvi,False,,0,False,"Language Identification using XGBoost. Code for training and application of a language identification model. Trained on the WiLI-2018 database, the classifier achieves an accuracy of 85.97% on the WiLi test dataset for 235 languages.",[],r/LanguageTechnology,False,6,,0,,False,t3_m6ycet,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,False,,1616009460.0,text,6,,,text,github.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m6ycet,True,,scientist_1337,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m6ycet/language_identification_using_xgboost_code_for/,all_ads,False,https://github.com/PhilWicke/Language_Identifier,30199,1615980660.0,0,,False,https://github.com/PhilWicke/Language_Identifier,,,,,0
524,,LanguageTechnology,"I can understand if there are two inputs, like in next sentence prediction, where the inputs need to be separated. But why do we have a 2nd separator token at the end. 

I am especially curious about cases where there is a single input. And in that case, is a CLS token even needed?",t2_1bqxd32j,False,,0,False,Why do we need a [SEP] token needed for the end of the input for a transformer model?,[],r/LanguageTechnology,False,6,,0,,False,t3_m6urd0,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1615994590.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I can understand if there are two inputs, like in next sentence prediction, where the inputs need to be separated. But why do we have a 2nd separator token at the end. &lt;/p&gt;

&lt;p&gt;I am especially curious about cases where there is a single input. And in that case, is a CLS token even needed?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m6urd0,True,,AdditionalWay,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m6urd0/why_do_we_need_a_sep_token_needed_for_the_end_of/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m6urd0/why_do_we_need_a_sep_token_needed_for_the_end_of/,30199,1615965790.0,0,,False,,,,,,282
525,,LanguageTechnology,"Apologies, I realize this is tangentially related to this subreddit, but the TTS and NLP community is small and (perhaps I am wrong) fairly overlapped. Just looking to hear about experiences using one over another. Trying to use this on very low resource languages with a very small linguistics team for commercial purposes, so apologies for being vague. We initially tried Festival/FestVox before realizing that calling that 'a hot mess' is a serious understatement.",t2_3fo8uwa3,False,,0,False,Anyone have experience doing TTS with Mozzilla TTS or Ossian (especially for low-resource languages)?,[],r/LanguageTechnology,False,6,,0,,False,t3_m6m8xm,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1615965478.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Apologies, I realize this is tangentially related to this subreddit, but the TTS and NLP community is small and (perhaps I am wrong) fairly overlapped. Just looking to hear about experiences using one over another. Trying to use this on very low resource languages with a very small linguistics team for commercial purposes, so apologies for being vague. We initially tried Festival/FestVox before realizing that calling that &amp;#39;a hot mess&amp;#39; is a serious understatement.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m6m8xm,True,,wutzvill,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m6m8xm/anyone_have_experience_doing_tts_with_mozzilla/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m6m8xm/anyone_have_experience_doing_tts_with_mozzilla/,30199,1615936678.0,0,,False,,,,,,467
526,,LanguageTechnology,,,False,,0,False,Confusion choosing between two CS PhD programs! Please help! 🙏,[],r/LanguageTechnology,False,6,,0,,False,t3_m6kww9,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,,,False,,,{},,False,,1615961642.0,text,6,,,,self.gradadmissions,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m6kww9,True,,[deleted],,3,False,all_ads,False,[],,dark,/r/LanguageTechnology/comments/m6kww9/confusion_choosing_between_two_cs_phd_programs/,all_ads,False,/r/gradadmissions/comments/m59zpc/confusion_choosing_between_two_cs_phd_programs/,30199,1615932842.0,0,,False,/r/gradadmissions/comments/m59zpc/confusion_choosing_between_two_cs_phd_programs/,"[{'approved_at_utc': None, 'subreddit': 'gradadmissions', 'selftext': 'Both offered full funding with fellowships! Any comments would be highly appreciated!\n\nThese two are NLP PhD programs.\n\n[View Poll](https://www.reddit.com/poll/m59zpc)', 'user_reports': [], 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Confusion choosing between two CS PhD programs! Please help! 🙏', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/gradadmissions', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_m59zpc', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.36, 'author_flair_background_color': '', 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'thumbnail': '', 'edited': 1615987753.0, 'author_flair_css_class': None, 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1615802065.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'domain': 'self.gradadmissions', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Both offered full funding with fellowships! Any comments would be highly appreciated!&lt;/p&gt;\n\n&lt;p&gt;These two are NLP PhD programs.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.reddit.com/poll/m59zpc""&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2tn62', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'm59zpc', 'is_robot_indexable': True, 'report_reasons': None, 'author': '[deleted]', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'poll_data': {'user_won_amount': None, 'tournament_id': None, 'voting_end_timestamp': 1616378065771, 'options': [{'text': 'University of Alberta, Canada', 'vote_count': 76, 'id': '6621124'}, {'text': 'University of Utah, USA', 'vote_count': 31, 'id': '6621125'}], 'user_selection': None, 'is_prediction': False, 'resolved_option_id': None, 'total_vote_count': 107, 'total_stake_amount': None}, 'author_flair_text_color': 'dark', 'permalink': '/r/gradadmissions/comments/m59zpc/confusion_choosing_between_two_cs_phd_programs/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'mod_reports': [], 'url': 'https://www.reddit.com/r/gradadmissions/comments/m59zpc/confusion_choosing_between_two_cs_phd_programs/', 'subreddit_subscribers': 119417, 'created_utc': 1615773265.0, 'num_crossposts': 3, 'media': None, 'is_video': False}]",t3_m59zpc,,,0
527,,LanguageTechnology,"I am trying to find the difference between the following examples:

&gt;I am writing this sentence on a Reddit thread.  
&gt;  
&gt;I am right in this sentence on a ready thread.

The output should be something like

&gt;I am --- this sentence on a --- thread

or

&gt;{""writing"" : ""right in"", ""Reddit"" : ""ready""}

I had a very uneducated idea of looping each word from one of the sentences against the other one. Then define a threshold of how many words to go forward in hope of finding the exact word in the second sentence (3-4 words for example) and give up if there is no occurrence. However this method falls short on many points.


Is this a known NLP task? If so, I would appreciate to know what is the keyword for this problem.

I am wondering if someone know/used/developed some algorithm/package/functionality for a similar task? I try to keep my development in Python, so I would appreciate Python suggestions as well as any algorithmic suggestions.


Thanks!

Edit: Wow people, thanks a lot! All of the responses were helpful and creative. I have found my solution and leaving this post here for others who are looking for similar solutions

Edit 2: Come on, guys. What's up with down voting all these people's comments? All these people were trying to help and I can tell they spend time on it. Please let's respect it, let's discuss if there is wrong information and let's be supportive to each other. We are here to share and grow, right?",t2_qovg5,False,,0,False,Finding the distance between two sentences that that share mostly the same words.,[],r/LanguageTechnology,False,6,,0,,False,t3_m681fh,False,dark,0.75,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,1615967790.0,,[],{},,True,,1615926857.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to find the difference between the following examples:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I am writing this sentence on a Reddit thread.  &lt;/p&gt;

&lt;p&gt;I am right in this sentence on a ready thread.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The output should be something like&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I am --- this sentence on a --- thread&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;{&amp;quot;writing&amp;quot; : &amp;quot;right in&amp;quot;, &amp;quot;Reddit&amp;quot; : &amp;quot;ready&amp;quot;}&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I had a very uneducated idea of looping each word from one of the sentences against the other one. Then define a threshold of how many words to go forward in hope of finding the exact word in the second sentence (3-4 words for example) and give up if there is no occurrence. However this method falls short on many points.&lt;/p&gt;

&lt;p&gt;Is this a known NLP task? If so, I would appreciate to know what is the keyword for this problem.&lt;/p&gt;

&lt;p&gt;I am wondering if someone know/used/developed some algorithm/package/functionality for a similar task? I try to keep my development in Python, so I would appreciate Python suggestions as well as any algorithmic suggestions.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;

&lt;p&gt;Edit: Wow people, thanks a lot! All of the responses were helpful and creative. I have found my solution and leaving this post here for others who are looking for similar solutions&lt;/p&gt;

&lt;p&gt;Edit 2: Come on, guys. What&amp;#39;s up with down voting all these people&amp;#39;s comments? All these people were trying to help and I can tell they spend time on it. Please let&amp;#39;s respect it, let&amp;#39;s discuss if there is wrong information and let&amp;#39;s be supportive to each other. We are here to share and grow, right?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m681fh,True,,hichoshanemi,,11,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m681fh/finding_the_distance_between_two_sentences_that/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m681fh/finding_the_distance_between_two_sentences_that/,30199,1615898057.0,0,,False,,,,,,1455
528,,LanguageTechnology,"Hello, fellow NLPers,  


I am currently working on a low-resource language that practically has no raw corpus whatsoever.  The language in question is an Arabic dialect and it's mainly used on social media or for chatting. Consequently, I was forced to collect data from public Facebook pages in order to build a raw corpus for such dialect. The purpose behind this is to build a general transformer language model (such as BERT), that can then be used/fine-tuned on other specific tasks. The issue I have is the legality and ethicality of publishing such corpus on a scientific paper. In short here are few questions that I need some help with:  
1. Is it legal/ethically correct to just publish the corpus as is for research purposes?  
2. If not, is it possible to instead apply some preprocessing techniques to the corpus in a way that makes this ethical?  
3. Are there any publications that you know of that have done something similar or related to collecting data from Facebook and so forth?  


Any information you can provide is very appreciated.

Thanks a lot and have a great day.",t2_429oy0lo,False,,0,False,Facebook comments to build a language model,[],r/LanguageTechnology,False,6,,0,,False,t3_m62ma4,False,dark,0.92,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,True,,1615904482.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, fellow NLPers,  &lt;/p&gt;

&lt;p&gt;I am currently working on a low-resource language that practically has no raw corpus whatsoever.  The language in question is an Arabic dialect and it&amp;#39;s mainly used on social media or for chatting. Consequently, I was forced to collect data from public Facebook pages in order to build a raw corpus for such dialect. The purpose behind this is to build a general transformer language model (such as BERT), that can then be used/fine-tuned on other specific tasks. The issue I have is the legality and ethicality of publishing such corpus on a scientific paper. In short here are few questions that I need some help with:&lt;br/&gt;
1. Is it legal/ethically correct to just publish the corpus as is for research purposes?&lt;br/&gt;
2. If not, is it possible to instead apply some preprocessing techniques to the corpus in a way that makes this ethical?&lt;br/&gt;
3. Are there any publications that you know of that have done something similar or related to collecting data from Facebook and so forth?  &lt;/p&gt;

&lt;p&gt;Any information you can provide is very appreciated.&lt;/p&gt;

&lt;p&gt;Thanks a lot and have a great day.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m62ma4,True,,le-zakkaz,,12,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m62ma4/facebook_comments_to_build_a_language_model/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m62ma4/facebook_comments_to_build_a_language_model/,30199,1615875682.0,0,,False,,,,,,1093
529,,LanguageTechnology,,t2_pc6ycvi,False,,0,False,"Computational Linguistics study shows how Metaphors, Figurative Framings and Sentiments are changing in the Covid-19 discourse over the course of the Pandemic. The War-Terminology is increasingly literal with the onset of the BlackLivesMatter Protests in 2020.",[],r/LanguageTechnology,False,6,,0,,False,t3_m66b5b,False,dark,0.75,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,False,,1615920628.0,text,6,,,text,frontiersin.org,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m66b5b,True,,scientist_1337,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m66b5b/computational_linguistics_study_shows_how/,all_ads,False,https://www.frontiersin.org/articles/10.3389/fcomm.2021.651997/full,30199,1615891828.0,0,,False,https://www.frontiersin.org/articles/10.3389/fcomm.2021.651997/full,,,,,0
530,,LanguageTechnology,"*Okay! Thanks to /u/*[*themajesticcalf*](https://www.reddit.com/user/themajesticcalf) *for pointing out the google doc to me on the original post. This community is great, and thank you all for not doing crazy things to it.*

Back to the original post. It's great to get as much information and resources possible out to our community. Here at Nvidia, a cool thing we do is ""Data Science of the Day"" (DSotD). It's tips and information from subject matter experts to better equip anyone doing data science/machine learning to perform their job to the maximum potential.

Today's DSotD is about embedding SQL code into python to query tables at blazing speeds, with the help of GPUs. Click the link (and then the picture) to check out the blog post on how to perform this task. [Embed SQL into Python for Blazing Speeds](https://forums.developer.nvidia.com/t/embed-your-sql-query-into-your-python-code-and-let-it-rip-on-a-gpu/170506?u=kasmith) .

Also, don't forget to register for our **FREE** conference, [GTC 2021 FREE REGISTRATION](https://www.nvidia.com/en-us/gtc/?ncid=GTCS21-NVKASMITH) , to have access to a week of great talks and presentations that show real world scenarios/applications where data science/machine learning skills like these above can help solve complicated problems.",,False,,0,False,Embed Your SQL Query Into Your Python Code and Let It Rip on a GPU - Data Science of the Day,[],r/LanguageTechnology,False,6,,0,,False,t3_m6aya3,False,dark,0.4,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,,,False,,,{},,True,,1615935507.0,text,6,,,,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;em&gt;Okay! Thanks to /u/&lt;/em&gt;&lt;a href=""https://www.reddit.com/user/themajesticcalf""&gt;&lt;em&gt;themajesticcalf&lt;/em&gt;&lt;/a&gt; &lt;em&gt;for pointing out the google doc to me on the original post. This community is great, and thank you all for not doing crazy things to it.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Back to the original post. It&amp;#39;s great to get as much information and resources possible out to our community. Here at Nvidia, a cool thing we do is &amp;quot;Data Science of the Day&amp;quot; (DSotD). It&amp;#39;s tips and information from subject matter experts to better equip anyone doing data science/machine learning to perform their job to the maximum potential.&lt;/p&gt;

&lt;p&gt;Today&amp;#39;s DSotD is about embedding SQL code into python to query tables at blazing speeds, with the help of GPUs. Click the link (and then the picture) to check out the blog post on how to perform this task. &lt;a href=""https://forums.developer.nvidia.com/t/embed-your-sql-query-into-your-python-code-and-let-it-rip-on-a-gpu/170506?u=kasmith""&gt;Embed SQL into Python for Blazing Speeds&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;Also, don&amp;#39;t forget to register for our &lt;strong&gt;FREE&lt;/strong&gt; conference, &lt;a href=""https://www.nvidia.com/en-us/gtc/?ncid=GTCS21-NVKASMITH""&gt;GTC 2021 FREE REGISTRATION&lt;/a&gt; , to have access to a week of great talks and presentations that show real world scenarios/applications where data science/machine learning skills like these above can help solve complicated problems.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m6aya3,True,,[deleted],,0,True,all_ads,False,[],,dark,/r/LanguageTechnology/comments/m6aya3/embed_your_sql_query_into_your_python_code_and/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m6aya3/embed_your_sql_query_into_your_python_code_and/,30199,1615906707.0,0,,False,,,,,,1291
531,,LanguageTechnology,"Hi NLP folks,

I have developed a library for NLP with different transformer architectures in Tensorflow 2.0 . The natural question will be, how it is different compared to hugging Face. Hugging face is amazing, no doubt in that, But the Tf 2.0 implementation of Hugging face is way slower compared to PT implementation. Which actually made me think why? 

The lack of proper serialization of models is the main reason. To do so, we actually needed to re-design the code. tf-transformers if serialization and more.

As per the benchmarks for **text generation using GPT2 and t5 models, it is 80 % faster than HF TF implementations and even faster than PT implementations.** All the codes + benchmarks + tutorials released. 

## Unique Features

&amp;#x200B;

* **Faster Auto Regressive Decoding** using Tensorflow2. Faster than PyTorch in most experiments (V100 GPU). **80%** faster compared to existing TF based libraries (relative difference) Refer [benchmark code](https://github.com/legacyai/tf-transformers/blob/main/tests/notebooks/benchmarks).
* Complete **TFlite** support for **BERT, RoBERTA, T5, Albert, mt5** for all down stream tasks except text-generation
* **Faster sentence-piece alignment** (no more LCS overhead)
* **Variable batch text generation** for Encoder only models like GPT2
* No more hassle of writing long codes for **TFRecords. minimal and simple**.
* Off the shelf support for auto-batching **tf.data.dataset** or **tf.ragged** tensors
* Pass dictionary outputs directly to loss functions inside tf.keras.Model.fit  
 using **model.compile2** . Refer [examples](https://github.com/legacyai/tf-transformers/blob/main/src/tf_transformers/notebooks/tutorials) or [blog](https://legacyai-org.medium.com/tf-transformers-f7722536ba61)
* Multiple mask modes like **causal**, **user-defined**, **prefix** by changing one argument . Refer [examples](https://github.com/legacyai/tf-transformers/blob/main/src/tf_transformers/notebooks/tutorials) or [blog](https://legacyai-org.medium.com/tf-transformers-f7722536ba61)

&amp;#x200B;

[https://github.com/legacyai/tf-transformers](https://github.com/legacyai/tf-transformers)

&amp;#x200B;

Please share your thoughts, comments and feedback .",t2_87tdfx60,False,,0,False,tf-transformers : State of the art faster NLP in Tensorflow 2.0 . 80 % faster to existing TF based libraries.,[],r/LanguageTechnology,False,6,,0,,False,t3_m5j2hi,False,dark,0.98,,public,59,0,{},,False,[],,False,False,,{},,False,59,,False,False,,False,,[],{},,True,,1615838231.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi NLP folks,&lt;/p&gt;

&lt;p&gt;I have developed a library for NLP with different transformer architectures in Tensorflow 2.0 . The natural question will be, how it is different compared to hugging Face. Hugging face is amazing, no doubt in that, But the Tf 2.0 implementation of Hugging face is way slower compared to PT implementation. Which actually made me think why? &lt;/p&gt;

&lt;p&gt;The lack of proper serialization of models is the main reason. To do so, we actually needed to re-design the code. tf-transformers if serialization and more.&lt;/p&gt;

&lt;p&gt;As per the benchmarks for &lt;strong&gt;text generation using GPT2 and t5 models, it is 80 % faster than HF TF implementations and even faster than PT implementations.&lt;/strong&gt; All the codes + benchmarks + tutorials released. &lt;/p&gt;

&lt;h2&gt;Unique Features&lt;/h2&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Faster Auto Regressive Decoding&lt;/strong&gt; using Tensorflow2. Faster than PyTorch in most experiments (V100 GPU). &lt;strong&gt;80%&lt;/strong&gt; faster compared to existing TF based libraries (relative difference) Refer &lt;a href=""https://github.com/legacyai/tf-transformers/blob/main/tests/notebooks/benchmarks""&gt;benchmark code&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Complete &lt;strong&gt;TFlite&lt;/strong&gt; support for &lt;strong&gt;BERT, RoBERTA, T5, Albert, mt5&lt;/strong&gt; for all down stream tasks except text-generation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Faster sentence-piece alignment&lt;/strong&gt; (no more LCS overhead)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variable batch text generation&lt;/strong&gt; for Encoder only models like GPT2&lt;/li&gt;
&lt;li&gt;No more hassle of writing long codes for &lt;strong&gt;TFRecords. minimal and simple&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Off the shelf support for auto-batching &lt;strong&gt;tf.data.dataset&lt;/strong&gt; or &lt;strong&gt;tf.ragged&lt;/strong&gt; tensors&lt;/li&gt;
&lt;li&gt;Pass dictionary outputs directly to loss functions inside tf.keras.Model.fit&lt;br/&gt;
using &lt;strong&gt;model.compile2&lt;/strong&gt; . Refer &lt;a href=""https://github.com/legacyai/tf-transformers/blob/main/src/tf_transformers/notebooks/tutorials""&gt;examples&lt;/a&gt; or &lt;a href=""https://legacyai-org.medium.com/tf-transformers-f7722536ba61""&gt;blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Multiple mask modes like &lt;strong&gt;causal&lt;/strong&gt;, &lt;strong&gt;user-defined&lt;/strong&gt;, &lt;strong&gt;prefix&lt;/strong&gt; by changing one argument . Refer &lt;a href=""https://github.com/legacyai/tf-transformers/blob/main/src/tf_transformers/notebooks/tutorials""&gt;examples&lt;/a&gt; or &lt;a href=""https://legacyai-org.medium.com/tf-transformers-f7722536ba61""&gt;blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/legacyai/tf-transformers""&gt;https://github.com/legacyai/tf-transformers&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Please share your thoughts, comments and feedback .&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m5j2hi,True,,aintnosunshine2,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m5j2hi/tftransformers_state_of_the_art_faster_nlp_in/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m5j2hi/tftransformers_state_of_the_art_faster_nlp_in/,30199,1615809431.0,0,,False,,,,,,2210
532,,LanguageTechnology,"Hey guys! Im planning to develop a complaint redressal system. So I have continuously flowing in complaints. The aim is to identify a issue that is being written in majority of the complaints. Is there any paper/ article that can help me achieve this?

I have found many solutions, that go well with static data, that is when you have no Incoming data, and are computing on whatever's available, but here I have continuously incoming data, so I will need an appropriate ML/NLP algorithm to find the common issues faced in all the complaints.
Thanks in advance for your help",t2_8n5d56qx,False,,0,False,Complaint redressal system,[],r/LanguageTechnology,False,6,,0,,False,t3_m63e18,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1615907920.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys! Im planning to develop a complaint redressal system. So I have continuously flowing in complaints. The aim is to identify a issue that is being written in majority of the complaints. Is there any paper/ article that can help me achieve this?&lt;/p&gt;

&lt;p&gt;I have found many solutions, that go well with static data, that is when you have no Incoming data, and are computing on whatever&amp;#39;s available, but here I have continuously incoming data, so I will need an appropriate ML/NLP algorithm to find the common issues faced in all the complaints.
Thanks in advance for your help&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m63e18,True,,ChandlerBingggggggg,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m63e18/complaint_redressal_system/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m63e18/complaint_redressal_system/,30199,1615879120.0,0,,False,,,,,,573
533,,LanguageTechnology,,t2_5smymm7p,False,,0,False,Into NLP - Tokenization,[],r/LanguageTechnology,False,6,,0,,False,t3_m5kidb,False,dark,0.83,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,False,,1615843175.0,text,6,,,text,qualicen.de,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m5kidb,True,,QualicenDS,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m5kidb/into_nlp_tokenization/,all_ads,False,https://www.qualicen.de/nlp-article-3-numerous-language-pieces-tokenization/,30199,1615814375.0,0,,False,https://www.qualicen.de/nlp-article-3-numerous-language-pieces-tokenization/,,,,,0
534,,LanguageTechnology,"So I want to be more active in all the subs and let everyone know the best available material out there. NVIDIA has been steam rolling the Kaggle competitions using some subject matter expertise and the NVIDIA platform. Here are four stellar Kaggle Grandmasters from NVIDIA talking a short history of state-of-the-art NLP models and best practices using Hugging Face.

It's an hour long on no speed up, but legitimately a great source for anyone new or experienced in NLP.

[Building World-Class NLP Models with Transformers and Hugging Face](https://bit.ly/2Nq5yqs)",,False,,0,False,NVIDIA Grandmaster Series – Building World-Class NLP Models with Transformers and Hugging Face,[],r/LanguageTechnology,False,6,,0,,False,t3_m5lje2,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,,,False,,,{},,True,,1615846293.0,text,6,,,,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I want to be more active in all the subs and let everyone know the best available material out there. NVIDIA has been steam rolling the Kaggle competitions using some subject matter expertise and the NVIDIA platform. Here are four stellar Kaggle Grandmasters from NVIDIA talking a short history of state-of-the-art NLP models and best practices using Hugging Face.&lt;/p&gt;

&lt;p&gt;It&amp;#39;s an hour long on no speed up, but legitimately a great source for anyone new or experienced in NLP.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://bit.ly/2Nq5yqs""&gt;Building World-Class NLP Models with Transformers and Hugging Face&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m5lje2,True,,[deleted],,0,True,all_ads,False,[],,dark,/r/LanguageTechnology/comments/m5lje2/nvidia_grandmaster_series_building_worldclass_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m5lje2/nvidia_grandmaster_series_building_worldclass_nlp/,30199,1615817493.0,0,,False,,,,,,566
535,,LanguageTechnology,"I’m doing some internet sleuthing but I’m mostly seeing text classification, are there other applications?",t2_7wo3mdn,False,,0,False,Are there other supervised ml tasks in NLP besides text classification?,[],r/LanguageTechnology,False,6,,0,,False,t3_m5ols9,False,dark,0.66,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1615854617.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m doing some internet sleuthing but I’m mostly seeing text classification, are there other applications?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m5ols9,True,,edwardsrk,,12,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m5ols9/are_there_other_supervised_ml_tasks_in_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m5ols9/are_there_other_supervised_ml_tasks_in_nlp/,30199,1615825817.0,0,,False,,,,,,106
536,,LanguageTechnology,"which Duc dataset gives good evaluation rouge score in text summarization? like is it DUC 2001 or DUC 2002,...?can anyone provide me a link to refer it.",t2_aan0jaaa,False,,0,False,COMPARISION OF DUC DATASETS ACCURACY,[],r/LanguageTechnology,False,6,,0,,False,t3_m5j9bt,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1615838891.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;which Duc dataset gives good evaluation rouge score in text summarization? like is it DUC 2001 or DUC 2002,...?can anyone provide me a link to refer it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m5j9bt,True,,Flimsy_Eye8287,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m5j9bt/comparision_of_duc_datasets_accuracy/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m5j9bt/comparision_of_duc_datasets_accuracy/,30199,1615810091.0,0,,False,,,,,,152
537,,LanguageTechnology," Below is the original code : 

 `import nltk`

&amp;#x200B;

`# flight grammar rules`

`flight_grammar = nltk.CFG.fromstring(""""""`

  `S -&gt; NP VP | VP`

  `VP -&gt; V NP | V NP PP`

  `PP -&gt; P NP`

  `NP -&gt; Prop | Det N | Det N PP`

  `V -&gt; ""walked"" | ""book"" | ""prefer"" | ""gave"" | ""want""`

  `Prop -&gt; ""Jack"" | ""John"" | ""I"" | ""Houston""`

  `Det -&gt; ""a"" | ""an"" | ""the"" | ""my"" | ""that""`

  `N -&gt; ""dog"" | ""bone"" | ""flight""`

  `P -&gt; ""in"" | ""on"" | ""by"" | ""with"" | ""to"" | ""through""`

  `"""""")`

&amp;#x200B;

`# make a recursive descent parser and parse the sentence`

`rd_parser = nltk.RecursiveDescentParser(flight_grammar)`

&amp;#x200B;

`#define first sentence`

`senttext = ""I prefer a flight through Houston""`

`#tokenize sentence by splitting on white space`

`sentlist = senttext.split()`

`# run the parse function on the tokenized sentence and print the tree strucutre`

`for tree in rd_parser.parse(sentlist):`

	`print (tree)`

It is able to parse the sentence mentioned in the code, but fails to do parse this sentence : """"Jack walked with the dog""

Modifying the rule to : `VP -&gt; V NP | V NP PP | V PP`, can parse that as well. However, I am unable to parse the following sentences : 

""John gave the dog a bone"" , ""I want to book that flight""",t2_4gosxw7f,False,,0,False,How to parse a sentence by writing grammar rules in NLTK CFG ?,[],r/LanguageTechnology,False,6,,0,,False,t3_m5ally,False,dark,0.84,,public,4,1,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1615804012.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Below is the original code : &lt;/p&gt;

&lt;p&gt;&lt;code&gt;import nltk&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# flight grammar rules&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;flight_grammar = nltk.CFG.fromstring(&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;S -&amp;gt; NP VP | VP&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;VP -&amp;gt; V NP | V NP PP&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;PP -&amp;gt; P NP&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;NP -&amp;gt; Prop | Det N | Det N PP&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;V -&amp;gt; &amp;quot;walked&amp;quot; | &amp;quot;book&amp;quot; | &amp;quot;prefer&amp;quot; | &amp;quot;gave&amp;quot; | &amp;quot;want&amp;quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Prop -&amp;gt; &amp;quot;Jack&amp;quot; | &amp;quot;John&amp;quot; | &amp;quot;I&amp;quot; | &amp;quot;Houston&amp;quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Det -&amp;gt; &amp;quot;a&amp;quot; | &amp;quot;an&amp;quot; | &amp;quot;the&amp;quot; | &amp;quot;my&amp;quot; | &amp;quot;that&amp;quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;N -&amp;gt; &amp;quot;dog&amp;quot; | &amp;quot;bone&amp;quot; | &amp;quot;flight&amp;quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;P -&amp;gt; &amp;quot;in&amp;quot; | &amp;quot;on&amp;quot; | &amp;quot;by&amp;quot; | &amp;quot;with&amp;quot; | &amp;quot;to&amp;quot; | &amp;quot;through&amp;quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;quot;&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# make a recursive descent parser and parse the sentence&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rd_parser = nltk.RecursiveDescentParser(flight_grammar)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#define first sentence&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;senttext = &amp;quot;I prefer a flight through Houston&amp;quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#tokenize sentence by splitting on white space&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sentlist = senttext.split()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# run the parse function on the tokenized sentence and print the tree strucutre&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for tree in rd_parser.parse(sentlist):&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`print (tree)`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It is able to parse the sentence mentioned in the code, but fails to do parse this sentence : &amp;quot;&amp;quot;Jack walked with the dog&amp;quot;&lt;/p&gt;

&lt;p&gt;Modifying the rule to : &lt;code&gt;VP -&amp;gt; V NP | V NP PP | V PP&lt;/code&gt;, can parse that as well. However, I am unable to parse the following sentences : &lt;/p&gt;

&lt;p&gt;&amp;quot;John gave the dog a bone&amp;quot; , &amp;quot;I want to book that flight&amp;quot;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 30, 'id': 'award_b4ff447e-05a5-42dc-9002-63568807cfe6', 'penny_donate': None, 'award_sub_type': 'PREMIUM', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_128.png', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'A glowing commendation for all to see', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'All-Seeing Upvote', 'resized_static_icons': [{'url': 'https://external-preview.redd.it?width=16&amp;height=16&amp;auto=webp&amp;s=d88c9a453f8ac38850b7a8241cfe5804b7b4905d', 'width': 16, 'height': 16}, {'url': 'https://external-preview.redd.it?width=32&amp;height=32&amp;auto=webp&amp;s=96a25019eb75878bdec4f6c012540f3baffbb1b2', 'width': 32, 'height': 32}, {'url': 'https://external-preview.redd.it?width=48&amp;height=48&amp;auto=webp&amp;s=1a51d27d75afde3fbde8bba84f9338f511211461', 'width': 48, 'height': 48}, {'url': 'https://external-preview.redd.it?width=64&amp;height=64&amp;auto=webp&amp;s=96af5ec460b05669ed60224cb0619bb8884abe27', 'width': 64, 'height': 64}, {'url': 'https://external-preview.redd.it?width=128&amp;height=128&amp;auto=webp&amp;s=2d3e648ed2302e6258673051ca5291f57beb29d4', 'width': 128, 'height': 128}], 'icon_format': 'APNG', 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m5ally,True,,DistrictFrequent9359,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m5ally/how_to_parse_a_sentence_by_writing_grammar_rules/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m5ally/how_to_parse_a_sentence_by_writing_grammar_rules/,30199,1615775212.0,0,,False,,,,,,1277
538,,LanguageTechnology,"I'm trying to rank words in a corpus of political speeches. TF-IDF seems to work really nice to identify ""important"" words, much better than raw frequency at least.

I'm wondering if there are any alternate or similar techniques to TF-IDF to rank the importance of words in a corpus.

Thanks!",t2_1vi2bryz,False,,0,False,Alternate approaches to TF-IDF?,[],r/LanguageTechnology,False,6,,0,,False,t3_m51t0z,False,dark,0.91,,public,15,0,{},,False,[],,False,False,,{},,False,15,,False,False,,False,,[],{},,True,,1615777996.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to rank words in a corpus of political speeches. TF-IDF seems to work really nice to identify &amp;quot;important&amp;quot; words, much better than raw frequency at least.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m wondering if there are any alternate or similar techniques to TF-IDF to rank the importance of words in a corpus.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m51t0z,True,,nubonaga,,28,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m51t0z/alternate_approaches_to_tfidf/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m51t0z/alternate_approaches_to_tfidf/,30199,1615749196.0,0,,False,,,,,,292
539,,LanguageTechnology,"Hi all,

I'm wondering if there are SOTA approach for automatic keyword extraction. I have following questions, I hope someone answer them.

1 are there any good data set to work on keyword extraction

2 what are the best ML and DL techniques used for it

3 are there any tools to prepare data for keyword extraction

4 is it possible to solve this problem using unsupervised approach

5 what are the key performance metrics to measure performance

Ps: my question is not limited to NER

Cheers",t2_4inu94cw,False,,0,False,Best approach for automatic key word extraction,[],r/LanguageTechnology,False,6,,0,,False,t3_m5f6vk,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1615821644.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m wondering if there are SOTA approach for automatic keyword extraction. I have following questions, I hope someone answer them.&lt;/p&gt;

&lt;p&gt;1 are there any good data set to work on keyword extraction&lt;/p&gt;

&lt;p&gt;2 what are the best ML and DL techniques used for it&lt;/p&gt;

&lt;p&gt;3 are there any tools to prepare data for keyword extraction&lt;/p&gt;

&lt;p&gt;4 is it possible to solve this problem using unsupervised approach&lt;/p&gt;

&lt;p&gt;5 what are the key performance metrics to measure performance&lt;/p&gt;

&lt;p&gt;Ps: my question is not limited to NER&lt;/p&gt;

&lt;p&gt;Cheers&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m5f6vk,True,,gubberex,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m5f6vk/best_approach_for_automatic_key_word_extraction/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m5f6vk/best_approach_for_automatic_key_word_extraction/,30199,1615792844.0,0,,False,,,,,,494
540,,LanguageTechnology,,t2_5fsp2x6v,False,,0,False,Structured Nearest Neighbour Learning for Few-Shot NER | Research Papers Summary 012,[],r/LanguageTechnology,False,6,,0,,False,t3_m5226j,False,dark,1.0,,public,8,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/bbhCN3rVwrU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Structured Nearest Neighbour Learning for Few-Shot NER | Research Papers Summary 012', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/bbhCN3rVwrU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/bbhCN3rVwrU/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/bbhCN3rVwrU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/m5226j', 'height': 200}",,False,8,,False,False,,False,,[],{},,False,,1615778691.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m5226j,True,,RyanAI100,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m5226j/structured_nearest_neighbour_learning_for_fewshot/,all_ads,False,https://youtu.be/bbhCN3rVwrU,30199,1615749891.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Structured Nearest Neighbour Learning for Few-Shot NER | Research Papers Summary 012', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/bbhCN3rVwrU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/bbhCN3rVwrU/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,https://youtu.be/bbhCN3rVwrU,,,,,0
541,,LanguageTechnology,"I'm looking to learn the theory, latest research, and development best practices around chatbots and conversational AI. I'm open to all mediums (e.g., books, lecture videos, courses, articles). 

So far I'm aware of the Jurafsky &amp; Martin [chapter](https://web.stanford.edu/~jurafsky/slp3/) on it. I know Rasa should have some material floating around somewhere, but my feeling is that it'll be too specific to the Rasa library. 

Any resources y'all have really enjoyed/found useful?",t2_3khic7tq,False,,0,False,Learning resources for chatbots / conversational AI?,[],r/LanguageTechnology,False,6,,0,,False,t3_m58hnw,False,dark,0.67,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1615797213.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m looking to learn the theory, latest research, and development best practices around chatbots and conversational AI. I&amp;#39;m open to all mediums (e.g., books, lecture videos, courses, articles). &lt;/p&gt;

&lt;p&gt;So far I&amp;#39;m aware of the Jurafsky &amp;amp; Martin &lt;a href=""https://web.stanford.edu/%7Ejurafsky/slp3/""&gt;chapter&lt;/a&gt; on it. I know Rasa should have some material floating around somewhere, but my feeling is that it&amp;#39;ll be too specific to the Rasa library. &lt;/p&gt;

&lt;p&gt;Any resources y&amp;#39;all have really enjoyed/found useful?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m58hnw,True,,EazyStrides,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m58hnw/learning_resources_for_chatbots_conversational_ai/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m58hnw/learning_resources_for_chatbots_conversational_ai/,30199,1615768413.0,0,,False,,,,,,487
542,,LanguageTechnology,"I have audio files with two speakers and I want to have speech to text conversation. For this I plan on using Huggingface. But I also want to separate text from the two speakers so I need diarization as well.

Any tips or suggestions based on your experience so I don't make the same mistakes. 

I see pyannote and Bob from idiap as potential options but I haven't used them before.",t2_9m8l2f9x,False,,0,False,Is there a python based speaker diarization system you would recommend?,[],r/LanguageTechnology,False,6,,0,,False,t3_m557kh,False,dark,0.75,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1615787444.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have audio files with two speakers and I want to have speech to text conversation. For this I plan on using Huggingface. But I also want to separate text from the two speakers so I need diarization as well.&lt;/p&gt;

&lt;p&gt;Any tips or suggestions based on your experience so I don&amp;#39;t make the same mistakes. &lt;/p&gt;

&lt;p&gt;I see pyannote and Bob from idiap as potential options but I haven&amp;#39;t used them before.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m557kh,True,,Advanced-Hedgehog-95,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m557kh/is_there_a_python_based_speaker_diarization/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m557kh/is_there_a_python_based_speaker_diarization/,30199,1615758644.0,0,,False,,,,,,382
543,,LanguageTechnology,"When performing mini-batch stochastic gradient descent in an NLP setting, what does the batch size represent? If I am training a character-level LSTM language model with batch size 100, does this mean I process 100 characters, 100 words or 100 sentences? 

&amp;#x200B;

Suppose it meant characters. And so I process 100 characters, average the gradients, update parameters, but what happens next? Do I re-initialize the model and input a new sequence of 100 characters sampled randomly from a corpus? Or do I just input the 100th to 200th characters that continue the first sequence? If the latter, at what point do I sample an entirely new sequence of characters from the corpus?",t2_128h0c,False,,0,False,"When batch size = n, does this mean parameter updates are performed after processing n characters, words, or sentences? [mini-batch stochastic gradient descent]",[],r/LanguageTechnology,False,6,,0,,False,t3_m587yq,False,dark,0.6,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1615796349.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;When performing mini-batch stochastic gradient descent in an NLP setting, what does the batch size represent? If I am training a character-level LSTM language model with batch size 100, does this mean I process 100 characters, 100 words or 100 sentences? &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Suppose it meant characters. And so I process 100 characters, average the gradients, update parameters, but what happens next? Do I re-initialize the model and input a new sequence of 100 characters sampled randomly from a corpus? Or do I just input the 100th to 200th characters that continue the first sequence? If the latter, at what point do I sample an entirely new sequence of characters from the corpus?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m587yq,True,,synysterbates,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m587yq/when_batch_size_n_does_this_mean_parameter/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m587yq/when_batch_size_n_does_this_mean_parameter/,30199,1615767549.0,0,,False,,,,,,681
544,,LanguageTechnology,"Hi all

I am wondering if there's any good implementation of latent aspect rating analysis (LARA). I googled it but surprisingly no libraries support it. I'm too dumb to go through paper and implement it myself. I don't even know how to prepare data for it.

Also now I'm curious how does the guys at Amazon do it?

Cheers",t2_4inu94cw,False,,0,False,LARA implementation,[],r/LanguageTechnology,False,6,,0,,False,t3_m4sdkp,False,dark,0.7,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1615744495.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all&lt;/p&gt;

&lt;p&gt;I am wondering if there&amp;#39;s any good implementation of latent aspect rating analysis (LARA). I googled it but surprisingly no libraries support it. I&amp;#39;m too dumb to go through paper and implement it myself. I don&amp;#39;t even know how to prepare data for it.&lt;/p&gt;

&lt;p&gt;Also now I&amp;#39;m curious how does the guys at Amazon do it?&lt;/p&gt;

&lt;p&gt;Cheers&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m4sdkp,True,,gubberex,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m4sdkp/lara_implementation/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m4sdkp/lara_implementation/,30199,1615715695.0,0,,False,,,,,,322
545,,LanguageTechnology,"I am actively seeking help somebody who is good at NLP in finishing my assignment. The raw data is in an XML file. I need to build probably a LSTM.model. if you are interested, please ping me for more details. Thanks.",t2_4w9dvamk,False,,0,False,Seeking help in NLP assignment,[],r/LanguageTechnology,False,6,,0,,False,t3_m51y7p,False,dark,0.11,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1615778389.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am actively seeking help somebody who is good at NLP in finishing my assignment. The raw data is in an XML file. I need to build probably a LSTM.model. if you are interested, please ping me for more details. Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m51y7p,True,,vbukkala,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m51y7p/seeking_help_in_nlp_assignment/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m51y7p/seeking_help_in_nlp_assignment/,30199,1615749589.0,0,,False,,,,,,217
546,,LanguageTechnology,can anyone send me DUC dataset for text summarization??,t2_aan0jaaa,False,,0,False,DUC DATASET,[],r/LanguageTechnology,False,6,,0,,False,t3_m4qcw4,False,dark,0.33,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1615734777.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;can anyone send me DUC dataset for text summarization??&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m4qcw4,True,,Flimsy_Eye8287,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m4qcw4/duc_dataset/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m4qcw4/duc_dataset/,30199,1615705977.0,0,,False,,,,,,55
547,,LanguageTechnology,"&amp;#x200B;

&amp;#x200B;

Hello everyone,

I am a senior student of English Language and Literature with a specialization in Linguistics. I am currently interested in pursuing a MA degree in Computational Linguistics but I am afraid that I am not qualified enough given that I lack basic programming skills.

I have already applied to the Language Technology MA degree in Uppsala University and I am planning to apply to both Stuttgart and Tuebingen Computational Linguistics programmes.

Do you think that is possible to be accepted with solely a linguistic background?

Do you have any other similar MA programmes in mind?

Thank you in advance",t2_9q1deko2,False,,0,False,MA in CompLing with a Linguistic background,[],r/LanguageTechnology,False,6,,0,,False,t3_m4as1e,False,dark,0.93,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,False,,False,,[],{},,True,,1615684967.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Hello everyone,&lt;/p&gt;

&lt;p&gt;I am a senior student of English Language and Literature with a specialization in Linguistics. I am currently interested in pursuing a MA degree in Computational Linguistics but I am afraid that I am not qualified enough given that I lack basic programming skills.&lt;/p&gt;

&lt;p&gt;I have already applied to the Language Technology MA degree in Uppsala University and I am planning to apply to both Stuttgart and Tuebingen Computational Linguistics programmes.&lt;/p&gt;

&lt;p&gt;Do you think that is possible to be accepted with solely a linguistic background?&lt;/p&gt;

&lt;p&gt;Do you have any other similar MA programmes in mind?&lt;/p&gt;

&lt;p&gt;Thank you in advance&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m4as1e,True,,Suspicious_Grocery64,,18,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m4as1e/ma_in_compling_with_a_linguistic_background/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m4as1e/ma_in_compling_with_a_linguistic_background/,30199,1615656167.0,0,,False,,,,,,648
548,,LanguageTechnology,"Text Summarization is the task of shortening a given document while maintaining the most important information. In general, a good summarizer should generate a summary that is syntactically accurate, semantically correct, coherent, and non-redundant. While extractive methods tend to have better performance on the first two aspects, they are typically less coherent and more redundant than abstractive ones, where new sentences are often generated by sentence fusion and compression, which helps detecting and removing redundancy. This work talks about existing and also proposes new techniques to deal with redundancy in long document summarisation.

Paper walkthrough: https://youtu.be/GFUwKDGYkuI",t2_hkv9s,False,,0,False,Systematically Exploring Redundancy Reduction in Summarizing Long Documents (Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_m46g0o,False,dark,0.88,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1615671477.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Text Summarization is the task of shortening a given document while maintaining the most important information. In general, a good summarizer should generate a summary that is syntactically accurate, semantically correct, coherent, and non-redundant. While extractive methods tend to have better performance on the first two aspects, they are typically less coherent and more redundant than abstractive ones, where new sentences are often generated by sentence fusion and compression, which helps detecting and removing redundancy. This work talks about existing and also proposes new techniques to deal with redundancy in long document summarisation.&lt;/p&gt;

&lt;p&gt;Paper walkthrough: &lt;a href=""https://youtu.be/GFUwKDGYkuI""&gt;https://youtu.be/GFUwKDGYkuI&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m46g0o,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m46g0o/systematically_exploring_redundancy_reduction_in/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m46g0o/systematically_exploring_redundancy_reduction_in/,30199,1615642677.0,0,,False,,,,,,700
549,,LanguageTechnology,Out-domain random inputs unwantedly giving very high confidence value. What are the available ways to get rid of it? What to do when we have no negative class data or the negative class is too big to cover?,t2_86zivv0h,False,,0,False,What are the ways to handle out of domain inputs for text classification?,[],r/LanguageTechnology,False,6,,0,,False,t3_m46mz4,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1615672174.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Out-domain random inputs unwantedly giving very high confidence value. What are the available ways to get rid of it? What to do when we have no negative class data or the negative class is too big to cover?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m46mz4,True,,hafizcse031,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m46mz4/what_are_the_ways_to_handle_out_of_domain_inputs/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m46mz4/what_are_the_ways_to_handle_out_of_domain_inputs/,30199,1615643374.0,0,,False,,,,,,206
550,,LanguageTechnology,"I am an aspiring NLP student and I had this bizarre or some may say amateurish question. I want to know whether this field of AI has good future or not. I know prediction about AGI is like asking when will Avatar sequels come. I just wanted to know if we achieved human level in language, will the field of NLP die or will it move towards Super Intelligence?

So in short I want to know whether NLP will have a good future in terms of research or job opportunities in this century. If this question sounded like non sense, ignore this post, or if you are interested in this question please respond. I am struck at this question for days, so please help me with this.",t2_9x4y5w88,False,,0,False,Will NLP have a good future even if we reach AGI?,[],r/LanguageTechnology,False,6,,0,,False,t3_m3nb23,False,dark,0.78,,public,13,0,{},,False,[],,False,False,,{},,False,13,,False,False,,False,,[],{},,True,,1615601150.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am an aspiring NLP student and I had this bizarre or some may say amateurish question. I want to know whether this field of AI has good future or not. I know prediction about AGI is like asking when will Avatar sequels come. I just wanted to know if we achieved human level in language, will the field of NLP die or will it move towards Super Intelligence?&lt;/p&gt;

&lt;p&gt;So in short I want to know whether NLP will have a good future in terms of research or job opportunities in this century. If this question sounded like non sense, ignore this post, or if you are interested in this question please respond. I am struck at this question for days, so please help me with this.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m3nb23,True,,dhassvishnu99,,17,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m3nb23/will_nlp_have_a_good_future_even_if_we_reach_agi/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m3nb23/will_nlp_have_a_good_future_even_if_we_reach_agi/,30199,1615572350.0,4,,False,,,,,,666
551,,LanguageTechnology,"Does anyone have experience with extracting dates from text? An example follows. Any pointers to code, papers, API's will be highly appreciated. I've tried out a few API's and tools already, and they did not work. Please reply only if you know that your tool/API of choice will work. Thanks!

Text: ""I signed contract with a vendor to provide a dance floor for my wedding. The Wedding was planned for March 21, 2020. The contract was signed in February.""

Desired output: ""March 21, 2020; February 1, 2020"".",t2_5c73x6y5,False,,0,False,Date extraction from text code/API's,[],r/LanguageTechnology,False,6,,0,,False,t3_m3s1vk,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1615614539.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Does anyone have experience with extracting dates from text? An example follows. Any pointers to code, papers, API&amp;#39;s will be highly appreciated. I&amp;#39;ve tried out a few API&amp;#39;s and tools already, and they did not work. Please reply only if you know that your tool/API of choice will work. Thanks!&lt;/p&gt;

&lt;p&gt;Text: &amp;quot;I signed contract with a vendor to provide a dance floor for my wedding. The Wedding was planned for March 21, 2020. The contract was signed in February.&amp;quot;&lt;/p&gt;

&lt;p&gt;Desired output: &amp;quot;March 21, 2020; February 1, 2020&amp;quot;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m3s1vk,True,,Jake_Bluuse,,18,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m3s1vk/date_extraction_from_text_codeapis/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m3s1vk/date_extraction_from_text_codeapis/,30199,1615585739.0,0,,False,,,,,,507
552,,LanguageTechnology,"Seq2seq models are advantageous for their ability to process text inputs without a constrained length. This tutorial covers how to build, train, and test a seq2seq model for text summarization using Keras. 

Article link: [https://blog.paperspace.com/implement-seq2seq-for-text-summarization-keras/](https://blog.paperspace.com/implement-seq2seq-for-text-summarization-keras/)

You can also run the full code on a free GPU: [https://ml-showcase.paperspace.com/projects/text-summarization-with-seq2seq-models](https://ml-showcase.paperspace.com/projects/text-summarization-with-seq2seq-models)

For more information on the theory behind seq2seq models (architecture, applications, and the data loading/processing steps for the model implemented here), check out [Part 1](https://blog.paperspace.com/introduction-to-seq2seq-models/). 

Comments and questions welcome!",t2_15en0l,False,,0,False,[Tutorial] How to Implement Seq2Seq Models for Text Summarization with Keras,[],r/LanguageTechnology,False,6,,0,,False,t3_m3lk2k,False,dark,0.9,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1615596360.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Seq2seq models are advantageous for their ability to process text inputs without a constrained length. This tutorial covers how to build, train, and test a seq2seq model for text summarization using Keras. &lt;/p&gt;

&lt;p&gt;Article link: &lt;a href=""https://blog.paperspace.com/implement-seq2seq-for-text-summarization-keras/""&gt;https://blog.paperspace.com/implement-seq2seq-for-text-summarization-keras/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can also run the full code on a free GPU: &lt;a href=""https://ml-showcase.paperspace.com/projects/text-summarization-with-seq2seq-models""&gt;https://ml-showcase.paperspace.com/projects/text-summarization-with-seq2seq-models&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information on the theory behind seq2seq models (architecture, applications, and the data loading/processing steps for the model implemented here), check out &lt;a href=""https://blog.paperspace.com/introduction-to-seq2seq-models/""&gt;Part 1&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;Comments and questions welcome!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m3lk2k,True,,hellopaperspace,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m3lk2k/tutorial_how_to_implement_seq2seq_models_for_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m3lk2k/tutorial_how_to_implement_seq2seq_models_for_text/,30199,1615567560.0,0,,False,,,,,,865
553,,LanguageTechnology,"Hello,

I am new to NLP so please bear with me. I am wondering if there is a common solution/framework to extracting constraint phrases from a string.


Examples and what I wanted to detect:

Shirt not more than $10. (More than $10)

Topic published  Before January first.  (before January first)

Pants that are exactly $10. (exactly $10)

I did find a library called LexNLP but I cannot use this framework.

https://lexpredict-lexnlp.readthedocs.io/en/docs-0.1.6/modules/extract_en_constraints.html

Thanks in advance.",t2_4v2wiakw,False,,0,False,Constraint Detection,[],r/LanguageTechnology,False,6,,0,,False,t3_m3h92a,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,1615554914.0,,[],{},,True,,1615583498.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am new to NLP so please bear with me. I am wondering if there is a common solution/framework to extracting constraint phrases from a string.&lt;/p&gt;

&lt;p&gt;Examples and what I wanted to detect:&lt;/p&gt;

&lt;p&gt;Shirt not more than $10. (More than $10)&lt;/p&gt;

&lt;p&gt;Topic published  Before January first.  (before January first)&lt;/p&gt;

&lt;p&gt;Pants that are exactly $10. (exactly $10)&lt;/p&gt;

&lt;p&gt;I did find a library called LexNLP but I cannot use this framework.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://lexpredict-lexnlp.readthedocs.io/en/docs-0.1.6/modules/extract_en_constraints.html""&gt;https://lexpredict-lexnlp.readthedocs.io/en/docs-0.1.6/modules/extract_en_constraints.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m3h92a,True,,mrkresc,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m3h92a/constraint_detection/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m3h92a/constraint_detection/,30199,1615554698.0,0,,False,,,,,,520
554,,LanguageTechnology,"Hi all! I'm new in NLP and still learning, so I'm sorry, this is a noob post. I'm a bit confused with all the information on the internet and I could really use some advice from people with experience.

The problem is that I haven't worked with other people's trained models yet. I get the process of training and evaluating my own ML/deep learning models, but I'm not sure how to proceed with this task and I don't have an expert hotline to ask for help (except for Reddit).

Task: For a course, I need to evaluate the performance of an NER ML model, and compare that to the performance of a deep learning NER model.

At my disposal I have:

* The training data used to train these models + a test and a dev set.
* A large labelled NER corpus of which parts were used to train both of these models.
* The deep learning model works with HuggingFace Transformers (which I'm completely new to, and I don't know if this changes anything).
* Both models have a command-line interface that provide bite-sized NER-tagged output when you load in a .txt file. (e.g.:  ""my name is Rob"" would return the token such as ""Rob"" + label ""B-PER"" for ""person"")

My current plan of action:

* Stripping the test data .txt file from all tags to get ""unseen data"" to test the trained models on.
* Let the command-line tools of both the deep learning model and the ML model generate labelled output on this test data.
* Calculate F1, confusion matrix, etc. on 1) the labelled output of these command-line tools and 2) the labelled ""golden standard"" test data set.

My questions:

* Main question: I don't need to retrain these models to evaluate them, right? Am I correct to assume that I can just work with the labelled output the models provide on the unseen test data set + the golden standard labelled test data, and calculate evaluation metrics on that?
* Is there a better way to evaluate these models beside metrics such as F1?  Can I also do other things to evaluate and compare these models?
* Is there a way to evaluate the performance of the models for each of the NER tags separately?

Thank you, any input is most welcome! ",t2_3xt0icx5,False,,0,False,Beginner questions about NER model evaluation.,[],r/LanguageTechnology,False,6,,0,,False,t3_m3i0ie,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1615586105.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all! I&amp;#39;m new in NLP and still learning, so I&amp;#39;m sorry, this is a noob post. I&amp;#39;m a bit confused with all the information on the internet and I could really use some advice from people with experience.&lt;/p&gt;

&lt;p&gt;The problem is that I haven&amp;#39;t worked with other people&amp;#39;s trained models yet. I get the process of training and evaluating my own ML/deep learning models, but I&amp;#39;m not sure how to proceed with this task and I don&amp;#39;t have an expert hotline to ask for help (except for Reddit).&lt;/p&gt;

&lt;p&gt;Task: For a course, I need to evaluate the performance of an NER ML model, and compare that to the performance of a deep learning NER model.&lt;/p&gt;

&lt;p&gt;At my disposal I have:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The training data used to train these models + a test and a dev set.&lt;/li&gt;
&lt;li&gt;A large labelled NER corpus of which parts were used to train both of these models.&lt;/li&gt;
&lt;li&gt;The deep learning model works with HuggingFace Transformers (which I&amp;#39;m completely new to, and I don&amp;#39;t know if this changes anything).&lt;/li&gt;
&lt;li&gt;Both models have a command-line interface that provide bite-sized NER-tagged output when you load in a .txt file. (e.g.:  &amp;quot;my name is Rob&amp;quot; would return the token such as &amp;quot;Rob&amp;quot; + label &amp;quot;B-PER&amp;quot; for &amp;quot;person&amp;quot;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My current plan of action:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Stripping the test data .txt file from all tags to get &amp;quot;unseen data&amp;quot; to test the trained models on.&lt;/li&gt;
&lt;li&gt;Let the command-line tools of both the deep learning model and the ML model generate labelled output on this test data.&lt;/li&gt;
&lt;li&gt;Calculate F1, confusion matrix, etc. on 1) the labelled output of these command-line tools and 2) the labelled &amp;quot;golden standard&amp;quot; test data set.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My questions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Main question: I don&amp;#39;t need to retrain these models to evaluate them, right? Am I correct to assume that I can just work with the labelled output the models provide on the unseen test data set + the golden standard labelled test data, and calculate evaluation metrics on that?&lt;/li&gt;
&lt;li&gt;Is there a better way to evaluate these models beside metrics such as F1?  Can I also do other things to evaluate and compare these models?&lt;/li&gt;
&lt;li&gt;Is there a way to evaluate the performance of the models for each of the NER tags separately?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank you, any input is most welcome! &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m3i0ie,True,,Melancholic_kitten,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m3i0ie/beginner_questions_about_ner_model_evaluation/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m3i0ie/beginner_questions_about_ner_model_evaluation/,30199,1615557305.0,0,,False,,,,,,2115
555,,LanguageTechnology,"I am trying to build a really good spell corrector for our search engine. We have a lot of domain specific terms so using an off the shelf one is not going to work well. I found the Peter Norvig spelling corrector post but am having trouble finding more sophisticated (and also scalable) spelling correctors. I need a model that can smartly pick between a set of generated corrections, as the most common one with the lowest edit distance is often not the best (it's right about 65% of the time). I switched to using a contextual corrector based on bigrams which made a small improvement (68% accurate), but am looking for something better. The type of error affects the likelihood of a correction being right as some errors are more likely than others. There's also the issue of speed, as doing an edit distance algo is inherently slow. AFAICT this doesn't seem to be actively being researched, unless i am looking in the wrong places. Is this problem seen as 'solved' by the broader NLP community?",t2_gn9lh,False,,0,False,State of the Art Spelling Correction,[],r/LanguageTechnology,False,6,,0,,False,t3_m3mva9,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1615599960.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to build a really good spell corrector for our search engine. We have a lot of domain specific terms so using an off the shelf one is not going to work well. I found the Peter Norvig spelling corrector post but am having trouble finding more sophisticated (and also scalable) spelling correctors. I need a model that can smartly pick between a set of generated corrections, as the most common one with the lowest edit distance is often not the best (it&amp;#39;s right about 65% of the time). I switched to using a contextual corrector based on bigrams which made a small improvement (68% accurate), but am looking for something better. The type of error affects the likelihood of a correction being right as some errors are more likely than others. There&amp;#39;s also the issue of speed, as doing an edit distance algo is inherently slow. AFAICT this doesn&amp;#39;t seem to be actively being researched, unless i am looking in the wrong places. Is this problem seen as &amp;#39;solved&amp;#39; by the broader NLP community?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m3mva9,True,,simonhughes22,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m3mva9/state_of_the_art_spelling_correction/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m3mva9/state_of_the_art_spelling_correction/,30199,1615571160.0,0,,False,,,,,,999
556,,LanguageTechnology,"I'm reading a couple of papers by Smolensky and Goldrick on their Gradient Symbolic Computation and similar models unifying neural networks and symbolic computation. (Ex. [Cho, Goldrick &amp; Smolensky (2017)](https://faculty.wcas.northwestern.edu/matt-goldrick/publications/pdfs/GSC_LV_final3.pdf) or [Goldrick &amp; Smolensky (2016)](http://roa.rutgers.edu/content/article/files/1552_smolensky_1.pdf)
Really awesome! 

Does anyone of you know where to find a concrete example implemented in Python or R?",t2_6g45cibc,False,,0,False,Gradient symbolic Computation... Do you know concrete implementations?,[],r/LanguageTechnology,False,6,,0,,False,t3_m3l7q7,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1615595401.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m reading a couple of papers by Smolensky and Goldrick on their Gradient Symbolic Computation and similar models unifying neural networks and symbolic computation. (Ex. &lt;a href=""https://faculty.wcas.northwestern.edu/matt-goldrick/publications/pdfs/GSC_LV_final3.pdf""&gt;Cho, Goldrick &amp;amp; Smolensky (2017)&lt;/a&gt; or &lt;a href=""http://roa.rutgers.edu/content/article/files/1552_smolensky_1.pdf""&gt;Goldrick &amp;amp; Smolensky (2016)&lt;/a&gt;
Really awesome! &lt;/p&gt;

&lt;p&gt;Does anyone of you know where to find a concrete example implemented in Python or R?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m3l7q7,True,,tonio9120,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m3l7q7/gradient_symbolic_computation_do_you_know/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m3l7q7/gradient_symbolic_computation_do_you_know/,30199,1615566601.0,2,,False,,,,,,505
557,,LanguageTechnology,"So I'm running gpt-2 simple in a google colab and I have two simple problems.

Firstly, the text generation speed is far too slow. With minimal input for the medium model I get around 12+ seconds to generate.

So to try and solve this problem I downgraded the model to 124M but the model wasnt capable of a basic conversation.

I need a model that's not only capable of generating text fast but is also just as effect as gpt-2 355M if not more effective.

I'm wondering if any of you may know a gpt-2 alternative or a way to customize gpt-2 simple to generate faster?",t2_auvwk62n,False,,0,False,Gpt-2 simple isnt good enough for the chatbot I want.,[],r/LanguageTechnology,False,6,,0,,False,t3_m39se2,False,dark,0.67,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1615552514.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I&amp;#39;m running gpt-2 simple in a google colab and I have two simple problems.&lt;/p&gt;

&lt;p&gt;Firstly, the text generation speed is far too slow. With minimal input for the medium model I get around 12+ seconds to generate.&lt;/p&gt;

&lt;p&gt;So to try and solve this problem I downgraded the model to 124M but the model wasnt capable of a basic conversation.&lt;/p&gt;

&lt;p&gt;I need a model that&amp;#39;s not only capable of generating text fast but is also just as effect as gpt-2 355M if not more effective.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m wondering if any of you may know a gpt-2 alternative or a way to customize gpt-2 simple to generate faster?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m39se2,True,,smellybellyman69,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m39se2/gpt2_simple_isnt_good_enough_for_the_chatbot_i/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m39se2/gpt2_simple_isnt_good_enough_for_the_chatbot_i/,30199,1615523714.0,0,,False,,,,,,567
558,,LanguageTechnology,"It seems that both spaCy and transform based models (like [https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)) are suite for good entity extraction in English. So it's hard for me to know which one I should use...

Do you have an opinion on this in terms of accuracy and performance ?

Thanks!",t2_9xrf8qmw,False,,0,False,SpaCy VS Transformers for NER,[],r/LanguageTechnology,False,6,,0,,False,t3_m2s4ko,False,dark,0.95,,public,17,0,{},,False,[],,False,False,,{},,False,17,,False,False,,False,,[],{},,True,,1615503686.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;It seems that both spaCy and transform based models (like &lt;a href=""https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english""&gt;https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english&lt;/a&gt;) are suite for good entity extraction in English. So it&amp;#39;s hard for me to know which one I should use...&lt;/p&gt;

&lt;p&gt;Do you have an opinion on this in terms of accuracy and performance ?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m2s4ko,True,,software38,,16,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m2s4ko/spacy_vs_transformers_for_ner/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m2s4ko/spacy_vs_transformers_for_ner/,30199,1615474886.0,0,,False,,,,,,388
559,,LanguageTechnology,,t2_hkv9s,False,,0,False,On Generating Extended Summaries of Long Documents (Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_m2pkl0,False,dark,0.81,,public,12,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Inc63mLLInA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'On Generating Extended Summaries of Long Documents (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Inc63mLLInA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Inc63mLLInA/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Inc63mLLInA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/m2pkl0', 'height': 200}",,False,12,,False,False,,False,,[],{},,False,,1615496506.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m2pkl0,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m2pkl0/on_generating_extended_summaries_of_long/,all_ads,False,https://youtu.be/Inc63mLLInA,30199,1615467706.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'On Generating Extended Summaries of Long Documents (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Inc63mLLInA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Inc63mLLInA/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/Inc63mLLInA,,,,,0
560,,LanguageTechnology,"I'm fairly new to NLP and would like to try and detect similarities in two different short-paragraph texts that I think are coming from the same author.

For example, a person or bot sends an English-language text message with about 100 words.  Then I get another text message with about 150 words, but the words used and the abbreviations are common (e.g. ""St."" with a period used for street).  

&amp;nbsp;

Is there an NLP library/tool that you can recommend that I can use to analyze 2 text corpus to get a percentage confidence that the two texts were created by the same author (authorship identification?)? Sort of like a plagiarism detector or a library/tool that can check for semantical identicalities (not sure if the manner of use of period, commas, etc. are also detected).

&amp;nbsp;

I would love to use a  library/environment that uses python but if not, please suggest any other (even if its a hosted website that allows you to paste in texts to compare).

Thanks!",t2_a48goybs,False,,0,False,Libraries/tools to determine if same authorship of short text,[],r/LanguageTechnology,False,6,,0,,False,t3_m2xugm,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1615517912.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m fairly new to NLP and would like to try and detect similarities in two different short-paragraph texts that I think are coming from the same author.&lt;/p&gt;

&lt;p&gt;For example, a person or bot sends an English-language text message with about 100 words.  Then I get another text message with about 150 words, but the words used and the abbreviations are common (e.g. &amp;quot;St.&amp;quot; with a period used for street).  &lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Is there an NLP library/tool that you can recommend that I can use to analyze 2 text corpus to get a percentage confidence that the two texts were created by the same author (authorship identification?)? Sort of like a plagiarism detector or a library/tool that can check for semantical identicalities (not sure if the manner of use of period, commas, etc. are also detected).&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;I would love to use a  library/environment that uses python but if not, please suggest any other (even if its a hosted website that allows you to paste in texts to compare).&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m2xugm,True,,Bikerrrrrrr,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m2xugm/librariestools_to_determine_if_same_authorship_of/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m2xugm/librariestools_to_determine_if_same_authorship_of/,30199,1615489112.0,0,,False,,,,,,982
561,,LanguageTechnology,looking for a paraphrase dataset for Slovak. Any link ?,t2_xf374,False,,0,False,Paraphrase Dataset for Slovak,[],r/LanguageTechnology,False,6,,0,,False,t3_m2vbko,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1615511709.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;looking for a paraphrase dataset for Slovak. Any link ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m2vbko,True,,thak123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m2vbko/paraphrase_dataset_for_slovak/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m2vbko/paraphrase_dataset_for_slovak/,30199,1615482909.0,0,,False,,,,,,55
562,,LanguageTechnology,"Hi,

&amp;#x200B;

I'm pretty new to NLP and was experimenting with spacy to do serveral basic NLP tasks. Now I wanted to do some sentiment analysis.

First I tried Textblob, but it only works for English. After some research I thought using Hugging/Face Transformers might be a better solution.

What do you think is the best option for doing a sentiment analysis? Preferably I would like to use pretrained models.

&amp;#x200B;

Thanks!",t2_sdfge43,False,,0,False,Sentiment Analysis with Transformers vs. Textblob,[],r/LanguageTechnology,False,6,,0,,False,t3_m2pnrk,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1615496789.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m pretty new to NLP and was experimenting with spacy to do serveral basic NLP tasks. Now I wanted to do some sentiment analysis.&lt;/p&gt;

&lt;p&gt;First I tried Textblob, but it only works for English. After some research I thought using Hugging/Face Transformers might be a better solution.&lt;/p&gt;

&lt;p&gt;What do you think is the best option for doing a sentiment analysis? Preferably I would like to use pretrained models.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m2pnrk,True,,ProKellaSK,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m2pnrk/sentiment_analysis_with_transformers_vs_textblob/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m2pnrk/sentiment_analysis_with_transformers_vs_textblob/,30199,1615467989.0,0,,False,,,,,,438
563,,LanguageTechnology,"Has anyone ever tried to do NLP Topic Modeling using Reddit jokes? Currently I'm trying to figure out the best topics for reddit jokes but I'm having a hard time how as the scores using LDA, LSI and HDP are very low. Scores are only around 0.3, 0.2 and the words are just son, man, beer. It just doesn't make sense. My dataset contains jokes from the subreddit dadjokes, 3amjokes, antijokes, darkjokes and jokes.

Here's my code - [https://github.com/ZL63388/c6\_sprint4/blob/main/Reddit\_Joke\_Classifier.ipynb](https://github.com/ZL63388/c6_sprint4/blob/main/Reddit_Joke_Classifier.ipynb)",t2_apbi47i8,False,,0,False,Topic Modeling using Reddit jokes,[],r/LanguageTechnology,False,6,,0,,False,t3_m2l47p,False,dark,0.85,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,1615450719.0,,[],{},,True,,1615478449.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Has anyone ever tried to do NLP Topic Modeling using Reddit jokes? Currently I&amp;#39;m trying to figure out the best topics for reddit jokes but I&amp;#39;m having a hard time how as the scores using LDA, LSI and HDP are very low. Scores are only around 0.3, 0.2 and the words are just son, man, beer. It just doesn&amp;#39;t make sense. My dataset contains jokes from the subreddit dadjokes, 3amjokes, antijokes, darkjokes and jokes.&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s my code - &lt;a href=""https://github.com/ZL63388/c6_sprint4/blob/main/Reddit_Joke_Classifier.ipynb""&gt;https://github.com/ZL63388/c6_sprint4/blob/main/Reddit_Joke_Classifier.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m2l47p,True,,ZL63388,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m2l47p/topic_modeling_using_reddit_jokes/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m2l47p/topic_modeling_using_reddit_jokes/,30199,1615449649.0,0,,False,,,,,,590
564,,LanguageTechnology,"Hi, Has anyone here has done topic Modelling on the DUC 2004 plain text dataset? 

I'm not having good results. If you don't know about the dataset, it is similar to a news dataset. 

Can someone guide me to a quality tutorial as i have gone through every video and article and it doesn't seem cover my problem.",t2_65fqlohg,False,,0,False,Topic Modelling (LDA) on DUC 2004 dataset,[],r/LanguageTechnology,False,6,,0,,False,t3_m2t10g,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1615506000.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, Has anyone here has done topic Modelling on the DUC 2004 plain text dataset? &lt;/p&gt;

&lt;p&gt;I&amp;#39;m not having good results. If you don&amp;#39;t know about the dataset, it is similar to a news dataset. &lt;/p&gt;

&lt;p&gt;Can someone guide me to a quality tutorial as i have gone through every video and article and it doesn&amp;#39;t seem cover my problem.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m2t10g,True,,usmannkhan,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m2t10g/topic_modelling_lda_on_duc_2004_dataset/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m2t10g/topic_modelling_lda_on_duc_2004_dataset/,30199,1615477200.0,0,,False,,,,,,311
565,,LanguageTechnology,"I'm asking the question here because I know that some of you liked the idea behind [nlpcloud.io](https://nlpcloud.io), so here's the question: which NLP models should we now add to NLP Cloud?

We initially proposed all the spaCy pre-trained models for NER and POS tagging, and we then heard a lot of users asking for transformer-based models. That's why we added the following transformer-based models last week:

* Facebook's Bart Large MNLI model, for **text classification**
* Facebook's Bart Large CNN model, for **text summarization**
* Deepset's Roberta Base Squad 2 model, for **question answering**
* DistilBERT Base Uncased Finetuned SST-2 English model, for **sentiment analysis**

We are now wondering which models we should add next. Should we add more transformer-based models? Or pre-trained models from another framework? What would you find the most useful for your own situation that we are lacking for the moment?

Thanks a lot!",t2_4z4m2qcs,False,,0,False,Next models on nlpcloud.io?,[],r/LanguageTechnology,False,6,,0,,False,t3_m2sjh5,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,1619936587.0,,[],{},,True,,1615504705.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m asking the question here because I know that some of you liked the idea behind &lt;a href=""https://nlpcloud.io""&gt;nlpcloud.io&lt;/a&gt;, so here&amp;#39;s the question: which NLP models should we now add to NLP Cloud?&lt;/p&gt;

&lt;p&gt;We initially proposed all the spaCy pre-trained models for NER and POS tagging, and we then heard a lot of users asking for transformer-based models. That&amp;#39;s why we added the following transformer-based models last week:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Facebook&amp;#39;s Bart Large MNLI model, for &lt;strong&gt;text classification&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Facebook&amp;#39;s Bart Large CNN model, for &lt;strong&gt;text summarization&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Deepset&amp;#39;s Roberta Base Squad 2 model, for &lt;strong&gt;question answering&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;DistilBERT Base Uncased Finetuned SST-2 English model, for &lt;strong&gt;sentiment analysis&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are now wondering which models we should add next. Should we add more transformer-based models? Or pre-trained models from another framework? What would you find the most useful for your own situation that we are lacking for the moment?&lt;/p&gt;

&lt;p&gt;Thanks a lot!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m2sjh5,True,,juliensalinas,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m2sjh5/next_models_on_nlpcloudio/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m2sjh5/next_models_on_nlpcloudio/,30199,1615475905.0,0,,False,,,,,,946
566,,LanguageTechnology,,t2_77cigax1,False,,0,False,"Video introduction to the join functions of the dplyr package in R programming. The tutorial provides programming examples and explains the difference between inner, left, right, full, semi, and anti joins",[],r/LanguageTechnology,False,6,,0,,False,t3_m2pjst,False,dark,1.0,,public,1,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Yg-pNqzDuN4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Join Data with dplyr in R (6 Examples) | inner, left, righ, full, semi &amp; anti', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Yg-pNqzDuN4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Statistics Globe', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Yg-pNqzDuN4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCyHEww8_SCdxZvEnkCfi55w'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Yg-pNqzDuN4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/m2pjst', 'height': 200}",,False,1,,False,False,,False,,[],{},,False,,1615496436.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m2pjst,True,,JoachimSchork,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m2pjst/video_introduction_to_the_join_functions_of_the/,all_ads,False,https://youtu.be/Yg-pNqzDuN4,30199,1615467636.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Join Data with dplyr in R (6 Examples) | inner, left, righ, full, semi &amp; anti', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Yg-pNqzDuN4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Statistics Globe', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Yg-pNqzDuN4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCyHEww8_SCdxZvEnkCfi55w'}}",False,https://youtu.be/Yg-pNqzDuN4,,,,,0
567,,LanguageTechnology,"I know that bert has been used extensively for semantic search; getting similar documents, articles, etc. Word embeddings capture the context better; but is there a way to implement fuzzy string matching with bert's word embeddings, or is tfidf the better solution for that?",t2_3iqsgk0g,False,,0,False,Semantic Search and Fuzzy string matching,[],r/LanguageTechnology,False,6,,0,,False,t3_m1zob5,False,dark,1.0,,public,21,0,{},,False,[],,False,False,,{},,False,21,,False,False,,False,,[],{},,True,,1615417984.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know that bert has been used extensively for semantic search; getting similar documents, articles, etc. Word embeddings capture the context better; but is there a way to implement fuzzy string matching with bert&amp;#39;s word embeddings, or is tfidf the better solution for that?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m1zob5,True,,wholestars,,13,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m1zob5/semantic_search_and_fuzzy_string_matching/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m1zob5/semantic_search_and_fuzzy_string_matching/,30199,1615389184.0,0,,False,,,,,,274
568,,LanguageTechnology,I've heard on twitter that NAACL 2021 final decisions are out. Anyone got the news yet?,t2_kjk0q,False,,0,False,NAACL 2021 results are out,[],r/LanguageTechnology,False,6,,0,,False,t3_m268vf,False,dark,1.0,,public,9,1,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{'gid_1': 1},,True,,1615433535.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve heard on twitter that NAACL 2021 final decisions are out. Anyone got the news yet?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m268vf,True,,scan33scan33,,10,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m268vf/naacl_2021_results_are_out/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m268vf/naacl_2021_results_are_out/,30199,1615404735.0,0,,False,,,,,,87
569,,LanguageTechnology,Trying to practice my NLP and was hoping for some good paper implementation suggestions. Any ideas? I could simply pick papers related exactly to what my research focuses on but I guess I am wondering what papers people think would provide general valuable skills.,t2_5x92moet,False,,0,False,Paper Implementation Suggestions,[],r/LanguageTechnology,False,6,,0,,False,t3_m2a8bn,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1615442779.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Trying to practice my NLP and was hoping for some good paper implementation suggestions. Any ideas? I could simply pick papers related exactly to what my research focuses on but I guess I am wondering what papers people think would provide general valuable skills.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m2a8bn,True,,Inside_Today4604,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m2a8bn/paper_implementation_suggestions/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m2a8bn/paper_implementation_suggestions/,30199,1615413979.0,0,,False,,,,,,264
570,,LanguageTechnology," 

Hey all, I'm an NVIDIA Senior Data Scientist and have a activate personal account on the sub here and across many ML/AI/DL sub-reddits. I think some of the best discussions and debates I've been apart of or read have come on these subs. I'd love to just throw in a plug here and have you all join NVIDIA virtually for our GTC conference this year. Like the title says, the ""granddaddy's"" of modern AI are going to be speaking at the conference! This is a great opportunity to get online and listen to some quality talks and science across many domains and product talks etc.

Here's a sign up link, I get zero commission for this, just want to share the love.[https://www.nvidia.com/en-us/gtc/?ncid=GTCS21-NVKASMITH](https://www.nvidia.com/en-us/gtc/?ncid=GTCS21-NVKASMITH)

Also here are some bullets of what GTC has in store.

* GTC is a **free** online conference and happening **April 12-16** with live sessions across the world.
* **Keynote** requires **no registration** and happens **Monday, April 12, at 8:30 AM PST**, with a second broadcast at **6:00 PM PST** for APAC audiences.
* The conference will have live webinars, on-demand sessions, posters. Connect With Experts, DLI (with a fee), and multiple panels that include industry pioneers, researchers, developers, start-ups, venture capitalists, and more.
* There is an amazing line-up of speakers including Gordon Bell award winners, AI pioneers, Oscar-winning artists, and thought leaders from every industry.",,False,,0,False,"Turing Award Winners Yoshua Bengio, Geoffrey Hinton, and Yann LeCun to Speak at GTC21",[],r/LanguageTechnology,False,6,,0,,False,t3_m2d741,False,dark,0.71,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,,,False,,,{},,True,,1615450661.0,text,6,,,,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all, I&amp;#39;m an NVIDIA Senior Data Scientist and have a activate personal account on the sub here and across many ML/AI/DL sub-reddits. I think some of the best discussions and debates I&amp;#39;ve been apart of or read have come on these subs. I&amp;#39;d love to just throw in a plug here and have you all join NVIDIA virtually for our GTC conference this year. Like the title says, the &amp;quot;granddaddy&amp;#39;s&amp;quot; of modern AI are going to be speaking at the conference! This is a great opportunity to get online and listen to some quality talks and science across many domains and product talks etc.&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s a sign up link, I get zero commission for this, just want to share the love.&lt;a href=""https://www.nvidia.com/en-us/gtc/?ncid=GTCS21-NVKASMITH""&gt;https://www.nvidia.com/en-us/gtc/?ncid=GTCS21-NVKASMITH&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also here are some bullets of what GTC has in store.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GTC is a &lt;strong&gt;free&lt;/strong&gt; online conference and happening &lt;strong&gt;April 12-16&lt;/strong&gt; with live sessions across the world.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Keynote&lt;/strong&gt; requires &lt;strong&gt;no registration&lt;/strong&gt; and happens &lt;strong&gt;Monday, April 12, at 8:30 AM PST&lt;/strong&gt;, with a second broadcast at &lt;strong&gt;6:00 PM PST&lt;/strong&gt; for APAC audiences.&lt;/li&gt;
&lt;li&gt;The conference will have live webinars, on-demand sessions, posters. Connect With Experts, DLI (with a fee), and multiple panels that include industry pioneers, researchers, developers, start-ups, venture capitalists, and more.&lt;/li&gt;
&lt;li&gt;There is an amazing line-up of speakers including Gordon Bell award winners, AI pioneers, Oscar-winning artists, and thought leaders from every industry.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m2d741,True,,[deleted],,1,True,all_ads,False,[],,dark,/r/LanguageTechnology/comments/m2d741/turing_award_winners_yoshua_bengio_geoffrey/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m2d741/turing_award_winners_yoshua_bengio_geoffrey/,30199,1615421861.0,0,,False,,,,,,1478
571,,LanguageTechnology,"Greetings!

I'm a CompSci major interested in NLP - however I'm very new to NLP. I currently know Spanish and Japanese - and am making a tool that allows users to learn Japanese easier. It's very similar to the windows snipping tool, once you've made a selection, it uses OCR to scan a bitmap for text. My main question is: 

How can I tokenize sentences into individual words, when Japanese doesn't have spaces? 

In English we can simply split up sentences based on spaces, but Japanese text does not have this.

Example: この文にスぺースがありません。 (English: ""In this sentence, there are no spaces"")

The goal I'm working towards is being able to click each word within the sentence, allowing for the program to do SQL queries to a Japanese datasbe containing linguistic information. This will then display back to the user things like the word's meaning, and how to read it. I have the data and database part set up, it's specifically the process of breaking Japanese sentences into easily tokenized strings. 

&amp;#x200B;

Apologies for any mistakes I might have made in writing this, I'm still relatively new to NLP and Data Science in general!",t2_139c5y,False,,0,False,Tokenizing / picking words out of non-english languages,[],r/LanguageTechnology,False,6,,0,,False,t3_m22r5j,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1615425571.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Greetings!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m a CompSci major interested in NLP - however I&amp;#39;m very new to NLP. I currently know Spanish and Japanese - and am making a tool that allows users to learn Japanese easier. It&amp;#39;s very similar to the windows snipping tool, once you&amp;#39;ve made a selection, it uses OCR to scan a bitmap for text. My main question is: &lt;/p&gt;

&lt;p&gt;How can I tokenize sentences into individual words, when Japanese doesn&amp;#39;t have spaces? &lt;/p&gt;

&lt;p&gt;In English we can simply split up sentences based on spaces, but Japanese text does not have this.&lt;/p&gt;

&lt;p&gt;Example: この文にスぺースがありません。 (English: &amp;quot;In this sentence, there are no spaces&amp;quot;)&lt;/p&gt;

&lt;p&gt;The goal I&amp;#39;m working towards is being able to click each word within the sentence, allowing for the program to do SQL queries to a Japanese datasbe containing linguistic information. This will then display back to the user things like the word&amp;#39;s meaning, and how to read it. I have the data and database part set up, it&amp;#39;s specifically the process of breaking Japanese sentences into easily tokenized strings. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Apologies for any mistakes I might have made in writing this, I&amp;#39;m still relatively new to NLP and Data Science in general!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m22r5j,True,,Doodle173,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m22r5j/tokenizing_picking_words_out_of_nonenglish/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m22r5j/tokenizing_picking_words_out_of_nonenglish/,30199,1615396771.0,0,,False,,,,,,1139
572,,LanguageTechnology,"Seq2seq models are advantageous for their ability to process text inputs without a constrained length. This tutorial covers encoder-decoder sequence-to-sequence models (seq2seq) in-depth. Topics covered include:

1. Seq2seq architecture
2. Applications
3. Implementing seq2seq for text summarization with Keras

Note that the third point only goes up until the data loading and text processing steps. Training and inference are covered in Part 2, which is coming out on Friday. :)

Article link: [https://blog.paperspace.com/introduction-to-seq2seq-models/](https://blog.paperspace.com/introduction-to-seq2seq-models/)

You can also run the full code on a free GPU: https://ml-showcase.paperspace.com/projects/text-summarization-with-seq2seq-models",t2_15en0l,False,,0,False,[Tutorial] Introduction to Encoder-Decoder Sequence-to-Sequence Models (Seq2Seq),[],r/LanguageTechnology,False,6,,0,,False,t3_m1zzyc,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1615418844.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Seq2seq models are advantageous for their ability to process text inputs without a constrained length. This tutorial covers encoder-decoder sequence-to-sequence models (seq2seq) in-depth. Topics covered include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Seq2seq architecture&lt;/li&gt;
&lt;li&gt;Applications&lt;/li&gt;
&lt;li&gt;Implementing seq2seq for text summarization with Keras&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note that the third point only goes up until the data loading and text processing steps. Training and inference are covered in Part 2, which is coming out on Friday. :)&lt;/p&gt;

&lt;p&gt;Article link: &lt;a href=""https://blog.paperspace.com/introduction-to-seq2seq-models/""&gt;https://blog.paperspace.com/introduction-to-seq2seq-models/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can also run the full code on a free GPU: &lt;a href=""https://ml-showcase.paperspace.com/projects/text-summarization-with-seq2seq-models""&gt;https://ml-showcase.paperspace.com/projects/text-summarization-with-seq2seq-models&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m1zzyc,True,,hellopaperspace,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m1zzyc/tutorial_introduction_to_encoderdecoder/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m1zzyc/tutorial_introduction_to_encoderdecoder/,30199,1615390044.0,0,,False,,,,,,748
573,,LanguageTechnology,Idk what this task is called in NLP. But is there any dataset that allows me to do this?,t2_gqagj9l,False,,0,False,How to predict a word in a corpus given its description,[],r/LanguageTechnology,False,6,,0,,False,t3_m1shj4,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1615391810.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Idk what this task is called in NLP. But is there any dataset that allows me to do this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m1shj4,True,,iCEChEshirE,,10,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m1shj4/how_to_predict_a_word_in_a_corpus_given_its/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m1shj4/how_to_predict_a_word_in_a_corpus_given_its/,30199,1615363010.0,0,,False,,,,,,88
574,,LanguageTechnology,"I have a doubt someone explain please: we have seen this classic example of why we use bert and other sentence level embedding rather than glove or doc2vec etc. for sentence level task because the same word ""bank"" used in different context will carry different meaning.

But if we are using it in two sentence :
1) man went to bank to withdraw money.
2) man went to bank for fishing.

Wouldn't adding bank embedding with deposit for 1st sentence and fishing for 2nd sentence be well enough to differentiate between sentences...so why do we do this sentence embedding when avg. of word embedding are just good enough, I don't see this reason justify the meaning behind it well enough.",t2_q1zjtoc,False,,0,False,Got Confused going through a documentation.,[],r/LanguageTechnology,False,6,,0,,False,t3_m20dgi,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1615419840.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a doubt someone explain please: we have seen this classic example of why we use bert and other sentence level embedding rather than glove or doc2vec etc. for sentence level task because the same word &amp;quot;bank&amp;quot; used in different context will carry different meaning.&lt;/p&gt;

&lt;p&gt;But if we are using it in two sentence :
1) man went to bank to withdraw money.
2) man went to bank for fishing.&lt;/p&gt;

&lt;p&gt;Wouldn&amp;#39;t adding bank embedding with deposit for 1st sentence and fishing for 2nd sentence be well enough to differentiate between sentences...so why do we do this sentence embedding when avg. of word embedding are just good enough, I don&amp;#39;t see this reason justify the meaning behind it well enough.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m20dgi,True,,172knot,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m20dgi/got_confused_going_through_a_documentation/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m20dgi/got_confused_going_through_a_documentation/,30199,1615391040.0,0,,False,,,,,,683
575,,LanguageTechnology,"Hi friends,

I have a huge documents(tickets) raised by users in company.But most of those are not more than 50 words and after preprocessing it came down to hardly 20-30 words
If I use Word2vec it is increasing the dimension to 300 
Still is it advisable to go for word2vec for proper relation between words or Custom word embeddings of less dimension",t2_94hg2kuk,False,,0,False,Custom Word Embeddings or Word2Vec,[],r/LanguageTechnology,False,6,,0,,False,t3_m1wiux,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1615408560.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi friends,&lt;/p&gt;

&lt;p&gt;I have a huge documents(tickets) raised by users in company.But most of those are not more than 50 words and after preprocessing it came down to hardly 20-30 words
If I use Word2vec it is increasing the dimension to 300 
Still is it advisable to go for word2vec for proper relation between words or Custom word embeddings of less dimension&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m1wiux,True,,Jambaladakipamba,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m1wiux/custom_word_embeddings_or_word2vec/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m1wiux/custom_word_embeddings_or_word2vec/,30199,1615379760.0,0,,False,,,,,,352
576,,LanguageTechnology,"Hey guys!

Hope you are all well.

I found the attractive tutorial in ACL 2018 '*Beyond Multiword Expressions: Processing Idioms and Metaphors',* and am very interested in this topic about modeling metaphors.  
But it was unfortunate that the link provided in ACL2018 ([https://www.angl.hu-berlin.de/department/staff-faculty/other/kordoni/acl18\_tutorial)](https://www.angl.hu-berlin.de/department/staff-faculty/other/kordoni/acl18_tutorial)) was down. 

Is there anyone willing to share a copy or a reachable link?

Thank you in advance!",t2_1994txsa,False,,0,False,Question About ACL2018 Tutorial5,[],r/LanguageTechnology,False,6,,0,,False,t3_m1sn0c,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1615392455.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys!&lt;/p&gt;

&lt;p&gt;Hope you are all well.&lt;/p&gt;

&lt;p&gt;I found the attractive tutorial in ACL 2018 &amp;#39;&lt;em&gt;Beyond Multiword Expressions: Processing Idioms and Metaphors&amp;#39;,&lt;/em&gt; and am very interested in this topic about modeling metaphors.&lt;br/&gt;
But it was unfortunate that the link provided in ACL2018 (&lt;a href=""https://www.angl.hu-berlin.de/department/staff-faculty/other/kordoni/acl18_tutorial""&gt;https://www.angl.hu-berlin.de/department/staff-faculty/other/kordoni/acl18_tutorial)&lt;/a&gt;) was down. &lt;/p&gt;

&lt;p&gt;Is there anyone willing to share a copy or a reachable link?&lt;/p&gt;

&lt;p&gt;Thank you in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m1sn0c,True,,XiaoLiu1028,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m1sn0c/question_about_acl2018_tutorial5/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m1sn0c/question_about_acl2018_tutorial5/,30199,1615363655.0,0,,False,,,,,,538
577,,LanguageTechnology,,t2_16gftjf,False,,0,False,Fun failure compilation from our gesture synthesis research,[],r/LanguageTechnology,False,6,,0,,False,t3_m18oa2,False,dark,0.81,,public,6,0,"{'content': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/-C2H1YojjWI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 267, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Failure compilation from my gesture synthesis research', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 267, 'html': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/-C2H1YojjWI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Taras Kucherenko', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/-C2H1YojjWI/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC62QjHPTaJR0yt1bOIcYfwA'}}",False,False,,"{'content': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/-C2H1YojjWI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 267, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/m18oa2', 'height': 200}",,False,6,,False,False,,False,,[],{},,False,,1615332232.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m18oa2,True,,Svito-zar,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m18oa2/fun_failure_compilation_from_our_gesture/,all_ads,False,https://youtu.be/-C2H1YojjWI,30199,1615303432.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Failure compilation from my gesture synthesis research', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 267, 'html': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/-C2H1YojjWI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Taras Kucherenko', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/-C2H1YojjWI/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC62QjHPTaJR0yt1bOIcYfwA'}}",False,https://youtu.be/-C2H1YojjWI,"[{'approved_at_utc': None, 'subreddit': 'MediaSynthesis', 'selftext': '', 'author_fullname': 't2_16gftjf', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Fun failure compilation from our gesture synthesis research', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MediaSynthesis', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'white', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_m18et1', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.85, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 12, 'total_awards_received': 0, 'media_embed': {'content': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/-C2H1YojjWI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 267, 'scrolling': False, 'height': 200}, 'author_flair_template_id': None, 'is_original_content': True, 'user_reports': [], 'secure_media': {'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Failure compilation from my gesture synthesis research', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 267, 'html': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/-C2H1YojjWI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Taras Kucherenko', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/-C2H1YojjWI/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC62QjHPTaJR0yt1bOIcYfwA'}}, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {'content': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/-C2H1YojjWI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 267, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/m18et1', 'height': 200}, 'link_flair_text': 'Research', 'can_mod_post': False, 'score': 12, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1615331513.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'youtu.be', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://youtu.be/-C2H1YojjWI', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '6f0cbb62-31fe-11e9-9618-0e6fae9351e8', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_f2bje', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'm18et1', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Svito-zar', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MediaSynthesis/comments/m18et1/fun_failure_compilation_from_our_gesture/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://youtu.be/-C2H1YojjWI', 'subreddit_subscribers': 23435, 'created_utc': 1615302713.0, 'num_crossposts': 1, 'media': {'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Failure compilation from my gesture synthesis research', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 267, 'html': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/-C2H1YojjWI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Taras Kucherenko', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/-C2H1YojjWI/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC62QjHPTaJR0yt1bOIcYfwA'}}, 'is_video': False}]",t3_m18et1,,,0
578,,LanguageTechnology,"Looking to train an NER model using your favorite transformer?

Checkout this new article [https://walidamamou.medium.com/how-to-fine-tune-bert...](https://walidamamou.medium.com/how-to-fine-tune-bert-transformer-with-spacy-3-6a90bfe57647?fbclid=IwAR0MrqwSjdlkJq3F58F_rGUqGRPHwaKnAzUyv-87Z6jzPtZMgRR2Bwfhqaw) on how to fine tune BERT transformer for NER using the newly released spaCy 3.

On a different note, we are looking for NLP experts to join our team (https://ubiai.tools), if anyone is interested please DM me!",t2_32tnavmg,False,,0,False,How to fine-tune BERT with Spacy 3: tutorial,[],r/LanguageTechnology,False,6,,0,,False,t3_m0pcrf,False,dark,0.9,,public,32,0,{},,False,[],,False,False,,{},,False,32,,False,False,,False,,[],{},,True,,1615264449.0,text,6,,,text,self.LanguageTechnology,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Looking to train an NER model using your favorite transformer?&lt;/p&gt;

&lt;p&gt;Checkout this new article &lt;a href=""https://walidamamou.medium.com/how-to-fine-tune-bert-transformer-with-spacy-3-6a90bfe57647?fbclid=IwAR0MrqwSjdlkJq3F58F_rGUqGRPHwaKnAzUyv-87Z6jzPtZMgRR2Bwfhqaw""&gt;https://walidamamou.medium.com/how-to-fine-tune-bert...&lt;/a&gt; on how to fine tune BERT transformer for NER using the newly released spaCy 3.&lt;/p&gt;

&lt;p&gt;On a different note, we are looking for NLP experts to join our team (&lt;a href=""https://ubiai.tools""&gt;https://ubiai.tools&lt;/a&gt;), if anyone is interested please DM me!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m0pcrf,True,,UBIAI,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m0pcrf/how_to_finetune_bert_with_spacy_3_tutorial/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m0pcrf/how_to_finetune_bert_with_spacy_3_tutorial/,30199,1615235649.0,0,,False,,,,,,518
579,,LanguageTechnology,"Hi all,

Hope everyone is keeping safe in these tough times. I'm a noob in NLP related technologies, I'm working on a lot of projects to consolidate my learnings. I want to understand the lifecycle of process of making a chat bot for that I want to learn NLU concepts can anyone suggest me some good resource for that. I'll really appreciate it",t2_4inu94cw,False,,0,False,NLU learning path,[],r/LanguageTechnology,False,6,,0,,False,t3_m0fxir,False,dark,1.0,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1615239772.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;Hope everyone is keeping safe in these tough times. I&amp;#39;m a noob in NLP related technologies, I&amp;#39;m working on a lot of projects to consolidate my learnings. I want to understand the lifecycle of process of making a chat bot for that I want to learn NLU concepts can anyone suggest me some good resource for that. I&amp;#39;ll really appreciate it&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m0fxir,True,,gubberex,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m0fxir/nlu_learning_path/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m0fxir/nlu_learning_path/,30199,1615210972.0,0,,False,,,,,,344
580,,LanguageTechnology,"# Intent and Action Classification,  analyze Chinese News and the Crypto market, train a classifier that understands 100+ languages, translate between 200 + languages, answer questions, summarize text, and much more in NLU 1.1.3 

## NLU 1.1.3 Release Notes
We are very excited to announce that the latest NLU release comes with a new pretrained Intent Classifier and NER Action Extractor for text related to
music, restaurants, and movies trained on the SNIPS dataset. Make sure to check out the models hub and the easy 1-liners for more info!

In addition to that, new NER and Embedding models for Bengali are now available

Finally, there is a new NLU Webinar with 9 accompanying tutorial notebooks which teach you  a lot of things and is segmented into the following parts :

- Part1: Easy 1 Liners 
  - Spell checking/Sentiment/POS/NER/ BERTtology embeddings
- Part2: Data analysis and NLP tasks on [Crypto News Headline dataset](https://www.kaggle.com/kashnitsky/news-about-major-cryptocurrencies-20132018-40k)
  - Preprocessing and extracting Emotions, Keywords, Named Entities and visualize them
- Part3: NLU Multi-Lingual 1 Liners with [Microsoft's Marian Models](https://marian-nmt.github.io/publications/)
  - Translate between 200+ languages (and classify lang afterward)
- Part 4: Data analysis and NLP tasks on [Chinese News Article Dataset](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/4_Unsupervise_Chinese_Keyword_Extraction_NER_and_Translation_from_Chinese_News.ipynb)
  - Word Segmentation, Lemmatization, Extract Keywords, Named Entities and translate to english
- Part 5: Train a sentiment Classifier that understands 100+ Languages
  - Train on a french sentiment dataset and predict the sentiment of 100+ languages with [language-agnostic BERT Sentence Embedding](https://arxiv.org/abs/2007.01852)
- Part 6: Question answering, Summarization, Squad and more with [Google's T5](https://arxiv.org/abs/1910.10683)
  - T5 Question answering and 18 + other NLP tasks ([SQUAD](https://arxiv.org/abs/1606.05250) / [GLUE](https://arxiv.org/abs/1804.07461) / [SUPER GLUE](https://super.gluebenchmark.com/))


### New Models

#### NLU 1.1.3 New Non-English Models

| Language | nlu.load() reference                                         | Spark NLP Model reference                                    | Type                  |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ | --------------------- |
| Bengali  | [bn.ner.cc_300d](https://nlp.johnsnowlabs.com/2021/02/10/bengali_cc_300d_bn.html) | [ bengaliner_cc_300d](https://nlp.johnsnowlabs.com/2021/02/10/bengali_cc_300d_bn.html) | NerDLModel    |
| Bengali  | [bn.embed](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html) | [bengali_cc_300d](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html) | NerDLModel            |
| Bengali  | [bn.embed.cc_300d](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html) | [bengali_cc_300d](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html) | Word Embeddings Model (Alias)    |
| Bengali  | [bn.embed.glove](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html) | [bengali_cc_300d](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html) |  Word Embeddings Model (Alias)|





#### NLU 1.1.3 New English Models

|Language | nlu.load() reference | Spark NLP Model reference | Type |
|---------|---------------------|----------------------------|------|
| English | [en.classify.snips](https://nlp.johnsnowlabs.com/2021/02/15/nerdl_snips_100d_en.html) |[nerdl_snips_100d](https://nlp.johnsnowlabs.com/2021/02/15/nerdl_snips_100d_en.html)     | NerDLModel |
| English | [en.ner.snips](https://nlp.johnsnowlabs.com/2021/02/15/classifierdl_use_snips_en.html) |[classifierdl_use_snips](https://nlp.johnsnowlabs.com/2021/02/15/classifierdl_use_snips_en.html)|ClassifierDLModel|




### New NLU Webinar
#### [State-of-the-art Natural Language Processing for 200+ Languages with 1 Line of code](https://events.johnsnowlabs.com/state-of-the-art-natural-language-processing-for-200-languages-with-1-line-of-code)


##### Talk Abstract 
Learn to harness the power of 1,000+ production-grade &amp; scalable NLP models for 200+ languages - all available with just 1 line of Python code by leveraging the open-source NLU library, which is powered by the widely popular Spark NLP.

John Snow Labs has delivered over 80 releases of Spark NLP to date, making it the most widely used NLP library in the enterprise and providing the AI community with state-of-the-art accuracy and scale for a variety of common NLP tasks. The most recent releases include pre-trained models for over 200 languages - including languages that do not use spaces for word segmentation algorithms like Chinese, Japanese, and Korean, and languages written from right to left like Arabic, Farsi, Urdu, and Hebrew. All software and models are free and open source under an Apache 2.0 license.

This webinar will show you how to leverage the multi-lingual capabilities of Spark NLP &amp; NLU - including automated language detection for up to 375 languages, and the ability to perform translation, named entity recognition, stopword removal, lemmatization, and more in a variety of language families. We will create Python code in real-time and solve these problems in just 30 minutes. The notebooks will then be made freely available online.

You can watch the [video here,](https://events.johnsnowlabs.com/state-of-the-art-natural-language-processing-for-200-languages-with-1-line-of-code) 

### NLU 1.1.3 New Notebooks and tutorials


#### New Webinar Notebooks

1. [NLU basics, easy 1-liners (Spellchecking, sentiment, NER, POS, BERT](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/0_liners_intro.ipynb)
2. [Analyze Crypto News dataset with Keyword extraction, NER, Emotional distribution, and stemming](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/1_NLU_base_features_on_dataset_with_YAKE_Lemma_Stemm_classifiers_NER_.ipynb)
3. [Translate Crypto News dataset between 300 Languages with the Marian Model (German, French, Hebrew examples)](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/2_multilingual_translation_with_marian_intro.ipynb)
4. [Translate Crypto News dataset between 300 Languages with the Marian Model (Hindi, Russian, Chinese examples)](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/3_more_multi_lingual_NLP_translation_Asian_languages_with_Marian.ipynb)
5. [Analyze Chinese News Headlines with Chinese Word Segmentation, Lemmatization, NER, and Keyword extraction](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/4_Unsupervise_Chinese_Keyword_Extraction_NER_and_Translation_from_Chinese_News.ipynb)
6. [Train a Sentiment Classifier that will understand 100+ languages on just a French Dataset with the powerful Language Agnostic Bert Embeddings](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/5_multi_lingual_sentiment_classifier_training_for_over_100_languages.ipynb)
7. [Summarize text and Answer Questions with T5](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/6_T5_question_answering_and_Text_summarization.ipynb)
8. [Solve any task in 1 line from SQUAD, GLUE and SUPER GLUE with T5](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/7_T5_SQUAD_GLUE_SUPER_GLUE_TASKS.ipynb)
9. [Overview of models for various languages](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/8_Multi_lingual_ner_pos_stop_words_sentiment_pretrained.ipynb)





#### New easy NLU 1-liners in NLU 1.1.3

####  [Detect actions in general commands related to music, restaurant, movies.](https://nlp.johnsnowlabs.com/2021/02/15/nerdl_snips_100d_en.html)


```python
nlu.load(""en.classify.snips"").predict(""book a spot for nona gray  myrtle and alison at a top-rated brasserie that is distant from wilson av on nov  the 4th  2030 that serves ouzeri"",output_level = ""document"")
```

outputs :

|                                               ner_confidence | entities                                                     | document                                                     | Entities_Classes                                             |
| -----------------------------------------------------------: | :----------------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| [1.0, 1.0, 0.9997000098228455, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9990000128746033, 1.0, 1.0, 1.0, 0.9965000152587891, 0.9998999834060669, 0.9567000269889832, 1.0, 1.0, 1.0, 0.9980000257492065, 0.9991999864578247, 0.9988999962806702, 1.0, 1.0, 0.9998999834060669] | ['nona gray myrtle and alison', 'top-rated', 'brasserie', 'distant', 'wilson av', 'nov the 4th 2030', 'ouzeri'] | book a spot for nona gray myrtle and alison at a top-rated brasserie that is distant from wilson av on nov the 4th 2030 that serves ouzeri | ['party_size_description', 'sort', 'restaurant_type', 'spatial_relation', 'poi', 'timeRange', 'cuisine'] |

####  [Named Entity Recognition (NER) Model in Bengali (bengaliner_cc_300d)](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html)


```python
# Bengali for: 'Iajuddin Ahmed passed Matriculation from Munshiganj High School in 1947 and Intermediate from Munshiganj Horganga College in 1950.'
nlu.load(""bn.ner.cc_300d"").predict(""১৯৪৮ সালে ইয়াজউদ্দিন আহম্মেদ মুন্সিগঞ্জ উচ্চ বিদ্যালয় থেকে মেট্রিক পাশ করেন এবং ১৯৫০ সালে মুন্সিগঞ্জ হরগঙ্গা কলেজ থেকে ইন্টারমেডিয়েট পাশ করেন"",output_level = ""document"")
```

outputs :

| ner_confidence                                                                                                                                                                                                                                                                                                                                                                                                                       | entities                                                                           | Entities_Classes   | document                                                                                                                         |
|---------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------|:--------------------------------------|
| [0.9987999796867371, 0.9854000210762024, 0.8604000210762024, 0.6686999797821045, 0.5289999842643738, 0.7009999752044678, 0.7684999704360962, 0.9979000091552734, 0.9976000189781189, 0.9930999875068665, 0.9994000196456909, 0.9879000186920166, 0.7407000064849854, 0.9215999841690063, 0.7657999992370605, 0.39419999718666077, 0.9124000072479248, 0.9932000041007996, 0.9919999837875366, 0.995199978351593, 0.9991999864578247] | ['সালে', 'ইয়াজউদ্দিন আহম্মেদ', 'মুন্সিগঞ্জ উচ্চ বিদ্যালয়', 'সালে', 'মুন্সিগঞ্জ হরগঙ্গা কলেজ'] | ['TIME', 'PER', 'ORG', 'TIME', 'ORG'] | ১৯৪৮ সালে ইয়াজউদ্দিন আহম্মেদ মুন্সিগঞ্জ উচ্চ বিদ্যালয় থেকে মেট্রিক পাশ করেন এবং ১৯৫০ সালে মুন্সিগঞ্জ হরগঙ্গা কলেজ থেকে ইন্টারমেডিয়েট পাশ করেন |

#### [Identify intent in general text - SNIPS dataset](https://nlp.johnsnowlabs.com/2021/02/15/classifierdl_use_snips_en.html)


```python
nlu.load(""en.ner.snips"").predict(""I want to bring six of us to a bistro in town that serves hot chicken sandwich that is within the same area"",output_level = ""document"")
```

outputs :


| document | snips | snips_confidence|
|----------|------|------------------|
| I want to bring six of us to a bistro in town that serves hot chicken sandwich that is within the same area | BookRestaurant |                  1 |


#### [Word Embeddings for Bengali (bengali_cc_300d)](https://nlp.johnsnowlabs.com/2021/02/10/bengali_cc_300d_bn.html)




```python
# Bengali for : 'Iajuddin Ahmed passed Matriculation from Munshiganj High School in 1947 and Intermediate from Munshiganj Horganga College in 1950.'
nlu.load(""bn.embed"").predict(""১৯৪৮ সালে ইয়াজউদ্দিন আহম্মেদ মুন্সিগঞ্জ উচ্চ বিদ্যালয় থেকে মেট্রিক পাশ করেন এবং ১৯৫০ সালে মুন্সিগঞ্জ হরগঙ্গা কলেজ থেকে ইন্টারমেডিয়েট পাশ করেন"",output_level = ""document"")
```

outputs :

|                                                     document | bn_embed_embeddings                                          |
| -----------------------------------------------------------: | :----------------------------------------------------------- |
| ১৯৪৮ সালে ইয়াজউদ্দিন আহম্মেদ মুন্সিগঞ্জ উচ্চ বিদ্যালয় থেকে মেট্রিক পাশ করেন এবং ১৯৫০ সালে মুন্সিগঞ্জ হরগঙ্গা কলেজ থেকে ইন্টারমেডিয়েট পাশ করেন | [-0.0828      0.0683      0.0215     ...  0.0679     -0.0484...] |



### NLU 1.1.3 Enhancements
- Added automatic conversion  to Sentence Embeddings of Word Embeddings when there is no Sentence Embedding Avaiable and a model needs the converted version to run.


### NLU 1.1.3 Bug Fixes
- Fixed a bug that caused `ur.sentiment` NLU pipeline to build incorrectly
- Fixed a bug that caused `sentiment.imdb.glove` NLU pipeline to build incorrectly
- Fixed a bug that caused `en.sentiment.glove.imdb` NLU pipeline to build incorrectly
- Fixed a bug that caused Spark 2.3.X environments to crash.

### NLU Installation

```bash
# PyPi
!pip install nlu pyspark==2.4.7
#Conda
# Install NLU from Anaconda/Conda
conda install -c johnsnowlabs nlu
```

### Additional NLU ressources

- [NLU Website](https://nlu.johnsnowlabs.com/)
- [All NLU Tutorial Notebooks](https://nlu.johnsnowlabs.com/docs/en/notebooks)
- [NLU Videos and Blogposts on NLU](https://nlp.johnsnowlabs.com/learn#pythons-nlu-library)
- [NLU on Github](https://github.com/JohnSnowLabs/nlu)
- [Suggestions or Questions? Contact us in Slack!](https://join.slack.com/t/spark-nlp/shared_invite/zt-lutct9gm-kuUazcyFKhuGY3_0AMkxqA)",t2_53n73cus,False,,0,False,"Intent and Action Classification, analyze Chinese News and the Crypto market, train a classifier that understands 100+ languages, translate between 200 + languages, answer questions, summarize text and much more on NLU 1.1.3",[],r/LanguageTechnology,False,6,,0,,False,t3_m0chul,False,dark,1.0,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,1615197648.0,,[],{},,True,,1615226127.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;h1&gt;Intent and Action Classification,  analyze Chinese News and the Crypto market, train a classifier that understands 100+ languages, translate between 200 + languages, answer questions, summarize text, and much more in NLU 1.1.3&lt;/h1&gt;

&lt;h2&gt;NLU 1.1.3 Release Notes&lt;/h2&gt;

&lt;p&gt;We are very excited to announce that the latest NLU release comes with a new pretrained Intent Classifier and NER Action Extractor for text related to
music, restaurants, and movies trained on the SNIPS dataset. Make sure to check out the models hub and the easy 1-liners for more info!&lt;/p&gt;

&lt;p&gt;In addition to that, new NER and Embedding models for Bengali are now available&lt;/p&gt;

&lt;p&gt;Finally, there is a new NLU Webinar with 9 accompanying tutorial notebooks which teach you  a lot of things and is segmented into the following parts :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Part1: Easy 1 Liners 

&lt;ul&gt;
&lt;li&gt;Spell checking/Sentiment/POS/NER/ BERTtology embeddings&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Part2: Data analysis and NLP tasks on &lt;a href=""https://www.kaggle.com/kashnitsky/news-about-major-cryptocurrencies-20132018-40k""&gt;Crypto News Headline dataset&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Preprocessing and extracting Emotions, Keywords, Named Entities and visualize them&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Part3: NLU Multi-Lingual 1 Liners with &lt;a href=""https://marian-nmt.github.io/publications/""&gt;Microsoft&amp;#39;s Marian Models&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Translate between 200+ languages (and classify lang afterward)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Part 4: Data analysis and NLP tasks on &lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/4_Unsupervise_Chinese_Keyword_Extraction_NER_and_Translation_from_Chinese_News.ipynb""&gt;Chinese News Article Dataset&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Word Segmentation, Lemmatization, Extract Keywords, Named Entities and translate to english&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Part 5: Train a sentiment Classifier that understands 100+ Languages

&lt;ul&gt;
&lt;li&gt;Train on a french sentiment dataset and predict the sentiment of 100+ languages with &lt;a href=""https://arxiv.org/abs/2007.01852""&gt;language-agnostic BERT Sentence Embedding&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Part 6: Question answering, Summarization, Squad and more with &lt;a href=""https://arxiv.org/abs/1910.10683""&gt;Google&amp;#39;s T5&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;T5 Question answering and 18 + other NLP tasks (&lt;a href=""https://arxiv.org/abs/1606.05250""&gt;SQUAD&lt;/a&gt; / &lt;a href=""https://arxiv.org/abs/1804.07461""&gt;GLUE&lt;/a&gt; / &lt;a href=""https://super.gluebenchmark.com/""&gt;SUPER GLUE&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;New Models&lt;/h3&gt;

&lt;h4&gt;NLU 1.1.3 New Non-English Models&lt;/h4&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Bengali&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/10/bengali_cc_300d_bn.html""&gt;bn.ner.cc_300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/10/bengali_cc_300d_bn.html""&gt; bengaliner_cc_300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NerDLModel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Bengali&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html""&gt;bn.embed&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html""&gt;bengali_cc_300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NerDLModel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Bengali&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html""&gt;bn.embed.cc_300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html""&gt;bengali_cc_300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Word Embeddings Model (Alias)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Bengali&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html""&gt;bn.embed.glove&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html""&gt;bengali_cc_300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Word Embeddings Model (Alias)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;NLU 1.1.3 New English Models&lt;/h4&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/15/nerdl_snips_100d_en.html""&gt;en.classify.snips&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/15/nerdl_snips_100d_en.html""&gt;nerdl_snips_100d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NerDLModel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/15/classifierdl_use_snips_en.html""&gt;en.ner.snips&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/15/classifierdl_use_snips_en.html""&gt;classifierdl_use_snips&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ClassifierDLModel&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h3&gt;New NLU Webinar&lt;/h3&gt;

&lt;h4&gt;&lt;a href=""https://events.johnsnowlabs.com/state-of-the-art-natural-language-processing-for-200-languages-with-1-line-of-code""&gt;State-of-the-art Natural Language Processing for 200+ Languages with 1 Line of code&lt;/a&gt;&lt;/h4&gt;

&lt;h5&gt;Talk Abstract&lt;/h5&gt;

&lt;p&gt;Learn to harness the power of 1,000+ production-grade &amp;amp; scalable NLP models for 200+ languages - all available with just 1 line of Python code by leveraging the open-source NLU library, which is powered by the widely popular Spark NLP.&lt;/p&gt;

&lt;p&gt;John Snow Labs has delivered over 80 releases of Spark NLP to date, making it the most widely used NLP library in the enterprise and providing the AI community with state-of-the-art accuracy and scale for a variety of common NLP tasks. The most recent releases include pre-trained models for over 200 languages - including languages that do not use spaces for word segmentation algorithms like Chinese, Japanese, and Korean, and languages written from right to left like Arabic, Farsi, Urdu, and Hebrew. All software and models are free and open source under an Apache 2.0 license.&lt;/p&gt;

&lt;p&gt;This webinar will show you how to leverage the multi-lingual capabilities of Spark NLP &amp;amp; NLU - including automated language detection for up to 375 languages, and the ability to perform translation, named entity recognition, stopword removal, lemmatization, and more in a variety of language families. We will create Python code in real-time and solve these problems in just 30 minutes. The notebooks will then be made freely available online.&lt;/p&gt;

&lt;p&gt;You can watch the &lt;a href=""https://events.johnsnowlabs.com/state-of-the-art-natural-language-processing-for-200-languages-with-1-line-of-code""&gt;video here,&lt;/a&gt; &lt;/p&gt;

&lt;h3&gt;NLU 1.1.3 New Notebooks and tutorials&lt;/h3&gt;

&lt;h4&gt;New Webinar Notebooks&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/0_liners_intro.ipynb""&gt;NLU basics, easy 1-liners (Spellchecking, sentiment, NER, POS, BERT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/1_NLU_base_features_on_dataset_with_YAKE_Lemma_Stemm_classifiers_NER_.ipynb""&gt;Analyze Crypto News dataset with Keyword extraction, NER, Emotional distribution, and stemming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/2_multilingual_translation_with_marian_intro.ipynb""&gt;Translate Crypto News dataset between 300 Languages with the Marian Model (German, French, Hebrew examples)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/3_more_multi_lingual_NLP_translation_Asian_languages_with_Marian.ipynb""&gt;Translate Crypto News dataset between 300 Languages with the Marian Model (Hindi, Russian, Chinese examples)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/4_Unsupervise_Chinese_Keyword_Extraction_NER_and_Translation_from_Chinese_News.ipynb""&gt;Analyze Chinese News Headlines with Chinese Word Segmentation, Lemmatization, NER, and Keyword extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/5_multi_lingual_sentiment_classifier_training_for_over_100_languages.ipynb""&gt;Train a Sentiment Classifier that will understand 100+ languages on just a French Dataset with the powerful Language Agnostic Bert Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/6_T5_question_answering_and_Text_summarization.ipynb""&gt;Summarize text and Answer Questions with T5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/7_T5_SQUAD_GLUE_SUPER_GLUE_TASKS.ipynb""&gt;Solve any task in 1 line from SQUAD, GLUE and SUPER GLUE with T5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/8_Multi_lingual_ner_pos_stop_words_sentiment_pretrained.ipynb""&gt;Overview of models for various languages&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;New easy NLU 1-liners in NLU 1.1.3&lt;/h4&gt;

&lt;h4&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/15/nerdl_snips_100d_en.html""&gt;Detect actions in general commands related to music, restaurant, movies.&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;python
nlu.load(&amp;quot;en.classify.snips&amp;quot;).predict(&amp;quot;book a spot for nona gray  myrtle and alison at a top-rated brasserie that is distant from wilson av on nov  the 4th  2030 that serves ouzeri&amp;quot;,output_level = &amp;quot;document&amp;quot;)
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;outputs :&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""right""&gt;ner_confidence&lt;/th&gt;
&lt;th align=""left""&gt;entities&lt;/th&gt;
&lt;th align=""left""&gt;document&lt;/th&gt;
&lt;th align=""left""&gt;Entities_Classes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""right""&gt;[1.0, 1.0, 0.9997000098228455, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9990000128746033, 1.0, 1.0, 1.0, 0.9965000152587891, 0.9998999834060669, 0.9567000269889832, 1.0, 1.0, 1.0, 0.9980000257492065, 0.9991999864578247, 0.9988999962806702, 1.0, 1.0, 0.9998999834060669]&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;nona gray myrtle and alison&amp;#39;, &amp;#39;top-rated&amp;#39;, &amp;#39;brasserie&amp;#39;, &amp;#39;distant&amp;#39;, &amp;#39;wilson av&amp;#39;, &amp;#39;nov the 4th 2030&amp;#39;, &amp;#39;ouzeri&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;book a spot for nona gray myrtle and alison at a top-rated brasserie that is distant from wilson av on nov the 4th 2030 that serves ouzeri&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;party_size_description&amp;#39;, &amp;#39;sort&amp;#39;, &amp;#39;restaurant_type&amp;#39;, &amp;#39;spatial_relation&amp;#39;, &amp;#39;poi&amp;#39;, &amp;#39;timeRange&amp;#39;, &amp;#39;cuisine&amp;#39;]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html""&gt;Named Entity Recognition (NER) Model in Bengali (bengaliner_cc_300d)&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;Bengali for: &amp;#39;Iajuddin Ahmed passed Matriculation from Munshiganj High School in 1947 and Intermediate from Munshiganj Horganga College in 1950.&amp;#39;&lt;/h1&gt;

&lt;p&gt;nlu.load(&amp;quot;bn.ner.cc_300d&amp;quot;).predict(&amp;quot;১৯৪৮ সালে ইয়াজউদ্দিন আহম্মেদ মুন্সিগঞ্জ উচ্চ বিদ্যালয় থেকে মেট্রিক পাশ করেন এবং ১৯৫০ সালে মুন্সিগঞ্জ হরগঙ্গা কলেজ থেকে ইন্টারমেডিয়েট পাশ করেন&amp;quot;,output_level = &amp;quot;document&amp;quot;)
```&lt;/p&gt;

&lt;p&gt;outputs :&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""right""&gt;ner_confidence&lt;/th&gt;
&lt;th align=""left""&gt;entities&lt;/th&gt;
&lt;th align=""left""&gt;Entities_Classes&lt;/th&gt;
&lt;th align=""left""&gt;document&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""right""&gt;[0.9987999796867371, 0.9854000210762024, 0.8604000210762024, 0.6686999797821045, 0.5289999842643738, 0.7009999752044678, 0.7684999704360962, 0.9979000091552734, 0.9976000189781189, 0.9930999875068665, 0.9994000196456909, 0.9879000186920166, 0.7407000064849854, 0.9215999841690063, 0.7657999992370605, 0.39419999718666077, 0.9124000072479248, 0.9932000041007996, 0.9919999837875366, 0.995199978351593, 0.9991999864578247]&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;সালে&amp;#39;, &amp;#39;ইয়াজউদ্দিন আহম্মেদ&amp;#39;, &amp;#39;মুন্সিগঞ্জ উচ্চ বিদ্যালয়&amp;#39;, &amp;#39;সালে&amp;#39;, &amp;#39;মুন্সিগঞ্জ হরগঙ্গা কলেজ&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;TIME&amp;#39;, &amp;#39;PER&amp;#39;, &amp;#39;ORG&amp;#39;, &amp;#39;TIME&amp;#39;, &amp;#39;ORG&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;১৯৪৮ সালে ইয়াজউদ্দিন আহম্মেদ মুন্সিগঞ্জ উচ্চ বিদ্যালয় থেকে মেট্রিক পাশ করেন এবং ১৯৫০ সালে মুন্সিগঞ্জ হরগঙ্গা কলেজ থেকে ইন্টারমেডিয়েট পাশ করেন&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/15/classifierdl_use_snips_en.html""&gt;Identify intent in general text - SNIPS dataset&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;python
nlu.load(&amp;quot;en.ner.snips&amp;quot;).predict(&amp;quot;I want to bring six of us to a bistro in town that serves hot chicken sandwich that is within the same area&amp;quot;,output_level = &amp;quot;document&amp;quot;)
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;outputs :&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;document&lt;/th&gt;
&lt;th&gt;snips&lt;/th&gt;
&lt;th&gt;snips_confidence&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;I want to bring six of us to a bistro in town that serves hot chicken sandwich that is within the same area&lt;/td&gt;
&lt;td&gt;BookRestaurant&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/10/bengali_cc_300d_bn.html""&gt;Word Embeddings for Bengali (bengali_cc_300d)&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;Bengali for : &amp;#39;Iajuddin Ahmed passed Matriculation from Munshiganj High School in 1947 and Intermediate from Munshiganj Horganga College in 1950.&amp;#39;&lt;/h1&gt;

&lt;p&gt;nlu.load(&amp;quot;bn.embed&amp;quot;).predict(&amp;quot;১৯৪৮ সালে ইয়াজউদ্দিন আহম্মেদ মুন্সিগঞ্জ উচ্চ বিদ্যালয় থেকে মেট্রিক পাশ করেন এবং ১৯৫০ সালে মুন্সিগঞ্জ হরগঙ্গা কলেজ থেকে ইন্টারমেডিয়েট পাশ করেন&amp;quot;,output_level = &amp;quot;document&amp;quot;)
```&lt;/p&gt;

&lt;p&gt;outputs :&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""right""&gt;document&lt;/th&gt;
&lt;th align=""left""&gt;bn_embed_embeddings&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""right""&gt;১৯৪৮ সালে ইয়াজউদ্দিন আহম্মেদ মুন্সিগঞ্জ উচ্চ বিদ্যালয় থেকে মেট্রিক পাশ করেন এবং ১৯৫০ সালে মুন্সিগঞ্জ হরগঙ্গা কলেজ থেকে ইন্টারমেডিয়েট পাশ করেন&lt;/td&gt;
&lt;td align=""left""&gt;[-0.0828      0.0683      0.0215     ...  0.0679     -0.0484...]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h3&gt;NLU 1.1.3 Enhancements&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Added automatic conversion  to Sentence Embeddings of Word Embeddings when there is no Sentence Embedding Avaiable and a model needs the converted version to run.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;NLU 1.1.3 Bug Fixes&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Fixed a bug that caused &lt;code&gt;ur.sentiment&lt;/code&gt; NLU pipeline to build incorrectly&lt;/li&gt;
&lt;li&gt;Fixed a bug that caused &lt;code&gt;sentiment.imdb.glove&lt;/code&gt; NLU pipeline to build incorrectly&lt;/li&gt;
&lt;li&gt;Fixed a bug that caused &lt;code&gt;en.sentiment.glove.imdb&lt;/code&gt; NLU pipeline to build incorrectly&lt;/li&gt;
&lt;li&gt;Fixed a bug that caused Spark 2.3.X environments to crash.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;NLU Installation&lt;/h3&gt;

&lt;p&gt;```bash&lt;/p&gt;

&lt;h1&gt;PyPi&lt;/h1&gt;

&lt;p&gt;!pip install nlu pyspark==2.4.7&lt;/p&gt;

&lt;h1&gt;Conda&lt;/h1&gt;

&lt;h1&gt;Install NLU from Anaconda/Conda&lt;/h1&gt;

&lt;p&gt;conda install -c johnsnowlabs nlu
```&lt;/p&gt;

&lt;h3&gt;Additional NLU ressources&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/""&gt;NLU Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/docs/en/notebooks""&gt;All NLU Tutorial Notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlp.johnsnowlabs.com/learn#pythons-nlu-library""&gt;NLU Videos and Blogposts on NLU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu""&gt;NLU on Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://join.slack.com/t/spark-nlp/shared_invite/zt-lutct9gm-kuUazcyFKhuGY3_0AMkxqA""&gt;Suggestions or Questions? Contact us in Slack!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m0chul,True,,CKL-IT,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m0chul/intent_and_action_classification_analyze_chinese/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m0chul/intent_and_action_classification_analyze_chinese/,30199,1615197327.0,0,,False,,,,,,14754
581,,LanguageTechnology,"Hey there, I'm interested in experimenting with natural language processing to see if it's possible to perform some sort of object classification with a large dataset, most likely wikipedia. An example scenario would be where a user can formulate questions about what can and cannot be done with an object, and the model can answer these questions.

&amp;#x200B;

An example of this could be:  
User: ""can a cup be filled with water?""

Model: ""yes""

User: ""does a cat bark?""

Model: ""no""

&amp;#x200B;

I am aware that projects like GPT-3 and similar are \*very\* complex and can (to some degree) reason, however my intention with this is \*not\* to perform advanced language processing that requires advanced analysis. Also, the example provided above is not representative of what my goals are with this; the conversation may be represented with some simple class or structure that conveys the object you're asking about (cup/motorcycle), and the subject in question (can it be filled with water/does it bark). Therefore the purpose of this question does not pertain to the question asked by the user but the ability to parse text from an available database and extract useful facts and key information from the text.

&amp;#x200B;

Any advice? Thanks",t2_ol1zc,False,,0,False,NLP for a general object classification?,[],r/LanguageTechnology,False,6,,0,,False,t3_m0l9zv,False,dark,0.6,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1615254291.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey there, I&amp;#39;m interested in experimenting with natural language processing to see if it&amp;#39;s possible to perform some sort of object classification with a large dataset, most likely wikipedia. An example scenario would be where a user can formulate questions about what can and cannot be done with an object, and the model can answer these questions.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;An example of this could be:&lt;br/&gt;
User: &amp;quot;can a cup be filled with water?&amp;quot;&lt;/p&gt;

&lt;p&gt;Model: &amp;quot;yes&amp;quot;&lt;/p&gt;

&lt;p&gt;User: &amp;quot;does a cat bark?&amp;quot;&lt;/p&gt;

&lt;p&gt;Model: &amp;quot;no&amp;quot;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I am aware that projects like GPT-3 and similar are *very* complex and can (to some degree) reason, however my intention with this is *not* to perform advanced language processing that requires advanced analysis. Also, the example provided above is not representative of what my goals are with this; the conversation may be represented with some simple class or structure that conveys the object you&amp;#39;re asking about (cup/motorcycle), and the subject in question (can it be filled with water/does it bark). Therefore the purpose of this question does not pertain to the question asked by the user but the ability to parse text from an available database and extract useful facts and key information from the text.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Any advice? Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m0l9zv,True,,Firedan1176,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m0l9zv/nlp_for_a_general_object_classification/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m0l9zv/nlp_for_a_general_object_classification/,30199,1615225491.0,0,,False,,,,,,1253
582,,LanguageTechnology,"I have created a POS tagger with a decent accuracy (Ukrainian only, if that matters). I need to apply it in an existing UD parser. I know C# and also I have briefly looked at Python syntax, in case I need it. I can change my tagger to be compatible with a specific parser.

Could you suggest specific parsers that would allow me to use my own part-of-speech tagger?",t2_14iu3t,False,,0,False,Using own POS Tagger with a UD parser,[],r/LanguageTechnology,False,6,,0,,False,t3_m0dne1,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1615231211.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have created a POS tagger with a decent accuracy (Ukrainian only, if that matters). I need to apply it in an existing UD parser. I know C# and also I have briefly looked at Python syntax, in case I need it. I can change my tagger to be compatible with a specific parser.&lt;/p&gt;

&lt;p&gt;Could you suggest specific parsers that would allow me to use my own part-of-speech tagger?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m0dne1,True,,BolvangarBear,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m0dne1/using_own_pos_tagger_with_a_ud_parser/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m0dne1/using_own_pos_tagger_with_a_ud_parser/,30199,1615202411.0,0,,False,,,,,,365
583,,LanguageTechnology,,t2_5fsp2x6v,False,,0,False,Embeddings of Label Components for Fine-grained NER | Research Papers Summary 011,[],r/LanguageTechnology,False,6,,0,,False,t3_lzxalx,False,dark,1.0,,public,22,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/GBsu1YVBHsQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Embeddings of Label Components for Fine-grained NER | Research Papers Summary 011', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/GBsu1YVBHsQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/GBsu1YVBHsQ/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/GBsu1YVBHsQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lzxalx', 'height': 200}",,False,22,,False,False,,False,,[],{},,False,,1615172933.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lzxalx,True,,RyanAI100,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lzxalx/embeddings_of_label_components_for_finegrained/,all_ads,False,https://youtu.be/GBsu1YVBHsQ,30199,1615144133.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Embeddings of Label Components for Fine-grained NER | Research Papers Summary 011', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/GBsu1YVBHsQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/GBsu1YVBHsQ/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,https://youtu.be/GBsu1YVBHsQ,,,,,0
584,,LanguageTechnology,,t2_udqrw,False,,0,False,Start Japanese text processing without installing any tokenizer on your local environment (tokenizers are on docker container),[],r/LanguageTechnology,False,6,,0,,False,t3_m0dbiq,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,False,,1615229802.0,text,6,,,text,github.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m0dbiq,True,,mhiramatsu,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m0dbiq/start_japanese_text_processing_without_installing/,all_ads,False,https://github.com/himkt/konoha/releases/tag/v4.6.4,30199,1615201002.0,0,,False,https://github.com/himkt/konoha/releases/tag/v4.6.4,,,,,0
585,,LanguageTechnology,"If you want to learn about state-of-the-art hyperparameter optimization algorithms (HPO), in this article I’ll tell you what they are and how they work.

We cover:
- A bit about HPO Approaches 
- What is Bayesian Optimization, and why is this method effective? 
- How do state-of-the-art Hyperparameter Optimization algorithms work? 
- **Hyperband vs BOHB comparison**

[HyperBand vs. BOHB](https://neptune.ai/blog/hyperband-and-bohb-understanding-state-of-the-art-hyperparameter-optimization-algorithms?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=blog-hyperband-and-bohb-understanding-state-of-the-art-hyperparameter-optimization-algorithms&amp;utm_content=languagetechnology)",t2_5hfacnnv,False,,0,False,HyperBand and BOHB: understanding hyperparameter optimization algorithms,[],r/LanguageTechnology,False,6,,0,,False,t3_m0bdh3,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1615221197.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If you want to learn about state-of-the-art hyperparameter optimization algorithms (HPO), in this article I’ll tell you what they are and how they work.&lt;/p&gt;

&lt;p&gt;We cover:
- A bit about HPO Approaches 
- What is Bayesian Optimization, and why is this method effective? 
- How do state-of-the-art Hyperparameter Optimization algorithms work? 
- &lt;strong&gt;Hyperband vs BOHB comparison&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://neptune.ai/blog/hyperband-and-bohb-understanding-state-of-the-art-hyperparameter-optimization-algorithms?utm_source=reddit&amp;amp;utm_medium=post&amp;amp;utm_campaign=blog-hyperband-and-bohb-understanding-state-of-the-art-hyperparameter-optimization-algorithms&amp;amp;utm_content=languagetechnology""&gt;HyperBand vs. BOHB&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m0bdh3,True,,kk_ai,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m0bdh3/hyperband_and_bohb_understanding_hyperparameter/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m0bdh3/hyperband_and_bohb_understanding_hyperparameter/,30199,1615192397.0,0,,False,,,,,,688
586,,LanguageTechnology,"Not sure where else to ask this, but I figured people who traffic in machine learning circles might have some idea.

I'm  certain tools like this must exist in some corner of the internet. I'm looking for some sort of algorithm-generated word cloud tool that expands upon ideas/concepts/words fed to it. Preferably, the more words fed into it, the more refined/related the suggestions. Anybody know of any tools that may work like this, or of any other subs that might help find such a tool?",t2_146j4k,False,,0,False,Generator for related words/ ideas/ concepts?,[],r/LanguageTechnology,False,6,,0,,False,t3_m0802a,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,True,,False,,[],{},,True,,1615207391.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Not sure where else to ask this, but I figured people who traffic in machine learning circles might have some idea.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m  certain tools like this must exist in some corner of the internet. I&amp;#39;m looking for some sort of algorithm-generated word cloud tool that expands upon ideas/concepts/words fed to it. Preferably, the more words fed into it, the more refined/related the suggestions. Anybody know of any tools that may work like this, or of any other subs that might help find such a tool?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m0802a,True,,ethestiel,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m0802a/generator_for_related_words_ideas_concepts/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m0802a/generator_for_related_words_ideas_concepts/,30199,1615178591.0,0,,False,,,,,,491
587,,LanguageTechnology,"Hello!

I'm currently developing an NLP project and I'm stuck on something. I'm a NLP beginner (using Python) and I have some questions I'm hoping the community might help me address.

I've built a dataset of several hundred political speeches and I'm building Word Clouds out of the content of these speeches. To make the story short, I want to tokenize ""phrases"" or ""concepts""; for example the name of an institution or a commonly used phrase by politicians, so that my word cloud will reflect the actual ""phrase"" or ""concept"" instead of showing me the individual words.

I'm aware of the concept of PMI (Pointwise Mutual Information) and how it can help identify these patterns in my data. I haven't been able to find any resources that show a code pipeline to do this.

Any suggestions would be gladly appreciated.

Thanks in advance!",t2_1vi2bryz,False,,0,False,PMI for WordClouds,[],r/LanguageTechnology,False,6,,0,,False,t3_m020dj,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1615187049.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m currently developing an NLP project and I&amp;#39;m stuck on something. I&amp;#39;m a NLP beginner (using Python) and I have some questions I&amp;#39;m hoping the community might help me address.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve built a dataset of several hundred political speeches and I&amp;#39;m building Word Clouds out of the content of these speeches. To make the story short, I want to tokenize &amp;quot;phrases&amp;quot; or &amp;quot;concepts&amp;quot;; for example the name of an institution or a commonly used phrase by politicians, so that my word cloud will reflect the actual &amp;quot;phrase&amp;quot; or &amp;quot;concept&amp;quot; instead of showing me the individual words.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m aware of the concept of PMI (Pointwise Mutual Information) and how it can help identify these patterns in my data. I haven&amp;#39;t been able to find any resources that show a code pipeline to do this.&lt;/p&gt;

&lt;p&gt;Any suggestions would be gladly appreciated.&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m020dj,True,,nubonaga,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m020dj/pmi_for_wordclouds/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m020dj/pmi_for_wordclouds/,30199,1615158249.0,0,,False,,,,,,838
588,,LanguageTechnology,"I am trying to build a semantic search algorithm using Bert embeddings and using annoy for indexing. AnnoyIndex() is built to take in pooled embeddings (I think); but what about word embeddings?  For example a word embedding of 100 docs of shape\[100, 256, 768\].  An example of working with pooled embeddings shape (100, 768): - 

from annoy import AnnoyIndex

\# Bert emits 768 dim vectors

D = 768

n\_trees = 300

ann = AnnoyIndex(D, 'angular')

pooled

for index, embed in enumerate(pooled\_embeddings):

ann.add\_item(index, embed)

[ann.build](https://ann.build)(n\_trees)

&amp;#x200B;

Should I flatten the word vectors? should I reduce the dimensionality in some other way?",t2_3iqsgk0g,False,,0,False,ANNOY and Semantic Search,[],r/LanguageTechnology,False,6,,0,,False,t3_m01iex,False,dark,0.66,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1615185480.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to build a semantic search algorithm using Bert embeddings and using annoy for indexing. AnnoyIndex() is built to take in pooled embeddings (I think); but what about word embeddings?  For example a word embedding of 100 docs of shape[100, 256, 768].  An example of working with pooled embeddings shape (100, 768): - &lt;/p&gt;

&lt;p&gt;from annoy import AnnoyIndex&lt;/p&gt;

&lt;p&gt;# Bert emits 768 dim vectors&lt;/p&gt;

&lt;p&gt;D = 768&lt;/p&gt;

&lt;p&gt;n_trees = 300&lt;/p&gt;

&lt;p&gt;ann = AnnoyIndex(D, &amp;#39;angular&amp;#39;)&lt;/p&gt;

&lt;p&gt;pooled&lt;/p&gt;

&lt;p&gt;for index, embed in enumerate(pooled_embeddings):&lt;/p&gt;

&lt;p&gt;ann.add_item(index, embed)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://ann.build""&gt;ann.build&lt;/a&gt;(n_trees)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Should I flatten the word vectors? should I reduce the dimensionality in some other way?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,m01iex,True,,wholestars,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/m01iex/annoy_and_semantic_search/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/m01iex/annoy_and_semantic_search/,30199,1615156680.0,0,,False,,,,,,683
589,,LanguageTechnology,"I would like to be able to process text data from newspaper sites, tweets, or whatever piece of a text an user can select from a web site and run different NLP inferences (sentiment, polarity, entity recognition, etc.) Within the client side. 

I'm relatively new and ,I don't know, may be Tensorflow could be used? 

Any thoughts?",t2_4df6a1hn,False,,0,False,Is there a way to deploy NLP models into a Chrome extension?,[],r/LanguageTechnology,False,6,,0,,False,t3_lziqxi,False,dark,0.94,,public,16,1,{},,False,[],,False,False,,{},,False,16,,False,False,,False,,[],{},,True,,1615118826.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I would like to be able to process text data from newspaper sites, tweets, or whatever piece of a text an user can select from a web site and run different NLP inferences (sentiment, polarity, entity recognition, etc.) Within the client side. &lt;/p&gt;

&lt;p&gt;I&amp;#39;m relatively new and ,I don&amp;#39;t know, may be Tensorflow could be used? &lt;/p&gt;

&lt;p&gt;Any thoughts?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 30, 'id': 'award_b4ff447e-05a5-42dc-9002-63568807cfe6', 'penny_donate': None, 'award_sub_type': 'PREMIUM', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_128.png', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'A glowing commendation for all to see', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'All-Seeing Upvote', 'resized_static_icons': [{'url': 'https://external-preview.redd.it?width=16&amp;height=16&amp;auto=webp&amp;s=d88c9a453f8ac38850b7a8241cfe5804b7b4905d', 'width': 16, 'height': 16}, {'url': 'https://external-preview.redd.it?width=32&amp;height=32&amp;auto=webp&amp;s=96a25019eb75878bdec4f6c012540f3baffbb1b2', 'width': 32, 'height': 32}, {'url': 'https://external-preview.redd.it?width=48&amp;height=48&amp;auto=webp&amp;s=1a51d27d75afde3fbde8bba84f9338f511211461', 'width': 48, 'height': 48}, {'url': 'https://external-preview.redd.it?width=64&amp;height=64&amp;auto=webp&amp;s=96af5ec460b05669ed60224cb0619bb8884abe27', 'width': 64, 'height': 64}, {'url': 'https://external-preview.redd.it?width=128&amp;height=128&amp;auto=webp&amp;s=2d3e648ed2302e6258673051ca5291f57beb29d4', 'width': 128, 'height': 128}], 'icon_format': 'APNG', 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/Illuminati_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lziqxi,True,,JotaPe-exe,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lziqxi/is_there_a_way_to_deploy_nlp_models_into_a_chrome/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lziqxi/is_there_a_way_to_deploy_nlp_models_into_a_chrome/,30199,1615090026.0,0,,False,,,,,,331
590,,LanguageTechnology,"If one has a pre-trained embedding (e.g. fasttext, glove) What is the best way to create embedding vectors for a new word that was not originally in the vocabulary of the pretrained embedding. There is an interesting approach called ""a la carte embeddings"" ([https://arxiv.org/abs/1805.05388](https://arxiv.org/abs/1805.05388)) but seems to be only limited to GLoVe. Anyone aware of any other published or *ad hoc* methods for doing this for any flavour of embedding models?",t2_7dpje88o,False,,0,False,How to add OOV words into an already pre-trained embedding,[],r/LanguageTechnology,False,6,,0,,False,t3_lzm4q5,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1615132795.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If one has a pre-trained embedding (e.g. fasttext, glove) What is the best way to create embedding vectors for a new word that was not originally in the vocabulary of the pretrained embedding. There is an interesting approach called &amp;quot;a la carte embeddings&amp;quot; (&lt;a href=""https://arxiv.org/abs/1805.05388""&gt;https://arxiv.org/abs/1805.05388&lt;/a&gt;) but seems to be only limited to GLoVe. Anyone aware of any other published or &lt;em&gt;ad hoc&lt;/em&gt; methods for doing this for any flavour of embedding models?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lzm4q5,True,,neelankatan,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lzm4q5/how_to_add_oov_words_into_an_already_pretrained/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lzm4q5/how_to_add_oov_words_into_an_already_pretrained/,30199,1615103995.0,0,,False,,,,,,474
591,,LanguageTechnology,,t2_hkv9s,False,,0,False,tNodeEmbed: Node Embeddings with Temporal Graphs | ML with Graphs (Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_lzja5d,False,dark,0.91,,public,9,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Ol1UYsPvsT8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'tNodeEmbed: Node Embeddings with Temporal Graphs | ML with Graphs (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Ol1UYsPvsT8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Ol1UYsPvsT8/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Ol1UYsPvsT8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lzja5d', 'height': 200}",,False,9,,False,False,,False,,[],{},,False,,1615120853.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lzja5d,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lzja5d/tnodeembed_node_embeddings_with_temporal_graphs/,all_ads,False,https://youtu.be/Ol1UYsPvsT8,30199,1615092053.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'tNodeEmbed: Node Embeddings with Temporal Graphs | ML with Graphs (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Ol1UYsPvsT8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Ol1UYsPvsT8/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/Ol1UYsPvsT8,,,,,0
592,,LanguageTechnology,"I am trying to test whether a given vocabulary list contains the tokens necessary to reconstruct a corpus of text losslessly. That is, if a tokenizer trained on a corpus of text were to attempt to tokenize the training corpus according to its associated vocabulary, would it be able to tokenize and detokenize the entire training corpus without losing any text to \[unk\] tokens? Are there ways to test this?

Suppose you had a corpus of text - I’ll use a small one for this example

    corpus = ‘the cat in the hat’

Suppose you had a trained tokenizer which tokenizes according to its vocab list where each token id is simply the index of the token in the list.

    vocab = [‘t’, ‘h’, ‘e’, ‘ ‘, ‘c’, ‘a’, ‘i’, ‘n’]
    print(len(vocab))
    8
    tokens = tokenize(corpus, vocab))
    print(tokens)
    [0, 1, 2, 3, 4, 5, 0, 3, 6, 7, 3, 0, 1, 2, 3, 2, 5, 0]
    print(“Tokenized length: {}”.format(len(tokens)))
    Tokenized length: 18

If I then detokenize from here I can obviously reconstruct the corpus as it originally was. However, this process is suboptimal as we can combine tokens to reduce the length of the tokenized representation.

    vocab = [‘t’, ‘h’, ‘e’, ‘ ‘, ‘c’, ‘a’, ‘i’, ‘n’, ‘th’]
    print(len(vocab))
    9
    tokens = tokenize(corpus, vocab))
    print(tokens)
    [8, 2, 3, 4, 5, 0, 3, 6, 7, 3, 8, 2, 3, 2, 5, 0]
    print(“Tokenized length: {}”.format(len(tokens)))
    Tokenized length: 16

This is obviously still lossless, though it adds an extra token to the vocabulary list, which will increase the possibility space of a model trying to predict the next token in a sequence. But if I remove a necessary token, it becomes lossy. Supppose our tokenizer outputs ‘\[unk\]’ for all tokens not in the vocabulary and that the ‘\[unk\]’ token is always the last token in the vocabulary regardless of the content of the vocabulary.

    vocab = [‘t’, ‘h’, ‘e’, ‘ ‘, ‘c’, ‘a’, ‘i’, ‘th’]
    print(len(vocab))
    8
    tokens = tokenize(corpus, vocab))
    print(tokens)
    [7, 2, 3, 4, 5, 0, 3, 6, 8, 3, 7, 2, 3, 2, 5, 0]
    print(“Tokenized length: {}”.format(len(tokens)))
    Tokenized length: 16

The length of the representation does not change but we can tell that it is lossy just from the vocabulary, therefore the compression is not lossless.

    print(detokenize(tokens, vocab))
    [‘the cat i[unk] the hat’]

This example makes it easy to tell the compression is lossy because our vocabulary is small and made of suboptimally combined tokens. We can go further with the combination of the vocabulary and combine ‘t’, ‘h’, ‘e’, and ‘ ‘ to form a single token covering all instances of “the “. Since ‘e’ doesn’t occur outside of its place in ‘the’ we can delete it from the vocabulary list and still maintain the same level of loss in compression, ditto ‘i’ and ‘n’.

    vocab = [‘t’, ‘h’, ‘the  ‘, ‘ ‘, ‘c’, ‘a’, ‘in’]
    print(len(vocab))
    7
    tokens = tokenize(corpus, vocab)
    print(tokens)
    [2, 4, 5, 0, 3, 6, 3, 2, 1, 5, 0]
    print(len(tokens)
    11

Thus far this is the most optimally compressed form of the sentence (though I don’t claim for it to be the most optimally compressed) and it has the smallest vocabulary size, making it easier for a model to guess the next token. In all of these examples we have been able to tell unambiguously whether or not the compression is lossless just by eyeballing it.

However, there is more than one way to tokenize a sentence. If our tokenizer sees the ‘t’ and ‘h’ without the context of the ‘e ‘, it will run into a problem: \*\*there is no ‘e’ in our vocabulary list\*\* and, if not properly trained, it will be forced to replace that ‘e’ with an ‘\[unk\]’ token, making it lossy while also expanding the length of its representation significantly.

Therefore, even though we have just proven lossless compression is possible given this corpus and the last vocabulary list, if the recognition of the sequence by the tokenizer is poor, it won’t be capable of losslessly tokenizing even though the vocabulary list is suited for it. As corpus size increases, it becomes harder and harder to tell just by looking that the vocabulary list can losslessly reconstruct the corpus.

With something like SentencePiece, which defaults to a vocabulary size in the thousands, trying to reconstruct the Wiki-sentences corpus, which is millions and millions of sentences long, it becomes untenable to pore over every output vocabulary size and manually check if it can reconstruct the corpus.

Thus my question: given a vocabulary list and a corpus, is there an automatic way to tell that that it is \*possible\* to reconstruct the given corpus losslessly using the given vocabulary assuming a well trained tokenizer? In other words is there a function to determine if lossless tokenization is possible which returns true or false given an input corpus and vocabulary?",t2_3ygf2903,False,,0,False,Is it possible to test whether a tokenizer can losslessly tokenize and detokenize a given corpus solely from its vocabulary?,[],r/LanguageTechnology,False,6,,0,,False,t3_lzer14,False,dark,0.79,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,1615076130.0,,[],{},,True,,1615104655.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to test whether a given vocabulary list contains the tokens necessary to reconstruct a corpus of text losslessly. That is, if a tokenizer trained on a corpus of text were to attempt to tokenize the training corpus according to its associated vocabulary, would it be able to tokenize and detokenize the entire training corpus without losing any text to [unk] tokens? Are there ways to test this?&lt;/p&gt;

&lt;p&gt;Suppose you had a corpus of text - I’ll use a small one for this example&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;corpus = ‘the cat in the hat’
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Suppose you had a trained tokenizer which tokenizes according to its vocab list where each token id is simply the index of the token in the list.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vocab = [‘t’, ‘h’, ‘e’, ‘ ‘, ‘c’, ‘a’, ‘i’, ‘n’]
print(len(vocab))
8
tokens = tokenize(corpus, vocab))
print(tokens)
[0, 1, 2, 3, 4, 5, 0, 3, 6, 7, 3, 0, 1, 2, 3, 2, 5, 0]
print(“Tokenized length: {}”.format(len(tokens)))
Tokenized length: 18
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If I then detokenize from here I can obviously reconstruct the corpus as it originally was. However, this process is suboptimal as we can combine tokens to reduce the length of the tokenized representation.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vocab = [‘t’, ‘h’, ‘e’, ‘ ‘, ‘c’, ‘a’, ‘i’, ‘n’, ‘th’]
print(len(vocab))
9
tokens = tokenize(corpus, vocab))
print(tokens)
[8, 2, 3, 4, 5, 0, 3, 6, 7, 3, 8, 2, 3, 2, 5, 0]
print(“Tokenized length: {}”.format(len(tokens)))
Tokenized length: 16
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is obviously still lossless, though it adds an extra token to the vocabulary list, which will increase the possibility space of a model trying to predict the next token in a sequence. But if I remove a necessary token, it becomes lossy. Supppose our tokenizer outputs ‘[unk]’ for all tokens not in the vocabulary and that the ‘[unk]’ token is always the last token in the vocabulary regardless of the content of the vocabulary.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vocab = [‘t’, ‘h’, ‘e’, ‘ ‘, ‘c’, ‘a’, ‘i’, ‘th’]
print(len(vocab))
8
tokens = tokenize(corpus, vocab))
print(tokens)
[7, 2, 3, 4, 5, 0, 3, 6, 8, 3, 7, 2, 3, 2, 5, 0]
print(“Tokenized length: {}”.format(len(tokens)))
Tokenized length: 16
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The length of the representation does not change but we can tell that it is lossy just from the vocabulary, therefore the compression is not lossless.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;print(detokenize(tokens, vocab))
[‘the cat i[unk] the hat’]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This example makes it easy to tell the compression is lossy because our vocabulary is small and made of suboptimally combined tokens. We can go further with the combination of the vocabulary and combine ‘t’, ‘h’, ‘e’, and ‘ ‘ to form a single token covering all instances of “the “. Since ‘e’ doesn’t occur outside of its place in ‘the’ we can delete it from the vocabulary list and still maintain the same level of loss in compression, ditto ‘i’ and ‘n’.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vocab = [‘t’, ‘h’, ‘the  ‘, ‘ ‘, ‘c’, ‘a’, ‘in’]
print(len(vocab))
7
tokens = tokenize(corpus, vocab)
print(tokens)
[2, 4, 5, 0, 3, 6, 3, 2, 1, 5, 0]
print(len(tokens)
11
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thus far this is the most optimally compressed form of the sentence (though I don’t claim for it to be the most optimally compressed) and it has the smallest vocabulary size, making it easier for a model to guess the next token. In all of these examples we have been able to tell unambiguously whether or not the compression is lossless just by eyeballing it.&lt;/p&gt;

&lt;p&gt;However, there is more than one way to tokenize a sentence. If our tokenizer sees the ‘t’ and ‘h’ without the context of the ‘e ‘, it will run into a problem: **there is no ‘e’ in our vocabulary list** and, if not properly trained, it will be forced to replace that ‘e’ with an ‘[unk]’ token, making it lossy while also expanding the length of its representation significantly.&lt;/p&gt;

&lt;p&gt;Therefore, even though we have just proven lossless compression is possible given this corpus and the last vocabulary list, if the recognition of the sequence by the tokenizer is poor, it won’t be capable of losslessly tokenizing even though the vocabulary list is suited for it. As corpus size increases, it becomes harder and harder to tell just by looking that the vocabulary list can losslessly reconstruct the corpus.&lt;/p&gt;

&lt;p&gt;With something like SentencePiece, which defaults to a vocabulary size in the thousands, trying to reconstruct the Wiki-sentences corpus, which is millions and millions of sentences long, it becomes untenable to pore over every output vocabulary size and manually check if it can reconstruct the corpus.&lt;/p&gt;

&lt;p&gt;Thus my question: given a vocabulary list and a corpus, is there an automatic way to tell that that it is *possible* to reconstruct the given corpus losslessly using the given vocabulary assuming a well trained tokenizer? In other words is there a function to determine if lossless tokenization is possible which returns true or false given an input corpus and vocabulary?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lzer14,True,,DeepLearningStudent,,29,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/lzer14/is_it_possible_to_test_whether_a_tokenizer_can/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lzer14/is_it_possible_to_test_whether_a_tokenizer_can/,30199,1615075855.0,0,,False,,,,,True,4866
593,,LanguageTechnology,"Hi all. I'm looking for resources that computer science students can use to learn more about linguistics. As my professor likes to put it, we're good at the P part of NLP but aren't the best at the NL part. Any help is greatly appreciated!",t2_2f0qmtvi,False,,0,False,Linguistic resources?,[],r/LanguageTechnology,False,6,,0,,False,t3_lyw5v9,False,dark,0.9,,public,14,0,{},,False,[],,False,False,,{},,False,14,,False,False,,False,,[],{},,True,,1615042603.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all. I&amp;#39;m looking for resources that computer science students can use to learn more about linguistics. As my professor likes to put it, we&amp;#39;re good at the P part of NLP but aren&amp;#39;t the best at the NL part. Any help is greatly appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lyw5v9,True,,eskaordaeiri,,12,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lyw5v9/linguistic_resources/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lyw5v9/linguistic_resources/,30199,1615013803.0,0,,False,,,,,,239
594,,LanguageTechnology,"Hi everyone 

I am building an MT engine and looking for En-Zh bilingual medical corpora to download. If anyone can share it here with me will be much appreciated!",t2_7oxl1l14,False,,0,False,Looking for En-Zh bilingual medical corpora,[],r/LanguageTechnology,False,6,,0,,False,t3_lz0gqq,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1615061979.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone &lt;/p&gt;

&lt;p&gt;I am building an MT engine and looking for En-Zh bilingual medical corpora to download. If anyone can share it here with me will be much appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lz0gqq,True,,bbsweettea,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lz0gqq/looking_for_enzh_bilingual_medical_corpora/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lz0gqq/looking_for_enzh_bilingual_medical_corpora/,30199,1615033179.0,0,,False,,,,,,163
595,,LanguageTechnology,"The most recent episode of the [Futurati Podcast](https://www.youtube.com/channel/UCRSov16ZLE2UgekgBTgnrjw/videos) is a big one. [We had Jungwon Byun and Andreas Stuhlmüller on](https://www.youtube.com/watch?v=Lef5q1xPTU0) to talk about their startup ['Ought'](https://ought.org/) and, to the best of my knowledge, this is the first public, long-form discussion of their work around.

(It's also probably our funniest episode.)

Their ambition is to wrap a sleek GUI around advanced language models to build a platform which could transform scholarship, education, research, and almost every other place people think about stuff.

The process is powered by GPT-3, and mostly boils down to teaching it how to do something you want it to do by showing it a couple of examples. To complete a list of potential essay topics you'd just show it 3-4 essay topics, and it'd respond by showing you a few more.

The more you interact with it, the better it gets.

There's all sorts of subtlety and detail, but that's the essence of it.

This may not sound all that impressive, but consider what it means. You can have Elicit (a separate spinoff of Ought) generate counterarguments to your position, brainstorm failure modes (and potential solutions) to a course of action, summarize papers, and rephrase a statement as a question or in a more emotionally positive tone.

The team is working on some integrations to extend these capabilities. Soon enough, Elicit will be able to connect to databases of published scientific papers, newspapers, blogs, or audio transcripts. When you ask it a research question, it'll be able to link out to millions of documents and offer high-level overviews of every major theme; it'll be able to test your comprehensions by asking you questions as you read; it'll be able to assemble concept hierarchies; it'll be able to extract all the figures from scientific papers and summarize them; it'll be able to extract all the proper names, find where those people are located, get their email addresses where available, and write them messages inviting them on your podcast.

We might one day be able to train a model on Einstein or Feynman and create lectures in their style.

What's more, people can share workflows they've developed. If I work out a good approach to learning about the subdisciplines of a field, for example, I can make that available to anyone to save them the effort of discovering it on their own.

There will be algorithms of thought that can make detailed, otherwise inaccessible aspects of other people's cognitive processes available.

And this is just researchers. It could help teachers dynamically adjust material on the basis of up-to-the-minute assessments of student performance. It could handle rudimentary aspects of therapy. It could help people retrain if they've been displaced by automation. It could summarize case law. It could help develop language skills in children.

I don't know if the future will look the way we hope it will, but I do think something like this could power huge parts of the knowledge work economy in the future, making everyone dramatically more productive.

It's tremendously exciting, and I'm honored to have been able to learn about it directly.",t2_a5rrv,False,,0,False,"How NLP might revolutionize scholarship, collaboration, and reasoning.",[],r/LanguageTechnology,False,6,,0,,False,t3_lyhs6j,False,dark,0.86,,public,19,0,{},,False,[],,False,False,,{},,False,19,,False,False,,False,,[],{},,True,,1614995909.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The most recent episode of the &lt;a href=""https://www.youtube.com/channel/UCRSov16ZLE2UgekgBTgnrjw/videos""&gt;Futurati Podcast&lt;/a&gt; is a big one. &lt;a href=""https://www.youtube.com/watch?v=Lef5q1xPTU0""&gt;We had Jungwon Byun and Andreas Stuhlmüller on&lt;/a&gt; to talk about their startup &lt;a href=""https://ought.org/""&gt;&amp;#39;Ought&amp;#39;&lt;/a&gt; and, to the best of my knowledge, this is the first public, long-form discussion of their work around.&lt;/p&gt;

&lt;p&gt;(It&amp;#39;s also probably our funniest episode.)&lt;/p&gt;

&lt;p&gt;Their ambition is to wrap a sleek GUI around advanced language models to build a platform which could transform scholarship, education, research, and almost every other place people think about stuff.&lt;/p&gt;

&lt;p&gt;The process is powered by GPT-3, and mostly boils down to teaching it how to do something you want it to do by showing it a couple of examples. To complete a list of potential essay topics you&amp;#39;d just show it 3-4 essay topics, and it&amp;#39;d respond by showing you a few more.&lt;/p&gt;

&lt;p&gt;The more you interact with it, the better it gets.&lt;/p&gt;

&lt;p&gt;There&amp;#39;s all sorts of subtlety and detail, but that&amp;#39;s the essence of it.&lt;/p&gt;

&lt;p&gt;This may not sound all that impressive, but consider what it means. You can have Elicit (a separate spinoff of Ought) generate counterarguments to your position, brainstorm failure modes (and potential solutions) to a course of action, summarize papers, and rephrase a statement as a question or in a more emotionally positive tone.&lt;/p&gt;

&lt;p&gt;The team is working on some integrations to extend these capabilities. Soon enough, Elicit will be able to connect to databases of published scientific papers, newspapers, blogs, or audio transcripts. When you ask it a research question, it&amp;#39;ll be able to link out to millions of documents and offer high-level overviews of every major theme; it&amp;#39;ll be able to test your comprehensions by asking you questions as you read; it&amp;#39;ll be able to assemble concept hierarchies; it&amp;#39;ll be able to extract all the figures from scientific papers and summarize them; it&amp;#39;ll be able to extract all the proper names, find where those people are located, get their email addresses where available, and write them messages inviting them on your podcast.&lt;/p&gt;

&lt;p&gt;We might one day be able to train a model on Einstein or Feynman and create lectures in their style.&lt;/p&gt;

&lt;p&gt;What&amp;#39;s more, people can share workflows they&amp;#39;ve developed. If I work out a good approach to learning about the subdisciplines of a field, for example, I can make that available to anyone to save them the effort of discovering it on their own.&lt;/p&gt;

&lt;p&gt;There will be algorithms of thought that can make detailed, otherwise inaccessible aspects of other people&amp;#39;s cognitive processes available.&lt;/p&gt;

&lt;p&gt;And this is just researchers. It could help teachers dynamically adjust material on the basis of up-to-the-minute assessments of student performance. It could handle rudimentary aspects of therapy. It could help people retrain if they&amp;#39;ve been displaced by automation. It could summarize case law. It could help develop language skills in children.&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t know if the future will look the way we hope it will, but I do think something like this could power huge parts of the knowledge work economy in the future, making everyone dramatically more productive.&lt;/p&gt;

&lt;p&gt;It&amp;#39;s tremendously exciting, and I&amp;#39;m honored to have been able to learn about it directly.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lyhs6j,True,,tmf1988,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lyhs6j/how_nlp_might_revolutionize_scholarship/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lyhs6j/how_nlp_might_revolutionize_scholarship/,30199,1614967109.0,0,,False,,,,,,3233
596,,LanguageTechnology,"We just released several important additions to the NLP Cloud API.

Many users were asking us for transformers-based models in addition to our existing spaCy models. So here we go, NLP Cloud now serves some of the best transformers-based models with PyTorch from [Hugging Face](https://huggingface.co/models):

* Facebook's Bart Large MNLI model, for **text classification**     
* Facebook's Bart Large CNN model, for **text summarization** 
* Deepset's Roberta Base Squad 2 model, for **question answering**
* DistilBERT Base Uncased Finetuned SST-2 English model, for **sentiment analysis**

Of  course our spaCy models are still available for Named Entity Recognition and POS tagging. And you can even upload your own spaCy  models. Important change: from now on, all the **large** spaCy models will be available for free.

All our models are available **for free** for a limited amount of requests per minute, and pricing for paid plans is very fair.

Hope you will like it, and can't wait for your feedbacks and suggestions!

Website: [https://nlpcloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=3ec10c28-ab0d-11eb-bcbc-0242ac130002)

Documentation: [https://docs.nlpcloud.io](https://nlpcloud.io)",t2_4z4m2qcs,False,,0,False,NLP Cloud now serves transformers-based models,[],r/LanguageTechnology,False,6,,0,,False,t3_lya2zi,False,dark,1.0,,public,19,0,{},,False,[],,False,False,,{},,False,19,,False,False,,1619936702.0,,[],{},,True,,1614973474.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We just released several important additions to the NLP Cloud API.&lt;/p&gt;

&lt;p&gt;Many users were asking us for transformers-based models in addition to our existing spaCy models. So here we go, NLP Cloud now serves some of the best transformers-based models with PyTorch from &lt;a href=""https://huggingface.co/models""&gt;Hugging Face&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Facebook&amp;#39;s Bart Large MNLI model, for &lt;strong&gt;text classification&lt;/strong&gt;     &lt;/li&gt;
&lt;li&gt;Facebook&amp;#39;s Bart Large CNN model, for &lt;strong&gt;text summarization&lt;/strong&gt; &lt;/li&gt;
&lt;li&gt;Deepset&amp;#39;s Roberta Base Squad 2 model, for &lt;strong&gt;question answering&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;DistilBERT Base Uncased Finetuned SST-2 English model, for &lt;strong&gt;sentiment analysis&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Of  course our spaCy models are still available for Named Entity Recognition and POS tagging. And you can even upload your own spaCy  models. Important change: from now on, all the &lt;strong&gt;large&lt;/strong&gt; spaCy models will be available for free.&lt;/p&gt;

&lt;p&gt;All our models are available &lt;strong&gt;for free&lt;/strong&gt; for a limited amount of requests per minute, and pricing for paid plans is very fair.&lt;/p&gt;

&lt;p&gt;Hope you will like it, and can&amp;#39;t wait for your feedbacks and suggestions!&lt;/p&gt;

&lt;p&gt;Website: &lt;a href=""https://nlpcloud.io/?utm_source=reddit&amp;amp;utm_campaign=3ec10c28-ab0d-11eb-bcbc-0242ac130002""&gt;https://nlpcloud.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Documentation: &lt;a href=""https://nlpcloud.io""&gt;https://docs.nlpcloud.io&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lya2zi,True,,juliensalinas,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lya2zi/nlp_cloud_now_serves_transformersbased_models/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lya2zi/nlp_cloud_now_serves_transformersbased_models/,30199,1614944674.0,0,,False,,,,,,1220
597,,LanguageTechnology,"Hi, this isn't really a question asking for help with code syntax, more so on the general theory / workflow side of things. 

I'm doing a small Python project where I'm trying to extract relevant job 'requirements' from UX Researcher job listings on Google. I've scraped a load of jobs, cleaned them using NLTK, Spacy etc and I have saved each job description as a text file respectively. I also made one large text file corpus with all the cleaned job descriptions in. 

I've heard that tf-idf is a good way to go about extracting just the employers' desired 'requirements' from these job descriptions (i.e., 'PhD in Psychology' or 'Agile and Scrum experience'). 

My problem however is i'm not sure in what format to load the documents in to process using tf-idf. Should I load in all of the documents which each contain one job description, or use the one text file which has all of the documents in itself? My thinking is I need to load each individual document given that tf-idf uses document frequency right? Any help would be appreciated, thanks!",t2_48eyl33r,False,,0,False,Help with tf-idf for job description extraction,[],r/LanguageTechnology,False,6,,0,,False,t3_lyml6j,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1615009202.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, this isn&amp;#39;t really a question asking for help with code syntax, more so on the general theory / workflow side of things. &lt;/p&gt;

&lt;p&gt;I&amp;#39;m doing a small Python project where I&amp;#39;m trying to extract relevant job &amp;#39;requirements&amp;#39; from UX Researcher job listings on Google. I&amp;#39;ve scraped a load of jobs, cleaned them using NLTK, Spacy etc and I have saved each job description as a text file respectively. I also made one large text file corpus with all the cleaned job descriptions in. &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve heard that tf-idf is a good way to go about extracting just the employers&amp;#39; desired &amp;#39;requirements&amp;#39; from these job descriptions (i.e., &amp;#39;PhD in Psychology&amp;#39; or &amp;#39;Agile and Scrum experience&amp;#39;). &lt;/p&gt;

&lt;p&gt;My problem however is i&amp;#39;m not sure in what format to load the documents in to process using tf-idf. Should I load in all of the documents which each contain one job description, or use the one text file which has all of the documents in itself? My thinking is I need to load each individual document given that tf-idf uses document frequency right? Any help would be appreciated, thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lyml6j,True,,crowpup783,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lyml6j/help_with_tfidf_for_job_description_extraction/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lyml6j/help_with_tfidf_for_job_description_extraction/,30199,1614980402.0,0,,False,,,,,,1053
598,,LanguageTechnology,"Hello guys.

I am trying to summarize (compare approaches) articles at seekingalpha. To compare various approaches I've created oracle summaries (5 sentences) using greedy algorithm as described in [Bertsumm](https://arxiv.org/pdf/1903.10318.pdf) . As an input to create oracle summaries I have used the author conclusion (can be seen here in section [Conclusion Thoughts and Outlook](https://seekingalpha.com/article/4411395-project-1m-higher-yields-bite-in-february)). 

Fully excited I have jumped into trying novel approaches and I have used for example [PreSumm](https://github.com/nlpyang/PreSumm) (BertSumExt). The problem is that the articles are much longer than PreSumm can process. I solved this problem by dividing the article into blocks of length of 512 tokens, then summarized each block by itself and merged the blocks together. I repeated this process until the final output was between 100 - 200 subword tokens. Allright, it works but...

Then I have tried [TextRank](https://radimrehurek.com/gensim_3.8.3/summarization/summariser.html) that has no length constraints. I selected the ratio of the output to be in the length of 5 sentences which is usually between 100 - 200 subword tokens. 

Finally I have compared the outputs using ROUGE F R1,R2 and RL score and to my suprise the TextRank yielded the score aroung 50 at R1 and RL, and around 39 at R2. On the other hand the PreSumm (or [facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn)) showed score only around 23 at R1 or 14 at R2.

I have also tried to implement trigram blocking for TextRank which havent significanlty lowered the results.

Do you have any suggestions why the current models compared to TextRank perform so much worse? Might it be because of the ""block by block summarization""? I have also tried to find out if creating oracle summaries as described in Bertsumm favors graph based summarization algorithms but I was not able to come up with any reasonable conclusion (yes or not). Do you guys have any hints what I can improve (besides training new model) when using huge language models to summarize longer inputs? Or is it normal that the current approaches fails at longer inputs (at least in my case using above mentioned approach)

Thanks for your suggestions :-)",t2_1n85cze8,False,,0,False,NLP - Summarization - SeekingAlpha - interesting observation,[],r/LanguageTechnology,False,6,,0,,False,t3_lyf161,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1614988982.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello guys.&lt;/p&gt;

&lt;p&gt;I am trying to summarize (compare approaches) articles at seekingalpha. To compare various approaches I&amp;#39;ve created oracle summaries (5 sentences) using greedy algorithm as described in &lt;a href=""https://arxiv.org/pdf/1903.10318.pdf""&gt;Bertsumm&lt;/a&gt; . As an input to create oracle summaries I have used the author conclusion (can be seen here in section &lt;a href=""https://seekingalpha.com/article/4411395-project-1m-higher-yields-bite-in-february""&gt;Conclusion Thoughts and Outlook&lt;/a&gt;). &lt;/p&gt;

&lt;p&gt;Fully excited I have jumped into trying novel approaches and I have used for example &lt;a href=""https://github.com/nlpyang/PreSumm""&gt;PreSumm&lt;/a&gt; (BertSumExt). The problem is that the articles are much longer than PreSumm can process. I solved this problem by dividing the article into blocks of length of 512 tokens, then summarized each block by itself and merged the blocks together. I repeated this process until the final output was between 100 - 200 subword tokens. Allright, it works but...&lt;/p&gt;

&lt;p&gt;Then I have tried &lt;a href=""https://radimrehurek.com/gensim_3.8.3/summarization/summariser.html""&gt;TextRank&lt;/a&gt; that has no length constraints. I selected the ratio of the output to be in the length of 5 sentences which is usually between 100 - 200 subword tokens. &lt;/p&gt;

&lt;p&gt;Finally I have compared the outputs using ROUGE F R1,R2 and RL score and to my suprise the TextRank yielded the score aroung 50 at R1 and RL, and around 39 at R2. On the other hand the PreSumm (or &lt;a href=""https://huggingface.co/facebook/bart-large-cnn""&gt;facebook/bart-large-cnn&lt;/a&gt;) showed score only around 23 at R1 or 14 at R2.&lt;/p&gt;

&lt;p&gt;I have also tried to implement trigram blocking for TextRank which havent significanlty lowered the results.&lt;/p&gt;

&lt;p&gt;Do you have any suggestions why the current models compared to TextRank perform so much worse? Might it be because of the &amp;quot;block by block summarization&amp;quot;? I have also tried to find out if creating oracle summaries as described in Bertsumm favors graph based summarization algorithms but I was not able to come up with any reasonable conclusion (yes or not). Do you guys have any hints what I can improve (besides training new model) when using huge language models to summarize longer inputs? Or is it normal that the current approaches fails at longer inputs (at least in my case using above mentioned approach)&lt;/p&gt;

&lt;p&gt;Thanks for your suggestions :-)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lyf161,True,,LBahnhof,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lyf161/nlp_summarization_seekingalpha_interesting/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lyf161/nlp_summarization_seekingalpha_interesting/,30199,1614960182.0,0,,False,,,,,,2284
599,,LanguageTechnology,"Hello,

I am trying to test my Dialogflow CX agent with voice, but I am having difficulties understanding where to start. I followed the instructions on this github page:

[https://github.com/googleapis/python-dialogflow-cx](https://github.com/googleapis/python-dialogflow-cx)

But when I try to run my python script to test my voice file, it gives me an error.

    grpc._channel._MultiThreadedRendezvous: &lt;_MultiThreadedRendezvous of RPC that terminated with: 
    status = StatusCode.UNKNOWN 
    details = ""Exception iterating requests!"" 
    debug_error_string = ""None""

I've also tried following the instructions on this page which gives me a script similarly to the one related to the github:

[https://cloud.google.com/dialogflow/cx/docs/how/detect-intent-stream](https://cloud.google.com/dialogflow/cx/docs/how/detect-intent-stream)

Has anyone successfully worked with Dialogflow and if so, what steps did you take to enable voice input?

I need to make a demonstration and I decided to use the ES agent, but also having difficulties with it recognizing what I want to say. When I say ""Hello LuPal"", it'll give me a response of ""Hello loophole"" (??). Thank you for your response, I'd really appreciate any suggestions!",t2_abpu2z4k,False,,0,False,Dialogflow CX - Issues with Enabling Voice Input,[],r/LanguageTechnology,False,6,,0,,False,t3_lyo4qi,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1615013871.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am trying to test my Dialogflow CX agent with voice, but I am having difficulties understanding where to start. I followed the instructions on this github page:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/googleapis/python-dialogflow-cx""&gt;https://github.com/googleapis/python-dialogflow-cx&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;But when I try to run my python script to test my voice file, it gives me an error.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grpc._channel._MultiThreadedRendezvous: &amp;lt;_MultiThreadedRendezvous of RPC that terminated with: 
status = StatusCode.UNKNOWN 
details = &amp;quot;Exception iterating requests!&amp;quot; 
debug_error_string = &amp;quot;None&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&amp;#39;ve also tried following the instructions on this page which gives me a script similarly to the one related to the github:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://cloud.google.com/dialogflow/cx/docs/how/detect-intent-stream""&gt;https://cloud.google.com/dialogflow/cx/docs/how/detect-intent-stream&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Has anyone successfully worked with Dialogflow and if so, what steps did you take to enable voice input?&lt;/p&gt;

&lt;p&gt;I need to make a demonstration and I decided to use the ES agent, but also having difficulties with it recognizing what I want to say. When I say &amp;quot;Hello LuPal&amp;quot;, it&amp;#39;ll give me a response of &amp;quot;Hello loophole&amp;quot; (??). Thank you for your response, I&amp;#39;d really appreciate any suggestions!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lyo4qi,True,,pineapplepipie,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lyo4qi/dialogflow_cx_issues_with_enabling_voice_input/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lyo4qi/dialogflow_cx_issues_with_enabling_voice_input/,30199,1614985071.0,0,,False,,,,,,1231
600,,LanguageTechnology,"Hello!

&amp;#x200B;

 My company is going to make available many PDF files for the public.

&amp;#x200B;

 Which settings/encoding should I recommend them to use in order for the software to be as much OCR-friendly as possible? For example, saving a MS Word file in PDF instead of scanning a printed image. (but that's a non-technical example)",t2_u20mf,False,,0,False,Hosting PDF for public view: how to make it as much OCR-friendly as possible?,[],r/LanguageTechnology,False,6,,0,,False,t3_lyg9e9,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1614992281.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;My company is going to make available many PDF files for the public.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Which settings/encoding should I recommend them to use in order for the software to be as much OCR-friendly as possible? For example, saving a MS Word file in PDF instead of scanning a printed image. (but that&amp;#39;s a non-technical example)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lyg9e9,True,,HardBender,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lyg9e9/hosting_pdf_for_public_view_how_to_make_it_as/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lyg9e9/hosting_pdf_for_public_view_how_to_make_it_as/,30199,1614963481.0,0,,False,,,,,,344
601,,LanguageTechnology,"Based on some additional research I am revising a question I asked yesterday. So new question, how does event extraction differ from information extraction? Could I use information extraction type code to accomplish event extraction from a written corpus? Any help would be much appreciated!",t2_4qae118k,False,,0,False,How does event extraction differ from information extraction/ retrieval?,[],r/LanguageTechnology,False,6,,0,,False,t3_lydxyz,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,1614962992.0,,[],{},,True,,1614985941.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Based on some additional research I am revising a question I asked yesterday. So new question, how does event extraction differ from information extraction? Could I use information extraction type code to accomplish event extraction from a written corpus? Any help would be much appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lydxyz,True,,Massive-Marzipan,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lydxyz/how_does_event_extraction_differ_from_information/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lydxyz/how_does_event_extraction_differ_from_information/,30199,1614957141.0,0,,False,,,,,,291
602,,LanguageTechnology,"Machine learning i taking over everything, including training text, speech, and language prediction models to do what they need to do. What's the need for rules in the NLP space anymore? Rules are for non-technical linguists and grammar writers, us NLP people are long past that and are doing it all with ML and neural nets.

Rule-based NLP is dead. Am I wrong? Prove me wrong, please. What USE is there for rule-based models in this field when we have machine learning models trained on mountains of meticulously-labeled data? Maybe if you didn't have any annotated labeled data, you might want to use rules in a pinch, but that's all ad hoc bullshit that will have to keep building up more and more as you find more and more things you didn't think of that will force you to make new rules. With ML all of those little things you don't think of are picked up in training so it knows how to deal with them right off the bat.",t2_aoa700i4,False,,0,False,Is rule-based NLP officially dead?,[],r/LanguageTechnology,False,6,,0,,False,t3_ly68xm,False,dark,0.61,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1614956151.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Machine learning i taking over everything, including training text, speech, and language prediction models to do what they need to do. What&amp;#39;s the need for rules in the NLP space anymore? Rules are for non-technical linguists and grammar writers, us NLP people are long past that and are doing it all with ML and neural nets.&lt;/p&gt;

&lt;p&gt;Rule-based NLP is dead. Am I wrong? Prove me wrong, please. What USE is there for rule-based models in this field when we have machine learning models trained on mountains of meticulously-labeled data? Maybe if you didn&amp;#39;t have any annotated labeled data, you might want to use rules in a pinch, but that&amp;#39;s all ad hoc bullshit that will have to keep building up more and more as you find more and more things you didn&amp;#39;t think of that will force you to make new rules. With ML all of those little things you don&amp;#39;t think of are picked up in training so it knows how to deal with them right off the bat.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ly68xm,True,,hqadn,,27,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ly68xm/is_rulebased_nlp_officially_dead/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ly68xm/is_rulebased_nlp_officially_dead/,30199,1614927351.0,0,,False,,,,,,925
603,,LanguageTechnology,"I thought this could be of interest. A team of 40+ changemakers built an NLP analysis pipeline for feature extraction, scraping Twitter, Google, and 1200 PDF files through automated APIs.   


https://omdena.com/blog/nlp-analysis/",t2_69ezr99,False,,0,False,Visualizing the Distribution of Several $Billions of #NGO Grants through Natural Language Processing (Case Study),[],r/LanguageTechnology,False,6,,0,,False,t3_lyczme,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1614983319.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I thought this could be of interest. A team of 40+ changemakers built an NLP analysis pipeline for feature extraction, scraping Twitter, Google, and 1200 PDF files through automated APIs.   &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://omdena.com/blog/nlp-analysis/""&gt;https://omdena.com/blog/nlp-analysis/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lyczme,True,,Lordobba,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lyczme/visualizing_the_distribution_of_several_billions/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lyczme/visualizing_the_distribution_of_several_billions/,30199,1614954519.0,0,,False,,,,,,230
604,,LanguageTechnology,I heard it takes around a month. I haven't heard back in 6 months.,t2_7w808r92,False,,0,False,How long did it take to get access to gpt3?,[],r/LanguageTechnology,False,6,,0,,False,t3_lyalog,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1614975430.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I heard it takes around a month. I haven&amp;#39;t heard back in 6 months.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lyalog,True,,Kind_Potato1241,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lyalog/how_long_did_it_take_to_get_access_to_gpt3/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lyalog/how_long_did_it_take_to_get_access_to_gpt3/,30199,1614946630.0,0,,False,,,,,,66
605,,LanguageTechnology,"Can anyone explain how event extraction coding techniques differ from topic modeling? A lot of the code examples I’ve looked at seem to be using topic modeling then organizing discerned topics by date. Is there more to event extraction than that? I assume there is. I feel like I must be missing something and I hope someone here has some insight. Also, if anyone has any materials regarding event extraction that are helpful I would love to hear about them. Thanks all!",t2_4qae118k,False,,0,False,How does event extraction differ from topic modeling?,[],r/LanguageTechnology,False,6,,0,,False,t3_lxsgio,False,dark,1.0,,public,13,0,{},,False,[],,False,False,,{},,False,13,,False,False,,False,,[],{},,True,,1614913608.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can anyone explain how event extraction coding techniques differ from topic modeling? A lot of the code examples I’ve looked at seem to be using topic modeling then organizing discerned topics by date. Is there more to event extraction than that? I assume there is. I feel like I must be missing something and I hope someone here has some insight. Also, if anyone has any materials regarding event extraction that are helpful I would love to hear about them. Thanks all!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lxsgio,True,,Massive-Marzipan,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lxsgio/how_does_event_extraction_differ_from_topic/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lxsgio/how_does_event_extraction_differ_from_topic/,30199,1614884808.0,0,,False,,,,,,470
606,,LanguageTechnology,"I'm a bit of a newbie when it comes to image captioning and was wondering if it would make sense to feed an image and a *ground-truth* *caption* to a pre-trained image-captioning model and visualize the attention scores over the image for every word? I.e. in order to see how much the model *would have had* to rely on the image features as opposed to the language model in order to generate such a caption?  If so, does anyone have experience implementing this or could point me to a repo that does?",t2_m7nob,False,,0,False,Attention over Ground-Truth Image Caption?,[],r/LanguageTechnology,False,6,,0,,False,t3_lxldzm,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1614895936.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a bit of a newbie when it comes to image captioning and was wondering if it would make sense to feed an image and a &lt;em&gt;ground-truth&lt;/em&gt; &lt;em&gt;caption&lt;/em&gt; to a pre-trained image-captioning model and visualize the attention scores over the image for every word? I.e. in order to see how much the model &lt;em&gt;would have had&lt;/em&gt; to rely on the image features as opposed to the language model in order to generate such a caption?  If so, does anyone have experience implementing this or could point me to a repo that does?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lxldzm,True,,ptitpainsuedois,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lxldzm/attention_over_groundtruth_image_caption/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lxldzm/attention_over_groundtruth_image_caption/,30199,1614867136.0,0,,False,,,,,,500
607,,LanguageTechnology,,t2_z8lug,False,,0,False,King -Man +Woman = King ?,[],r/LanguageTechnology,False,6,,0,,False,t3_lxu1ct,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,False,,1614917635.0,text,6,,,text,blog.esciencecenter.nl,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lxu1ct,True,,martin_m_n_novy,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lxu1ct/king_man_woman_king/,all_ads,False,https://blog.esciencecenter.nl/king-man-woman-king-9a7fd2935a85,30199,1614888835.0,0,,False,https://blog.esciencecenter.nl/king-man-woman-king-9a7fd2935a85,,,,,0
608,,LanguageTechnology,I am looking to figure out a way to show how news events unfold over time : ie how early articles described an event like the us capitol riot versus what they say now. I’m guessing semantic similarity is the best way to achieve this? Does anyone have other/ better ideas? I’m fairly new to nlp and appreciate any help anyone can provide. I have gotten great help from this subreddit in the past.,t2_4qae118k,False,,0,False,How to best determine changing news event coverage— semantic similarity or no?,[],r/LanguageTechnology,False,6,,0,,False,t3_lxtrsz,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1614916985.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am looking to figure out a way to show how news events unfold over time : ie how early articles described an event like the us capitol riot versus what they say now. I’m guessing semantic similarity is the best way to achieve this? Does anyone have other/ better ideas? I’m fairly new to nlp and appreciate any help anyone can provide. I have gotten great help from this subreddit in the past.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lxtrsz,True,,Massive-Marzipan,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lxtrsz/how_to_best_determine_changing_news_event/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lxtrsz/how_to_best_determine_changing_news_event/,30199,1614888185.0,0,,False,,,,,,395
609,,LanguageTechnology,"Hey, I've created a tutorial on how to perform a Shapiro-Wilk normality test in the R programming language: [https://statisticsglobe.com/shapiro-wilk-normality-test-in-r](https://statisticsglobe.com/shapiro-wilk-normality-test-in-r)",t2_77cigax1,False,,0,False,Tutorial on how to perform a Shapiro-Wilk normality test,[],r/LanguageTechnology,False,6,,0,,False,t3_lxgx5l,False,dark,0.57,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1614879586.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, I&amp;#39;ve created a tutorial on how to perform a Shapiro-Wilk normality test in the R programming language: &lt;a href=""https://statisticsglobe.com/shapiro-wilk-normality-test-in-r""&gt;https://statisticsglobe.com/shapiro-wilk-normality-test-in-r&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lxgx5l,True,,JoachimSchork,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lxgx5l/tutorial_on_how_to_perform_a_shapirowilk/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lxgx5l/tutorial_on_how_to_perform_a_shapirowilk/,30199,1614850786.0,0,,False,,,,,,232
610,,LanguageTechnology,"I’m trying to build a model that will decide if a review is positive or negative. I’d like to use the above methods but am unsure if they can all be used together. I think my order of operations would be clean data(case, punctuation, stop words etc) then get bigrams, as a bag of bigrams instead of just words? And then get the tf-idf scores for these bigrams. Use those scores with with something like naive bayes classifier to make my predictions? Is this doable? Or Will I mangle some methods that aren’t really meant to be used together?",t2_7wo3mdn,False,,0,False,"Using bag or words, bigrams and tf-idf together",[],r/LanguageTechnology,False,6,,0,,False,t3_lx47ms,False,dark,0.99,,public,13,0,{},,False,[],,False,False,,{},,False,13,,False,False,,1614805404.0,,[],{},,True,,1614834009.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m trying to build a model that will decide if a review is positive or negative. I’d like to use the above methods but am unsure if they can all be used together. I think my order of operations would be clean data(case, punctuation, stop words etc) then get bigrams, as a bag of bigrams instead of just words? And then get the tf-idf scores for these bigrams. Use those scores with with something like naive bayes classifier to make my predictions? Is this doable? Or Will I mangle some methods that aren’t really meant to be used together?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lx47ms,True,,edwardsrk,,11,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lx47ms/using_bag_or_words_bigrams_and_tfidf_together/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lx47ms/using_bag_or_words_bigrams_and_tfidf_together/,30199,1614805209.0,1,,False,,,,,,541
611,,LanguageTechnology,,t2_hkv9s,False,,0,False,Combining BERT with Static Word Embedding for Categorizing Social Media | Research Paper Walkthrough,[],r/LanguageTechnology,False,6,,0,,False,t3_lwzppi,False,dark,0.86,,public,10,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/VqlA_ALWQdM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Combining BERT with Static Word Embedding for Categorizing Social Media | Research Paper Walkthrough', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/VqlA_ALWQdM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/VqlA_ALWQdM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/VqlA_ALWQdM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lwzppi', 'height': 200}",,False,10,,False,False,,False,,[],{},,False,,1614822199.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lwzppi,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lwzppi/combining_bert_with_static_word_embedding_for/,all_ads,False,https://youtu.be/VqlA_ALWQdM,30199,1614793399.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Combining BERT with Static Word Embedding for Categorizing Social Media | Research Paper Walkthrough', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/VqlA_ALWQdM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/VqlA_ALWQdM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/VqlA_ALWQdM,,,,,0
612,,LanguageTechnology,"We recently wrote a paper that investigates whether pretrained language models can use their internal knowledge to detect🩺 and discard🩹 undesired behaviors and reduce biases in their own outputs: [https://arxiv.org/abs/2103.00453](https://arxiv.org/abs/2103.00453)

This is still a very early draft. We plan to extend the paper along several axes and to conduct extensive further experiments, so I'd be happy to hear your thoughts 😊",t2_383etasr,False,,0,False,"""All terrorists are [MASK]"": Self-Diagnosis and Self-Debiasing for Pretrained Language Models",[],r/LanguageTechnology,False,6,,0,,False,t3_lwqxqd,False,dark,0.95,,public,28,0,{},,False,[],,False,False,,{},,False,28,,False,False,,False,,[],{},,True,,1614797027.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We recently wrote a paper that investigates whether pretrained language models can use their internal knowledge to detect🩺 and discard🩹 undesired behaviors and reduce biases in their own outputs: &lt;a href=""https://arxiv.org/abs/2103.00453""&gt;https://arxiv.org/abs/2103.00453&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is still a very early draft. We plan to extend the paper along several axes and to conduct extensive further experiments, so I&amp;#39;d be happy to hear your thoughts 😊&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lwqxqd,True,,timoschick,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lwqxqd/all_terrorists_are_mask_selfdiagnosis_and/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lwqxqd/all_terrorists_are_mask_selfdiagnosis_and/,30199,1614768227.0,1,,False,,,,,,432
613,,LanguageTechnology,"I have two corpora, one with 5k positive review and one with 5k negative reviews. If use a bag of words or bag of bigrams method to do sentiment analysis, should I keep the the corpora separate or do I rage each review in each corpus as pos/neg and merge them?",t2_7wo3mdn,False,,0,False,"For a bag of bigrams, do you tag things positive or negative?",[],r/LanguageTechnology,False,6,,0,,False,t3_lxb80e,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1614856139.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have two corpora, one with 5k positive review and one with 5k negative reviews. If use a bag of words or bag of bigrams method to do sentiment analysis, should I keep the the corpora separate or do I rage each review in each corpus as pos/neg and merge them?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lxb80e,True,,edwardsrk,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lxb80e/for_a_bag_of_bigrams_do_you_tag_things_positive/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lxb80e/for_a_bag_of_bigrams_do_you_tag_things_positive/,30199,1614827339.0,0,,False,,,,,,260
614,,LanguageTechnology,I am reading a paper and it uses the LIWC to estimate risk of certain mental health disorders. I am confused if LIWC is an example of machine learning. Or whether it is part of processing the data to be used in a ML algorithm. Thanks in advance,t2_960cvruz,False,,0,False,Is LIWC machine learning?,[],r/LanguageTechnology,False,6,,0,,False,t3_lx629u,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1614839281.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am reading a paper and it uses the LIWC to estimate risk of certain mental health disorders. I am confused if LIWC is an example of machine learning. Or whether it is part of processing the data to be used in a ML algorithm. Thanks in advance&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lx629u,True,,copperthecavachon,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lx629u/is_liwc_machine_learning/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lx629u/is_liwc_machine_learning/,30199,1614810481.0,0,,False,,,,,,244
615,,LanguageTechnology,"Like if you were a data scientist working at an actual company Doing nlp, what would your average corpus size be? I’m writing a paper on serializing classifiers and trimming down/preprocessing data but only have exposure to smaller data sets we would use in a class room and was wondering what the typical load would look like. Thank you!",t2_7wo3mdn,False,,0,False,Is a corpus of 5million reviews large or small or midsized for a corpus?,[],r/LanguageTechnology,False,6,,0,,False,t3_lx33zm,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1614830968.0,text,6,,,text,self.LanguageTechnology,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Like if you were a data scientist working at an actual company Doing nlp, what would your average corpus size be? I’m writing a paper on serializing classifiers and trimming down/preprocessing data but only have exposure to smaller data sets we would use in a class room and was wondering what the typical load would look like. Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lx33zm,True,,edwardsrk,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lx33zm/is_a_corpus_of_5million_reviews_large_or_small_or/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lx33zm/is_a_corpus_of_5million_reviews_large_or_small_or/,30199,1614802168.0,0,,False,,,,,,338
616,,LanguageTechnology,"Hi all, I'm supposed to evaluate a Dutch DL NER model (Huggingface) for a course, and compare it to a rule-based model. I'm feeling a bit lost and don't know where to start. Does anyone have any tips/good practices? What metrics should I use, and are there other evaluation techniques besides metrics? I appreciate all input!",t2_3xt0icx5,False,,0,False,Best/good practices for evaluating deep learning NER models?,[],r/LanguageTechnology,False,6,,0,,False,t3_lx6lps,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1614840885.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, I&amp;#39;m supposed to evaluate a Dutch DL NER model (Huggingface) for a course, and compare it to a rule-based model. I&amp;#39;m feeling a bit lost and don&amp;#39;t know where to start. Does anyone have any tips/good practices? What metrics should I use, and are there other evaluation techniques besides metrics? I appreciate all input!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lx6lps,True,,Melancholic_kitten,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lx6lps/bestgood_practices_for_evaluating_deep_learning/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lx6lps/bestgood_practices_for_evaluating_deep_learning/,30199,1614812085.0,0,,False,,,,,,325
617,,LanguageTechnology,"I'm a hobbyist with no background in machine learning. Lately, I have been entertaining the idea of creating a very minimalist story prompt generator for use in solo role playing games (the tabletop sort). I want it to generate only an adjective and a noun, such as ""open cage"", ""endless parade"", or ""timely arrival""; so I data-mined some books for adjectives and nouns that are commonly used together. Now my problem is that these pairs that I have extracted cover a wild range of themes, and in most cases are not applicable to the situation the game is in. I at least want to be able to separate them into some general categories like ""objects"", ""events"", ""threats"" etc. I was wondering if I can somehow do this with machine learning. 

Since I don't fully know the capabilities of the current models, I'm not sure about the doability of what I want to do (i.e. labeling two-word strings). So far, I've tried using Logistic Regression with only a single category and 200 datapoints (half belonging to the category, half not). The model was only able to label 2 out of 7 new pairs correctly, not very successful. I haven't tried doing the same with more data, but my gut feeling tells me that what the model is learning is the relatedness of each token in a pair to the category label (this might be an obvious thing, but like I said, I'm a noob). If that is the case, it is not very ideal for me because I have tens of thousands of pairs and I cannot cover each single word related to a specific category by coding a part of the data by hand.  

Is there a good way to tackle this problem? Is it doable? Can you give me some pointers about where to look before I invest tens of hours in a fruitless endeavor?",t2_113skr,False,,0,False,Labeling adjective+noun pairs with themes,[],r/LanguageTechnology,False,6,,0,,False,t3_lx0lfd,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1614824403.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a hobbyist with no background in machine learning. Lately, I have been entertaining the idea of creating a very minimalist story prompt generator for use in solo role playing games (the tabletop sort). I want it to generate only an adjective and a noun, such as &amp;quot;open cage&amp;quot;, &amp;quot;endless parade&amp;quot;, or &amp;quot;timely arrival&amp;quot;; so I data-mined some books for adjectives and nouns that are commonly used together. Now my problem is that these pairs that I have extracted cover a wild range of themes, and in most cases are not applicable to the situation the game is in. I at least want to be able to separate them into some general categories like &amp;quot;objects&amp;quot;, &amp;quot;events&amp;quot;, &amp;quot;threats&amp;quot; etc. I was wondering if I can somehow do this with machine learning. &lt;/p&gt;

&lt;p&gt;Since I don&amp;#39;t fully know the capabilities of the current models, I&amp;#39;m not sure about the doability of what I want to do (i.e. labeling two-word strings). So far, I&amp;#39;ve tried using Logistic Regression with only a single category and 200 datapoints (half belonging to the category, half not). The model was only able to label 2 out of 7 new pairs correctly, not very successful. I haven&amp;#39;t tried doing the same with more data, but my gut feeling tells me that what the model is learning is the relatedness of each token in a pair to the category label (this might be an obvious thing, but like I said, I&amp;#39;m a noob). If that is the case, it is not very ideal for me because I have tens of thousands of pairs and I cannot cover each single word related to a specific category by coding a part of the data by hand.  &lt;/p&gt;

&lt;p&gt;Is there a good way to tackle this problem? Is it doable? Can you give me some pointers about where to look before I invest tens of hours in a fruitless endeavor?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lx0lfd,True,,SilvioDylan,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lx0lfd/labeling_adjectivenoun_pairs_with_themes/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lx0lfd/labeling_adjectivenoun_pairs_with_themes/,30199,1614795603.0,0,,False,,,,,,1711
618,,LanguageTechnology,"I'm using Google AI Platform. When importing a JSONL file with various HTML entities, the encoding changes during the import. So for example, a registered name like **Nintendo®** is appearing in my dataset as **NintendoÂ®** and a trademarked name like **Animal Crossing™** is appearing as **Animal Crossingâ¢**.

When it comes to training an accurate model, should I be selecting the longer and stranger encoded version for the label since technically that's all part of the name (encoded differently)? Or am I missing a way to better encode my Google datasets?

**Edit:** More information... I'm uploading it in UTF-8 format and it appears correctly when viewed in Google Cloud Storage but during labeling it's appearing differently (ISO-8859-1 format). I guess my main question is only is it okay to proceed with labeling in this format (since I assume all data coming in will be converted in the same fashion)?",t2_u900733,False,,0,False,Named Entity Recognition: What's the best way to handle entity labeling for variations in HTML encoding for symbols like trademarks(™) and registered trademarks(®)?,[],r/LanguageTechnology,False,6,,0,,False,t3_lwz8zb,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,1614803098.0,,[],{},,True,,1614821017.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m using Google AI Platform. When importing a JSONL file with various HTML entities, the encoding changes during the import. So for example, a registered name like &lt;strong&gt;Nintendo®&lt;/strong&gt; is appearing in my dataset as &lt;strong&gt;NintendoÂ®&lt;/strong&gt; and a trademarked name like &lt;strong&gt;Animal Crossing™&lt;/strong&gt; is appearing as &lt;strong&gt;Animal Crossingâ¢&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;When it comes to training an accurate model, should I be selecting the longer and stranger encoded version for the label since technically that&amp;#39;s all part of the name (encoded differently)? Or am I missing a way to better encode my Google datasets?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; More information... I&amp;#39;m uploading it in UTF-8 format and it appears correctly when viewed in Google Cloud Storage but during labeling it&amp;#39;s appearing differently (ISO-8859-1 format). I guess my main question is only is it okay to proceed with labeling in this format (since I assume all data coming in will be converted in the same fashion)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lwz8zb,True,,UniverseSimulation,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lwz8zb/named_entity_recognition_whats_the_best_way_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lwz8zb/named_entity_recognition_whats_the_best_way_to/,30199,1614792217.0,0,,False,,,,,,914
619,,LanguageTechnology,"I've been wanting to spend some time reading through research to gain technical computational linguistics knowledge. Are there any more basic papers I can explore in the realm of research? I'm beginning my CS degree, so my knowledge base is pretty sparse at this point, but I'm exploring topics that interest me like Universal Grammar or Phonology/Prosody.",t2_5yziyy8x,False,,0,False,Beginner Research Papers to Read,[],r/LanguageTechnology,False,6,,0,,False,t3_lwkniu,False,dark,1.0,,public,21,0,{},,False,[],,False,False,,{},,False,21,,False,False,,False,,[],{},,True,,1614772858.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been wanting to spend some time reading through research to gain technical computational linguistics knowledge. Are there any more basic papers I can explore in the realm of research? I&amp;#39;m beginning my CS degree, so my knowledge base is pretty sparse at this point, but I&amp;#39;m exploring topics that interest me like Universal Grammar or Phonology/Prosody.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lwkniu,True,,FocalFactual,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lwkniu/beginner_research_papers_to_read/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lwkniu/beginner_research_papers_to_read/,30199,1614744058.0,0,,False,,,,,,356
620,,LanguageTechnology,"Do you think it would be splitting it up into sentences and using a BERT to generate a single average doc embedding from which you could calculate cosines distance on, or should it be employing some kind of long document transformer?",t2_x37tk,False,,0,False,Best way to do unsupervised long document similarity?,[],r/LanguageTechnology,False,6,,0,,False,t3_lx1e15,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1614826440.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Do you think it would be splitting it up into sentences and using a BERT to generate a single average doc embedding from which you could calculate cosines distance on, or should it be employing some kind of long document transformer?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lx1e15,True,,Maltemusen,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lx1e15/best_way_to_do_unsupervised_long_document/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lx1e15/best_way_to_do_unsupervised_long_document/,30199,1614797640.0,0,,False,,,,,,233
621,,LanguageTechnology,"Hello all!

I apologize of this is the wrong place to ask this question, but I am starting a PhD program in ECE next year focusing on SLP. I mainly have experience with NLP and was wondering if different ial equations are useful for Speech? In my final quarter of undergrad I can either take ODE or Multivariate stats and was wondering if I could get some insight on which one may be more useful?

Thank you!",t2_10f652,False,,0,False,Academic Course Question,[],r/LanguageTechnology,False,6,,0,,False,t3_lx15vi,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1614825867.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all!&lt;/p&gt;

&lt;p&gt;I apologize of this is the wrong place to ask this question, but I am starting a PhD program in ECE next year focusing on SLP. I mainly have experience with NLP and was wondering if different ial equations are useful for Speech? In my final quarter of undergrad I can either take ODE or Multivariate stats and was wondering if I could get some insight on which one may be more useful?&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lx15vi,True,,Runninganddogs979,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lx15vi/academic_course_question/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lx15vi/academic_course_question/,30199,1614797067.0,0,,False,,,,,,408
622,,LanguageTechnology,"I have a large db with questions and answers but the problem is that there is no context. So the question is, how can I benefit from this data to train a model? Which type of a model (downstream task) could I train with it? I think I am onto something but can't find out what and how.",t2_7tb2j,False,,0,False,How to benefit from Q&amp;A dataset,[],r/LanguageTechnology,False,6,,0,,False,t3_lwwunt,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1614815040.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a large db with questions and answers but the problem is that there is no context. So the question is, how can I benefit from this data to train a model? Which type of a model (downstream task) could I train with it? I think I am onto something but can&amp;#39;t find out what and how.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lwwunt,True,,gevezex,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lwwunt/how_to_benefit_from_qa_dataset/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lwwunt/how_to_benefit_from_qa_dataset/,30199,1614786240.0,0,,False,,,,,,284
623,,LanguageTechnology,"Hey all, 
Hope it’s okay to post link to surveys here.

I’m currently writing my final year dissertation on Automatic Quiz generation  and need data about the quality of the generated questions.

[form](https://forms.gle/TMYcr63HjhVN33P77) 

Thank you for your time!
Any feedback would be appreciated!",t2_8vzdo03r,False,,0,False,Help with dissertation survey - Automatic Quiz Qeneration,[],r/LanguageTechnology,False,6,,0,,False,t3_lws3fe,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1614801118.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all, 
Hope it’s okay to post link to surveys here.&lt;/p&gt;

&lt;p&gt;I’m currently writing my final year dissertation on Automatic Quiz generation  and need data about the quality of the generated questions.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://forms.gle/TMYcr63HjhVN33P77""&gt;form&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Thank you for your time!
Any feedback would be appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lws3fe,True,,finance_and_kebabs,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lws3fe/help_with_dissertation_survey_automatic_quiz/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lws3fe/help_with_dissertation_survey_automatic_quiz/,30199,1614772318.0,0,,False,,,,,,301
624,,LanguageTechnology,"Hello, community! We are running a machine-learning startup with a focus on #LanguageTechnology ([https://reason.al/](https://reason.al/?utm_source=reddit.com&amp;utm_medium=referral&amp;utm_campaign=BL-share-comm-RD-001)) and started to publish very practical and hands-on blog posts on machine-learning and broader coding insights.

Our latest one is on why and how to set up a machine-learning pipeline from home:

[https://reason.al/blog/ml-from-home-1](https://reason.al/blog/ml-from-home-1/?utm_source=linkedin.com&amp;utm_medium=referral&amp;utm_campaign=BL-share-comm-LI-001)

I hope that's of interest for you here, would be great to see you subscribe to the (monthly) newsletter, too!",t2_909vsfqe,False,,0,False,Why and how to set up a machine-learning pipeline from the home office,[],r/LanguageTechnology,False,6,,0,,False,t3_lwr173,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,1614769195.0,,[],{},,True,,1614797399.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, community! We are running a machine-learning startup with a focus on #LanguageTechnology (&lt;a href=""https://reason.al/?utm_source=reddit.com&amp;amp;utm_medium=referral&amp;amp;utm_campaign=BL-share-comm-RD-001""&gt;https://reason.al/&lt;/a&gt;) and started to publish very practical and hands-on blog posts on machine-learning and broader coding insights.&lt;/p&gt;

&lt;p&gt;Our latest one is on why and how to set up a machine-learning pipeline from home:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://reason.al/blog/ml-from-home-1/?utm_source=linkedin.com&amp;amp;utm_medium=referral&amp;amp;utm_campaign=BL-share-comm-LI-001""&gt;https://reason.al/blog/ml-from-home-1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I hope that&amp;#39;s of interest for you here, would be great to see you subscribe to the (monthly) newsletter, too!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lwr173,True,,natalieberlin,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lwr173/why_and_how_to_set_up_a_machinelearning_pipeline/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lwr173/why_and_how_to_set_up_a_machinelearning_pipeline/,30199,1614768599.0,0,,False,,,,,,694
625,,LanguageTechnology,"Apologies for the super-beginner's level question -- I've tried looking around, and there's not a ton out there. 

I understand at least vaguely what's meant when we talk about what the Queries, Keys, and Values in networks with Self-Attention. What I'm not fully understanding is how these Matrices are learned during training. How does the LM converge on these final Matrices?",t2_cc1kr,False,,0,False,"How are the Q,K &amp; V matrices learned in Self-Attention networks?",[],r/LanguageTechnology,False,6,,0,,False,t3_lwcriy,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1614748625.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Apologies for the super-beginner&amp;#39;s level question -- I&amp;#39;ve tried looking around, and there&amp;#39;s not a ton out there. &lt;/p&gt;

&lt;p&gt;I understand at least vaguely what&amp;#39;s meant when we talk about what the Queries, Keys, and Values in networks with Self-Attention. What I&amp;#39;m not fully understanding is how these Matrices are learned during training. How does the LM converge on these final Matrices?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lwcriy,True,,ryan516,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lwcriy/how_are_the_qk_v_matrices_learned_in/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lwcriy/how_are_the_qk_v_matrices_learned_in/,30199,1614719825.0,0,,False,,,,,,378
626,,LanguageTechnology,"Hi all, I just finished my subreddit browser that organizes subs by tags.  It also queries wikipedia for pages for each tag, analyzes those pages with Watson, and color codes them based on the main emotion.  I'm having fun learning relationships between concepts.  Hope you do too.  www.dao.af",t2_amyzfjaz,False,,0,False,NLU subreddit search app,[],r/LanguageTechnology,False,6,,0,,False,t3_lwjv0e,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1614770225.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, I just finished my subreddit browser that organizes subs by tags.  It also queries wikipedia for pages for each tag, analyzes those pages with Watson, and color codes them based on the main emotion.  I&amp;#39;m having fun learning relationships between concepts.  Hope you do too.  &lt;a href=""http://www.dao.af""&gt;www.dao.af&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lwjv0e,True,,dao-af,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lwjv0e/nlu_subreddit_search_app/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lwjv0e/nlu_subreddit_search_app/,30199,1614741425.0,0,,False,,,,,,293
627,,LanguageTechnology,,t2_ozpva,False,,0,False,Does anyone know Chinese version for otter.ai?,[],r/LanguageTechnology,False,6,,0,,False,t3_lwjeeh,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1614768742.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lwjeeh,True,,wilsonckao,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lwjeeh/does_anyone_know_chinese_version_for_otterai/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lwjeeh/does_anyone_know_chinese_version_for_otterai/,30199,1614739942.0,0,,False,,,,,,0
628,,LanguageTechnology,"Over the past few decades, Deep neural network-based models have been developed to complete a broad range of tasks. Some of them are mainly designed to process and generate coherent texts in multiple languages, answer questions about a text, translate texts, and create summaries of the online content.

Several Deep learning systems are already available with linguistic capabilities, for instance, text analysis tools, in the form of applications for real-time translation, and virtual assistants such as Alexa, Bixby, Siri, Google Assistant, and Cortana. Some of the above systems use a specific deep-learning model called Multilingual BERT (mBERT). mBERT is released by Google and is trained on approximately 100 languages simultaneously. 

Paper summary: [https://www.marktechpost.com/2021/03/01/researchers-from-stanford-uci-and-uc-santa-barbara-conducted-a-study-to-understand-how-the-mbert-model-encodes-grammatical-features/](https://www.marktechpost.com/2021/03/01/researchers-from-stanford-uci-and-uc-santa-barbara-conducted-a-study-to-understand-how-the-mbert-model-encodes-grammatical-features/) 

Paper: [https://arxiv.org/abs/2101.11043](https://arxiv.org/abs/2101.11043) 

Github: [https://github.com/toizzy/deep-subjecthood](https://github.com/toizzy/deep-subjecthood)",t2_4wudjgid,False,,0,False,"Researchers From Stanford, UCI and UC Santa Barbara Conducted a Study to Understand How The mBERT Model Encodes Grammatical Features",[],r/LanguageTechnology,False,6,,0,,False,t3_lvv25w,False,dark,0.98,,public,35,0,{},,False,[],,False,False,,{},,False,35,,False,False,,False,,[],{},,True,,1614692830.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Over the past few decades, Deep neural network-based models have been developed to complete a broad range of tasks. Some of them are mainly designed to process and generate coherent texts in multiple languages, answer questions about a text, translate texts, and create summaries of the online content.&lt;/p&gt;

&lt;p&gt;Several Deep learning systems are already available with linguistic capabilities, for instance, text analysis tools, in the form of applications for real-time translation, and virtual assistants such as Alexa, Bixby, Siri, Google Assistant, and Cortana. Some of the above systems use a specific deep-learning model called Multilingual BERT (mBERT). mBERT is released by Google and is trained on approximately 100 languages simultaneously. &lt;/p&gt;

&lt;p&gt;Paper summary: &lt;a href=""https://www.marktechpost.com/2021/03/01/researchers-from-stanford-uci-and-uc-santa-barbara-conducted-a-study-to-understand-how-the-mbert-model-encodes-grammatical-features/""&gt;https://www.marktechpost.com/2021/03/01/researchers-from-stanford-uci-and-uc-santa-barbara-conducted-a-study-to-understand-how-the-mbert-model-encodes-grammatical-features/&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/abs/2101.11043""&gt;https://arxiv.org/abs/2101.11043&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Github: &lt;a href=""https://github.com/toizzy/deep-subjecthood""&gt;https://github.com/toizzy/deep-subjecthood&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lvv25w,True,,techsucker,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lvv25w/researchers_from_stanford_uci_and_uc_santa/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lvv25w/researchers_from_stanford_uci_and_uc_santa/,30199,1614664030.0,0,,False,,,,,,1285
629,,LanguageTechnology,"Are you working on fact-checking, detection of misinformation, hate speech or other problems related to trust and truth online? You can now **submit a paper or talk proposal to the Truth and Trust Online 2021 (TTO 2021). https://truthandtrustonline.com/call-for-papers-2/**

The annual Conference for Truth and Trust Online is organised as a unique collaboration between practitioners, technologists, academics and platforms, to share, discuss, and collaborate on useful technical innovations and research in the space. This year’s **TTO is virtual and will take place online Oct 7-8 2021.** 

We welcome technical papers of the following types: surveys, methods, reproduction papers, resource papers, case studies. 

**Topics of interest include:**

* Misinformation and disinformation 
* Trustworthiness of COVID-19 news and guidance
* Hate speech
* Online harassment and cyberbullying
* Credibility and fake reviews
* Hyper-partisanship and bias
* Image/video/audio verification
* Fake amplification, polarization, and echo chambers
* Transparency in content and source moderation 
* Privacy and anonymity requirements

We encourage wide participation from all interested parties and stakeholders on online media, including academics, startups and large industry, non-profit organizations and governmental institutions.

**Technical paper submission deadline: July 30, 2021**

**Talk proposal submission deadline: August 13, 2021**

More details can be found: https://truthandtrustonline.com/call-for-papers-2/",t2_86xv3ene,False,,0,False,Conference for Truth and Trust Online Calls for Paper and Talk Proposal Submissions,[],r/LanguageTechnology,False,6,,0,,False,t3_lwfp2m,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1614756951.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Are you working on fact-checking, detection of misinformation, hate speech or other problems related to trust and truth online? You can now &lt;strong&gt;submit a paper or talk proposal to the Truth and Trust Online 2021 (TTO 2021). &lt;a href=""https://truthandtrustonline.com/call-for-papers-2/""&gt;https://truthandtrustonline.com/call-for-papers-2/&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The annual Conference for Truth and Trust Online is organised as a unique collaboration between practitioners, technologists, academics and platforms, to share, discuss, and collaborate on useful technical innovations and research in the space. This year’s &lt;strong&gt;TTO is virtual and will take place online Oct 7-8 2021.&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;We welcome technical papers of the following types: surveys, methods, reproduction papers, resource papers, case studies. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Topics of interest include:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Misinformation and disinformation &lt;/li&gt;
&lt;li&gt;Trustworthiness of COVID-19 news and guidance&lt;/li&gt;
&lt;li&gt;Hate speech&lt;/li&gt;
&lt;li&gt;Online harassment and cyberbullying&lt;/li&gt;
&lt;li&gt;Credibility and fake reviews&lt;/li&gt;
&lt;li&gt;Hyper-partisanship and bias&lt;/li&gt;
&lt;li&gt;Image/video/audio verification&lt;/li&gt;
&lt;li&gt;Fake amplification, polarization, and echo chambers&lt;/li&gt;
&lt;li&gt;Transparency in content and source moderation &lt;/li&gt;
&lt;li&gt;Privacy and anonymity requirements&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We encourage wide participation from all interested parties and stakeholders on online media, including academics, startups and large industry, non-profit organizations and governmental institutions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Technical paper submission deadline: July 30, 2021&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Talk proposal submission deadline: August 13, 2021&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;More details can be found: &lt;a href=""https://truthandtrustonline.com/call-for-papers-2/""&gt;https://truthandtrustonline.com/call-for-papers-2/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lwfp2m,True,,kochkinael,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lwfp2m/conference_for_truth_and_trust_online_calls_for/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lwfp2m/conference_for_truth_and_trust_online_calls_for/,30199,1614728151.0,0,,False,,,,,,1513
630,,LanguageTechnology,"Hi,

I've peripherally worked with NLP in the past mainly through my work with Python and machine learning. I'd love to get more in-depth knowledge and so would my company, so much so they've offered to pay for any NLP course I want.

Does anyone have any good recommendations for NLP courses?",t2_6nh12,False,,0,False,NLP course,[],r/LanguageTechnology,False,6,,0,,False,t3_lwenyp,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1614753953.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve peripherally worked with NLP in the past mainly through my work with Python and machine learning. I&amp;#39;d love to get more in-depth knowledge and so would my company, so much so they&amp;#39;ve offered to pay for any NLP course I want.&lt;/p&gt;

&lt;p&gt;Does anyone have any good recommendations for NLP courses?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lwenyp,True,,afrochum,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lwenyp/nlp_course/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lwenyp/nlp_course/,30199,1614725153.0,0,,False,,,,,,293
631,,LanguageTechnology,,t2_trjf2,False,,0,False,Why a YouTube Chat About Chess Got Flagged for Hate Speech,[],r/LanguageTechnology,False,6,,0,,False,t3_lw98tw,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,False,,1614739207.0,text,6,,,text,wired.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lw98tw,True,,adammathias,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lw98tw/why_a_youtube_chat_about_chess_got_flagged_for/,all_ads,False,https://www.wired.com/story/why-youtube-chat-chess-flagged-hate-speech/,30199,1614710407.0,0,,False,https://www.wired.com/story/why-youtube-chat-chess-flagged-hate-speech/,,,,,0
632,,LanguageTechnology,"Hi everyone, we are a marketing company and going to start an annotation project for entity sentiment analysis. Can you please share what are best practices for starting an NLP annotation project? What is most efficient approach? What techniques are mostly used to automate the annotation process?",t2_a9eqcnhr,False,,0,False,[D] Annotation tool for entity sentiment analysis,[],r/LanguageTechnology,False,6,,0,,False,t3_lw07f5,False,dark,0.81,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1614714517.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, we are a marketing company and going to start an annotation project for entity sentiment analysis. Can you please share what are best practices for starting an NLP annotation project? What is most efficient approach? What techniques are mostly used to automate the annotation process?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lw07f5,True,,KarlaNour96,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lw07f5/d_annotation_tool_for_entity_sentiment_analysis/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lw07f5/d_annotation_tool_for_entity_sentiment_analysis/,30199,1614685717.0,0,,False,,,,,,297
633,,LanguageTechnology,,t2_5alofkdd,False,,0,False,"Paper ""M6: A Chinese Multimodal Pretrainer"". Dataset contains 1900GB of images and 292GB of text. Models contain 10B parameters and 100B (Mixture-of-Experts) parameters. Images shown are text-to-image examples from the paper. Paper link is in a comment.",[],r/LanguageTechnology,False,6,,0,,False,t3_lvv7fc,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,False,,1614693412.0,text,6,,,text,reddit.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lvv7fc,True,,Wiskkey,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lvv7fc/paper_m6_a_chinese_multimodal_pretrainer_dataset/,all_ads,False,https://www.reddit.com/gallery/lvv2mo,30199,1614664612.0,0,,False,https://www.reddit.com/gallery/lvv2mo,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': '', 'author_fullname': 't2_5alofkdd', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'is_gallery': True, 'title': '[R] Paper ""M6: A Chinese Multimodal Pretrainer"". Dataset contains 1900GB of images and 292GB of text. Models contain 10B parameters and 100B (Mixture-of-Experts) parameters. Images shown are text-to-image examples from the paper. Paper link is in a comment.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'three', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'129bwvzpxjk61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 90, 'x': 108, 'u': 'https://preview.redd.it/129bwvzpxjk61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6e34622f394843876559708653e9e8852cea6aa1'}, {'y': 181, 'x': 216, 'u': 'https://preview.redd.it/129bwvzpxjk61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bf3b81bc9e820d350fc98e8c46d9e08b7c739c14'}, {'y': 268, 'x': 320, 'u': 'https://preview.redd.it/129bwvzpxjk61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9561210767fc122ca281c1a284c1df14dfad25e6'}, {'y': 536, 'x': 640, 'u': 'https://preview.redd.it/129bwvzpxjk61.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=add2918f80b2081d9c03c94c0e6e4a280c4bc276'}], 's': {'y': 570, 'x': 680, 'u': 'https://preview.redd.it/129bwvzpxjk61.png?width=680&amp;format=png&amp;auto=webp&amp;s=11b9920599013cebe3f9e25d5562f6651a95a07c'}, 'id': '129bwvzpxjk61'}, 'gu2uoxkqxjk61': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 89, 'x': 108, 'u': 'https://preview.redd.it/gu2uoxkqxjk61.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a99500311776c78056793c029a89ac0c70f0f3e1'}, {'y': 178, 'x': 216, 'u': 'https://preview.redd.it/gu2uoxkqxjk61.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=55c8e8670a4a2f19aeed43a2f1c8f3e51b3f9ca1'}, {'y': 264, 'x': 320, 'u': 'https://preview.redd.it/gu2uoxkqxjk61.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=39cd8e90d27c4e2fbfed7233652c9ddd3f324d51'}, {'y': 528, 'x': 640, 'u': 'https://preview.redd.it/gu2uoxkqxjk61.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3a4964c26dac659a2594c1dcc19164d38c29ac9e'}], 's': {'y': 561, 'x': 680, 'u': 'https://preview.redd.it/gu2uoxkqxjk61.jpg?width=680&amp;format=pjpg&amp;auto=webp&amp;s=8df0a006f1cb884993afe79ed3997d0147587e7b'}, 'id': 'gu2uoxkqxjk61'}}, 'name': 't3_lvv2mo', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 116, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'gallery_data': {'items': [{'media_id': '129bwvzpxjk61', 'id': 30788218}, {'media_id': 'gu2uoxkqxjk61', 'id': 30788219}]}, 'link_flair_text': 'Research', 'can_mod_post': False, 'score': 116, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1614692880.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'reddit.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.reddit.com/gallery/lvv2mo', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'lvv2mo', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Wiskkey', 'discussion_type': None, 'num_comments': 22, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/lvv2mo/r_paper_m6_a_chinese_multimodal_pretrainer/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/gallery/lvv2mo', 'subreddit_subscribers': 1930713, 'created_utc': 1614664080.0, 'num_crossposts': 3, 'media': None, 'is_video': False}]",t3_lvv2mo,,,0
634,,LanguageTechnology,,t2_3xwr441a,False,,0,False,Corpus Research Study Advice,[],r/LanguageTechnology,False,6,,0,,False,t3_lvvdgi,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1614694086.0,text,6,,,text,self.linguistics,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lvvdgi,True,,FranciumSenpai,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lvvdgi/corpus_research_study_advice/,all_ads,False,/r/linguistics/comments/lvqp2v/corpus_research_study_advice/,30199,1614665286.0,0,,False,/r/linguistics/comments/lvqp2v/corpus_research_study_advice/,"[{'approved_at_utc': None, 'subreddit': 'linguistics', 'selftext': ""Hi there, I'm currently an univ student studying linguistics. I am doing an independent corpus study on how language use has changed with telecommunication, but it has been difficult finding a data analysis tool since turning the webpages I'm using into HTML documents and then trying to get the text from them is rather difficult and inefficient - it takes far too long and doesn't get most of the text from the sources I use. A post might contain over 13k comments and it'll only grab the first 20.\n\nDoes anyone know of any free tools I can use to aid in my research? I'm really at my wit's end, especially since it's not like I have $1K+ lying around to buy the software I would need when I'd only be using it for about 2 months out of the entire year.\n\nAny suggestions are open, thank you very much."", 'author_fullname': 't2_3xwr441a', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Corpus Research Study Advice', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/linguistics', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_lvqp2v', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1614677673.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.linguistics', 'allow_live_comments': True, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there, I&amp;#39;m currently an univ student studying linguistics. I am doing an independent corpus study on how language use has changed with telecommunication, but it has been difficult finding a data analysis tool since turning the webpages I&amp;#39;m using into HTML documents and then trying to get the text from them is rather difficult and inefficient - it takes far too long and doesn&amp;#39;t get most of the text from the sources I use. A post might contain over 13k comments and it&amp;#39;ll only grab the first 20.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know of any free tools I can use to aid in my research? I&amp;#39;m really at my wit&amp;#39;s end, especially since it&amp;#39;s not like I have $1K+ lying around to buy the software I would need when I&amp;#39;d only be using it for about 2 months out of the entire year.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions are open, thank you very much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2qhos', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'lvqp2v', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'FranciumSenpai', 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/linguistics/comments/lvqp2v/corpus_research_study_advice/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/linguistics/comments/lvqp2v/corpus_research_study_advice/', 'subreddit_subscribers': 249863, 'created_utc': 1614648873.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_lvqp2v,,,0
635,,LanguageTechnology,"Hey guys!

This week, LAMA ([https://lamaai.io](https://lamaai.io)) has a couple of updates. Let's start with this weeks AI news!

You can find the video [here](https://www.lamaai.io/posts/ai-360-01-03-2021-unified-transformer-sebastian-ruder-openais-dall-e-glom-and-studiogan), but as for the key highlights:

* [Facebook AI Research](https://ai.facebook.com/) announce a new multi-modal Transformer architecture, [UniT](https://arxiv.org/abs/2102.10772)
* [Sebastian Ruder updates](https://ruder.io/recent-advances-lm-fine-tuning/) us on the latest advances in language model fine-tuning
* [OpenAI](http://openai.com/) have news about DALL-E
* Geoffrey Hinton [proposes an idea paper](https://arxiv.org/abs/2102.12627) he dubs GLOM
* [StudioGAN](https://github.com/POSTECH-CVLab/PyTorch-StudioGAN) is introduced: A PyTorch library for SoTA GAN models

&amp;#x200B;

Would you like to know how we can use Machine Learning to detect COVID symptoms? Imperial College's **Björn Schuller** is going to be presenting  his recent and topical work on detecting COVID symptoms through the use of Computer Audition (think Computer Vision but for audio instead!). As a little introduction, Björn is a Full Professor at the University of Augsburg in Germany, where he is also Chair of Embedded Intelligence for Health Care and Wellbeing. He is also a Professor of Artificial Intelligence at Imperial College London and heads GLAM (Group for Language, Audio and Music). He has over 1000 publications which feature his name (🤯) and his recent research interests focus on audio and multi-modal approaches to emotion detection. Björn will be discussing his paper: [COVID-19 and Computer Audition](https://arxiv.org/abs/2003.11117) which was written during the outbreak last year. In this paper, he overviews the usage of speech and sound analysis by artificial intelligence/machine learning to detect a presence of COVID. If you're interested in attending the talk, register on the eventbrite: [https://www.eventbrite.com/e/bjorn-schuller-lama-ai-covid-19-and-computer-audition-tickets-143203512561](https://www.eventbrite.com/e/bjorn-schuller-lama-ai-covid-19-and-computer-audition-tickets-143203512561)

&amp;#x200B;

Finally, last week we had a paper presentation on the current state of AI's progress towards Natural Language Understanding. You can find the video/talk [here](https://www.lamaai.io/posts/progress-towards-natural-language-understanding)! As for some key points from the talk:

* (Bender and Koller, 2020) discuss the question whether a system exposed only to the form of language in its training data, can in principle learn its meaning
* They underline their arguments with multiple thought experiments and a comparison to human children language acquisition which is grounded in the real world and in interaction with others
* The NLP research community is called to reflect on the current research trends and to take a more top-down approach by asking “whether the hill we are climbing so rapidly is the right hill”
* (Linzen, 2020) discusses common evaluation practices in NLP research and their limitations
* He proposes a new evaluation paradigm which takes into consideration pre-training corpora of different sizes, as well as normative and efficiency attributes while comparing ML models to each other.",t2_aej9we1h,False,,0,False,"LAMA AI's weekly news, updates, and events.",[],r/LanguageTechnology,False,6,,0,,False,t3_lvnwe4,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1614669964.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys!&lt;/p&gt;

&lt;p&gt;This week, LAMA (&lt;a href=""https://lamaai.io""&gt;https://lamaai.io&lt;/a&gt;) has a couple of updates. Let&amp;#39;s start with this weeks AI news!&lt;/p&gt;

&lt;p&gt;You can find the video &lt;a href=""https://www.lamaai.io/posts/ai-360-01-03-2021-unified-transformer-sebastian-ruder-openais-dall-e-glom-and-studiogan""&gt;here&lt;/a&gt;, but as for the key highlights:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://ai.facebook.com/""&gt;Facebook AI Research&lt;/a&gt; announce a new multi-modal Transformer architecture, &lt;a href=""https://arxiv.org/abs/2102.10772""&gt;UniT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://ruder.io/recent-advances-lm-fine-tuning/""&gt;Sebastian Ruder updates&lt;/a&gt; us on the latest advances in language model fine-tuning&lt;/li&gt;
&lt;li&gt;&lt;a href=""http://openai.com/""&gt;OpenAI&lt;/a&gt; have news about DALL-E&lt;/li&gt;
&lt;li&gt;Geoffrey Hinton &lt;a href=""https://arxiv.org/abs/2102.12627""&gt;proposes an idea paper&lt;/a&gt; he dubs GLOM&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/POSTECH-CVLab/PyTorch-StudioGAN""&gt;StudioGAN&lt;/a&gt; is introduced: A PyTorch library for SoTA GAN models&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Would you like to know how we can use Machine Learning to detect COVID symptoms? Imperial College&amp;#39;s &lt;strong&gt;Björn Schuller&lt;/strong&gt; is going to be presenting  his recent and topical work on detecting COVID symptoms through the use of Computer Audition (think Computer Vision but for audio instead!). As a little introduction, Björn is a Full Professor at the University of Augsburg in Germany, where he is also Chair of Embedded Intelligence for Health Care and Wellbeing. He is also a Professor of Artificial Intelligence at Imperial College London and heads GLAM (Group for Language, Audio and Music). He has over 1000 publications which feature his name (🤯) and his recent research interests focus on audio and multi-modal approaches to emotion detection. Björn will be discussing his paper: &lt;a href=""https://arxiv.org/abs/2003.11117""&gt;COVID-19 and Computer Audition&lt;/a&gt; which was written during the outbreak last year. In this paper, he overviews the usage of speech and sound analysis by artificial intelligence/machine learning to detect a presence of COVID. If you&amp;#39;re interested in attending the talk, register on the eventbrite: &lt;a href=""https://www.eventbrite.com/e/bjorn-schuller-lama-ai-covid-19-and-computer-audition-tickets-143203512561""&gt;https://www.eventbrite.com/e/bjorn-schuller-lama-ai-covid-19-and-computer-audition-tickets-143203512561&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Finally, last week we had a paper presentation on the current state of AI&amp;#39;s progress towards Natural Language Understanding. You can find the video/talk &lt;a href=""https://www.lamaai.io/posts/progress-towards-natural-language-understanding""&gt;here&lt;/a&gt;! As for some key points from the talk:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(Bender and Koller, 2020) discuss the question whether a system exposed only to the form of language in its training data, can in principle learn its meaning&lt;/li&gt;
&lt;li&gt;They underline their arguments with multiple thought experiments and a comparison to human children language acquisition which is grounded in the real world and in interaction with others&lt;/li&gt;
&lt;li&gt;The NLP research community is called to reflect on the current research trends and to take a more top-down approach by asking “whether the hill we are climbing so rapidly is the right hill”&lt;/li&gt;
&lt;li&gt;(Linzen, 2020) discusses common evaluation practices in NLP research and their limitations&lt;/li&gt;
&lt;li&gt;He proposes a new evaluation paradigm which takes into consideration pre-training corpora of different sizes, as well as normative and efficiency attributes while comparing ML models to each other.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lvnwe4,True,,lamaai_io,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lvnwe4/lama_ais_weekly_news_updates_and_events/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lvnwe4/lama_ais_weekly_news_updates_and_events/,30199,1614641164.0,0,,False,,,,,,3317
636,,LanguageTechnology,"Hi, 

I recently got admitted to a good PhD program in the US to work on natural language processing. The advisor is great too. A question to this reddit community - do you think going for a PhD in this domain is worth it? 

Background: Im already working at a great organization where my learning curve is increasing for the last two years. Im getting to work on state of the art things, I get to read papers, implement them, come up with my own architectures etc. My peers are very supportive, and Im assuming this is paving path for great industry opportunities in general. 

However, I applied for a PhD for two reasons: I want to learn more about the domain and stick to an area to get more expertise, understand the theory etc. And eventually I want to be a research lead, for which I think a PhD will provide me with immense credibility. However the idea of starting a PhD at 27, and going back to school and that lifestyle for another five years is very very scary. Im starting to have cold feet. 

Any words of wisdom from someone in the process, or someone who has been through this? 

Thank you so much!",t2_cz9tkqu,False,,0,False,[Need Advice]PhD in NLP @ reputed US institute/Prof. Worth it?,[],r/LanguageTechnology,False,6,,0,,False,t3_lvbkfr,False,dark,1.0,,public,14,0,{},,False,[],,False,False,,{},,False,14,,False,False,,False,,[],{},,True,,1614640257.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, &lt;/p&gt;

&lt;p&gt;I recently got admitted to a good PhD program in the US to work on natural language processing. The advisor is great too. A question to this reddit community - do you think going for a PhD in this domain is worth it? &lt;/p&gt;

&lt;p&gt;Background: Im already working at a great organization where my learning curve is increasing for the last two years. Im getting to work on state of the art things, I get to read papers, implement them, come up with my own architectures etc. My peers are very supportive, and Im assuming this is paving path for great industry opportunities in general. &lt;/p&gt;

&lt;p&gt;However, I applied for a PhD for two reasons: I want to learn more about the domain and stick to an area to get more expertise, understand the theory etc. And eventually I want to be a research lead, for which I think a PhD will provide me with immense credibility. However the idea of starting a PhD at 27, and going back to school and that lifestyle for another five years is very very scary. Im starting to have cold feet. &lt;/p&gt;

&lt;p&gt;Any words of wisdom from someone in the process, or someone who has been through this? &lt;/p&gt;

&lt;p&gt;Thank you so much!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lvbkfr,True,,naboo_random,,21,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lvbkfr/need_advicephd_in_nlp_reputed_us_instituteprof/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lvbkfr/need_advicephd_in_nlp_reputed_us_instituteprof/,30199,1614611457.0,0,,False,,,,,,1114
637,,LanguageTechnology,"My goal is to have a network I can feed large amounts of text and complex questions to receive full length through answers.

Firstly is such a thing even possible with current models?  


Here is an example of what I want to be done

Input: the entire book of animal farm

Question: How does the book animal farm depict the issues with communism.

Full length answer. Animal farm depicts the failure of communism in several ways. The book shows us that people working with equal pay results in laziness among certain individuals and people having less motivation to work harder jobs. ETC ETC

Answer length should be anywhere from sentence s to paragraphs.

Also Id prefer that the model would have a higher bias to include reasoning for the questions.

For example Bob is 20 years old. Is Bob 30 years old?

Answer: No, Bob cannot be 30 years old because he is 20 years old.

This might be ambitious but the goal is something that I could use continuously through high school/college to save a ton of time. Maybe ill have to wait for bigger models like GPT-4 Im not sure but im hoping anyone here can give me some pointers?",t2_am21sk7f,False,,0,False,Any pre-trained longformers for this task?,[],r/LanguageTechnology,False,6,,0,,False,t3_lvjkoo,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1614659181.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My goal is to have a network I can feed large amounts of text and complex questions to receive full length through answers.&lt;/p&gt;

&lt;p&gt;Firstly is such a thing even possible with current models?  &lt;/p&gt;

&lt;p&gt;Here is an example of what I want to be done&lt;/p&gt;

&lt;p&gt;Input: the entire book of animal farm&lt;/p&gt;

&lt;p&gt;Question: How does the book animal farm depict the issues with communism.&lt;/p&gt;

&lt;p&gt;Full length answer. Animal farm depicts the failure of communism in several ways. The book shows us that people working with equal pay results in laziness among certain individuals and people having less motivation to work harder jobs. ETC ETC&lt;/p&gt;

&lt;p&gt;Answer length should be anywhere from sentence s to paragraphs.&lt;/p&gt;

&lt;p&gt;Also Id prefer that the model would have a higher bias to include reasoning for the questions.&lt;/p&gt;

&lt;p&gt;For example Bob is 20 years old. Is Bob 30 years old?&lt;/p&gt;

&lt;p&gt;Answer: No, Bob cannot be 30 years old because he is 20 years old.&lt;/p&gt;

&lt;p&gt;This might be ambitious but the goal is something that I could use continuously through high school/college to save a ton of time. Maybe ill have to wait for bigger models like GPT-4 Im not sure but im hoping anyone here can give me some pointers?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lvjkoo,True,,Horosoros,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lvjkoo/any_pretrained_longformers_for_this_task/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lvjkoo/any_pretrained_longformers_for_this_task/,30199,1614630381.0,0,,False,,,,,,1124
638,,LanguageTechnology,"Is there a website where I can upload a snippet of speech and clone a voice? Just for demo purposes. 

I tried the method in this link but it didn’t work. https://youtu.be/CkZJ1knpZmo

Do you know of a good resource?",t2_8wbdz21a,False,,0,False,Where can I find an online resource of voice cloning?,[],r/LanguageTechnology,False,6,,0,,False,t3_lv4ipg,False,dark,0.89,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1614618364.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there a website where I can upload a snippet of speech and clone a voice? Just for demo purposes. &lt;/p&gt;

&lt;p&gt;I tried the method in this link but it didn’t work. &lt;a href=""https://youtu.be/CkZJ1knpZmo""&gt;https://youtu.be/CkZJ1knpZmo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Do you know of a good resource?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lv4ipg,True,,VoiceTech,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lv4ipg/where_can_i_find_an_online_resource_of_voice/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lv4ipg/where_can_i_find_an_online_resource_of_voice/,30199,1614589564.0,0,,False,,,,,,216
639,,LanguageTechnology,"Hello,

I am a NLP noob so please forgive my naive questions. I have a problem and looking for pointers to find a solution.

I have a list of keywords and have loads of unstructured data. Based on the keywords in my list, I want to extract information like values from the text. For example I have a sentence like 'His age is 15'. and my keyword is 'AGE'. Now I want to extract the age and the number from the text. THe thing is since the text is unstructured, the exact semantics of the sentence would differ in different texts. So I am looking to find/implement some solution that identifies the keyword (Simple enough) and extracts the associated information. I have tried using regexp, they work sometimes and sometimes start getting too complex to extract the info. Any help would be appreciated.

Thanks",t2_nfdoy,False,,0,False,Extracting information from text,[],r/LanguageTechnology,False,6,,0,,False,t3_lv9k9u,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1614635091.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am a NLP noob so please forgive my naive questions. I have a problem and looking for pointers to find a solution.&lt;/p&gt;

&lt;p&gt;I have a list of keywords and have loads of unstructured data. Based on the keywords in my list, I want to extract information like values from the text. For example I have a sentence like &amp;#39;His age is 15&amp;#39;. and my keyword is &amp;#39;AGE&amp;#39;. Now I want to extract the age and the number from the text. THe thing is since the text is unstructured, the exact semantics of the sentence would differ in different texts. So I am looking to find/implement some solution that identifies the keyword (Simple enough) and extracts the associated information. I have tried using regexp, they work sometimes and sometimes start getting too complex to extract the info. Any help would be appreciated.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lv9k9u,True,,ary0007,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lv9k9u/extracting_information_from_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lv9k9u/extracting_information_from_text/,30199,1614606291.0,0,,False,,,,,,809
640,,LanguageTechnology,,t2_5smymm7p,False,,0,False,Into NLP - Fuzzy String Matching and the Edit Distance,[],r/LanguageTechnology,False,6,,0,,False,t3_lv6z3i,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1614627224.0,text,6,,,text,qualicen.de,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lv6z3i,True,,QualicenDS,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lv6z3i/into_nlp_fuzzy_string_matching_and_the_edit/,all_ads,False,https://www.qualicen.de/into-nlp-2-fuzzy-string-matching-and-the-edit-distance/,30199,1614598424.0,0,,False,https://www.qualicen.de/into-nlp-2-fuzzy-string-matching-and-the-edit-distance/,,,,,0
641,,LanguageTechnology,"Hello everyone!

I built an end-to-end tutorial to train an emotion classifier using SqueezeBERT, a state-of-the-art lightweight version of BERT.  
I got +0.9 on test data in a few lines of code.  
I thought I'd share with you my approach and the code so that you can maybe apply it to your projects.

The trained model is also available so that you can use it on your own data without retraining from scratch.

* Here's the code: [https://github.com/ahmedbesbes/multi-label-sentiment-classifier](https://github.com/ahmedbesbes/multi-label-sentiment-classifier)
* and the blog post: [https://medium.com/@ahmedbesbes/training-a-multi-label-emotion-classifier-with-tez-and-pytorch-af04c899a63a](https://medium.com/@ahmedbesbes/training-a-multi-label-emotion-classifier-with-tez-and-pytorch-af04c899a63a)

Please let me know if you have an issue

Feedbacks more than welcome :)

Best!",t2_10lnxu,False,,0,False,Training a Multi-Label Emotion Classifier with Tez and PyTorch to detect +20 different emotions,[],r/LanguageTechnology,False,6,,0,,False,t3_lurnhc,False,dark,1.0,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1614578189.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;

&lt;p&gt;I built an end-to-end tutorial to train an emotion classifier using SqueezeBERT, a state-of-the-art lightweight version of BERT.&lt;br/&gt;
I got +0.9 on test data in a few lines of code.&lt;br/&gt;
I thought I&amp;#39;d share with you my approach and the code so that you can maybe apply it to your projects.&lt;/p&gt;

&lt;p&gt;The trained model is also available so that you can use it on your own data without retraining from scratch.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Here&amp;#39;s the code: &lt;a href=""https://github.com/ahmedbesbes/multi-label-sentiment-classifier""&gt;https://github.com/ahmedbesbes/multi-label-sentiment-classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;and the blog post: &lt;a href=""https://medium.com/@ahmedbesbes/training-a-multi-label-emotion-classifier-with-tez-and-pytorch-af04c899a63a""&gt;https://medium.com/@ahmedbesbes/training-a-multi-label-emotion-classifier-with-tez-and-pytorch-af04c899a63a&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Please let me know if you have an issue&lt;/p&gt;

&lt;p&gt;Feedbacks more than welcome :)&lt;/p&gt;

&lt;p&gt;Best!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lurnhc,True,,ahmedbesbes,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lurnhc/training_a_multilabel_emotion_classifier_with_tez/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lurnhc/training_a_multilabel_emotion_classifier_with_tez/,30199,1614549389.0,0,,False,,,,,,881
642,,LanguageTechnology,,t2_hkv9s,False,,0,False,SummPip: Multi-Document Summarization with Sentence Graph Compression | Research Paper Walkthrough,[],r/LanguageTechnology,False,6,,0,,False,t3_luk23p,False,dark,1.0,,public,20,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/1jwUOMQVCo4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'SummPip: Multi-Document Summarization with Sentence Graph Compression | Research Paper Walkthrough', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/1jwUOMQVCo4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/1jwUOMQVCo4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/1jwUOMQVCo4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/luk23p', 'height': 200}",,False,20,,False,False,,False,,[],{},,False,,1614558794.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,luk23p,True,,prakhar21,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/luk23p/summpip_multidocument_summarization_with_sentence/,all_ads,False,https://youtu.be/1jwUOMQVCo4,30199,1614529994.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'SummPip: Multi-Document Summarization with Sentence Graph Compression | Research Paper Walkthrough', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/1jwUOMQVCo4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/1jwUOMQVCo4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/1jwUOMQVCo4,,,,,0
643,,LanguageTechnology,,t2_5fsp2x6v,False,,0,False,An effective joint sentence and token labelling method for Low-Resource NER | Research Papers Summary 010,[],r/LanguageTechnology,False,6,,0,,False,t3_lunnjs,False,dark,0.88,,public,12,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/HR5qa1ElLeA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Joint Sentence and Token Labelling for NER | Research Papers Summary 010', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/HR5qa1ElLeA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/HR5qa1ElLeA/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/HR5qa1ElLeA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lunnjs', 'height': 200}",,False,12,,False,False,,False,,[],{},,False,,1614567930.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lunnjs,True,,RyanAI100,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lunnjs/an_effective_joint_sentence_and_token_labelling/,all_ads,False,https://youtu.be/HR5qa1ElLeA,30199,1614539130.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Joint Sentence and Token Labelling for NER | Research Papers Summary 010', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/HR5qa1ElLeA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/HR5qa1ElLeA/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,https://youtu.be/HR5qa1ElLeA,,,,,0
644,,LanguageTechnology,"Scroll down to bottom for TLDR.

So ive been working on fine-tuning GPT2 to be capable of reading comprehension using CoQa dataset. It works good but my goal is to create a system that can read an entire book and then answer questions given about the book. The problem is there is too much data being sent into the bot within a single input. It can handle multiple paragraphs quite well and perhaps even an essay but a book is just too much data. There are a few solutions ive thought of such as using reading comprehension to create a pre-generated data set with each paragraph of the book followed by generated questions. However, even using google colab such data generation would take an excruciatingly long time. If there are 400 paragraphs it would take nearly 3 hours to finish the data generation. I also thought I could perhaps remove words like ""You, but or"" etc those kind of conjunctions and pronouns that don't at to the story. But I feel as if its comprehension capabilities may be diminished. Perhaps such a task would not be feasible until I can get my hands on GPT-3?

Anyways ill break down the methods that I have thought of

Method 1: Take the specified book and allow GPT-2 to generate reading comprehension dataset from it. Then feed that data back into the model so that it learns specific things about said book. ETA 4+ hours

Method 2: Take the book and remove conjunctions and pronouns from the text. then do the same as method 1

ETA &lt;2 hours

Method 3: No clue how this would work but perhaps I could just throw the whole book into the training data along with reading comprehension data to see if it might pick up on some of the questions Eta &lt; 1 hour

Method 4: Find a dataset that specifically answers questions from a variety of books and add that to CoQa. Eta &lt; 1 hour + how long to find said dataset.

The end goal is to create a system where I can have an AI answer questions about a book. Which would completely remove any need for me to read said book. Also be just as capable of reading pages from a text book and completing questions. Such a system would greatly enhance my ability to complete school work and free up hours of wasted time on learning about 50 year old books.

TLDR:

Book is too big to feed into GPT-2 reading comprehension model. How do I design/train my model to quickly answer questions about entire books?

My Question to you:

If you are reading, my questions to you are exactly which methods I should approach and is this challenge even feasible using the current GPT-2 model.",t2_am21sk7f,False,,0,False,I have an issue with GPT-2 Reading comprehension for a task.,[],r/LanguageTechnology,False,6,,0,,False,t3_lunlfh,False,dark,0.9,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,1614539380.0,,[],{},,True,,1614567786.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Scroll down to bottom for TLDR.&lt;/p&gt;

&lt;p&gt;So ive been working on fine-tuning GPT2 to be capable of reading comprehension using CoQa dataset. It works good but my goal is to create a system that can read an entire book and then answer questions given about the book. The problem is there is too much data being sent into the bot within a single input. It can handle multiple paragraphs quite well and perhaps even an essay but a book is just too much data. There are a few solutions ive thought of such as using reading comprehension to create a pre-generated data set with each paragraph of the book followed by generated questions. However, even using google colab such data generation would take an excruciatingly long time. If there are 400 paragraphs it would take nearly 3 hours to finish the data generation. I also thought I could perhaps remove words like &amp;quot;You, but or&amp;quot; etc those kind of conjunctions and pronouns that don&amp;#39;t at to the story. But I feel as if its comprehension capabilities may be diminished. Perhaps such a task would not be feasible until I can get my hands on GPT-3?&lt;/p&gt;

&lt;p&gt;Anyways ill break down the methods that I have thought of&lt;/p&gt;

&lt;p&gt;Method 1: Take the specified book and allow GPT-2 to generate reading comprehension dataset from it. Then feed that data back into the model so that it learns specific things about said book. ETA 4+ hours&lt;/p&gt;

&lt;p&gt;Method 2: Take the book and remove conjunctions and pronouns from the text. then do the same as method 1&lt;/p&gt;

&lt;p&gt;ETA &amp;lt;2 hours&lt;/p&gt;

&lt;p&gt;Method 3: No clue how this would work but perhaps I could just throw the whole book into the training data along with reading comprehension data to see if it might pick up on some of the questions Eta &amp;lt; 1 hour&lt;/p&gt;

&lt;p&gt;Method 4: Find a dataset that specifically answers questions from a variety of books and add that to CoQa. Eta &amp;lt; 1 hour + how long to find said dataset.&lt;/p&gt;

&lt;p&gt;The end goal is to create a system where I can have an AI answer questions about a book. Which would completely remove any need for me to read said book. Also be just as capable of reading pages from a text book and completing questions. Such a system would greatly enhance my ability to complete school work and free up hours of wasted time on learning about 50 year old books.&lt;/p&gt;

&lt;p&gt;TLDR:&lt;/p&gt;

&lt;p&gt;Book is too big to feed into GPT-2 reading comprehension model. How do I design/train my model to quickly answer questions about entire books?&lt;/p&gt;

&lt;p&gt;My Question to you:&lt;/p&gt;

&lt;p&gt;If you are reading, my questions to you are exactly which methods I should approach and is this challenge even feasible using the current GPT-2 model.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lunlfh,True,,Horosoros,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lunlfh/i_have_an_issue_with_gpt2_reading_comprehension/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lunlfh/i_have_an_issue_with_gpt2_reading_comprehension/,30199,1614538986.0,0,,False,,,,,,2547
645,,LanguageTechnology,"Hi all, 

Starting to make a switch toward NLP research in my Ph.D. and find I struggle to read many papers which use terminology from linguistics. Does anyone have any book suggestions that might help me better understand grammar / linguistic terminology? I am a native English speaker, I just am not familiar, for example, with Chomsky's surface vs deep structures, what frame semantics are, etc. 

Thanks!",t2_5x92moet,False,,0,False,Struggling With Terminology From Linguistics,[],r/LanguageTechnology,False,6,,0,,False,t3_lujkt5,False,dark,1.0,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1614557523.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, &lt;/p&gt;

&lt;p&gt;Starting to make a switch toward NLP research in my Ph.D. and find I struggle to read many papers which use terminology from linguistics. Does anyone have any book suggestions that might help me better understand grammar / linguistic terminology? I am a native English speaker, I just am not familiar, for example, with Chomsky&amp;#39;s surface vs deep structures, what frame semantics are, etc. &lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lujkt5,True,,Inside_Today4604,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lujkt5/struggling_with_terminology_from_linguistics/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lujkt5/struggling_with_terminology_from_linguistics/,30199,1614528723.0,0,,False,,,,,,408
646,,LanguageTechnology,"Hi everyone. The title is the question. I'm just wondering whether there may be any language models like BERT that were pretrained on domain-specific corpora.

For example, the BioBERT model is known to have been pretrained on biomedical text, and hence achieves better performance than BERT on biomedical NLP tasks.

Would there be anything else along the same lines? Thanks.",t2_m8kccne,False,,0,False,Does anyone know of any domain-specific pretrained language models?,[],r/LanguageTechnology,False,6,,0,,False,t3_luvxbh,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1614589747.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone. The title is the question. I&amp;#39;m just wondering whether there may be any language models like BERT that were pretrained on domain-specific corpora.&lt;/p&gt;

&lt;p&gt;For example, the BioBERT model is known to have been pretrained on biomedical text, and hence achieves better performance than BERT on biomedical NLP tasks.&lt;/p&gt;

&lt;p&gt;Would there be anything else along the same lines? Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,luvxbh,True,,Seankala,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/luvxbh/does_anyone_know_of_any_domainspecific_pretrained/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/luvxbh/does_anyone_know_of_any_domainspecific_pretrained/,30199,1614560947.0,0,,False,,,,,,376
647,,LanguageTechnology,"Early days of research in Natural language processing established long-term dependencies. It also brought the vanishing gradient problem in front of us because nascent models were handling the input sequences one by one, without parallelization. Recently, revolutionary transformer-based architectures and the self-attention mechanisms have enabled token pairs’ interactions across complete sequences resulting in modeling arbitrary dependencies in a constant number of layers. The above helped achieve state-of-the-art performance across many Natural language processing tasks.

However, these advantages came to a high-cost barrier because the transformer-based networks’ memory and computational requirements grow quadratically with sequence length. The above results in significant efficiency bottlenecks when dealing with long sequences. Researchers from the University of Wisconsin-Madison, American Family Insurance, UC Berkeley, and Google Brain propose Nyströmformer. Nyströmformer is an O(n) approximation in both memory and time for self-attention. The above is designed to rescue us from the[ quadratic cost](https://syncedreview.com/2020/07/31/applying-linearly-scalable-transformers-to-model-longer-protein-sequences/) associated with the long input sequences

Paper Summary: https://www.marktechpost.com/2021/02/27/university-of-wisconsin-madison-uc-berkeley-and-google-brain-introduce-nystromformer-a-nystrom-based-algorithm-for-approximating-self-attention/

Paper: https://arxiv.org/pdf/2102.03902.pdf

Github: https://github.com/mlpen/Nystromformer",t2_2wsvqwhg,False,,0,False,"University of Wisconsin-Madison, UC Berkeley, and Google Brain Introduce Nystromformer: A Nystrom-based Algorithm for Approximating Self-Attention",[],r/LanguageTechnology,False,6,,0,,False,t3_lu37h4,False,dark,0.97,,public,31,0,{},,False,[],,False,False,,{},,False,31,,False,False,,False,,[],{},,True,,1614507975.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Early days of research in Natural language processing established long-term dependencies. It also brought the vanishing gradient problem in front of us because nascent models were handling the input sequences one by one, without parallelization. Recently, revolutionary transformer-based architectures and the self-attention mechanisms have enabled token pairs’ interactions across complete sequences resulting in modeling arbitrary dependencies in a constant number of layers. The above helped achieve state-of-the-art performance across many Natural language processing tasks.&lt;/p&gt;

&lt;p&gt;However, these advantages came to a high-cost barrier because the transformer-based networks’ memory and computational requirements grow quadratically with sequence length. The above results in significant efficiency bottlenecks when dealing with long sequences. Researchers from the University of Wisconsin-Madison, American Family Insurance, UC Berkeley, and Google Brain propose Nyströmformer. Nyströmformer is an O(n) approximation in both memory and time for self-attention. The above is designed to rescue us from the&lt;a href=""https://syncedreview.com/2020/07/31/applying-linearly-scalable-transformers-to-model-longer-protein-sequences/""&gt; quadratic cost&lt;/a&gt; associated with the long input sequences&lt;/p&gt;

&lt;p&gt;Paper Summary: &lt;a href=""https://www.marktechpost.com/2021/02/27/university-of-wisconsin-madison-uc-berkeley-and-google-brain-introduce-nystromformer-a-nystrom-based-algorithm-for-approximating-self-attention/""&gt;https://www.marktechpost.com/2021/02/27/university-of-wisconsin-madison-uc-berkeley-and-google-brain-introduce-nystromformer-a-nystrom-based-algorithm-for-approximating-self-attention/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/pdf/2102.03902.pdf""&gt;https://arxiv.org/pdf/2102.03902.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Github: &lt;a href=""https://github.com/mlpen/Nystromformer""&gt;https://github.com/mlpen/Nystromformer&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lu37h4,True,,ai-lover,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lu37h4/university_of_wisconsinmadison_uc_berkeley_and/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lu37h4/university_of_wisconsinmadison_uc_berkeley_and/,30199,1614479175.0,0,,False,,,,,,1567
648,,LanguageTechnology,"like this: 

[https://towardsdatascience.com/data-to-text-generation-with-t5-building-a-simple-yet-advanced-nlg-model-b5cce5a6df45](https://towardsdatascience.com/data-to-text-generation-with-t5-building-a-simple-yet-advanced-nlg-model-b5cce5a6df45) 

Sample Input: ""Teacher"", ""Great"".

Output: ""He is a great teacher and everyone needs to learn from him""

&amp;#x200B;

I want to do this for a university project and  I don't have a lot of time. do you think this is a good idea or should I change the topic. 

I have a dataset of sample outputs. I am new to this. Any help is appreciated thanks.",t2_6fwexyi9,False,,0,False,How to generate sentences from a set of keywords?,[],r/LanguageTechnology,False,6,,0,,False,t3_luam24,False,dark,0.82,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1614528529.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;like this: &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://towardsdatascience.com/data-to-text-generation-with-t5-building-a-simple-yet-advanced-nlg-model-b5cce5a6df45""&gt;https://towardsdatascience.com/data-to-text-generation-with-t5-building-a-simple-yet-advanced-nlg-model-b5cce5a6df45&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Sample Input: &amp;quot;Teacher&amp;quot;, &amp;quot;Great&amp;quot;.&lt;/p&gt;

&lt;p&gt;Output: &amp;quot;He is a great teacher and everyone needs to learn from him&amp;quot;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I want to do this for a university project and  I don&amp;#39;t have a lot of time. do you think this is a good idea or should I change the topic. &lt;/p&gt;

&lt;p&gt;I have a dataset of sample outputs. I am new to this. Any help is appreciated thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,luam24,True,,roymustang261,,10,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/luam24/how_to_generate_sentences_from_a_set_of_keywords/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/luam24/how_to_generate_sentences_from_a_set_of_keywords/,30199,1614499729.0,0,,False,,,,,,597
649,,LanguageTechnology,"The github description of Apache Superset is:

&gt; Apache Superset is a Data Visualization and Data Exploration Platform 

ref: https://github.com/apache/superset

The data I am looking at is semi-structured text (html), and I can reach to some structured data (knowledge base) related to the texts.

Questions:

A) Do you use Superset for exploring textual data? How?

B) Is there another tool similar to Superset that is specifically crafted to visualize / explore / analyze text such a html?

C) If there is no existing library / app / api / service, what would be the bare minimal features you would need in such tool?",t2_9dhmh,False,,0,False,Apache Superset for textual data ?,[],r/LanguageTechnology,False,6,,0,,False,t3_lucpm8,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1614536853.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The github description of Apache Superset is:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Apache Superset is a Data Visualization and Data Exploration Platform &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ref: &lt;a href=""https://github.com/apache/superset""&gt;https://github.com/apache/superset&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The data I am looking at is semi-structured text (html), and I can reach to some structured data (knowledge base) related to the texts.&lt;/p&gt;

&lt;p&gt;Questions:&lt;/p&gt;

&lt;p&gt;A) Do you use Superset for exploring textual data? How?&lt;/p&gt;

&lt;p&gt;B) Is there another tool similar to Superset that is specifically crafted to visualize / explore / analyze text such a html?&lt;/p&gt;

&lt;p&gt;C) If there is no existing library / app / api / service, what would be the bare minimal features you would need in such tool?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lucpm8,True,,amirouche,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lucpm8/apache_superset_for_textual_data/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lucpm8/apache_superset_for_textual_data/,30199,1614508053.0,0,,False,,,,,,623
650,,LanguageTechnology,"So...yeah this has really been on the back of my mind and it may be more appropriate for r/TooAfraidToAsk but yeah...

I'm working on a relation extraction research project, and I've noticed that almost every single paper that I read is written by something related to China (school, institution, author, etc.).

Is there some sort of reason why this might be? Or have I just not read enough papers?",t2_m8kccne,False,,0,False,"Is it just me, or are the majority of research papers dealing with relation extraction written by Chinese institutions?",[],r/LanguageTechnology,False,6,,0,,False,t3_lugq47,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1614549591.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So...yeah this has really been on the back of my mind and it may be more appropriate for &lt;a href=""/r/TooAfraidToAsk""&gt;r/TooAfraidToAsk&lt;/a&gt; but yeah...&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working on a relation extraction research project, and I&amp;#39;ve noticed that almost every single paper that I read is written by something related to China (school, institution, author, etc.).&lt;/p&gt;

&lt;p&gt;Is there some sort of reason why this might be? Or have I just not read enough papers?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lugq47,True,,Seankala,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lugq47/is_it_just_me_or_are_the_majority_of_research/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lugq47/is_it_just_me_or_are_the_majority_of_research/,30199,1614520791.0,0,,False,,,,,,399
651,,LanguageTechnology,"Hi,

I am currently working on a topic modelling project with gensim and i have some issues, regarding the tf\_idf vectors for adding a new document.

&amp;#x200B;

The preprocessed and tokenized data is stored in a pandas dataframe (using spaCy). I splitted the data into 2 parts train and test. (2 seperate dataframes). 

Gensim requires to first create a dictionary. The dictionary is expandable, so i first created it with only the train data and afterwards expand it with the test data, if there are possibly some new tokens.

Afterward the corpus is created ( which are basically just BOW vectors). The tf-idf vectorizer is created on top of this corpus and then applyed on the corpus:

""""""

dataset = corpora.Dictionary()

dataset.add\_documents(dataframe\['tokens'\].tolist())

corpus = \[dataset.doc2bow(text) for text in dataframe\['tokens'\].tolist()\]

tfidf\_vectorizer = TfidfModel(corpus)

tfidf = tfidf\_vectorizer\[corpus\]

""""""

&amp;#x200B;

Question 1: For applying the model to a new document, i would need the tfidf vectors of the new document. Do i have to just add the document to the old corpus, create a new tfidf\_vectorizer with that corpus and then just apply the vectorizer to only the new document? Or am i missing something?

Question 2: As i saw in the documentation, the model is applied to a new document with the following code( i am using LsiModel):

""""""

model = LsiModel(tfidf\_vect\[corpus\], id2word=dataset)

vector = model\[tfidf\_vect\[new\_corpus\]

""""""

Print(vector) prints :  &lt;gensim.interfaces.TransformedCorpus at 0x245468431&gt;. 

How can i visualize / just print the resulting vector ( i guess it should be the topic distribution of the new document)? Or am i doing something wrong here ?",t2_5eyplvnc,False,,0,False,Applying gensim topic model on new document,[],r/LanguageTechnology,False,6,,0,,False,t3_lufpot,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1614546674.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I am currently working on a topic modelling project with gensim and i have some issues, regarding the tf_idf vectors for adding a new document.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The preprocessed and tokenized data is stored in a pandas dataframe (using spaCy). I splitted the data into 2 parts train and test. (2 seperate dataframes). &lt;/p&gt;

&lt;p&gt;Gensim requires to first create a dictionary. The dictionary is expandable, so i first created it with only the train data and afterwards expand it with the test data, if there are possibly some new tokens.&lt;/p&gt;

&lt;p&gt;Afterward the corpus is created ( which are basically just BOW vectors). The tf-idf vectorizer is created on top of this corpus and then applyed on the corpus:&lt;/p&gt;

&lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;

&lt;p&gt;dataset = corpora.Dictionary()&lt;/p&gt;

&lt;p&gt;dataset.add_documents(dataframe[&amp;#39;tokens&amp;#39;].tolist())&lt;/p&gt;

&lt;p&gt;corpus = [dataset.doc2bow(text) for text in dataframe[&amp;#39;tokens&amp;#39;].tolist()]&lt;/p&gt;

&lt;p&gt;tfidf_vectorizer = TfidfModel(corpus)&lt;/p&gt;

&lt;p&gt;tfidf = tfidf_vectorizer[corpus]&lt;/p&gt;

&lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Question 1: For applying the model to a new document, i would need the tfidf vectors of the new document. Do i have to just add the document to the old corpus, create a new tfidf_vectorizer with that corpus and then just apply the vectorizer to only the new document? Or am i missing something?&lt;/p&gt;

&lt;p&gt;Question 2: As i saw in the documentation, the model is applied to a new document with the following code( i am using LsiModel):&lt;/p&gt;

&lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;

&lt;p&gt;model = LsiModel(tfidf_vect[corpus], id2word=dataset)&lt;/p&gt;

&lt;p&gt;vector = model[tfidf_vect[new_corpus]&lt;/p&gt;

&lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;

&lt;p&gt;Print(vector) prints :  &amp;lt;gensim.interfaces.TransformedCorpus at 0x245468431&amp;gt;. &lt;/p&gt;

&lt;p&gt;How can i visualize / just print the resulting vector ( i guess it should be the topic distribution of the new document)? Or am i doing something wrong here ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lufpot,True,,p-dog1,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lufpot/applying_gensim_topic_model_on_new_document/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lufpot/applying_gensim_topic_model_on_new_document/,30199,1614517874.0,0,,False,,,,,,1744
652,,LanguageTechnology,,t2_5alofkdd,False,,0,False,Paper: Investigating the Limitations of the Transformers with Simple Arithmetic Tasks,[],r/LanguageTechnology,False,6,,0,,False,t3_lu1m4b,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,False,,1614502576.0,text,6,,,text,arxiv.org,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lu1m4b,True,,Wiskkey,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lu1m4b/paper_investigating_the_limitations_of_the/,all_ads,False,https://arxiv.org/abs/2102.13019,30199,1614473776.0,1,,False,https://arxiv.org/abs/2102.13019,,,,,0
653,,LanguageTechnology,"Hi everyone! 

I come from a language/linguistics background. I got my BA in Spanish with a minor in Linguistics. I’ve taken classes like Syntax, phonetics, phonology, language technology, Hispanic linguistics, and “understanding machine learning.”

I’m looking to apply to masters in Voice Tech, comp ling or NLP this year and I would like to boost the technical side of my knowledge and CV so that the universities see that I could be a good fit for their programs. 

I’ve already completed two courses on Python on Coursera and Codecademy, and I trained a language detection model using naive bayes to distinguish between Spanish and Portuguese texts. I am really eager to keep learning, I just don’t know where to start or what direction to go in.

Are there any online courses or portfolio projects you would recommend to show admissions that I have some background knowledge? Unfortunately I can’t re-enroll at my university at the moment as they are already half way through the semester and it would cost thousands. 

Thanks in advance!",,False,,0,False,Best MOOCs/Projects for MS in CL/NLP (Linguistics Background),[],r/LanguageTechnology,False,6,,0,,False,t3_ltqyv2,False,dark,1.0,,public,15,0,{},,False,[],,False,False,,{},,False,15,,False,,,1614445759.0,,,{},,True,,1614470163.0,text,6,,,,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone! &lt;/p&gt;

&lt;p&gt;I come from a language/linguistics background. I got my BA in Spanish with a minor in Linguistics. I’ve taken classes like Syntax, phonetics, phonology, language technology, Hispanic linguistics, and “understanding machine learning.”&lt;/p&gt;

&lt;p&gt;I’m looking to apply to masters in Voice Tech, comp ling or NLP this year and I would like to boost the technical side of my knowledge and CV so that the universities see that I could be a good fit for their programs. &lt;/p&gt;

&lt;p&gt;I’ve already completed two courses on Python on Coursera and Codecademy, and I trained a language detection model using naive bayes to distinguish between Spanish and Portuguese texts. I am really eager to keep learning, I just don’t know where to start or what direction to go in.&lt;/p&gt;

&lt;p&gt;Are there any online courses or portfolio projects you would recommend to show admissions that I have some background knowledge? Unfortunately I can’t re-enroll at my university at the moment as they are already half way through the semester and it would cost thousands. &lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ltqyv2,True,,[deleted],,5,True,all_ads,False,[],,dark,/r/LanguageTechnology/comments/ltqyv2/best_moocsprojects_for_ms_in_clnlp_linguistics/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ltqyv2/best_moocsprojects_for_ms_in_clnlp_linguistics/,30199,1614441363.0,0,,False,,,,,,1044
654,,LanguageTechnology,"There is a new 1-year Master’s program at the University of Groningen (the Netherlands) dedicated exclusively to voice/speech tech. Focus is on speech recognition and voice synthesis. 

This is an interdisciplinary program for students from CS, AI,  Linguistics, or similar. 

https://youtu.be/297BY6uTHB8
https://www.rug.nl/masters/voice-technology/",t2_8wbdz21a,False,,0,False,New Masters program in Voice and Speech Tech (in Europe),[],r/LanguageTechnology,False,6,,0,,False,t3_ltxrhd,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1614490125.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;There is a new 1-year Master’s program at the University of Groningen (the Netherlands) dedicated exclusively to voice/speech tech. Focus is on speech recognition and voice synthesis. &lt;/p&gt;

&lt;p&gt;This is an interdisciplinary program for students from CS, AI,  Linguistics, or similar. &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://youtu.be/297BY6uTHB8""&gt;https://youtu.be/297BY6uTHB8&lt;/a&gt;
&lt;a href=""https://www.rug.nl/masters/voice-technology/""&gt;https://www.rug.nl/masters/voice-technology/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ltxrhd,True,,VoiceTech,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ltxrhd/new_masters_program_in_voice_and_speech_tech_in/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ltxrhd/new_masters_program_in_voice_and_speech_tech_in/,30199,1614461325.0,0,,False,,,,,,350
655,,LanguageTechnology,"Hi everyone!  


I am trying to find a solution for following problem:  
I have a large data frame where documents are labeled with &gt;400 different category names.   
I built a simple ""data explorer"" that takes a free-text query as user input and runs fuzzywuzzy string matching over those category names.

For example, user searches for ""climate change"" and the fuzzy string matching returns the top 10 most similar category names - let's say ""climate\_change\_mitigation"", ""climate\_change\_programme"", etc...Then the user can select all the desired categories and the data-frame is filtered accordingly. So this is quite simple but works well.

Now, I would like to also take some contextual information into account. For example, when user searches for ""plastic"" the fuzzy string matching, of course, only matches with category names that contain the string ""plastic"" or very similar. However, I would like to also obtain categories such as ""Clean Oceans"", ""Marine Ecosystem"", etc.

I was thinking of training a basic word/doc2vec model on the documents, but I am not sure how to use it to match similarity between a string and another lists of strings (the category names), also perhaps this could help in some cases but usually the simple fuzzy string matching would outperform it still I suppose.

Any ideas?

Thanks!",t2_10qlg0,False,,0,False,Contextual matching of category names,[],r/LanguageTechnology,False,6,,0,,False,t3_ltjqjk,False,dark,1.0,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1614443283.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone!  &lt;/p&gt;

&lt;p&gt;I am trying to find a solution for following problem:&lt;br/&gt;
I have a large data frame where documents are labeled with &amp;gt;400 different category names.&lt;br/&gt;
I built a simple &amp;quot;data explorer&amp;quot; that takes a free-text query as user input and runs fuzzywuzzy string matching over those category names.&lt;/p&gt;

&lt;p&gt;For example, user searches for &amp;quot;climate change&amp;quot; and the fuzzy string matching returns the top 10 most similar category names - let&amp;#39;s say &amp;quot;climate_change_mitigation&amp;quot;, &amp;quot;climate_change_programme&amp;quot;, etc...Then the user can select all the desired categories and the data-frame is filtered accordingly. So this is quite simple but works well.&lt;/p&gt;

&lt;p&gt;Now, I would like to also take some contextual information into account. For example, when user searches for &amp;quot;plastic&amp;quot; the fuzzy string matching, of course, only matches with category names that contain the string &amp;quot;plastic&amp;quot; or very similar. However, I would like to also obtain categories such as &amp;quot;Clean Oceans&amp;quot;, &amp;quot;Marine Ecosystem&amp;quot;, etc.&lt;/p&gt;

&lt;p&gt;I was thinking of training a basic word/doc2vec model on the documents, but I am not sure how to use it to match similarity between a string and another lists of strings (the category names), also perhaps this could help in some cases but usually the simple fuzzy string matching would outperform it still I suppose.&lt;/p&gt;

&lt;p&gt;Any ideas?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ltjqjk,True,,xsliartII,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ltjqjk/contextual_matching_of_category_names/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ltjqjk/contextual_matching_of_category_names/,30199,1614414483.0,0,,False,,,,,,1326
656,,LanguageTechnology,"Hey! I have a bunch of questions for a project of mine.

1. For word embeddings, do I need to perform other similarity measures like Tf-Idf? Cause I read somewhere about BoW and Skip-Gram methods used for creating embeddings. 


2. What do we do with word embeddings? Like once we get the word vectors, what is the next step in the process? I'm trying to make a sentiment analysis project with LSTM and word embeddings but I'm not sure how to use the word embeddings. 


3. Are embeddings, features? If not, how do I extract features from data? 

I'm a noob so I'll probably be asking more questions here.",t2_5xndvr4d,False,,0,False,Some questions regarding word embeddings,[],r/LanguageTechnology,False,6,,0,,False,t3_ltgppq,False,dark,0.91,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,1614402471.0,,[],{},,True,,1614430744.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey! I have a bunch of questions for a project of mine.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;For word embeddings, do I need to perform other similarity measures like Tf-Idf? Cause I read somewhere about BoW and Skip-Gram methods used for creating embeddings. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What do we do with word embeddings? Like once we get the word vectors, what is the next step in the process? I&amp;#39;m trying to make a sentiment analysis project with LSTM and word embeddings but I&amp;#39;m not sure how to use the word embeddings. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Are embeddings, features? If not, how do I extract features from data? &lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I&amp;#39;m a noob so I&amp;#39;ll probably be asking more questions here.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ltgppq,True,,Anup_Kodlekere,,11,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ltgppq/some_questions_regarding_word_embeddings/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ltgppq/some_questions_regarding_word_embeddings/,30199,1614401944.0,0,,False,,,,,,605
657,,LanguageTechnology,"A team of researchers at UC Berkeley, University of Maryland, and UC Irvine conducted a study to identify that can cause instability in the GPT-3 language model. The team proposes a contextual calibration procedure that consistently improves GPT-3 accuracy across diverse prompt format choices and examples.

GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. Large language models have significantly improved their few-shot performance, with top models like GPT-3. Few-shot learning allows users to prototype NLP models swiftly. It enables non-technical users to create NLP systems and efficiently reuse models reducing system memory and complexity.

Paper Summary: [https://www.marktechpost.com/2021/02/26/researchers-from-uc-berkeley-university-of-maryland-and-uc-irvine-introduce-a-new-contextual-calibration-approach-that-significantly-improves-gpt-3-accuracy-up-to-30/](https://www.marktechpost.com/2021/02/26/researchers-from-uc-berkeley-university-of-maryland-and-uc-irvine-introduce-a-new-contextual-calibration-approach-that-significantly-improves-gpt-3-accuracy-up-to-30/) 

Paper Source: [https://arxiv.org/pdf/2102.09690.pdf](https://arxiv.org/pdf/2102.09690.pdf) 

Github: [https://github.com/tonyzhaozh/few-shot-learning](https://github.com/tonyzhaozh/few-shot-learning)",t2_4wudjgid,False,,0,False,"Researchers From UC Berkeley, University of Maryland, and UC Irvine Introduce A new Contextual Calibration Approach That Significantly Improves GPT-3 Accuracy Up to 30%",[],r/LanguageTechnology,False,6,,0,,False,t3_lt0z99,False,dark,0.98,,public,28,0,{},,False,[],,False,False,,{},,False,28,,False,False,,False,,[],{},,True,,1614384861.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;A team of researchers at UC Berkeley, University of Maryland, and UC Irvine conducted a study to identify that can cause instability in the GPT-3 language model. The team proposes a contextual calibration procedure that consistently improves GPT-3 accuracy across diverse prompt format choices and examples.&lt;/p&gt;

&lt;p&gt;GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. Large language models have significantly improved their few-shot performance, with top models like GPT-3. Few-shot learning allows users to prototype NLP models swiftly. It enables non-technical users to create NLP systems and efficiently reuse models reducing system memory and complexity.&lt;/p&gt;

&lt;p&gt;Paper Summary: &lt;a href=""https://www.marktechpost.com/2021/02/26/researchers-from-uc-berkeley-university-of-maryland-and-uc-irvine-introduce-a-new-contextual-calibration-approach-that-significantly-improves-gpt-3-accuracy-up-to-30/""&gt;https://www.marktechpost.com/2021/02/26/researchers-from-uc-berkeley-university-of-maryland-and-uc-irvine-introduce-a-new-contextual-calibration-approach-that-significantly-improves-gpt-3-accuracy-up-to-30/&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Paper Source: &lt;a href=""https://arxiv.org/pdf/2102.09690.pdf""&gt;https://arxiv.org/pdf/2102.09690.pdf&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Github: &lt;a href=""https://github.com/tonyzhaozh/few-shot-learning""&gt;https://github.com/tonyzhaozh/few-shot-learning&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lt0z99,True,,techsucker,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lt0z99/researchers_from_uc_berkeley_university_of/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lt0z99/researchers_from_uc_berkeley_university_of/,30199,1614356061.0,0,,False,,,,,,1346
658,,LanguageTechnology,"Hi.

I have a text multi-class text classification problem, in which I'm trying to classify different subreddits' comments using a very simple TFIDF + PCA + SVM pipeline. What I'm really keen to know is that how different keywords in each class contribute to this classification problem. How can I do this? I have around 10 classes each having 5000 comments with 30 words in average.",t2_kpq8t,False,,0,False,How to extract keywords important to a text classification problem?,[],r/LanguageTechnology,False,6,,0,,False,t3_lt5k5m,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1614396224.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi.&lt;/p&gt;

&lt;p&gt;I have a text multi-class text classification problem, in which I&amp;#39;m trying to classify different subreddits&amp;#39; comments using a very simple TFIDF + PCA + SVM pipeline. What I&amp;#39;m really keen to know is that how different keywords in each class contribute to this classification problem. How can I do this? I have around 10 classes each having 5000 comments with 30 words in average.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lt5k5m,True,,quit_daedalus,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lt5k5m/how_to_extract_keywords_important_to_a_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lt5k5m/how_to_extract_keywords_important_to_a_text/,30199,1614367424.0,0,,False,,,,,,383
659,,LanguageTechnology,,t2_2zyh29i,False,,0,False,Intro to AI Bias,[],r/LanguageTechnology,False,6,,0,,False,t3_lt9p6b,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,False,,1614407623.0,text,6,,,text,beluis3d.medium.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lt9p6b,True,,beluis3d,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lt9p6b/intro_to_ai_bias/,all_ads,False,https://beluis3d.medium.com/introduction-to-bias-in-ai-5058429ba0e,30199,1614378823.0,0,,False,https://beluis3d.medium.com/introduction-to-bias-in-ai-5058429ba0e,,,,,0
660,,LanguageTechnology,"We're building an ML-(graph)-based tool for file &amp; knowledge management and are just onboarding our first alpha &amp; beta users 🙆🏼‍♀️ !

(Just building our little community here [r/reasonal](https://www.reddit.com/r/reasonal/))

You probably know how difficult it is to keep track of knowledge snippets, files, and links. **In fact,** [**30%**](https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/the-social-economy) **of the average ""knowledge worker's"" time is spent searching and locating files, as well as communicating &amp; collaborating internally 🤯**

We are happy to finally bring some ""intelligence"" into our work tools and combine it with an intuitive UX. If you would like to see how our tool can bring structure, transparency, and insights to your work content across your Google Drive, Dropbox, Slack, MS Teams, and browser bookmarks, add yourself to the waitlist and get early access to our beta version: ➡️ [https://reason.al](https://reason.al/?utm_source=reddit.com&amp;utm_medium=referral&amp;utm_campaign=WL-comm-rd-003-LT)

❗ If you fill in the survey and are onboarded on the closed beta, you'll receive 5 premium accounts for a lifetime (to be used by yourself or given to your friends 😻). The premium account gets all features and integrations and includes one workspace with up to 10 members 👍🏼

💡 In a nutshell, the underlying ML surfaces the right content among work files or highlights outdated/unreliable/faulty files before you open them. It is based on (combined 😇 ) more than 10 years of our founders' [/u/btabibian](https://www.reddit.com/u/btabibian/) [/u/musically\_ut](https://www.reddit.com/u/musically_ut/) research:

[https://dl.acm.org/doi/abs/10.1145/3038912.3052672](https://dl.acm.org/doi/abs/10.1145/3038912.3052672)

[https://arxiv.org/abs/1905.05305](https://arxiv.org/abs/1905.05305)

[http://proceedings.mlr.press/v124/tabibian20a](http://proceedings.mlr.press/v124/tabibian20a)

[https://www.pnas.org/content/116/10/3988.short](https://www.pnas.org/content/116/10/3988.short)

[https://dl.acm.org/doi/abs/10.1145/3018661.3018685](https://dl.acm.org/doi/abs/10.1145/3018661.3018685)

[https://dl.acm.org/doi/abs/10.1145/2939672.2939875](https://dl.acm.org/doi/abs/10.1145/2939672.2939875)

[https://arxiv.org/abs/1805.09360](https://arxiv.org/abs/1805.09360)",t2_909vsfqe,False,,0,False,Bringing deep learning into our work tools: Waitlist open &amp; Feedback welcome!,[],r/LanguageTechnology,False,6,,0,,False,t3_lsuhqw,False,dark,0.83,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1614362207.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We&amp;#39;re building an ML-(graph)-based tool for file &amp;amp; knowledge management and are just onboarding our first alpha &amp;amp; beta users 🙆🏼‍♀️ !&lt;/p&gt;

&lt;p&gt;(Just building our little community here &lt;a href=""https://www.reddit.com/r/reasonal/""&gt;r/reasonal&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;You probably know how difficult it is to keep track of knowledge snippets, files, and links. &lt;strong&gt;In fact,&lt;/strong&gt; &lt;a href=""https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/the-social-economy""&gt;&lt;strong&gt;30%&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;of the average &amp;quot;knowledge worker&amp;#39;s&amp;quot; time is spent searching and locating files, as well as communicating &amp;amp; collaborating internally 🤯&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We are happy to finally bring some &amp;quot;intelligence&amp;quot; into our work tools and combine it with an intuitive UX. If you would like to see how our tool can bring structure, transparency, and insights to your work content across your Google Drive, Dropbox, Slack, MS Teams, and browser bookmarks, add yourself to the waitlist and get early access to our beta version: ➡️ &lt;a href=""https://reason.al/?utm_source=reddit.com&amp;amp;utm_medium=referral&amp;amp;utm_campaign=WL-comm-rd-003-LT""&gt;https://reason.al&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;❗ If you fill in the survey and are onboarded on the closed beta, you&amp;#39;ll receive 5 premium accounts for a lifetime (to be used by yourself or given to your friends 😻). The premium account gets all features and integrations and includes one workspace with up to 10 members 👍🏼&lt;/p&gt;

&lt;p&gt;💡 In a nutshell, the underlying ML surfaces the right content among work files or highlights outdated/unreliable/faulty files before you open them. It is based on (combined 😇 ) more than 10 years of our founders&amp;#39; &lt;a href=""https://www.reddit.com/u/btabibian/""&gt;/u/btabibian&lt;/a&gt; &lt;a href=""https://www.reddit.com/u/musically_ut/""&gt;/u/musically_ut&lt;/a&gt; research:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://dl.acm.org/doi/abs/10.1145/3038912.3052672""&gt;https://dl.acm.org/doi/abs/10.1145/3038912.3052672&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://arxiv.org/abs/1905.05305""&gt;https://arxiv.org/abs/1905.05305&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""http://proceedings.mlr.press/v124/tabibian20a""&gt;http://proceedings.mlr.press/v124/tabibian20a&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.pnas.org/content/116/10/3988.short""&gt;https://www.pnas.org/content/116/10/3988.short&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://dl.acm.org/doi/abs/10.1145/3018661.3018685""&gt;https://dl.acm.org/doi/abs/10.1145/3018661.3018685&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://dl.acm.org/doi/abs/10.1145/2939672.2939875""&gt;https://dl.acm.org/doi/abs/10.1145/2939672.2939875&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://arxiv.org/abs/1805.09360""&gt;https://arxiv.org/abs/1805.09360&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lsuhqw,True,,natalieberlin,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lsuhqw/bringing_deep_learning_into_our_work_tools/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lsuhqw/bringing_deep_learning_into_our_work_tools/,30199,1614333407.0,0,,False,,,,,,2348
661,,LanguageTechnology,"Hello everyone,

  
I wrote an article on Information Retrieval and implemented a basic search engine in Rust.

Feedback is much appreciated :)

[https://luc-sydney-georges.medium.com/how-to-never-lose-your-stuff-again-pt-1-b06a9d4e0040](https://luc-sydney-georges.medium.com/how-to-never-lose-your-stuff-again-pt-1-b06a9d4e0040)",t2_xjybvkc,False,,0,False,Information Retrieval in Rust,[],r/LanguageTechnology,False,6,,0,,False,t3_lswr6g,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1614371368.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;

&lt;p&gt;I wrote an article on Information Retrieval and implemented a basic search engine in Rust.&lt;/p&gt;

&lt;p&gt;Feedback is much appreciated :)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://luc-sydney-georges.medium.com/how-to-never-lose-your-stuff-again-pt-1-b06a9d4e0040""&gt;https://luc-sydney-georges.medium.com/how-to-never-lose-your-stuff-again-pt-1-b06a9d4e0040&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lswr6g,True,,McPotates,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lswr6g/information_retrieval_in_rust/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lswr6g/information_retrieval_in_rust/,30199,1614342568.0,0,,False,,,,,,329
662,,LanguageTechnology,"Hi,

I have fined tuned a based-based classification model. I want to understand why model predicting a certain tag. Are there any tools for checking this behavior or study  what's going on inside the hood.",t2_xf374,False,,0,False,Debugging Transformer based classification model for its behavior,[],r/LanguageTechnology,False,6,,0,,False,t3_lsx7te,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1614373220.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I have fined tuned a based-based classification model. I want to understand why model predicting a certain tag. Are there any tools for checking this behavior or study  what&amp;#39;s going on inside the hood.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lsx7te,True,,thak123,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lsx7te/debugging_transformer_based_classification_model/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lsx7te/debugging_transformer_based_classification_model/,30199,1614344420.0,0,,False,,,,,,206
663,,LanguageTechnology,"I'm working on an unsupervised summary extraction problem for an under-resourced language.

I would prefer to use an out-of-the-box solution in the following form - I provide sentence embeddings (e.g. using a trained BERT model), and the text rank package uses them to find the most relevant ones.

However, a google search made me realize that existing solutions mostly rely on popular packages, e.g. PyTextRank works on top of spacy, and when I run it I get an error that 'noun\_chunks' syntax iterator is not implemented for my language.

Any suggestions how to do it better?",t2_gbqbt,False,,0,False,Is there a bare-bones text rank package for python?,[],r/LanguageTechnology,False,6,,0,,False,t3_lszpsn,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1614381301.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working on an unsupervised summary extraction problem for an under-resourced language.&lt;/p&gt;

&lt;p&gt;I would prefer to use an out-of-the-box solution in the following form - I provide sentence embeddings (e.g. using a trained BERT model), and the text rank package uses them to find the most relevant ones.&lt;/p&gt;

&lt;p&gt;However, a google search made me realize that existing solutions mostly rely on popular packages, e.g. PyTextRank works on top of spacy, and when I run it I get an error that &amp;#39;noun_chunks&amp;#39; syntax iterator is not implemented for my language.&lt;/p&gt;

&lt;p&gt;Any suggestions how to do it better?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lszpsn,True,,BorderLineGenius,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lszpsn/is_there_a_barebones_text_rank_package_for_python/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lszpsn/is_there_a_barebones_text_rank_package_for_python/,30199,1614352501.0,0,,False,,,,,,578
664,,LanguageTechnology,,t2_16gftjf,False,,0,False,[R] AAMAS 2021: A framework for integrating gesture generation models into interactive conversational agents,[],r/LanguageTechnology,False,6,,0,,False,t3_lszl8y,False,dark,1.0,,public,1,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/jhgUBS0125A?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'A framework for integrating gesture generation models into interactive conversational agents', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/jhgUBS0125A?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Taras Kucherenko', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/jhgUBS0125A/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC62QjHPTaJR0yt1bOIcYfwA'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/jhgUBS0125A?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lszl8y', 'height': 200}",,False,1,,False,False,,False,,[],{},,False,,1614380929.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lszl8y,True,,Svito-zar,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lszl8y/r_aamas_2021_a_framework_for_integrating_gesture/,all_ads,False,https://youtu.be/jhgUBS0125A,30199,1614352129.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'A framework for integrating gesture generation models into interactive conversational agents', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/jhgUBS0125A?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Taras Kucherenko', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/jhgUBS0125A/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC62QjHPTaJR0yt1bOIcYfwA'}}",False,https://youtu.be/jhgUBS0125A,,,,,0
665,,LanguageTechnology,"I’m curious about determining semantic meaning in cases where context implies more than what is available in the sentence. 

For example: “I only wear that shirt because she gave it to me”. The implied meaning is that “I” don’t like the shirt. 

Is there any dataset or tool to extract these implied or norm based meanings? Thank you!",t2_49qyxxsz,False,,0,False,"Want to extract semantic meaning from implied context and norms, I need direction of what to search for",[],r/LanguageTechnology,False,6,,0,,False,t3_lstkqa,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1614358116.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m curious about determining semantic meaning in cases where context implies more than what is available in the sentence. &lt;/p&gt;

&lt;p&gt;For example: “I only wear that shirt because she gave it to me”. The implied meaning is that “I” don’t like the shirt. &lt;/p&gt;

&lt;p&gt;Is there any dataset or tool to extract these implied or norm based meanings? Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lstkqa,True,,win10240,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lstkqa/want_to_extract_semantic_meaning_from_implied/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lstkqa/want_to_extract_semantic_meaning_from_implied/,30199,1614329316.0,0,,False,,,,,,334
666,,LanguageTechnology,"Hi everyone, 

i'm trying to implement a system that from the subtitles of movies/tv series extracts emotion that film causes. I found dataset named DailyDialog that contain 103K sentences labelled with 6 based Ekman's emotion plus no\_emotion label.

I'm trying to use BERT and i reached an 85% of accuracy. The problem is that dataset is very unbalanced. I'm trying to use BERT for data augmentation using it as masked language modelling (MLM) for generate new sentences with new words that fit in the context of the sentence. For every sentence the data aug generates ten sencentes. The problem is  that the word outputed by BERT it is not sure it is conforms to the emotion of the sencente. I can try to see with SenticNet if the word is related to the emotion of sentence.

Another thing is that Bert immediately goes into overfitting.

\- Anyone have any suggestions if the project is feasible?  
\- Do you know any datasets related to the described task?  
\- Do you have any suggestions on how to do a good data augmentation?",t2_al438vgy,False,,0,False,Emotion extraction from subtitles of the movies,[],r/LanguageTechnology,False,6,,0,,False,t3_lsygna,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1614377532.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;

&lt;p&gt;i&amp;#39;m trying to implement a system that from the subtitles of movies/tv series extracts emotion that film causes. I found dataset named DailyDialog that contain 103K sentences labelled with 6 based Ekman&amp;#39;s emotion plus no_emotion label.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m trying to use BERT and i reached an 85% of accuracy. The problem is that dataset is very unbalanced. I&amp;#39;m trying to use BERT for data augmentation using it as masked language modelling (MLM) for generate new sentences with new words that fit in the context of the sentence. For every sentence the data aug generates ten sencentes. The problem is  that the word outputed by BERT it is not sure it is conforms to the emotion of the sencente. I can try to see with SenticNet if the word is related to the emotion of sentence.&lt;/p&gt;

&lt;p&gt;Another thing is that Bert immediately goes into overfitting.&lt;/p&gt;

&lt;p&gt;- Anyone have any suggestions if the project is feasible?&lt;br/&gt;
- Do you know any datasets related to the described task?&lt;br/&gt;
- Do you have any suggestions on how to do a good data augmentation?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lsygna,True,,antomare94,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lsygna/emotion_extraction_from_subtitles_of_the_movies/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lsygna/emotion_extraction_from_subtitles_of_the_movies/,30199,1614348732.0,0,,False,,,,,,1033
667,,LanguageTechnology,,t2_7rr9jv7r,False,,0,False,"Given the vast amount of unstructured data captured in e-Health Records, there's an immediate high demand for NLP to facilitate automatic extraction &amp; structuring of clinical data for decision support",[],r/LanguageTechnology,False,6,,0,,False,t3_lsv8mc,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,False,,1614365440.0,text,6,,,text,re-work.co,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lsv8mc,True,,DeepLearnerJM,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lsv8mc/given_the_vast_amount_of_unstructured_data/,all_ads,False,https://www.re-work.co/events/ai-in-healthcare-summit-2021?utm_source=LinkedIn&amp;utm_medium=Message&amp;utm_campaign=JM_Healthcare_2021,30199,1614336640.0,0,,False,https://www.re-work.co/events/ai-in-healthcare-summit-2021?utm_source=LinkedIn&amp;utm_medium=Message&amp;utm_campaign=JM_Healthcare_2021,,,,,0
668,,LanguageTechnology,"
## NLU 1.1.2 Release Notes

We are very happy to announce NLU 1.1.2 has been released with the integration of 30+ models and pipelines Bengali Named Entity Recognition, Hindi Word Embeddings,
and state-of-the-art transformer based OntoNotes models and pipelines from the [incredible Spark NLP 2.7.3 Release](https://github.com/JohnSnowLabs/spark-nlp/releases/tag/2.7.3) in addition to a few bugfixes.  
In addition to that, there is a [new NLU Webinar video](https://www.youtube.com/watch?t=2141&amp;v=hJR9m3NYnwk&amp;feature=youtu.be) showcasing in detail 
how to use NLU to analyze a crypto news dataset to extract keywords unsupervised and predict sentimential/emotional distributions of the dataset and much more!

### [Python's NLU library: 1,000+ models, 200+ Languages, State of the Art Accuracy, 1 Line of code - NLU NYC/DC NLP Meetup Webinar](https://www.youtube.com/watch?t=2141&amp;v=hJR9m3NYnwk&amp;feature=youtu.be)
Using just 1 line of Python code by leveraging the NLU library, which is powered by the award-winning Spark NLP.

This webinar covers, using live coding in real-time,
how to deliver summarization, translation, unsupervised keyword extraction, emotion analysis,
question answering, spell checking, named entity recognition, document classification, and other common NLP tasks. T
his is all done with a single line of code, that works directly on Python strings or pandas data frames.
Since NLU is based on Spark NLP, no code changes are required to scale processing to multi-core or cluster environment - integrating natively with Ray, Dask, or Spark data frames.

The recent releases for Spark NLP and NLU include pre-trained models for over 200 languages and language detection for 375 languages.
This includes 20 languages families; non-Latin alphabets; languages that do not use spaces for word segmentation like
Chinese, Japanese, and Korean; and languages written from right to left like Arabic, Farsi, Urdu, and Hebrew.
We'll also cover some of the algorithms and models that are included. The code notebooks will be freely available online.

 

### NLU 1.1.2 New Models  and Pipelines

#### NLU 1.1.2 New Non-English Models

|Language | nlu.load() reference | Spark NLP Model reference | Type |
|---------|---------------------|----------------------------|------|
|Bengali | [bn.ner](https://nlp.johnsnowlabs.com/2021/01/27/ner_jifs_glove_840B_300d_bn.html) |[ner_jifs_glove_840B_300d](https://nlp.johnsnowlabs.com/2021/01/27/ner_jifs_glove_840B_300d_bn.html) | Word Embeddings Model (Alias) |
| Bengali  | [bn.ner.glove](https://nlp.johnsnowlabs.com/2021/01/27/ner_jifs_glove_840B_300d_bn.html) | [ner_jifs_glove_840B_300d](https://nlp.johnsnowlabs.com/2021/01/27/ner_jifs_glove_840B_300d_bn.html) | Word Embeddings Model (Alias) |
|Hindi|[hi.embed](https://nlp.johnsnowlabs.com/2021/02/03/hindi_cc_300d_hi.html)|[hindi_cc_300d](https://nlp.johnsnowlabs.com/2021/02/03/hindi_cc_300d_hi.html)|NerDLModel|
|Bengali | [bn.lemma](https://nlp.johnsnowlabs.com/2021/01/20/lemma_bn.html) |[lemma](https://nlp.johnsnowlabs.com/2021/01/20/lemma_bn.html) | Lemmatizer                    |
|Japanese | [ja.lemma](https://nlp.johnsnowlabs.com/2021/01/15/lemma_ja.html) |[lemma](https://nlp.johnsnowlabs.com/2021/01/15/lemma_ja.html) | Lemmatizer                    |
|Bihari | [bh.lemma](https://nlp.johnsnowlabs.com/2021/01/18/lemma_bh.html) |[lemma](https://nlp.johnsnowlabs.com/2021/01/18/lemma_bh.html) | Lemma                    |
|Amharic | [am.lemma](https://nlp.johnsnowlabs.com/2021/01/20/lemma_am.html) |[lemma](https://nlp.johnsnowlabs.com/2021/01/20/lemma_am.html) | Lemma                    |

#### NLU 1.1.2 New English Models and Pipelines

|Language | nlu.load() reference | Spark NLP Model reference | Type |
|---------|---------------------|----------------------------|------|
| English | [en.ner.onto.bert.small_l2_128](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L2_128_en.html) |[onto_small_bert_L2_128](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L2_128_en.html)     | NerDLModel |
| English | [en.ner.onto.bert.small_l4_256](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L4_256_en.html) |[onto_small_bert_L4_256](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L4_256_en.html)     | NerDLModel |
| English | [en.ner.onto.bert.small_l4_512](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L4_512_en.html) |[onto_small_bert_L4_512](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L4_512_en.html)     | NerDLModel |
| English | [en.ner.onto.bert.small_l8_512](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L8_512_en.html) |[onto_small_bert_L8_512](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L8_512_en.html)     | NerDLModel |
| English | [en.ner.onto.bert.cased_base](https://nlp.johnsnowlabs.com/2020/12/05/onto_bert_base_cased_en.html) |[onto_bert_base_cased](https://nlp.johnsnowlabs.com/2020/12/05/onto_bert_base_cased_en.html)     | NerDLModel |
| English | [en.ner.onto.bert.cased_large](https://nlp.johnsnowlabs.com/2020/12/05/onto_bert_large_cased_en.html) |[onto_bert_large_cased](https://nlp.johnsnowlabs.com/2020/12/05/onto_bert_large_cased_en.html)     | NerDLModel |
| English | [en.ner.onto.electra.uncased_small](https://nlp.johnsnowlabs.com/2020/12/05/onto_electra_small_uncased_en.html) |[onto_electra_small_uncased](https://nlp.johnsnowlabs.com/2020/12/05/onto_electra_small_uncased_en.html)     | NerDLModel |
| English  | [en.ner.onto.electra.uncased_base](https://nlp.johnsnowlabs.com/2020/12/05/onto_electra_base_uncased_en.html) |[onto_electra_base_uncased](https://nlp.johnsnowlabs.com/2020/12/05/onto_electra_base_uncased_en.html)     | NerDLModel |
| English | [en.ner.onto.electra.uncased_large](https://nlp.johnsnowlabs.com/2020/12/05/onto_electra_large_uncased_en.html) |[onto_electra_large_uncased](https://nlp.johnsnowlabs.com/2020/12/05/onto_electra_large_uncased_en.html)     | NerDLModel |
| English | [en.ner.onto.bert.tiny](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_tiny_en.html) | [onto_recognize_entities_bert_tiny](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_tiny_en.html) | Pipeline |
| English | [en.ner.onto.bert.mini](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_mini_en.html) |[onto_recognize_entities_bert_mini](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_mini_en.html)     | Pipeline |
| English | [en.ner.onto.bert.small](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_small_en.html) | [onto_recognize_entities_bert_small](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_small_en.html) | Pipeline |
| English | [en.ner.onto.bert.medium](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_medium_en.html) |[onto_recognize_entities_bert_medium](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_medium_en.html)     | Pipeline |
| English | [en.ner.onto.bert.base](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_base_en.html) |[onto_recognize_entities_bert_base](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_base_en.html)     | Pipeline |
|English|[en.ner.onto.bert.large](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_large_en.html)|[onto_recognize_entities_bert_large](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_large_en.html)|Pipeline|
|English|[en.ner.onto.electra.small](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_small_en.html)|[onto_recognize_entities_electra_small](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_small_en.html)|Pipeline|
|English|[en.ner.onto.electra.base](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_base_en.html)|[onto_recognize_entities_electra_base](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_base_en.html)|Pipeline|
|English|[en.ner.onto.large](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_large_en.html)|[onto_recognize_entities_electra_large](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_large_en.html)|Pipeline|



### New Tutorials and Notebooks

- [NYC/DC NLP Meetup Webinar video analyze Crypto News, Unsupervised Keywords, Translate between 300 Languages, Question Answering, Summerization, POS, NER in 1 line of code in almost just 20 minutes](https://www.youtube.com/watch?t=2141&amp;v=hJR9m3NYnwk&amp;feature=youtu.be)
- [NLU basics POS/NER/Sentiment Classification/BERTology Embeddings](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/NYC_DC_NLP_MEETUP/0_liners_intro.ipynb)
- [Explore Crypto Newsarticle dataset, unsupervised Keyword extraction, Stemming, Emotion/Sentiment distribution Analysis](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/NYC_DC_NLP_MEETUP/1_NLU_base_features_on_dataset_with_YAKE_Lemma_Stemm_classifiers_NER_.ipynb)
- [Translate between more than 300 Languages in 1 line of code with the Marian Models](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/NYC_DC_NLP_MEETUP/2_multilingual_translation_with_marian.ipynb)
- [New NLU 1.1.2 Models Showcase Notebooks, Bengali NER, Hindi Embeddings, 30 new_models](https://colab.research.google.com/github/JohnSnowLabs/nlu/blob/master/examples/release_notebooks/NLU1.1.2_Bengali_ner_Hindi_Embeddings_30_new_models.ipynb)


### NLU 1.1.2 Bug Fixes

- Fixed a bug that caused NER confidences not beeing extracted
- Fixed a bug that caused nlu.load('spell') to crash
- Fixed a bug that caused Uralic/Estonian/ET language models not to be loaded properly


### New  Easy NLU 1-liners in 1.1.2


#### [Named Entity Recognition for Bengali (GloVe 840B 300d)](https://nlp.johnsnowlabs.com/2021/01/27/ner_jifs_glove_840B_300d_bn.html)


```python
#Bengali for :  It began to be widely used in the United States in the early '90s.
nlu.load(""bn.ner"").predict(""৯০ এর দশকের শুরুর দিকে বৃহৎ আকারে মার্কিন যুক্তরাষ্ট্রে এর প্রয়োগের প্রক্রিয়া শুরু হয়'"")
```
output :

|   entities             | token     | Entities_classes   |   ner_confidence |
|:---------------------|:----------|:----------------------|-----------------:|
| ['মার্কিন যুক্তরাষ্ট্রে'] | ৯০        | ['LOC']               |           1      |
| ['মার্কিন যুক্তরাষ্ট্রে'] | এর        | ['LOC']               |           0.9999 |
| ['মার্কিন যুক্তরাষ্ট্রে'] | দশকের     | ['LOC']               |           1      |
| ['মার্কিন যুক্তরাষ্ট্রে'] | শুরুর       | ['LOC']               |           0.9969 |
| ['মার্কিন যুক্তরাষ্ট্রে'] | দিকে      | ['LOC']               |           1      |
| ['মার্কিন যুক্তরাষ্ট্রে'] | বৃহৎ       | ['LOC']               |           0.9994 |
| ['মার্কিন যুক্তরাষ্ট্রে'] | আকারে     | ['LOC']               |           1      |
| ['মার্কিন যুক্তরাষ্ট্রে'] | মার্কিন    | ['LOC']               |           0.9602 |
| ['মার্কিন যুক্তরাষ্ট্রে'] | যুক্তরাষ্ট্রে | ['LOC']               |           0.4134 |
| ['মার্কিন যুক্তরাষ্ট্রে'] | এর        | ['LOC']               |           1      |
| ['মার্কিন যুক্তরাষ্ট্রে'] | প্রয়োগের   | ['LOC']               |           1      |
| ['মার্কিন যুক্তরাষ্ট্রে'] | প্রক্রিয়া   | ['LOC']               |           1      |
| ['মার্কিন যুক্তরাষ্ট্রে'] | শুরু        | ['LOC']               |           0.9999 |
| ['মার্কিন যুক্তরাষ্ট্রে'] | হয়        | ['LOC']               |           1      |
| ['মার্কিন যুক্তরাষ্ট্রে'] | '         | ['LOC']               |           1      |


#### [Bengali Lemmatizer](https://nlp.johnsnowlabs.com/2021/01/20/lemma_bn.html)


```python
#Bengali for :  One morning in the marble-decorated building of Vaidyanatha, an obese monk was engaged in the enchantment of Duis and the milk service of one and a half Vaidyanatha. Give me two to eat
nlu.load(""bn.lemma"").predict(""একদিন প্রাতে বৈদ্যনাথের মার্বলমণ্ডিত দালানে একটি স্থূলোদর সন্ন্যাসী দুইসের মোহনভোগ এবং দেড়সের দুগ্ধ সেবায় নিযুক্ত আছে বৈদ্যনাথ গায়ে একখানি চাদর দিয়া জোড়করে একান্ত বিনীতভাবে ভূতলে বসিয়া ভক্তিভরে পবিত্র ভোজনব্যাপার নিরীক্ষণ করিতেছিলেন এমন সময় কোনোমতে দ্বারীদের দৃষ্টি এড়াইয়া জীর্ণদেহ বালক সহিত একটি অতি শীর্ণকায়া রমণী গৃহে প্রবেশ করিয়া ক্ষীণস্বরে কহিল বাবু দুটি খেতে দাও"")

```
output :

| lemma                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | document                                                                                                                                                                                                                                                                                                                                          |
|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ['একদিন', 'প্রাতঃ', 'বৈদ্যনাথ', 'মার্বলমণ্ডিত', 'দালান', 'এক', 'স্থূলউদর', 'সন্ন্যাসী', 'দুইসের', 'মোহনভোগ', 'এবং', 'দেড়সের', 'দুগ্ধ', 'সেবা', 'নিযুক্ত', 'আছে', 'বৈদ্যনাথ', 'গা', 'একখান', 'চাদর', 'দেওয়া', 'জোড়কর', 'একান্ত', 'বিনীতভাব', 'ভূতল', 'বসা', 'ভক্তিভরা', 'পবিত্র', 'ভোজনব্যাপার', 'নিরীক্ষণ', 'করা', 'এমন', 'সময়', 'কোনোমত', 'দ্বারী', 'দৃষ্টি', 'এড়ানো', 'জীর্ণদেহ', 'বালক', 'সহিত', 'এক', 'অতি', 'শীর্ণকায়া', 'রমণী', 'গৃহ', 'প্রবেশ', 'বিশ্বাস', 'ক্ষীণস্বর', 'কহা', 'বাবু', 'দুই', 'খাওয়া', 'দাওয়া'] | একদিন প্রাতে বৈদ্যনাথের মার্বলমণ্ডিত দালানে একটি স্থূলোদর সন্ন্যাসী দুইসের মোহনভোগ এবং দেড়সের দুগ্ধ সেবায় নিযুক্ত আছে বৈদ্যনাথ গায়ে একখানি চাদর দিয়া জোড়করে একান্ত বিনীতভাবে ভূতলে বসিয়া ভক্তিভরে পবিত্র ভোজনব্যাপার নিরীক্ষণ করিতেছিলেন এমন সময় কোনোমতে দ্বারীদের দৃষ্টি এড়াইয়া জীর্ণদেহ বালক সহিত একটি অতি শীর্ণকায়া রমণী গৃহে প্রবেশ করিয়া ক্ষীণস্বরে কহিল বাবু দুটি খেতে দাও |


#### [Japanese Lemmatizer](https://nlp.johnsnowlabs.com/2021/01/15/lemma_ja.html)


```python
#Japanese for :  Some residents were uncomfortable with this, but it seems that no one is now openly protesting or protesting.
nlu.load(""ja.lemma"").predict(""これに不快感を示す住民はいましたが,現在,表立って反対や抗議の声を挙げている住民はいないようです。"")

```
output :

| lemma                                                                                                                                                                                                                                                          | document                                                                                         |
|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------|
| ['これ', 'にる', '不快', '感', 'を', '示す', '住民', 'はる', 'いる', 'まする', 'たる', 'がる', ',', '現在', ',', '表立つ', 'てる', '反対', 'やる', '抗議', 'のる', '声', 'を', '挙げる', 'てる', 'いる', '住民', 'はる', 'いる', 'なぐ', 'よう', 'です', '。'] | これに不快感を示す住民はいましたが,現在,表立って反対や抗議の声を挙げている住民はいないようです。 |

#### [Aharic Lemmatizer](https://nlp.johnsnowlabs.com/2021/01/20/lemma_am.html)


```python
#Aharic for :  Bookmark the permalink.
nlu.load(""am.lemma"").predict(""መጽሐፉን መጽሐፍ ኡ ን አስያዛት አስያዝ ኧ ኣት ።"")

```
output  :

| lemma                                                | document                         |
|:-----------------------------------------------------|:---------------------------------|
| ['_', 'መጽሐፍ', 'ኡ', 'ን', '_', 'አስያዝ', 'ኧ', 'ኣት', '።'] | መጽሐፉን መጽሐፍ ኡ ን አስያዛት አስያዝ ኧ ኣት ። |

#### [Bhojpuri Lemmatizer](https://nlp.johnsnowlabs.com/2021/01/18/lemma_bh.html)


```python
#Bhojpuri for : In this event, participation of World Bhojpuri Conference, Purvanchal Ekta Manch, Veer Kunwar Singh Foundation, Purvanchal Bhojpuri Mahasabha, and Herf - Media.
nlu.load(""bh.lemma"").predict(""एह आयोजन में विश्व भोजपुरी सम्मेलन , पूर्वांचल एकता मंच , वीर कुँवर सिंह फाउन्डेशन , पूर्वांचल भोजपुरी महासभा , अउर हर्फ - मीडिया के सहभागिता बा ।"")
```

output :

| lemma                                                                                                                                                                                                                               | document                                                                                                                      |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------|
| ['एह', 'आयोजन', 'में', 'विश्व', 'भोजपुरी', 'सम्मेलन', 'COMMA', 'पूर्वांचल', 'एकता', 'मंच', 'COMMA', 'वीर', 'कुँवर', 'सिंह', 'फाउन्डेशन', 'COMMA', 'पूर्वांचल', 'भोजपुरी', 'महासभा', 'COMMA', 'अउर', 'हर्फ', '-', 'मीडिया', 'को', 'सहभागिता', 'बा', '।'] | एह आयोजन में विश्व भोजपुरी सम्मेलन , पूर्वांचल एकता मंच , वीर कुँवर सिंह फाउन्डेशन , पूर्वांचल भोजपुरी महासभा , अउर हर्फ - मीडिया के सहभागिता बा । |

#### [Named Entity Recognition - BERT Tiny (OntoNotes)](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L2_128_en.html)
```python
nlu.load(""en.ner.onto.bert.small_l2_128"").predict(""""""William Henry Gates III (born October 28, 1955) is an American business magnate,
 software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft,
  Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect,
   while also being the largest individual shareholder until May 2014.
    He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico;
     it went on to become the world's largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect.
     During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time
      role at Microsoft and full-time work at the Bill &amp; Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.
 He gradually transferred his duties to Ray Ozzie and Craig Mundie.
  He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella."""""",output_level = ""document"")
```

output  :

| ner_confidence | entities | Entities_classes                                          |
| :------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| [0.8536999821662903, 0.7195000052452087, 0.746...] | ['PERSON', 'DATE', 'NORP', 'ORG', 'ORG', 'PERSON', 'DATE', 'CARDINAL', 'DATE', 'DATE', 'GPE', 'GPE', 'PERSON', 'DATE', 'GPE', 'GPE'] | ['William Henry Gates III', 'October 28, 1955', 'American', 'Microsoft Corporation', 'Microsoft', 'Gates', 'May 2014', 'one', '1970s', '1980s', 'Seattle', 'Washington', 'Paul Allen', '1975', 'Albuquerque', 'New Mexico'] |

####  [Named Entity Recognition - BERT Mini (OntoNotes)](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L4_256_en.html)
```python
nlu.load(""en.ner.onto.bert.small_l4_256"").predict(""""""William Henry Gates III (born October 28, 1955) is an American business magnate,
 software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft,
  Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect,
   while also being the largest individual shareholder until May 2014.
    He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico;
     it went on to become the world's largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect.
     During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time
      role at Microsoft and full-time work at the Bill &amp; Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.
 He gradually transferred his duties to Ray Ozzie and Craig Mundie.
  He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella."""""",output_level = ""document"")
```

output :

|  ner_confidence	  | entities                                                                                                                                                                                                                                           | Entities_classes                                                                                                                     |
|---------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------|
|          [0.835099995136261, 0.40450000762939453, 0.331...] | ['William Henry Gates III', 'October 28, 1955', 'American', 'Microsoft Corporation', 'Microsoft', 'Gates', 'May 2014', 'one', '1970s and 1980s', 'Seattle', 'Washington', 'Gates', 'Microsoft', 'Paul Allen', '1975', 'Albuquerque', 'New Mexico'] | ['PERSON', 'DATE', 'NORP', 'ORG', 'ORG', 'ORG', 'DATE', 'CARDINAL', 'DATE', 'GPE', 'GPE', 'ORG', 'ORG', 'PERSON', 'DATE', 'GPE', 'GPE'] |




#### [Named Entity Recognition - BERT Small (OntoNotes)](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L4_512_en.html)

```python
nlu.load(""en.ner.onto.bert.small_l4_512"").predict(""""""William Henry Gates III (born October 28, 1955) is an American business magnate,
 software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft,
  Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect,
   while also being the largest individual shareholder until May 2014.
    He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico;
     it went on to become the world's largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect.
     During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time
      role at Microsoft and full-time work at the Bill &amp; Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.
 He gradually transferred his duties to Ray Ozzie and Craig Mundie.
  He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella."""""",output_level = ""document"")
```
output :

|   ner_confidence | entities                                                                                                                                                                                                                                               | Entities_classes                                                                                                                           |
|---------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------|
|              [0.964900016784668, 0.8299000263214111, 0.9607...]| ['William Henry Gates III', 'October 28, 1955', 'American', 'Microsoft Corporation', 'Microsoft', 'Gates', 'May 2014', 'one', 'the 1970s and 1980s', 'Seattle', 'Washington', 'Gates', 'Microsoft', 'Paul Allen', '1975', 'Albuquerque', 'New Mexico'] | ['PERSON', 'DATE', 'NORP', 'ORG', 'ORG', 'PERSON', 'DATE', 'CARDINAL', 'DATE', 'GPE', 'GPE', 'PERSON', 'ORG', 'PERSON', 'DATE', 'GPE', 'GPE'] |


#### [Named Entity Recognition - BERT Medium (OntoNotes)](https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L8_512_en.html)

```python
nlu.load(""en.ner.onto.bert.small_l8_512"").predict(""""""William Henry Gates III (born October 28, 1955) is an American business magnate,
 software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft,
  Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect,
   while also being the largest individual shareholder until May 2014.
    He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico;
     it went on to become the world's largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect.
     During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time
      role at Microsoft and full-time work at the Bill &amp; Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.
 He gradually transferred his duties to Ray Ozzie and Craig Mundie.
  He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella."""""",output_level = ""document"")
```
output :

| ner_confidence   | entities                                                                                                                                                                                                                           | Entities_classes                                                                                                        |
|---------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|
|        [0.916700005531311, 0.5873000025749207, 0.8816...] | ['William Henry Gates III', 'October 28, 1955', 'American', 'Microsoft Corporation', 'Microsoft', 'Gates', 'May 2014', 'the 1970s and 1980s', 'Seattle', 'Washington', 'Gates', 'Paul Allen', '1975', 'Albuquerque', 'New Mexico'] | ['PERSON', 'DATE', 'NORP', 'ORG', 'ORG', 'PERSON', 'DATE', 'DATE', 'GPE', 'GPE', 'PERSON', 'PERSON', 'DATE', 'GPE', 'GPE'] |



#### [Named Entity Recognition - BERT Base (OntoNotes)](https://nlp.johnsnowlabs.com/2020/12/05/onto_bert_base_cased_en.html)

```python
nlu.load(""en.ner.onto.bert.cased_base"").predict(""""""William Henry Gates III (born October 28, 1955) is an American business magnate,
 software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft,
  Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect,
   while also being the largest individual shareholder until May 2014.
    He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico;
     it went on to become the world's largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect.
     During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time
      role at Microsoft and full-time work at the Bill &amp; Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.
 He gradually transferred his duties to Ray Ozzie and Craig Mundie.
  He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella."""""",output_level = ""document"")
```
output :

|   ner_confidence | entities                                                                                                                                                                                                                                               | Entities_classes                                                                                                                           |
|---------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------|
|              [0.504800021648407, 0.47290000319480896, 0.462...] | ['William Henry Gates III', 'October 28, 1955', 'American', 'Microsoft Corporation', 'Microsoft', 'Gates', 'May 2014', 'one', 'the 1970s and 1980s', 'Seattle', 'Washington', 'Gates', 'Microsoft', 'Paul Allen', '1975', 'Albuquerque', 'New Mexico'] | ['PERSON', 'DATE', 'NORP', 'ORG', 'ORG', 'PERSON', 'DATE', 'CARDINAL', 'DATE', 'GPE', 'GPE', 'PERSON', 'ORG', 'PERSON', 'DATE', 'GPE', 'GPE'] |





### NLU Installation

```bash
# PyPi
!pip install nlu pyspark==2.4.7
#Conda
# Install NLU from Anaconda/Conda
conda install -c johnsnowlabs nlu
```

### Additional NLU ressources
- [NLU Website](https://nlu.johnsnowlabs.com/)
- [All NLU Tutorial Notebooks](https://nlu.johnsnowlabs.com/docs/en/notebooks)
- [NLU Videos and Blogposts on NLU](https://nlp.johnsnowlabs.com/learn#pythons-nlu-library)
- [NLU on Github](https://github.com/JohnSnowLabs/nlu)",t2_53n73cus,False,,0,False,"Train Multi-Lingual classifier for 100 languages in 1 Line, Hindi Word Embeddings, Bengali NER NYC/DC Meetup Webinar, in NLU 1.1.2",[],r/LanguageTechnology,False,6,,0,,False,t3_lsckqc,False,dark,0.77,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1614304714.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;h2&gt;NLU 1.1.2 Release Notes&lt;/h2&gt;

&lt;p&gt;We are very happy to announce NLU 1.1.2 has been released with the integration of 30+ models and pipelines Bengali Named Entity Recognition, Hindi Word Embeddings,
and state-of-the-art transformer based OntoNotes models and pipelines from the &lt;a href=""https://github.com/JohnSnowLabs/spark-nlp/releases/tag/2.7.3""&gt;incredible Spark NLP 2.7.3 Release&lt;/a&gt; in addition to a few bugfixes.&lt;br/&gt;
In addition to that, there is a &lt;a href=""https://www.youtube.com/watch?t=2141&amp;amp;v=hJR9m3NYnwk&amp;amp;feature=youtu.be""&gt;new NLU Webinar video&lt;/a&gt; showcasing in detail 
how to use NLU to analyze a crypto news dataset to extract keywords unsupervised and predict sentimential/emotional distributions of the dataset and much more!&lt;/p&gt;

&lt;h3&gt;&lt;a href=""https://www.youtube.com/watch?t=2141&amp;amp;v=hJR9m3NYnwk&amp;amp;feature=youtu.be""&gt;Python&amp;#39;s NLU library: 1,000+ models, 200+ Languages, State of the Art Accuracy, 1 Line of code - NLU NYC/DC NLP Meetup Webinar&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Using just 1 line of Python code by leveraging the NLU library, which is powered by the award-winning Spark NLP.&lt;/p&gt;

&lt;p&gt;This webinar covers, using live coding in real-time,
how to deliver summarization, translation, unsupervised keyword extraction, emotion analysis,
question answering, spell checking, named entity recognition, document classification, and other common NLP tasks. T
his is all done with a single line of code, that works directly on Python strings or pandas data frames.
Since NLU is based on Spark NLP, no code changes are required to scale processing to multi-core or cluster environment - integrating natively with Ray, Dask, or Spark data frames.&lt;/p&gt;

&lt;p&gt;The recent releases for Spark NLP and NLU include pre-trained models for over 200 languages and language detection for 375 languages.
This includes 20 languages families; non-Latin alphabets; languages that do not use spaces for word segmentation like
Chinese, Japanese, and Korean; and languages written from right to left like Arabic, Farsi, Urdu, and Hebrew.
We&amp;#39;ll also cover some of the algorithms and models that are included. The code notebooks will be freely available online.&lt;/p&gt;

&lt;h3&gt;NLU 1.1.2 New Models  and Pipelines&lt;/h3&gt;

&lt;h4&gt;NLU 1.1.2 New Non-English Models&lt;/h4&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Bengali&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/27/ner_jifs_glove_840B_300d_bn.html""&gt;bn.ner&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/27/ner_jifs_glove_840B_300d_bn.html""&gt;ner_jifs_glove_840B_300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Word Embeddings Model (Alias)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Bengali&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/27/ner_jifs_glove_840B_300d_bn.html""&gt;bn.ner.glove&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/27/ner_jifs_glove_840B_300d_bn.html""&gt;ner_jifs_glove_840B_300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Word Embeddings Model (Alias)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hindi&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/03/hindi_cc_300d_hi.html""&gt;hi.embed&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/03/hindi_cc_300d_hi.html""&gt;hindi_cc_300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NerDLModel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Bengali&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/20/lemma_bn.html""&gt;bn.lemma&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/20/lemma_bn.html""&gt;lemma&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Lemmatizer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Japanese&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/15/lemma_ja.html""&gt;ja.lemma&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/15/lemma_ja.html""&gt;lemma&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Lemmatizer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Bihari&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/18/lemma_bh.html""&gt;bh.lemma&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/18/lemma_bh.html""&gt;lemma&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Lemma&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Amharic&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/20/lemma_am.html""&gt;am.lemma&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/20/lemma_am.html""&gt;lemma&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Lemma&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;NLU 1.1.2 New English Models and Pipelines&lt;/h4&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L2_128_en.html""&gt;en.ner.onto.bert.small_l2_128&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L2_128_en.html""&gt;onto_small_bert_L2_128&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NerDLModel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L4_256_en.html""&gt;en.ner.onto.bert.small_l4_256&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L4_256_en.html""&gt;onto_small_bert_L4_256&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NerDLModel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L4_512_en.html""&gt;en.ner.onto.bert.small_l4_512&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L4_512_en.html""&gt;onto_small_bert_L4_512&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NerDLModel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L8_512_en.html""&gt;en.ner.onto.bert.small_l8_512&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L8_512_en.html""&gt;onto_small_bert_L8_512&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NerDLModel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_bert_base_cased_en.html""&gt;en.ner.onto.bert.cased_base&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_bert_base_cased_en.html""&gt;onto_bert_base_cased&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NerDLModel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_bert_large_cased_en.html""&gt;en.ner.onto.bert.cased_large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_bert_large_cased_en.html""&gt;onto_bert_large_cased&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NerDLModel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_electra_small_uncased_en.html""&gt;en.ner.onto.electra.uncased_small&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_electra_small_uncased_en.html""&gt;onto_electra_small_uncased&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NerDLModel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_electra_base_uncased_en.html""&gt;en.ner.onto.electra.uncased_base&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_electra_base_uncased_en.html""&gt;onto_electra_base_uncased&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NerDLModel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_electra_large_uncased_en.html""&gt;en.ner.onto.electra.uncased_large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_electra_large_uncased_en.html""&gt;onto_electra_large_uncased&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NerDLModel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_tiny_en.html""&gt;en.ner.onto.bert.tiny&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_tiny_en.html""&gt;onto_recognize_entities_bert_tiny&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Pipeline&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_mini_en.html""&gt;en.ner.onto.bert.mini&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_mini_en.html""&gt;onto_recognize_entities_bert_mini&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Pipeline&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_small_en.html""&gt;en.ner.onto.bert.small&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_small_en.html""&gt;onto_recognize_entities_bert_small&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Pipeline&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_medium_en.html""&gt;en.ner.onto.bert.medium&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_medium_en.html""&gt;onto_recognize_entities_bert_medium&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Pipeline&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_base_en.html""&gt;en.ner.onto.bert.base&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_base_en.html""&gt;onto_recognize_entities_bert_base&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Pipeline&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_large_en.html""&gt;en.ner.onto.bert.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_bert_large_en.html""&gt;onto_recognize_entities_bert_large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Pipeline&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_small_en.html""&gt;en.ner.onto.electra.small&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_small_en.html""&gt;onto_recognize_entities_electra_small&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Pipeline&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_base_en.html""&gt;en.ner.onto.electra.base&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_base_en.html""&gt;onto_recognize_entities_electra_base&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Pipeline&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_large_en.html""&gt;en.ner.onto.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_large_en.html""&gt;onto_recognize_entities_electra_large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Pipeline&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h3&gt;New Tutorials and Notebooks&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://www.youtube.com/watch?t=2141&amp;amp;v=hJR9m3NYnwk&amp;amp;feature=youtu.be""&gt;NYC/DC NLP Meetup Webinar video analyze Crypto News, Unsupervised Keywords, Translate between 300 Languages, Question Answering, Summerization, POS, NER in 1 line of code in almost just 20 minutes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/NYC_DC_NLP_MEETUP/0_liners_intro.ipynb""&gt;NLU basics POS/NER/Sentiment Classification/BERTology Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/NYC_DC_NLP_MEETUP/1_NLU_base_features_on_dataset_with_YAKE_Lemma_Stemm_classifiers_NER_.ipynb""&gt;Explore Crypto Newsarticle dataset, unsupervised Keyword extraction, Stemming, Emotion/Sentiment distribution Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/NYC_DC_NLP_MEETUP/2_multilingual_translation_with_marian.ipynb""&gt;Translate between more than 300 Languages in 1 line of code with the Marian Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://colab.research.google.com/github/JohnSnowLabs/nlu/blob/master/examples/release_notebooks/NLU1.1.2_Bengali_ner_Hindi_Embeddings_30_new_models.ipynb""&gt;New NLU 1.1.2 Models Showcase Notebooks, Bengali NER, Hindi Embeddings, 30 new_models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;NLU 1.1.2 Bug Fixes&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Fixed a bug that caused NER confidences not beeing extracted&lt;/li&gt;
&lt;li&gt;Fixed a bug that caused nlu.load(&amp;#39;spell&amp;#39;) to crash&lt;/li&gt;
&lt;li&gt;Fixed a bug that caused Uralic/Estonian/ET language models not to be loaded properly&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;New  Easy NLU 1-liners in 1.1.2&lt;/h3&gt;

&lt;h4&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/27/ner_jifs_glove_840B_300d_bn.html""&gt;Named Entity Recognition for Bengali (GloVe 840B 300d)&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;Bengali for :  It began to be widely used in the United States in the early &amp;#39;90s.&lt;/h1&gt;

&lt;p&gt;nlu.load(&amp;quot;bn.ner&amp;quot;).predict(&amp;quot;৯০ এর দশকের শুরুর দিকে বৃহৎ আকারে মার্কিন যুক্তরাষ্ট্রে এর প্রয়োগের প্রক্রিয়া শুরু হয়&amp;#39;&amp;quot;)
```
output :&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""left""&gt;entities&lt;/th&gt;
&lt;th align=""left""&gt;token&lt;/th&gt;
&lt;th align=""left""&gt;Entities_classes&lt;/th&gt;
&lt;th align=""right""&gt;ner_confidence&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;[&amp;#39;মার্কিন যুক্তরাষ্ট্রে&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;৯০&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;LOC&amp;#39;]&lt;/td&gt;
&lt;td align=""right""&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;[&amp;#39;মার্কিন যুক্তরাষ্ট্রে&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;এর&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;LOC&amp;#39;]&lt;/td&gt;
&lt;td align=""right""&gt;0.9999&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;[&amp;#39;মার্কিন যুক্তরাষ্ট্রে&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;দশকের&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;LOC&amp;#39;]&lt;/td&gt;
&lt;td align=""right""&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;[&amp;#39;মার্কিন যুক্তরাষ্ট্রে&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;শুরুর&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;LOC&amp;#39;]&lt;/td&gt;
&lt;td align=""right""&gt;0.9969&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;[&amp;#39;মার্কিন যুক্তরাষ্ট্রে&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;দিকে&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;LOC&amp;#39;]&lt;/td&gt;
&lt;td align=""right""&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;[&amp;#39;মার্কিন যুক্তরাষ্ট্রে&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;বৃহৎ&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;LOC&amp;#39;]&lt;/td&gt;
&lt;td align=""right""&gt;0.9994&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;[&amp;#39;মার্কিন যুক্তরাষ্ট্রে&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;আকারে&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;LOC&amp;#39;]&lt;/td&gt;
&lt;td align=""right""&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;[&amp;#39;মার্কিন যুক্তরাষ্ট্রে&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;মার্কিন&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;LOC&amp;#39;]&lt;/td&gt;
&lt;td align=""right""&gt;0.9602&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;[&amp;#39;মার্কিন যুক্তরাষ্ট্রে&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;যুক্তরাষ্ট্রে&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;LOC&amp;#39;]&lt;/td&gt;
&lt;td align=""right""&gt;0.4134&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;[&amp;#39;মার্কিন যুক্তরাষ্ট্রে&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;এর&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;LOC&amp;#39;]&lt;/td&gt;
&lt;td align=""right""&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;[&amp;#39;মার্কিন যুক্তরাষ্ট্রে&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;প্রয়োগের&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;LOC&amp;#39;]&lt;/td&gt;
&lt;td align=""right""&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;[&amp;#39;মার্কিন যুক্তরাষ্ট্রে&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;প্রক্রিয়া&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;LOC&amp;#39;]&lt;/td&gt;
&lt;td align=""right""&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;[&amp;#39;মার্কিন যুক্তরাষ্ট্রে&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;শুরু&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;LOC&amp;#39;]&lt;/td&gt;
&lt;td align=""right""&gt;0.9999&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;[&amp;#39;মার্কিন যুক্তরাষ্ট্রে&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;হয়&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;LOC&amp;#39;]&lt;/td&gt;
&lt;td align=""right""&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;[&amp;#39;মার্কিন যুক্তরাষ্ট্রে&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;&amp;#39;&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;LOC&amp;#39;]&lt;/td&gt;
&lt;td align=""right""&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/20/lemma_bn.html""&gt;Bengali Lemmatizer&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;Bengali for :  One morning in the marble-decorated building of Vaidyanatha, an obese monk was engaged in the enchantment of Duis and the milk service of one and a half Vaidyanatha. Give me two to eat&lt;/h1&gt;

&lt;p&gt;nlu.load(&amp;quot;bn.lemma&amp;quot;).predict(&amp;quot;একদিন প্রাতে বৈদ্যনাথের মার্বলমণ্ডিত দালানে একটি স্থূলোদর সন্ন্যাসী দুইসের মোহনভোগ এবং দেড়সের দুগ্ধ সেবায় নিযুক্ত আছে বৈদ্যনাথ গায়ে একখানি চাদর দিয়া জোড়করে একান্ত বিনীতভাবে ভূতলে বসিয়া ভক্তিভরে পবিত্র ভোজনব্যাপার নিরীক্ষণ করিতেছিলেন এমন সময় কোনোমতে দ্বারীদের দৃষ্টি এড়াইয়া জীর্ণদেহ বালক সহিত একটি অতি শীর্ণকায়া রমণী গৃহে প্রবেশ করিয়া ক্ষীণস্বরে কহিল বাবু দুটি খেতে দাও&amp;quot;)&lt;/p&gt;

&lt;p&gt;```
output :&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""left""&gt;lemma&lt;/th&gt;
&lt;th align=""left""&gt;document&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;[&amp;#39;একদিন&amp;#39;, &amp;#39;প্রাতঃ&amp;#39;, &amp;#39;বৈদ্যনাথ&amp;#39;, &amp;#39;মার্বলমণ্ডিত&amp;#39;, &amp;#39;দালান&amp;#39;, &amp;#39;এক&amp;#39;, &amp;#39;স্থূলউদর&amp;#39;, &amp;#39;সন্ন্যাসী&amp;#39;, &amp;#39;দুইসের&amp;#39;, &amp;#39;মোহনভোগ&amp;#39;, &amp;#39;এবং&amp;#39;, &amp;#39;দেড়সের&amp;#39;, &amp;#39;দুগ্ধ&amp;#39;, &amp;#39;সেবা&amp;#39;, &amp;#39;নিযুক্ত&amp;#39;, &amp;#39;আছে&amp;#39;, &amp;#39;বৈদ্যনাথ&amp;#39;, &amp;#39;গা&amp;#39;, &amp;#39;একখান&amp;#39;, &amp;#39;চাদর&amp;#39;, &amp;#39;দেওয়া&amp;#39;, &amp;#39;জোড়কর&amp;#39;, &amp;#39;একান্ত&amp;#39;, &amp;#39;বিনীতভাব&amp;#39;, &amp;#39;ভূতল&amp;#39;, &amp;#39;বসা&amp;#39;, &amp;#39;ভক্তিভরা&amp;#39;, &amp;#39;পবিত্র&amp;#39;, &amp;#39;ভোজনব্যাপার&amp;#39;, &amp;#39;নিরীক্ষণ&amp;#39;, &amp;#39;করা&amp;#39;, &amp;#39;এমন&amp;#39;, &amp;#39;সময়&amp;#39;, &amp;#39;কোনোমত&amp;#39;, &amp;#39;দ্বারী&amp;#39;, &amp;#39;দৃষ্টি&amp;#39;, &amp;#39;এড়ানো&amp;#39;, &amp;#39;জীর্ণদেহ&amp;#39;, &amp;#39;বালক&amp;#39;, &amp;#39;সহিত&amp;#39;, &amp;#39;এক&amp;#39;, &amp;#39;অতি&amp;#39;, &amp;#39;শীর্ণকায়া&amp;#39;, &amp;#39;রমণী&amp;#39;, &amp;#39;গৃহ&amp;#39;, &amp;#39;প্রবেশ&amp;#39;, &amp;#39;বিশ্বাস&amp;#39;, &amp;#39;ক্ষীণস্বর&amp;#39;, &amp;#39;কহা&amp;#39;, &amp;#39;বাবু&amp;#39;, &amp;#39;দুই&amp;#39;, &amp;#39;খাওয়া&amp;#39;, &amp;#39;দাওয়া&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;একদিন প্রাতে বৈদ্যনাথের মার্বলমণ্ডিত দালানে একটি স্থূলোদর সন্ন্যাসী দুইসের মোহনভোগ এবং দেড়সের দুগ্ধ সেবায় নিযুক্ত আছে বৈদ্যনাথ গায়ে একখানি চাদর দিয়া জোড়করে একান্ত বিনীতভাবে ভূতলে বসিয়া ভক্তিভরে পবিত্র ভোজনব্যাপার নিরীক্ষণ করিতেছিলেন এমন সময় কোনোমতে দ্বারীদের দৃষ্টি এড়াইয়া জীর্ণদেহ বালক সহিত একটি অতি শীর্ণকায়া রমণী গৃহে প্রবেশ করিয়া ক্ষীণস্বরে কহিল বাবু দুটি খেতে দাও&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/15/lemma_ja.html""&gt;Japanese Lemmatizer&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;Japanese for :  Some residents were uncomfortable with this, but it seems that no one is now openly protesting or protesting.&lt;/h1&gt;

&lt;p&gt;nlu.load(&amp;quot;ja.lemma&amp;quot;).predict(&amp;quot;これに不快感を示す住民はいましたが,現在,表立って反対や抗議の声を挙げている住民はいないようです。&amp;quot;)&lt;/p&gt;

&lt;p&gt;```
output :&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""left""&gt;lemma&lt;/th&gt;
&lt;th align=""left""&gt;document&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;[&amp;#39;これ&amp;#39;, &amp;#39;にる&amp;#39;, &amp;#39;不快&amp;#39;, &amp;#39;感&amp;#39;, &amp;#39;を&amp;#39;, &amp;#39;示す&amp;#39;, &amp;#39;住民&amp;#39;, &amp;#39;はる&amp;#39;, &amp;#39;いる&amp;#39;, &amp;#39;まする&amp;#39;, &amp;#39;たる&amp;#39;, &amp;#39;がる&amp;#39;, &amp;#39;,&amp;#39;, &amp;#39;現在&amp;#39;, &amp;#39;,&amp;#39;, &amp;#39;表立つ&amp;#39;, &amp;#39;てる&amp;#39;, &amp;#39;反対&amp;#39;, &amp;#39;やる&amp;#39;, &amp;#39;抗議&amp;#39;, &amp;#39;のる&amp;#39;, &amp;#39;声&amp;#39;, &amp;#39;を&amp;#39;, &amp;#39;挙げる&amp;#39;, &amp;#39;てる&amp;#39;, &amp;#39;いる&amp;#39;, &amp;#39;住民&amp;#39;, &amp;#39;はる&amp;#39;, &amp;#39;いる&amp;#39;, &amp;#39;なぐ&amp;#39;, &amp;#39;よう&amp;#39;, &amp;#39;です&amp;#39;, &amp;#39;。&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;これに不快感を示す住民はいましたが,現在,表立って反対や抗議の声を挙げている住民はいないようです。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/20/lemma_am.html""&gt;Aharic Lemmatizer&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;Aharic for :  Bookmark the permalink.&lt;/h1&gt;

&lt;p&gt;nlu.load(&amp;quot;am.lemma&amp;quot;).predict(&amp;quot;መጽሐፉን መጽሐፍ ኡ ን አስያዛት አስያዝ ኧ ኣት ።&amp;quot;)&lt;/p&gt;

&lt;p&gt;```
output  :&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""left""&gt;lemma&lt;/th&gt;
&lt;th align=""left""&gt;document&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;[&amp;#39;&lt;em&gt;&amp;#39;, &amp;#39;መጽሐፍ&amp;#39;, &amp;#39;ኡ&amp;#39;, &amp;#39;ን&amp;#39;, &amp;#39;&lt;/em&gt;&amp;#39;, &amp;#39;አስያዝ&amp;#39;, &amp;#39;ኧ&amp;#39;, &amp;#39;ኣት&amp;#39;, &amp;#39;።&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;መጽሐፉን መጽሐፍ ኡ ን አስያዛት አስያዝ ኧ ኣት ።&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/18/lemma_bh.html""&gt;Bhojpuri Lemmatizer&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;Bhojpuri for : In this event, participation of World Bhojpuri Conference, Purvanchal Ekta Manch, Veer Kunwar Singh Foundation, Purvanchal Bhojpuri Mahasabha, and Herf - Media.&lt;/h1&gt;

&lt;p&gt;nlu.load(&amp;quot;bh.lemma&amp;quot;).predict(&amp;quot;एह आयोजन में विश्व भोजपुरी सम्मेलन , पूर्वांचल एकता मंच , वीर कुँवर सिंह फाउन्डेशन , पूर्वांचल भोजपुरी महासभा , अउर हर्फ - मीडिया के सहभागिता बा ।&amp;quot;)
```&lt;/p&gt;

&lt;p&gt;output :&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""left""&gt;lemma&lt;/th&gt;
&lt;th align=""left""&gt;document&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;[&amp;#39;एह&amp;#39;, &amp;#39;आयोजन&amp;#39;, &amp;#39;में&amp;#39;, &amp;#39;विश्व&amp;#39;, &amp;#39;भोजपुरी&amp;#39;, &amp;#39;सम्मेलन&amp;#39;, &amp;#39;COMMA&amp;#39;, &amp;#39;पूर्वांचल&amp;#39;, &amp;#39;एकता&amp;#39;, &amp;#39;मंच&amp;#39;, &amp;#39;COMMA&amp;#39;, &amp;#39;वीर&amp;#39;, &amp;#39;कुँवर&amp;#39;, &amp;#39;सिंह&amp;#39;, &amp;#39;फाउन्डेशन&amp;#39;, &amp;#39;COMMA&amp;#39;, &amp;#39;पूर्वांचल&amp;#39;, &amp;#39;भोजपुरी&amp;#39;, &amp;#39;महासभा&amp;#39;, &amp;#39;COMMA&amp;#39;, &amp;#39;अउर&amp;#39;, &amp;#39;हर्फ&amp;#39;, &amp;#39;-&amp;#39;, &amp;#39;मीडिया&amp;#39;, &amp;#39;को&amp;#39;, &amp;#39;सहभागिता&amp;#39;, &amp;#39;बा&amp;#39;, &amp;#39;।&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;एह आयोजन में विश्व भोजपुरी सम्मेलन , पूर्वांचल एकता मंच , वीर कुँवर सिंह फाउन्डेशन , पूर्वांचल भोजपुरी महासभा , अउर हर्फ - मीडिया के सहभागिता बा ।&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L2_128_en.html""&gt;Named Entity Recognition - BERT Tiny (OntoNotes)&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;python
nlu.load(&amp;quot;en.ner.onto.bert.small_l2_128&amp;quot;).predict(&amp;quot;&amp;quot;&amp;quot;William Henry Gates III (born October 28, 1955) is an American business magnate,
 software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft,
  Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect,
   while also being the largest individual shareholder until May 2014.
    He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico;
     it went on to become the world&amp;#39;s largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect.
     During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time
      role at Microsoft and full-time work at the Bill &amp;amp; Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.
 He gradually transferred his duties to Ray Ozzie and Craig Mundie.
  He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella.&amp;quot;&amp;quot;&amp;quot;,output_level = &amp;quot;document&amp;quot;)
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;output  :&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""left""&gt;ner_confidence&lt;/th&gt;
&lt;th align=""left""&gt;entities&lt;/th&gt;
&lt;th align=""left""&gt;Entities_classes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;[0.8536999821662903, 0.7195000052452087, 0.746...]&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;PERSON&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;NORP&amp;#39;, &amp;#39;ORG&amp;#39;, &amp;#39;ORG&amp;#39;, &amp;#39;PERSON&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;CARDINAL&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;GPE&amp;#39;, &amp;#39;GPE&amp;#39;, &amp;#39;PERSON&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;GPE&amp;#39;, &amp;#39;GPE&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;William Henry Gates III&amp;#39;, &amp;#39;October 28, 1955&amp;#39;, &amp;#39;American&amp;#39;, &amp;#39;Microsoft Corporation&amp;#39;, &amp;#39;Microsoft&amp;#39;, &amp;#39;Gates&amp;#39;, &amp;#39;May 2014&amp;#39;, &amp;#39;one&amp;#39;, &amp;#39;1970s&amp;#39;, &amp;#39;1980s&amp;#39;, &amp;#39;Seattle&amp;#39;, &amp;#39;Washington&amp;#39;, &amp;#39;Paul Allen&amp;#39;, &amp;#39;1975&amp;#39;, &amp;#39;Albuquerque&amp;#39;, &amp;#39;New Mexico&amp;#39;]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L4_256_en.html""&gt;Named Entity Recognition - BERT Mini (OntoNotes)&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;python
nlu.load(&amp;quot;en.ner.onto.bert.small_l4_256&amp;quot;).predict(&amp;quot;&amp;quot;&amp;quot;William Henry Gates III (born October 28, 1955) is an American business magnate,
 software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft,
  Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect,
   while also being the largest individual shareholder until May 2014.
    He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico;
     it went on to become the world&amp;#39;s largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect.
     During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time
      role at Microsoft and full-time work at the Bill &amp;amp; Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.
 He gradually transferred his duties to Ray Ozzie and Craig Mundie.
  He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella.&amp;quot;&amp;quot;&amp;quot;,output_level = &amp;quot;document&amp;quot;)
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;output :&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""right""&gt;ner_confidence&lt;/th&gt;
&lt;th align=""left""&gt;entities&lt;/th&gt;
&lt;th align=""left""&gt;Entities_classes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""right""&gt;[0.835099995136261, 0.40450000762939453, 0.331...]&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;William Henry Gates III&amp;#39;, &amp;#39;October 28, 1955&amp;#39;, &amp;#39;American&amp;#39;, &amp;#39;Microsoft Corporation&amp;#39;, &amp;#39;Microsoft&amp;#39;, &amp;#39;Gates&amp;#39;, &amp;#39;May 2014&amp;#39;, &amp;#39;one&amp;#39;, &amp;#39;1970s and 1980s&amp;#39;, &amp;#39;Seattle&amp;#39;, &amp;#39;Washington&amp;#39;, &amp;#39;Gates&amp;#39;, &amp;#39;Microsoft&amp;#39;, &amp;#39;Paul Allen&amp;#39;, &amp;#39;1975&amp;#39;, &amp;#39;Albuquerque&amp;#39;, &amp;#39;New Mexico&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;PERSON&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;NORP&amp;#39;, &amp;#39;ORG&amp;#39;, &amp;#39;ORG&amp;#39;, &amp;#39;ORG&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;CARDINAL&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;GPE&amp;#39;, &amp;#39;GPE&amp;#39;, &amp;#39;ORG&amp;#39;, &amp;#39;ORG&amp;#39;, &amp;#39;PERSON&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;GPE&amp;#39;, &amp;#39;GPE&amp;#39;]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L4_512_en.html""&gt;Named Entity Recognition - BERT Small (OntoNotes)&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;python
nlu.load(&amp;quot;en.ner.onto.bert.small_l4_512&amp;quot;).predict(&amp;quot;&amp;quot;&amp;quot;William Henry Gates III (born October 28, 1955) is an American business magnate,
 software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft,
  Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect,
   while also being the largest individual shareholder until May 2014.
    He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico;
     it went on to become the world&amp;#39;s largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect.
     During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time
      role at Microsoft and full-time work at the Bill &amp;amp; Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.
 He gradually transferred his duties to Ray Ozzie and Craig Mundie.
  He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella.&amp;quot;&amp;quot;&amp;quot;,output_level = &amp;quot;document&amp;quot;)
&lt;/code&gt;
output :&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""right""&gt;ner_confidence&lt;/th&gt;
&lt;th align=""left""&gt;entities&lt;/th&gt;
&lt;th align=""left""&gt;Entities_classes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""right""&gt;[0.964900016784668, 0.8299000263214111, 0.9607...]&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;William Henry Gates III&amp;#39;, &amp;#39;October 28, 1955&amp;#39;, &amp;#39;American&amp;#39;, &amp;#39;Microsoft Corporation&amp;#39;, &amp;#39;Microsoft&amp;#39;, &amp;#39;Gates&amp;#39;, &amp;#39;May 2014&amp;#39;, &amp;#39;one&amp;#39;, &amp;#39;the 1970s and 1980s&amp;#39;, &amp;#39;Seattle&amp;#39;, &amp;#39;Washington&amp;#39;, &amp;#39;Gates&amp;#39;, &amp;#39;Microsoft&amp;#39;, &amp;#39;Paul Allen&amp;#39;, &amp;#39;1975&amp;#39;, &amp;#39;Albuquerque&amp;#39;, &amp;#39;New Mexico&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;PERSON&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;NORP&amp;#39;, &amp;#39;ORG&amp;#39;, &amp;#39;ORG&amp;#39;, &amp;#39;PERSON&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;CARDINAL&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;GPE&amp;#39;, &amp;#39;GPE&amp;#39;, &amp;#39;PERSON&amp;#39;, &amp;#39;ORG&amp;#39;, &amp;#39;PERSON&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;GPE&amp;#39;, &amp;#39;GPE&amp;#39;]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_small_bert_L8_512_en.html""&gt;Named Entity Recognition - BERT Medium (OntoNotes)&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;python
nlu.load(&amp;quot;en.ner.onto.bert.small_l8_512&amp;quot;).predict(&amp;quot;&amp;quot;&amp;quot;William Henry Gates III (born October 28, 1955) is an American business magnate,
 software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft,
  Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect,
   while also being the largest individual shareholder until May 2014.
    He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico;
     it went on to become the world&amp;#39;s largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect.
     During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time
      role at Microsoft and full-time work at the Bill &amp;amp; Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.
 He gradually transferred his duties to Ray Ozzie and Craig Mundie.
  He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella.&amp;quot;&amp;quot;&amp;quot;,output_level = &amp;quot;document&amp;quot;)
&lt;/code&gt;
output :&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""right""&gt;ner_confidence&lt;/th&gt;
&lt;th align=""left""&gt;entities&lt;/th&gt;
&lt;th align=""left""&gt;Entities_classes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""right""&gt;[0.916700005531311, 0.5873000025749207, 0.8816...]&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;William Henry Gates III&amp;#39;, &amp;#39;October 28, 1955&amp;#39;, &amp;#39;American&amp;#39;, &amp;#39;Microsoft Corporation&amp;#39;, &amp;#39;Microsoft&amp;#39;, &amp;#39;Gates&amp;#39;, &amp;#39;May 2014&amp;#39;, &amp;#39;the 1970s and 1980s&amp;#39;, &amp;#39;Seattle&amp;#39;, &amp;#39;Washington&amp;#39;, &amp;#39;Gates&amp;#39;, &amp;#39;Paul Allen&amp;#39;, &amp;#39;1975&amp;#39;, &amp;#39;Albuquerque&amp;#39;, &amp;#39;New Mexico&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;PERSON&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;NORP&amp;#39;, &amp;#39;ORG&amp;#39;, &amp;#39;ORG&amp;#39;, &amp;#39;PERSON&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;GPE&amp;#39;, &amp;#39;GPE&amp;#39;, &amp;#39;PERSON&amp;#39;, &amp;#39;PERSON&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;GPE&amp;#39;, &amp;#39;GPE&amp;#39;]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/onto_bert_base_cased_en.html""&gt;Named Entity Recognition - BERT Base (OntoNotes)&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;python
nlu.load(&amp;quot;en.ner.onto.bert.cased_base&amp;quot;).predict(&amp;quot;&amp;quot;&amp;quot;William Henry Gates III (born October 28, 1955) is an American business magnate,
 software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft,
  Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect,
   while also being the largest individual shareholder until May 2014.
    He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico;
     it went on to become the world&amp;#39;s largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect.
     During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time
      role at Microsoft and full-time work at the Bill &amp;amp; Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.
 He gradually transferred his duties to Ray Ozzie and Craig Mundie.
  He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella.&amp;quot;&amp;quot;&amp;quot;,output_level = &amp;quot;document&amp;quot;)
&lt;/code&gt;
output :&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""right""&gt;ner_confidence&lt;/th&gt;
&lt;th align=""left""&gt;entities&lt;/th&gt;
&lt;th align=""left""&gt;Entities_classes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""right""&gt;[0.504800021648407, 0.47290000319480896, 0.462...]&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;William Henry Gates III&amp;#39;, &amp;#39;October 28, 1955&amp;#39;, &amp;#39;American&amp;#39;, &amp;#39;Microsoft Corporation&amp;#39;, &amp;#39;Microsoft&amp;#39;, &amp;#39;Gates&amp;#39;, &amp;#39;May 2014&amp;#39;, &amp;#39;one&amp;#39;, &amp;#39;the 1970s and 1980s&amp;#39;, &amp;#39;Seattle&amp;#39;, &amp;#39;Washington&amp;#39;, &amp;#39;Gates&amp;#39;, &amp;#39;Microsoft&amp;#39;, &amp;#39;Paul Allen&amp;#39;, &amp;#39;1975&amp;#39;, &amp;#39;Albuquerque&amp;#39;, &amp;#39;New Mexico&amp;#39;]&lt;/td&gt;
&lt;td align=""left""&gt;[&amp;#39;PERSON&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;NORP&amp;#39;, &amp;#39;ORG&amp;#39;, &amp;#39;ORG&amp;#39;, &amp;#39;PERSON&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;CARDINAL&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;GPE&amp;#39;, &amp;#39;GPE&amp;#39;, &amp;#39;PERSON&amp;#39;, &amp;#39;ORG&amp;#39;, &amp;#39;PERSON&amp;#39;, &amp;#39;DATE&amp;#39;, &amp;#39;GPE&amp;#39;, &amp;#39;GPE&amp;#39;]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h3&gt;NLU Installation&lt;/h3&gt;

&lt;p&gt;```bash&lt;/p&gt;

&lt;h1&gt;PyPi&lt;/h1&gt;

&lt;p&gt;!pip install nlu pyspark==2.4.7&lt;/p&gt;

&lt;h1&gt;Conda&lt;/h1&gt;

&lt;h1&gt;Install NLU from Anaconda/Conda&lt;/h1&gt;

&lt;p&gt;conda install -c johnsnowlabs nlu
```&lt;/p&gt;

&lt;h3&gt;Additional NLU ressources&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/""&gt;NLU Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/docs/en/notebooks""&gt;All NLU Tutorial Notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlp.johnsnowlabs.com/learn#pythons-nlu-library""&gt;NLU Videos and Blogposts on NLU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu""&gt;NLU on Github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lsckqc,True,,CKL-IT,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lsckqc/train_multilingual_classifier_for_100_languages/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lsckqc/train_multilingual_classifier_for_100_languages/,30199,1614275914.0,0,,False,,,,,,33270
669,,LanguageTechnology,,t2_5alofkdd,False,,0,False,"OpenAI has released the paper associated with DALL-E: ""Zero-Shot Text-to-Image Generation""",[],r/LanguageTechnology,False,6,,0,,False,t3_lrxmgl,False,dark,0.97,,public,30,0,{},,False,[],,False,False,,{},,False,30,,False,False,,False,,[],{},,False,,1614254430.0,text,6,,,text,arxiv.org,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lrxmgl,True,,Wiskkey,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lrxmgl/openai_has_released_the_paper_associated_with/,all_ads,False,https://arxiv.org/abs/2102.12092,30199,1614225630.0,0,,False,https://arxiv.org/abs/2102.12092,,,,,0
670,,LanguageTechnology,,t2_21qaqh1p,False,,0,False,MBart Language Translation (English to Multilanguage) using Hugging Face,[],r/LanguageTechnology,False,6,,0,,False,t3_lsdh5m,False,dark,1.0,,public,2,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/oO7k5lH8Oe8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'MBart Language Translation (English to Tamil/Hindi) using Hugging Face | Python NLP', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/oO7k5lH8Oe8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/oO7k5lH8Oe8/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/oO7k5lH8Oe8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lsdh5m', 'height': 200}",,False,2,,False,False,,False,,[],{},,False,,1614307060.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lsdh5m,True,,dulldata,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lsdh5m/mbart_language_translation_english_to/,all_ads,False,https://youtu.be/oO7k5lH8Oe8,30199,1614278260.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'MBart Language Translation (English to Tamil/Hindi) using Hugging Face | Python NLP', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/oO7k5lH8Oe8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/oO7k5lH8Oe8/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,https://youtu.be/oO7k5lH8Oe8,,,,,0
671,,LanguageTechnology,"I was trying to catch up with transformer and gpt-2 implementation detail. So I have created some scripts to support retraining gpt-2 model using tensorflow2: [https://github.com/wenxichen/gpt-2](https://github.com/wenxichen/gpt-2)

Have fun!",t2_12jyfh,False,,0,False,Retrain GPT-2 model with tensorflow2,[],r/LanguageTechnology,False,6,,0,,False,t3_ls3tqc,False,dark,0.88,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1614278601.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was trying to catch up with transformer and gpt-2 implementation detail. So I have created some scripts to support retraining gpt-2 model using tensorflow2: &lt;a href=""https://github.com/wenxichen/gpt-2""&gt;https://github.com/wenxichen/gpt-2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Have fun!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ls3tqc,True,,wencc,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ls3tqc/retrain_gpt2_model_with_tensorflow2/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ls3tqc/retrain_gpt2_model_with_tensorflow2/,30199,1614249801.0,1,,False,,,,,,242
672,,LanguageTechnology,"I have short audio segments, lasting 5-10 seconds each, of a single speaker speaking a single sentence. I need gender recognition on this. I need an algorithm that I feed an audio file into, and it spit out a result of if this is a male speaker, or this is a female speaker. And I need to run this on tens of thousands of audio files in a dataset.

How do I do this? What features do I use to determine this? Do I need machine learning or can I use some fixed numbers to figure it out? How do I access these stats/numbers that I need to figure this out?",t2_akky2zpq,False,,0,False,How do you make a model for gender recognition of speech in an audio file?,[],r/LanguageTechnology,False,6,,0,,False,t3_lsapbl,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1614299954.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have short audio segments, lasting 5-10 seconds each, of a single speaker speaking a single sentence. I need gender recognition on this. I need an algorithm that I feed an audio file into, and it spit out a result of if this is a male speaker, or this is a female speaker. And I need to run this on tens of thousands of audio files in a dataset.&lt;/p&gt;

&lt;p&gt;How do I do this? What features do I use to determine this? Do I need machine learning or can I use some fixed numbers to figure it out? How do I access these stats/numbers that I need to figure this out?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lsapbl,True,,ionhfh,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lsapbl/how_do_you_make_a_model_for_gender_recognition_of/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lsapbl/how_do_you_make_a_model_for_gender_recognition_of/,30199,1614271154.0,0,,False,,,,,,553
673,,LanguageTechnology,"Hello, I'm quite new to parsing in NLP and I'm still confuse about some stuff.

1) I saw that CKY algorithm only work with CNF grammar, but on some research paper i saw they use CKY with PCFG and CCG. Can PCFG and CCG, be transformed into CNF?

2) I would like to make a parser from scratch, using my own lexicon and grammar extracted from a corpus in my language, with probabilistic method. I wanted to start with probabilistic CKY is it a good idea?

Thanks in advance Have a nice day!",t2_9nnxxp7o,False,,0,False,Parsing with statistical method,[],r/LanguageTechnology,False,6,,0,,False,t3_ls70lb,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1614289873.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I&amp;#39;m quite new to parsing in NLP and I&amp;#39;m still confuse about some stuff.&lt;/p&gt;

&lt;p&gt;1) I saw that CKY algorithm only work with CNF grammar, but on some research paper i saw they use CKY with PCFG and CCG. Can PCFG and CCG, be transformed into CNF?&lt;/p&gt;

&lt;p&gt;2) I would like to make a parser from scratch, using my own lexicon and grammar extracted from a corpus in my language, with probabilistic method. I wanted to start with probabilistic CKY is it a good idea?&lt;/p&gt;

&lt;p&gt;Thanks in advance Have a nice day!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ls70lb,True,,thib610,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ls70lb/parsing_with_statistical_method/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ls70lb/parsing_with_statistical_method/,30199,1614261073.0,0,,False,,,,,,487
674,,LanguageTechnology,,t2_5alofkdd,False,,0,False,"Text-to-image Google Colab notebook ""Aleph-Image: CLIPxDAll-E"" has been released. This notebook uses OpenAI's CLIP neural network to steer OpenAI's DALL-E image generator to try to match a given text description.",[],r/LanguageTechnology,False,6,,0,,False,t3_ls0nhf,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,False,,1614265690.0,text,6,,,text,self.MachineLearning,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ls0nhf,True,,Wiskkey,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ls0nhf/texttoimage_google_colab_notebook_alephimage/,all_ads,False,/r/MachineLearning/comments/ls0e0f/p_texttoimage_google_colab_notebook_alephimage/,30199,1614236890.0,0,,False,/r/MachineLearning/comments/ls0e0f/p_texttoimage_google_colab_notebook_alephimage/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': '[Google Colab notebook](https://colab.research.google.com/drive/1Q-TbYvASMPRMXCOQjkxxf72CXYjR_8Vp?usp=sharing). [Twitter reference](https://twitter.com/advadnoun/status/1364822183751471109).\n\n**Update**: ""DALL-E image generator"" in the post title is a reference to the [discrete VAE (variational autoencoder) used for DALL-E](https://www.reddit.com/r/MachineLearning/comments/lrroom/n_openai_has_released_the_encoder_and_decoder_for/). OpenAI [will not](https://github.com/openai/DALL-E/issues/4) release DALL-E in its entirety.\n\n**Update**: [A tweet from the developer](https://twitter.com/advadnoun/status/1364936070505238534), in reference to the white blotches in output images that often happen with the current version of notebook:\n\n&gt;Well, the white blotches have disappeared; more work to be done yet, but that\'s not bad!\n\n**Update**: Thanks to the users in the comments who suggested a temporary [developer-suggested fix](https://twitter.com/advadnoun/status/1364939598384623617) to reduce white blotches. To make this fix, change the line in ""Latent Coordinate"" that reads\n\n    normu = torch.nn.functional.gumbel_softmax(self.normu.view(1, 8192, -1), dim=-1).view(1, 8192, 64, 64)\n\nto\n\n    normu = torch.nn.functional.gumbel_softmax(self.normu.view(1, 8192, -1), dim=-1, tau = 1.5).view(1, 8192, 64, 64)\n\nby adding "", tau = 1.5"" (without quotes) after ""dim=-1"". The higher this parameter value is, apparently the lower the chance is of white blotches, but with the tradeoff of less sharpness. Some people have suggested trying 1.2, 1.7, or 2 instead of 1.5.\n\nI am not affiliated with this notebook or its developer.\n\nSee also: [List of sites/programs/projects that use OpenAI\'s CLIP neural network for steering image/video creation to match a text description](https://www.reddit.com/r/MachineLearning/comments/ldc6oc/p_list_of_sitesprogramsprojects_that_use_openais/).\n\nExample using text ""The boundary between consciousness and unconsciousness"":\n\nhttps://preview.redd.it/93r6bonvlkj61.png?width=512&amp;format=png&amp;auto=webp&amp;s=6505c22e4b02fe863054b081f662ee9f5db2bc95', 'author_fullname': 't2_5alofkdd', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[P] Text-to-image Google Colab notebook ""Aleph-Image: CLIPxDAll-E"" has been released. This notebook uses OpenAI\'s CLIP neural network to steer OpenAI\'s DALL-E image generator to try to match a given text description.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'four', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'93r6bonvlkj61': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 108, 'x': 108, 'u': 'https://preview.redd.it/93r6bonvlkj61.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad640b8a6a1720f04db88e7350cc2795e6161f57'}, {'y': 216, 'x': 216, 'u': 'https://preview.redd.it/93r6bonvlkj61.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=014f663338fef46951d9046647acb8e6aa6f1426'}, {'y': 320, 'x': 320, 'u': 'https://preview.redd.it/93r6bonvlkj61.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1c9102008898bb4fa131617b88627f94854ef321'}], 's': {'y': 512, 'x': 512, 'u': 'https://preview.redd.it/93r6bonvlkj61.png?width=512&amp;format=png&amp;auto=webp&amp;s=6505c22e4b02fe863054b081f662ee9f5db2bc95'}, 'id': '93r6bonvlkj61'}}, 'name': 't3_ls0e0f', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 140, 'total_awards_received': 3, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Project', 'can_mod_post': False, 'score': 140, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': 1614311067.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1614264618.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://colab.research.google.com/drive/1Q-TbYvASMPRMXCOQjkxxf72CXYjR_8Vp?usp=sharing""&gt;Google Colab notebook&lt;/a&gt;. &lt;a href=""https://twitter.com/advadnoun/status/1364822183751471109""&gt;Twitter reference&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: &amp;quot;DALL-E image generator&amp;quot; in the post title is a reference to the &lt;a href=""https://www.reddit.com/r/MachineLearning/comments/lrroom/n_openai_has_released_the_encoder_and_decoder_for/""&gt;discrete VAE (variational autoencoder) used for DALL-E&lt;/a&gt;. OpenAI &lt;a href=""https://github.com/openai/DALL-E/issues/4""&gt;will not&lt;/a&gt; release DALL-E in its entirety.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: &lt;a href=""https://twitter.com/advadnoun/status/1364936070505238534""&gt;A tweet from the developer&lt;/a&gt;, in reference to the white blotches in output images that often happen with the current version of notebook:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Well, the white blotches have disappeared; more work to be done yet, but that&amp;#39;s not bad!&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: Thanks to the users in the comments who suggested a temporary &lt;a href=""https://twitter.com/advadnoun/status/1364939598384623617""&gt;developer-suggested fix&lt;/a&gt; to reduce white blotches. To make this fix, change the line in &amp;quot;Latent Coordinate&amp;quot; that reads&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;normu = torch.nn.functional.gumbel_softmax(self.normu.view(1, 8192, -1), dim=-1).view(1, 8192, 64, 64)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;to&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;normu = torch.nn.functional.gumbel_softmax(self.normu.view(1, 8192, -1), dim=-1, tau = 1.5).view(1, 8192, 64, 64)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;by adding &amp;quot;, tau = 1.5&amp;quot; (without quotes) after &amp;quot;dim=-1&amp;quot;. The higher this parameter value is, apparently the lower the chance is of white blotches, but with the tradeoff of less sharpness. Some people have suggested trying 1.2, 1.7, or 2 instead of 1.5.&lt;/p&gt;\n\n&lt;p&gt;I am not affiliated with this notebook or its developer.&lt;/p&gt;\n\n&lt;p&gt;See also: &lt;a href=""https://www.reddit.com/r/MachineLearning/comments/ldc6oc/p_list_of_sitesprogramsprojects_that_use_openais/""&gt;List of sites/programs/projects that use OpenAI&amp;#39;s CLIP neural network for steering image/video creation to match a text description&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Example using text &amp;quot;The boundary between consciousness and unconsciousness&amp;quot;:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/93r6bonvlkj61.png?width=512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6505c22e4b02fe863054b081f662ee9f5db2bc95""&gt;https://preview.redd.it/93r6bonvlkj61.png?width=512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6505c22e4b02fe863054b081f662ee9f5db2bc95&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 3, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'ls0e0f', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Wiskkey', 'discussion_type': None, 'num_comments': 49, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/ls0e0f/p_texttoimage_google_colab_notebook_alephimage/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/ls0e0f/p_texttoimage_google_colab_notebook_alephimage/', 'subreddit_subscribers': 1930715, 'created_utc': 1614235818.0, 'num_crossposts': 6, 'media': None, 'is_video': False}]",t3_ls0e0f,,,0
675,,LanguageTechnology,"mt-archive.info is a really neat site, it's an archive of papers on machine translation. However, they're only in English. I know such research has been going on outside the Anglosphere as well since the beginning, do you know where I can find similar archives in other languages?",t2_kxps2,False,,0,False,Looking for machine translation papers in other languages,[],r/LanguageTechnology,False,6,,0,,False,t3_ls4kus,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1614281582.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;mt-archive.info is a really neat site, it&amp;#39;s an archive of papers on machine translation. However, they&amp;#39;re only in English. I know such research has been going on outside the Anglosphere as well since the beginning, do you know where I can find similar archives in other languages?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ls4kus,True,,Terpomo11,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ls4kus/looking_for_machine_translation_papers_in_other/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ls4kus/looking_for_machine_translation_papers_in_other/,30199,1614252782.0,1,,False,,,,,,280
676,,LanguageTechnology,"Hey, I've created a tutorial on how to calculate summary statistics by group in the R programming language: [https://statisticsglobe.com/summary-statistics-by-group-in-r](https://statisticsglobe.com/summary-statistics-by-group-in-r)",t2_77cigax1,False,,0,False,Tutorial on how to calculate summary statistics by group,[],r/LanguageTechnology,False,6,,0,,False,t3_ls0a0k,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1614264150.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, I&amp;#39;ve created a tutorial on how to calculate summary statistics by group in the R programming language: &lt;a href=""https://statisticsglobe.com/summary-statistics-by-group-in-r""&gt;https://statisticsglobe.com/summary-statistics-by-group-in-r&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ls0a0k,True,,JoachimSchork,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ls0a0k/tutorial_on_how_to_calculate_summary_statistics/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ls0a0k/tutorial_on_how_to_calculate_summary_statistics/,30199,1614235350.0,0,,False,,,,,,232
677,,LanguageTechnology,,t2_3hn41pel,False,,0,False,Swearing in programming,[],r/LanguageTechnology,False,6,,0,,False,t3_ls0cyn,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,False,,1614264499.0,text,6,,,text,self.github,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ls0cyn,True,,idan_huji,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ls0cyn/swearing_in_programming/,all_ads,False,/r/github/comments/lrznai/swearing_in_programming/,30199,1614235699.0,0,,False,/r/github/comments/lrznai/swearing_in_programming/,"[{'approved_at_utc': None, 'subreddit': 'github', 'selftext': 'I know that it sounds a bit funny, but this is for research.\n\nWe are looking for swear\xa0words that are typical to programming.\n\n[Swearing is related\xa0to quality,](https://arxiv.org/pdf/2007.10912.pdf) retention and more. \n\nWhile finding common swear is easy (see our [current model](https://github.com/evidencebp/commit-classification/blob/master/swearing_model.py)), finding the more unique ones is hard.\n\n&amp;#x200B;\n\nIf you have ideas, please add them [here](https://forms.gle/p9DznqMbCBNr6e1T6).', 'author_fullname': 't2_3hn41pel', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Swearing in programming', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/github', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_lrznai', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 46, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 46, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1614261619.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.github', 'allow_live_comments': True, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know that it sounds a bit funny, but this is for research.&lt;/p&gt;\n\n&lt;p&gt;We are looking for swear\xa0words that are typical to programming.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://arxiv.org/pdf/2007.10912.pdf""&gt;Swearing is related\xa0to quality,&lt;/a&gt; retention and more. &lt;/p&gt;\n\n&lt;p&gt;While finding common swear is easy (see our &lt;a href=""https://github.com/evidencebp/commit-classification/blob/master/swearing_model.py""&gt;current model&lt;/a&gt;), finding the more unique ones is hard.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If you have ideas, please add them &lt;a href=""https://forms.gle/p9DznqMbCBNr6e1T6""&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2s5m1', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'lrznai', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'idan_huji', 'discussion_type': None, 'num_comments': 27, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/github/comments/lrznai/swearing_in_programming/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/github/comments/lrznai/swearing_in_programming/', 'subreddit_subscribers': 51977, 'created_utc': 1614232819.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_lrznai,,,0
678,,LanguageTechnology,I want to create a sentiment analysis for a class using NLTK but not on movie reviews or Twitter data which seem to be the two most popular. Does anyone know where I could find a large set of sentiment tagged restaurant reviews? Thank you,t2_7wo3mdn,False,,0,False,Does anyone know where I could get some tagged pos/neg/neutral restaurant reviews?,[],r/LanguageTechnology,False,6,,0,,False,t3_lrqhlr,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1614238689.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to create a sentiment analysis for a class using NLTK but not on movie reviews or Twitter data which seem to be the two most popular. Does anyone know where I could find a large set of sentiment tagged restaurant reviews? Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lrqhlr,True,,edwardsrk,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lrqhlr/does_anyone_know_where_i_could_get_some_tagged/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lrqhlr/does_anyone_know_where_i_could_get_some_tagged/,30199,1614209889.0,0,,False,,,,,,238
679,,LanguageTechnology,,t2_trjf2,False,,0,False,Recent Advances in Language Model Fine-tuning,[],r/LanguageTechnology,False,6,,0,,False,t3_lr96o0,False,dark,0.98,,public,30,0,{},,False,[],,False,False,,{},,False,30,,False,False,,False,,[],{},,False,,1614189714.0,text,6,,,text,ruder.io,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lr96o0,True,,adammathias,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lr96o0/recent_advances_in_language_model_finetuning/,all_ads,False,https://ruder.io/recent-advances-lm-fine-tuning/,30199,1614160914.0,0,,False,https://ruder.io/recent-advances-lm-fine-tuning/,,,,,0
680,,LanguageTechnology,"Hello! I have a small question about cross lingual encoders (after having read numerous papers and going through so much code, I am still a little lost about something pretty basic lol)

Once we train cross lingual word embeddings (using a mapping method like [VecMap](https://github.com/artetxem/vecmap)), we get two resultant word embeddings: source_mapped and target_mapped. So when we use a ""cross-lingual"" encoder and we just copy the source_mapped embedding parameters into the encoder, how are we utilising the cross-lingual signal exactly? I would understand if the initial encoder mapping is done like source_mapped-&gt;target_mapped-&gt;decoder hidden state-&gt;backward pass-&gt;....-&gt; trained model, but that doesn't seem to be the case. So if someone has experience with cross-lingual encoders, could you explain how exactly it would work/be implement after getting the source and target embeddings mapped (similarly, how it would work if they were trained in a joint fashion)? Thank you so much!!",t2_13s4mru,False,,0,False,How do cross-lingual encoders work?,[],r/LanguageTechnology,False,6,,0,,False,t3_lrnvkh,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,1614203442.0,,[],{},,True,,1614231999.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! I have a small question about cross lingual encoders (after having read numerous papers and going through so much code, I am still a little lost about something pretty basic lol)&lt;/p&gt;

&lt;p&gt;Once we train cross lingual word embeddings (using a mapping method like &lt;a href=""https://github.com/artetxem/vecmap""&gt;VecMap&lt;/a&gt;), we get two resultant word embeddings: source_mapped and target_mapped. So when we use a &amp;quot;cross-lingual&amp;quot; encoder and we just copy the source_mapped embedding parameters into the encoder, how are we utilising the cross-lingual signal exactly? I would understand if the initial encoder mapping is done like source_mapped-&amp;gt;target_mapped-&amp;gt;decoder hidden state-&amp;gt;backward pass-&amp;gt;....-&amp;gt; trained model, but that doesn&amp;#39;t seem to be the case. So if someone has experience with cross-lingual encoders, could you explain how exactly it would work/be implement after getting the source and target embeddings mapped (similarly, how it would work if they were trained in a joint fashion)? Thank you so much!!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lrnvkh,True,,hayis4horses1,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lrnvkh/how_do_crosslingual_encoders_work/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lrnvkh/how_do_crosslingual_encoders_work/,30199,1614203199.0,0,,False,,,,,,1013
681,,LanguageTechnology,,t2_5alofkdd,False,,0,False,Paper: Calibrate Before Use: Improving Few-Shot Performance of GPT-3,[],r/LanguageTechnology,False,6,,0,,False,t3_lrschd,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,False,,1614243008.0,text,6,,,text,self.MachineLearning,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lrschd,True,,Wiskkey,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lrschd/paper_calibrate_before_use_improving_fewshot/,all_ads,False,/r/MachineLearning/comments/lpvb1z/r_calibrate_before_use_improving_fewshot/,30199,1614214208.0,0,,False,/r/MachineLearning/comments/lpvb1z/r_calibrate_before_use_improving_fewshot/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'New paper from BAIR on GPT-3.\n\nPaper: https://arxiv.org/abs/2102.09690\n\nAbstract: GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model\'s bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as ""N/A"". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2\'s average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.', 'author_fullname': 't2_9coqjy16', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[R] Calibrate Before Use: Improving Few-Shot Performance of GPT-3', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'three', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_lpvb1z', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 13, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Research', 'can_mod_post': False, 'score': 13, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1614046028.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;New paper from BAIR on GPT-3.&lt;/p&gt;\n\n&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/abs/2102.09690""&gt;https://arxiv.org/abs/2102.09690&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Abstract: GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model&amp;#39;s bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as &amp;quot;N/A&amp;quot;. We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2&amp;#39;s average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'lpvb1z', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Lanky_Ad2150', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/lpvb1z/r_calibrate_before_use_improving_fewshot/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/lpvb1z/r_calibrate_before_use_improving_fewshot/', 'subreddit_subscribers': 1930718, 'created_utc': 1614017228.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_lpvb1z,,,0
682,,LanguageTechnology,"I'm trying to think of a fun project to do for a class, and I found the problem of anaphora resolution interesting. 

Is this a problem that is essentially ""solved"" or is it still being actively researched? 

What are some state-of-the-art methods? Could I apply BERT? What are some papers I could read about?  Thank you!",t2_4cob5ceh,False,,0,False,Whats the state of art for anaphora resolution?,[],r/LanguageTechnology,False,6,,0,,False,t3_lrmumd,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1614227894.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to think of a fun project to do for a class, and I found the problem of anaphora resolution interesting. &lt;/p&gt;

&lt;p&gt;Is this a problem that is essentially &amp;quot;solved&amp;quot; or is it still being actively researched? &lt;/p&gt;

&lt;p&gt;What are some state-of-the-art methods? Could I apply BERT? What are some papers I could read about?  Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lrmumd,True,,beatleinabox,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lrmumd/whats_the_state_of_art_for_anaphora_resolution/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lrmumd/whats_the_state_of_art_for_anaphora_resolution/,30199,1614199094.0,0,,False,,,,,,321
683,,LanguageTechnology,"I am very interested in this topic but learning and getting academic certifications takes time and effort, as topic, is it worth it?",t2_5on262kw,False,,0,False,Question: What is the commercial value of NLP?,[],r/LanguageTechnology,False,6,,0,,False,t3_lrv476,False,dark,0.2,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,True,,False,,[],{},,True,,1614248422.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am very interested in this topic but learning and getting academic certifications takes time and effort, as topic, is it worth it?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lrv476,True,,W-K-C,,15,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lrv476/question_what_is_the_commercial_value_of_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lrv476/question_what_is_the_commercial_value_of_nlp/,30199,1614219622.0,0,,False,,,,,,132
684,,LanguageTechnology,"I have used pos tagging, stop words removal and lemmatization to pre process the given sentence. After that I've got the output as list of lists. 

    Somethin like this :   [['explain', 'VERB'], ['briefly', 'ADV'], ['working', 'NOUN'], ['merge', 'NOUN'], ['sort', 'NOUN'], ['.', 'PUNCT']] 

Now I have to convert this into feature vectors so that I could pass these vectors as an input to an classification model. But I'm not sure how to use pre processed text as an input to feature vectorizers.

If anybody has any inputs regarding this, it would be helpful. Any links that address the same problem would be appreciated. If it's not the sub to ask these questions, please guide me to the proper sub  
Thanks a lot in advance :)",t2_4gcy0nnj,False,,0,False,How to convert pre-processed text into feature vectors,[],r/LanguageTechnology,False,6,,0,,False,t3_lrj76a,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1614218712.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have used pos tagging, stop words removal and lemmatization to pre process the given sentence. After that I&amp;#39;ve got the output as list of lists. &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Somethin like this :   [[&amp;#39;explain&amp;#39;, &amp;#39;VERB&amp;#39;], [&amp;#39;briefly&amp;#39;, &amp;#39;ADV&amp;#39;], [&amp;#39;working&amp;#39;, &amp;#39;NOUN&amp;#39;], [&amp;#39;merge&amp;#39;, &amp;#39;NOUN&amp;#39;], [&amp;#39;sort&amp;#39;, &amp;#39;NOUN&amp;#39;], [&amp;#39;.&amp;#39;, &amp;#39;PUNCT&amp;#39;]] 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now I have to convert this into feature vectors so that I could pass these vectors as an input to an classification model. But I&amp;#39;m not sure how to use pre processed text as an input to feature vectorizers.&lt;/p&gt;

&lt;p&gt;If anybody has any inputs regarding this, it would be helpful. Any links that address the same problem would be appreciated. If it&amp;#39;s not the sub to ask these questions, please guide me to the proper sub&lt;br/&gt;
Thanks a lot in advance :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lrj76a,True,,suhas_bn_1412,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lrj76a/how_to_convert_preprocessed_text_into_feature/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lrj76a/how_to_convert_preprocessed_text_into_feature/,30199,1614189912.0,0,,False,,,,,,731
685,,LanguageTechnology,"Query expansion is the process of reformulating a given query to improve retrieval performance in information retrieval operations, particularly in the context of query understanding. 🤠
This paper uses BERT to find relevant chunks from the documents as possible augmentations to the given query. 🔥 

Paper Walkthrough: https://youtu.be/WAv6LsIJZbs",t2_hkv9s,False,,0,False,BERT-QE: Contextualized Query Expansion for Document Re-ranking (Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_lr8o8g,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1614187830.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Query expansion is the process of reformulating a given query to improve retrieval performance in information retrieval operations, particularly in the context of query understanding. 🤠
This paper uses BERT to find relevant chunks from the documents as possible augmentations to the given query. 🔥 &lt;/p&gt;

&lt;p&gt;Paper Walkthrough: &lt;a href=""https://youtu.be/WAv6LsIJZbs""&gt;https://youtu.be/WAv6LsIJZbs&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lr8o8g,True,,prakhar21,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lr8o8g/bertqe_contextualized_query_expansion_for/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lr8o8g/bertqe_contextualized_query_expansion_for/,30199,1614159030.0,0,,False,,,,,,347
686,,LanguageTechnology,"I'm a graduate student on the east coast working with a postdoc researcher focused on ML acceleration. We are trying to see what pain points researchers and engineers experience with their ML project to help direct our future research projects.

We have the belief that the team is capable of developing a solution towards accelerating BERT through training times and model sizes.

We are hoping to get some opinions from the community on whether this would be something for us as researchers worthwhile to pursue. We also would be interested in finding teams looking to test this solution in the future.

If you can spare 5 minutes, please help us by filling out this survey: [https://tufts.qualtrics.com/jfe/form/SV\_bI9I7IDELoJveC2](https://tufts.qualtrics.com/jfe/form/SV_bI9I7IDELoJveC2)",t2_acf9z0h5,False,,0,False,Any interest in BERT-related (or LDA) acceleration (5x)? &lt;5-min Survey&gt;,[],r/LanguageTechnology,False,6,,0,,False,t3_lqolib,False,dark,0.76,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,True,,1614131921.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a graduate student on the east coast working with a postdoc researcher focused on ML acceleration. We are trying to see what pain points researchers and engineers experience with their ML project to help direct our future research projects.&lt;/p&gt;

&lt;p&gt;We have the belief that the team is capable of developing a solution towards accelerating BERT through training times and model sizes.&lt;/p&gt;

&lt;p&gt;We are hoping to get some opinions from the community on whether this would be something for us as researchers worthwhile to pursue. We also would be interested in finding teams looking to test this solution in the future.&lt;/p&gt;

&lt;p&gt;If you can spare 5 minutes, please help us by filling out this survey: &lt;a href=""https://tufts.qualtrics.com/jfe/form/SV_bI9I7IDELoJveC2""&gt;https://tufts.qualtrics.com/jfe/form/SV_bI9I7IDELoJveC2&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lqolib,True,,mlnovice1,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lqolib/any_interest_in_bertrelated_or_lda_acceleration/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lqolib/any_interest_in_bertrelated_or_lda_acceleration/,30199,1614103121.0,0,,False,,,,,,792
687,,LanguageTechnology,"Standardizing/normalizing is known to help learning when fitting a model using gradient descent.

When using a mix of embeddings and non-text features, how should you approach standardization/normalization? 

a) Standardize/normalize everything per usual.

b) Standardize/normalize non-text features, but leave the word embeddings alone.

c) Don't standardize/normalize anything.

So far I'm leaning towards option c. My reasoning:

\- For embeddings extracted from transformers-based models, there's no sensible notion of mean and standard deviation (in the case of standardization) or maximum or minimum (in the case of normalization).

\- For a fixed vocabulary of word vectors (such as from word2vec), gensim supports norming all the vectors, but I worry that this procedure would lose useful information contained in the embeddings.

\- If you standardize/normalize non-text features only, the model would likely update weights associated with the word embeddings faster due to them likely being on a larger scale, which would hurt learning.",t2_3khic7tq,False,,0,False,Should you standardize/normalize embeddings when using them with a classifier?,[],r/LanguageTechnology,False,6,,0,,False,t3_lqm5kp,False,dark,1.0,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1614125843.0,text,6,,,text,self.LanguageTechnology,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Standardizing/normalizing is known to help learning when fitting a model using gradient descent.&lt;/p&gt;

&lt;p&gt;When using a mix of embeddings and non-text features, how should you approach standardization/normalization? &lt;/p&gt;

&lt;p&gt;a) Standardize/normalize everything per usual.&lt;/p&gt;

&lt;p&gt;b) Standardize/normalize non-text features, but leave the word embeddings alone.&lt;/p&gt;

&lt;p&gt;c) Don&amp;#39;t standardize/normalize anything.&lt;/p&gt;

&lt;p&gt;So far I&amp;#39;m leaning towards option c. My reasoning:&lt;/p&gt;

&lt;p&gt;- For embeddings extracted from transformers-based models, there&amp;#39;s no sensible notion of mean and standard deviation (in the case of standardization) or maximum or minimum (in the case of normalization).&lt;/p&gt;

&lt;p&gt;- For a fixed vocabulary of word vectors (such as from word2vec), gensim supports norming all the vectors, but I worry that this procedure would lose useful information contained in the embeddings.&lt;/p&gt;

&lt;p&gt;- If you standardize/normalize non-text features only, the model would likely update weights associated with the word embeddings faster due to them likely being on a larger scale, which would hurt learning.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lqm5kp,True,,EazyStrides,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lqm5kp/should_you_standardizenormalize_embeddings_when/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lqm5kp/should_you_standardizenormalize_embeddings_when/,30199,1614097043.0,0,,False,,,,,,1046
688,,LanguageTechnology,,t2_j2r31,False,,0,False,Master Thesis: Matching companies that are potential cooperation partners,[],r/LanguageTechnology,False,6,,0,,False,t3_lqdjhg,False,dark,0.89,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,False,,1614098237.0,text,6,,,text,self.MLQuestions,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lqdjhg,True,,Affenbob123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lqdjhg/master_thesis_matching_companies_that_are/,all_ads,False,/r/MLQuestions/comments/lqdh6b/master_thesis_matching_companies_that_are/,30199,1614069437.0,0,,False,/r/MLQuestions/comments/lqdh6b/master_thesis_matching_companies_that_are/,"[{'approved_at_utc': None, 'subreddit': 'MLQuestions', 'selftext': ""Hi all,\n\nI am currently looking at various topics for my master's thesis. One of the topics is about determining whether a company B is a possible cooperation partner for company A. Company A has defined some criteria that must be met for a possible cooperation and for each criteria a weight is provided how important it is. Examples for such criteria:\n\n* Products of company\n* Company structure\n* business activity\n* job postings\n* capabilities of the company\n* company size\n* Shareholders\n* Is the company doing research\n\nIn the thesis the NLP model BERT should be used. The question I have is whether there is any use case for the application of BERT at all. The matching/recommendation can't be done with BERT, right?\n\nOne could try to use BERT to extract data (company size, capabilities, what kind of job postings) from the company website with BERT. The problem is that currently there is no training data to do that.\n\nDo any of you have an idea how to use BERT for this thesis?\n\nThanks!"", 'author_fullname': 't2_j2r31', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Master Thesis: Matching companies that are potential cooperation partners', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MLQuestions', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_lqdh6b', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 13, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 13, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1614098032.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MLQuestions', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am currently looking at various topics for my master&amp;#39;s thesis. One of the topics is about determining whether a company B is a possible cooperation partner for company A. Company A has defined some criteria that must be met for a possible cooperation and for each criteria a weight is provided how important it is. Examples for such criteria:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Products of company&lt;/li&gt;\n&lt;li&gt;Company structure&lt;/li&gt;\n&lt;li&gt;business activity&lt;/li&gt;\n&lt;li&gt;job postings&lt;/li&gt;\n&lt;li&gt;capabilities of the company&lt;/li&gt;\n&lt;li&gt;company size&lt;/li&gt;\n&lt;li&gt;Shareholders&lt;/li&gt;\n&lt;li&gt;Is the company doing research&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In the thesis the NLP model BERT should be used. The question I have is whether there is any use case for the application of BERT at all. The matching/recommendation can&amp;#39;t be done with BERT, right?&lt;/p&gt;\n\n&lt;p&gt;One could try to use BERT to extract data (company size, capabilities, what kind of job postings) from the company website with BERT. The problem is that currently there is no training data to do that.&lt;/p&gt;\n\n&lt;p&gt;Do any of you have an idea how to use BERT for this thesis?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_30rel', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'lqdh6b', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Affenbob123', 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MLQuestions/comments/lqdh6b/master_thesis_matching_companies_that_are/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MLQuestions/comments/lqdh6b/master_thesis_matching_companies_that_are/', 'subreddit_subscribers': 32376, 'created_utc': 1614069232.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_lqdh6b,,,0
689,,LanguageTechnology,,t2_9np1uzua,False,,0,False,Have you ever had the opportunity to be in Europe and learn Slovak? 😄,[],r/LanguageTechnology,False,6,,0,,False,t3_lqnrvj,False,dark,0.43,,public,0,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/rULMctrwd78?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Naucio sam SLOVACKI ZA PAR MESECI | Ako sa veľmi rýchlo naučit po Slovensky?', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/rULMctrwd78?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Limited_Edition_ns', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/rULMctrwd78/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCsMG9GU8xIPnkKVQBgZZhkg'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/rULMctrwd78?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lqnrvj', 'height': 200}",,False,0,,False,False,,False,,[],{},,False,,1614129831.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lqnrvj,True,,limited_edition_ns,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lqnrvj/have_you_ever_had_the_opportunity_to_be_in_europe/,all_ads,False,https://youtu.be/rULMctrwd78,30199,1614101031.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Naucio sam SLOVACKI ZA PAR MESECI | Ako sa veľmi rýchlo naučit po Slovensky?', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/rULMctrwd78?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Limited_Edition_ns', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/rULMctrwd78/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCsMG9GU8xIPnkKVQBgZZhkg'}}",False,https://youtu.be/rULMctrwd78,,,,,0
690,,LanguageTechnology,[https://youtu.be/IeACb0nqK24](https://youtu.be/IeACb0nqK24),t2_4yukrxj9,False,,0,False,A unique self-assessment algorithm App that simulates changes taking place in personality over time based,[],r/LanguageTechnology,False,6,,0,,False,t3_lqg852,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1614108475.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://youtu.be/IeACb0nqK24""&gt;https://youtu.be/IeACb0nqK24&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lqg852,True,,moctidderwww2019,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lqg852/a_unique_selfassessment_algorithm_app_that/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lqg852/a_unique_selfassessment_algorithm_app_that/,30199,1614079675.0,0,,False,,,,,,60
691,,LanguageTechnology,,t2_hkv9s,False,,0,False,Hierarchical Transformers for Long Document Classification (Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_lpxl51,False,dark,0.87,,public,11,1,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3IOl5d9PZeM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Hierarchical Transformers for Long Document Classification (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3IOl5d9PZeM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3IOl5d9PZeM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3IOl5d9PZeM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lpxl51', 'height': 200}",,False,11,,False,False,,False,,[],{},,False,,1614051292.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lpxl51,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lpxl51/hierarchical_transformers_for_long_document/,all_ads,False,https://youtu.be/3IOl5d9PZeM,30199,1614022492.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Hierarchical Transformers for Long Document Classification (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3IOl5d9PZeM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3IOl5d9PZeM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/3IOl5d9PZeM,,,,,0
692,,LanguageTechnology,"I forgot where I saw it, and now I can't find it, so now I was hoping someone could tell me where it was.

Someone had been using NLP to create text adventure games.  I remember trying it, selecting a genre, and it was a mess but it was pretty interesting / good.  I wanted to try it again but I can't find it.  They were using some sort of neural net.  Everytime I checked my inventory it was different but still made for a novel game experience.",t2_6ibjn,False,,0,False,NLP generated Zork-like text adventure games,[],r/LanguageTechnology,False,6,,0,,False,t3_lq9ai6,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1614083688.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I forgot where I saw it, and now I can&amp;#39;t find it, so now I was hoping someone could tell me where it was.&lt;/p&gt;

&lt;p&gt;Someone had been using NLP to create text adventure games.  I remember trying it, selecting a genre, and it was a mess but it was pretty interesting / good.  I wanted to try it again but I can&amp;#39;t find it.  They were using some sort of neural net.  Everytime I checked my inventory it was different but still made for a novel game experience.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lq9ai6,True,,fschwiet,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lq9ai6/nlp_generated_zorklike_text_adventure_games/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lq9ai6/nlp_generated_zorklike_text_adventure_games/,30199,1614054888.0,0,,False,,,,,,447
693,,LanguageTechnology,"I am looking for library or code base to implement MTL. Any ideas on this 

Things on my checklist.

1. Should be able to mix different datasets.
2. Able to accumulate gradients for separate task and propagate.
3. Ability to define custom layers on pretrained models.
4. Single head or specific heads for MTL.
5. Implement my custom loss",t2_xf374,False,,0,False,Looking for a code base to implement multi-task learning in NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_lpl5ja,False,dark,0.83,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1614019773.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am looking for library or code base to implement MTL. Any ideas on this &lt;/p&gt;

&lt;p&gt;Things on my checklist.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Should be able to mix different datasets.&lt;/li&gt;
&lt;li&gt;Able to accumulate gradients for separate task and propagate.&lt;/li&gt;
&lt;li&gt;Ability to define custom layers on pretrained models.&lt;/li&gt;
&lt;li&gt;Single head or specific heads for MTL.&lt;/li&gt;
&lt;li&gt;Implement my custom loss&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lpl5ja,True,,thak123,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lpl5ja/looking_for_a_code_base_to_implement_multitask/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lpl5ja/looking_for_a_code_base_to_implement_multitask/,30199,1613990973.0,0,,False,,,,,,337
694,,LanguageTechnology,,t2_5fsp2x6v,False,,0,False,How to use Instance-based Learning to improve the INTERPRETABILITY of NER models | Research Papers Summary 009,[],r/LanguageTechnology,False,6,,0,,False,t3_lp45xe,False,dark,0.9,,public,16,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/OGiyOz4oy54?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Instance-Based Learning for NER | Research Papers Summary 009', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/OGiyOz4oy54?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/OGiyOz4oy54/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/OGiyOz4oy54?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lp45xe', 'height': 200}",,False,16,,False,False,,False,,[],{},,False,,1613963480.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lp45xe,True,,RyanAI100,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lp45xe/how_to_use_instancebased_learning_to_improve_the/,all_ads,False,https://youtu.be/OGiyOz4oy54,30199,1613934680.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Instance-Based Learning for NER | Research Papers Summary 009', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/OGiyOz4oy54?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/OGiyOz4oy54/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,https://youtu.be/OGiyOz4oy54,,,,,0
695,,LanguageTechnology,,t2_a7i59xms,False,,0,False,torch.nn.Embedding explained (+ Character-level language model),[],r/LanguageTechnology,False,6,,0,,False,t3_lp6afl,False,dark,0.89,,public,7,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/euwN5DHfLEo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'torch.nn.Embedding explained (+ Character-level language model)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/euwN5DHfLEo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'mildlyoverfitted', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/euwN5DHfLEo/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCYBSjwkGTK06NnDnFsOcR7g'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/euwN5DHfLEo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lp6afl', 'height': 200}",,False,7,,False,False,,False,,[],{},,False,,1613969380.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lp6afl,True,,mildlyoverfitted,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lp6afl/torchnnembedding_explained_characterlevel/,all_ads,False,https://youtu.be/euwN5DHfLEo,30199,1613940580.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'torch.nn.Embedding explained (+ Character-level language model)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/euwN5DHfLEo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'mildlyoverfitted', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/euwN5DHfLEo/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCYBSjwkGTK06NnDnFsOcR7g'}}",False,https://youtu.be/euwN5DHfLEo,,,,,0
696,,LanguageTechnology," 

In case you are looking for a real-world project (not a hackathon) to make an impact while building up your project portfolio, here is a collaborative two-month project where 50 engineers from all around the world will work together. 

You can apply here [https://omdena.com/projects/bias/](https://omdena.com/projects/bias/?fbclid=IwAR2WIXjaZuY8vNTMm8uUDqE57cqHftC8NlWQzSLnSiTLrLBV4y6IgnuYd_Q)",t2_69ezr99,False,,0,False,NLP Real World Challenge to Detect Bias &amp; Misinformation in Articles,[],r/LanguageTechnology,False,6,,0,,False,t3_lotgl4,False,dark,0.94,,public,24,0,{},,False,[],,False,False,,{},,False,24,,False,False,,False,,[],{},,True,,1613928132.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In case you are looking for a real-world project (not a hackathon) to make an impact while building up your project portfolio, here is a collaborative two-month project where 50 engineers from all around the world will work together. &lt;/p&gt;

&lt;p&gt;You can apply here &lt;a href=""https://omdena.com/projects/bias/?fbclid=IwAR2WIXjaZuY8vNTMm8uUDqE57cqHftC8NlWQzSLnSiTLrLBV4y6IgnuYd_Q""&gt;https://omdena.com/projects/bias/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lotgl4,True,,Lordobba,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lotgl4/nlp_real_world_challenge_to_detect_bias/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lotgl4/nlp_real_world_challenge_to_detect_bias/,30199,1613899332.0,0,,False,,,,,,397
697,,LanguageTechnology,"I am curious to hear about your opinions regarding future advancements and possible solutions to the issues that exist now in the field of language technology.

What do you see as the main issues and do you think something radically has to change in the way we process language data in order to solve those issues? 

Do you think that new paradigms will emerge in the years to come? I guess that's impossible to predict but do you see a tendency towards new ways of processing languages?

What are you excited about the most?",t2_14edhs,False,,0,False,Future advancements and unsolved problems,[],r/LanguageTechnology,False,6,,0,,False,t3_lou3qo,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1613930909.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am curious to hear about your opinions regarding future advancements and possible solutions to the issues that exist now in the field of language technology.&lt;/p&gt;

&lt;p&gt;What do you see as the main issues and do you think something radically has to change in the way we process language data in order to solve those issues? &lt;/p&gt;

&lt;p&gt;Do you think that new paradigms will emerge in the years to come? I guess that&amp;#39;s impossible to predict but do you see a tendency towards new ways of processing languages?&lt;/p&gt;

&lt;p&gt;What are you excited about the most?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lou3qo,True,,bakee_aphex,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lou3qo/future_advancements_and_unsolved_problems/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lou3qo/future_advancements_and_unsolved_problems/,30199,1613902109.0,0,,False,,,,,,525
698,,LanguageTechnology,"I am looking for problems where BERT has been shown to perform poorly. Additionally, what are some English to English NLP (or any other - same language to the same language)  tasks where fine-tuning GPT-2 is not helpful at all?",t2_a1hqdli,False,,0,False,"What are some classification tasks where BERT-based models don't work well? In a similar vein, what are some generative tasks where fine-tuning GPT-2/LM does not work well?",[],r/LanguageTechnology,False,6,,0,,False,t3_lomb87,False,dark,0.95,,public,17,0,{},,False,[],,False,False,,{},,False,17,,False,False,,False,,[],{},,True,,1613901269.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am looking for problems where BERT has been shown to perform poorly. Additionally, what are some English to English NLP (or any other - same language to the same language)  tasks where fine-tuning GPT-2 is not helpful at all?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lomb87,True,,flerakml,,16,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lomb87/what_are_some_classification_tasks_where/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lomb87/what_are_some_classification_tasks_where/,30199,1613872469.0,1,,False,,,,,,227
699,,LanguageTechnology,"

&gt;tween 0 and 1 by a sigmoid activation function.
Our proposed model (model (b) in Fig. 1) uses additional information from
final hidden states of input tokens t1, t2,...tN . The BERT’s sequence output
is fetched into time-distributed fully connected dense layer with the number of
neurons equal to the number of labels. The output of this layer is pooled in
two different ways using the max-pooling and the average-pooling. Max-pooling
outputs the maximum of each feature across all tokens, thus it reacts on strong
class-related keywords. The average-pooling, on the other hand, outputs the
average of each feature over the sequence and thus attends to all tokens in
the sequence evenly. To compute average-pooling, we clipped all features into
interval [−1, 1] to intentionally suppress the influence of strong keywords. When
computing pooled values, we ignored all padding tokens (denoted as [PAD]).
Finally, the features generated from the pooled output and the output for the
[CLS] token are summed together. 

https://link.springer.com/chapter/10.1007%2F978-3-030-58323-1_23

From this paper, mean pool is best, followed by max pool, cls token rep, and sep token rep. 

https://arxiv.org/pdf/1910.07973.pdf",t2_10efjmjx,False,,0,False,"I was wondering if there were any new insights for text representation using transformers, did a lit search, here are a few highlights",[],r/LanguageTechnology,False,6,,0,,False,t3_lorzly,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1613922166.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;blockquote&gt;
&lt;p&gt;tween 0 and 1 by a sigmoid activation function.
Our proposed model (model (b) in Fig. 1) uses additional information from
final hidden states of input tokens t1, t2,...tN . The BERT’s sequence output
is fetched into time-distributed fully connected dense layer with the number of
neurons equal to the number of labels. The output of this layer is pooled in
two different ways using the max-pooling and the average-pooling. Max-pooling
outputs the maximum of each feature across all tokens, thus it reacts on strong
class-related keywords. The average-pooling, on the other hand, outputs the
average of each feature over the sequence and thus attends to all tokens in
the sequence evenly. To compute average-pooling, we clipped all features into
interval [−1, 1] to intentionally suppress the influence of strong keywords. When
computing pooled values, we ignored all padding tokens (denoted as [PAD]).
Finally, the features generated from the pooled output and the output for the
[CLS] token are summed together. &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=""https://link.springer.com/chapter/10.1007%2F978-3-030-58323-1_23""&gt;https://link.springer.com/chapter/10.1007%2F978-3-030-58323-1_23&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;From this paper, mean pool is best, followed by max pool, cls token rep, and sep token rep. &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://arxiv.org/pdf/1910.07973.pdf""&gt;https://arxiv.org/pdf/1910.07973.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lorzly,True,,BatmantoshReturns,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lorzly/i_was_wondering_if_there_were_any_new_insights/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lorzly/i_was_wondering_if_there_were_any_new_insights/,30199,1613893366.0,0,,False,,,,,,1217
700,,LanguageTechnology,"Hi. I'm currently trying to download the TACRED dataset. According the Stanford NLP's website, you can download it for free from LDC if you're an LDC member or pay $25.

At the TACRED page on LDC (https://catalog.ldc.upenn.edu/LDC2018T24) I see at the bottom ""Available Media"" and nothing under ""View Fees."" I don't see any way to download the data though.

On my account page it says that the administrator for my school hasn't approved my account yet, but would this matter? I'm still an LDC member even if I'm not officially verified to be a member of my school, right?

Any tips are appreciated. Thanks.",t2_m8kccne,False,,0,False,How do you download data from the Linguistic Data Consortium?,[],r/LanguageTechnology,False,6,,0,,False,t3_lorvz4,False,dark,0.6,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1613921731.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi. I&amp;#39;m currently trying to download the TACRED dataset. According the Stanford NLP&amp;#39;s website, you can download it for free from LDC if you&amp;#39;re an LDC member or pay $25.&lt;/p&gt;

&lt;p&gt;At the TACRED page on LDC (&lt;a href=""https://catalog.ldc.upenn.edu/LDC2018T24""&gt;https://catalog.ldc.upenn.edu/LDC2018T24&lt;/a&gt;) I see at the bottom &amp;quot;Available Media&amp;quot; and nothing under &amp;quot;View Fees.&amp;quot; I don&amp;#39;t see any way to download the data though.&lt;/p&gt;

&lt;p&gt;On my account page it says that the administrator for my school hasn&amp;#39;t approved my account yet, but would this matter? I&amp;#39;m still an LDC member even if I&amp;#39;m not officially verified to be a member of my school, right?&lt;/p&gt;

&lt;p&gt;Any tips are appreciated. Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lorvz4,True,,Seankala,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lorvz4/how_do_you_download_data_from_the_linguistic_data/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lorvz4/how_do_you_download_data_from_the_linguistic_data/,30199,1613892931.0,0,,False,,,,,,607
701,,LanguageTechnology,"Hey guys. Working on a document-level relation extraction (DocRE) task and I'm wondering what kind of datasets there may be.

In the general domain I've only been able to come across DocRED, and there are also the BC5CDR and GDA datasets in the biomedical domain. I'm wondering what else may be out there that I've missed.

Thanks!",t2_m8kccne,False,,0,False,Does anybody know of relation extraction datasets that are at the document level?,[],r/LanguageTechnology,False,6,,0,,False,t3_loncah,False,dark,0.75,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1613904844.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys. Working on a document-level relation extraction (DocRE) task and I&amp;#39;m wondering what kind of datasets there may be.&lt;/p&gt;

&lt;p&gt;In the general domain I&amp;#39;ve only been able to come across DocRED, and there are also the BC5CDR and GDA datasets in the biomedical domain. I&amp;#39;m wondering what else may be out there that I&amp;#39;ve missed.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,loncah,True,,Seankala,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/loncah/does_anybody_know_of_relation_extraction_datasets/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/loncah/does_anybody_know_of_relation_extraction_datasets/,30199,1613876044.0,0,,False,,,,,,331
702,,LanguageTechnology,,t2_hkv9s,False,,0,False,Nucleus Sampling: The Curious Case of Neural Text Degeneration (Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_lo4ymm,False,dark,0.86,,public,14,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/dCORspO2yVY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Nucleus Sampling: The Curious Case of Neural Text Degeneration (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/dCORspO2yVY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/dCORspO2yVY/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/dCORspO2yVY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lo4ymm', 'height': 200}",,False,14,,False,False,,False,,[],{},,False,,1613848012.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lo4ymm,True,,prakhar21,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lo4ymm/nucleus_sampling_the_curious_case_of_neural_text/,all_ads,False,https://youtu.be/dCORspO2yVY,30199,1613819212.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Nucleus Sampling: The Curious Case of Neural Text Degeneration (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/dCORspO2yVY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/dCORspO2yVY/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/dCORspO2yVY,,,,,0
703,,LanguageTechnology,"Hi everyone, any idea as to where I can find a good NLP developer? I tried Upwork but it seems like there are more AI generalists than NLP specialists.

Specifically, 

I need a AI/NLP Developer with AWS Lambda experience to train and deploy GPT-2 model. 

End goal to make a chatbot that will be used as a ‘friend’ for lonely people (see www.replika.ai [advanced] and chatty-app https://apps.apple.com/ca/app/chatty-your-robot-friend/id1020581003 [less advanced]). The chatty-app is based on GPT-2 architecture and is what the first iteration should be similar to.

The first iteration of this project will be a multi user web app that will communicate back and forth with a machine learning API (accessible via high performance gPRC protobufs) running on AWS lambda endpoint. The final product should be maintained 100% by amazon so I won’t need a system engineer/admin.

The GPT-2 model itself will need to be fine tuned on an empathetic dialogue corpus (which I will provide). To keep server costs low, the training should be done via GPU but once trained, the model should use CPU. I am open to any suggestions that will help keep on-going operational server costs lows (for ex. Imposing a slight delay 1-2 seconds between user questions and chatbot response). Ideally the architecture should allow for:

- multi-user support;
- up/down voting of responses so that the model can adapt to each user (via beam search [ex. You could create a tensor of weights assigned to multiple outputs from each question, have the model generate multiple output sequences via beam search, and then promote or penalize the weight associated with that response based on the upvote/downvote]); and
- allow for transfer learning so that the model output can be biased towards end user’s topical and stylistic preferences via Q&amp;A script (or another method).

Any suggestions regarding the overall app and NLP architecture are welcome.",t2_fi1kd,False,,0,False,Finding good NLP developers?,[],r/LanguageTechnology,False,6,,0,,False,t3_lnof83,False,dark,0.74,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,True,,1613792101.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, any idea as to where I can find a good NLP developer? I tried Upwork but it seems like there are more AI generalists than NLP specialists.&lt;/p&gt;

&lt;p&gt;Specifically, &lt;/p&gt;

&lt;p&gt;I need a AI/NLP Developer with AWS Lambda experience to train and deploy GPT-2 model. &lt;/p&gt;

&lt;p&gt;End goal to make a chatbot that will be used as a ‘friend’ for lonely people (see &lt;a href=""http://www.replika.ai""&gt;www.replika.ai&lt;/a&gt; [advanced] and chatty-app &lt;a href=""https://apps.apple.com/ca/app/chatty-your-robot-friend/id1020581003""&gt;https://apps.apple.com/ca/app/chatty-your-robot-friend/id1020581003&lt;/a&gt; [less advanced]). The chatty-app is based on GPT-2 architecture and is what the first iteration should be similar to.&lt;/p&gt;

&lt;p&gt;The first iteration of this project will be a multi user web app that will communicate back and forth with a machine learning API (accessible via high performance gPRC protobufs) running on AWS lambda endpoint. The final product should be maintained 100% by amazon so I won’t need a system engineer/admin.&lt;/p&gt;

&lt;p&gt;The GPT-2 model itself will need to be fine tuned on an empathetic dialogue corpus (which I will provide). To keep server costs low, the training should be done via GPU but once trained, the model should use CPU. I am open to any suggestions that will help keep on-going operational server costs lows (for ex. Imposing a slight delay 1-2 seconds between user questions and chatbot response). Ideally the architecture should allow for:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;multi-user support;&lt;/li&gt;
&lt;li&gt;up/down voting of responses so that the model can adapt to each user (via beam search [ex. You could create a tensor of weights assigned to multiple outputs from each question, have the model generate multiple output sequences via beam search, and then promote or penalize the weight associated with that response based on the upvote/downvote]); and&lt;/li&gt;
&lt;li&gt;allow for transfer learning so that the model output can be biased towards end user’s topical and stylistic preferences via Q&amp;amp;A script (or another method).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Any suggestions regarding the overall app and NLP architecture are welcome.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lnof83,True,,seudointellect,,13,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lnof83/finding_good_nlp_developers/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lnof83/finding_good_nlp_developers/,30199,1613763301.0,0,,False,,,,,,1922
704,,LanguageTechnology,,t2_47kojffj,False,,0,False,Do you think OpenAI's GPT3 is good enough to pass the Turing Test? / The world's largest scale Turing Test,[],r/LanguageTechnology,False,6,,0,,False,t3_lnpjqu,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,False,,1613794987.0,text,6,,,text,self.artificial,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lnpjqu,True,,theaicore,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lnpjqu/do_you_think_openais_gpt3_is_good_enough_to_pass/,all_ads,False,/r/artificial/comments/lncumk/do_you_think_openais_gpt3_is_good_enough_to_pass/,30199,1613766187.0,0,,False,/r/artificial/comments/lncumk/do_you_think_openais_gpt3_is_good_enough_to_pass/,"[{'approved_at_utc': None, 'subreddit': 'artificial', 'selftext': 'I finally managed to get access to GPT3 🙌 and am curious about this question so have created a web application to test it. At a pre-scheduled time, thousands of people from around the world will go on to the app and enter a chat interface. There is a 50-50 chance that they are matched to another visitor or GPT3. Through messaging back and forth, they have to figure out who is on the other side, Ai or human.\n\nWhat do you think the results will be?\n\n[The Imitation Game project](https://www.theaicore.com/imitationgame?utm_source=reddit)\n\nA key consideration is that rather than limiting it just to skilled interrogators, this project is more about if GPT3 can fool the general population so it differs from the classic Turing Test in that way. Another difference is that when matched with a human, they are both the ""interrogator"" instead of just one person interrogating and the other trying to prove they are not a computer.\n\n&amp;#x200B;\n\nUPDATE: Even though I have access to GPT3, they did not approve me using it in this application to am using a different chatbot technology.', 'author_fullname': 't2_47kojffj', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ""Do you think OpenAI's GPT3 is good enough to pass the Turing Test? / The world's largest scale Turing Test"", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/artificial', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_lncumk', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.89, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 65, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Project', 'can_mod_post': False, 'score': 65, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': 1614774824.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1613759723.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.artificial', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I finally managed to get access to GPT3 🙌 and am curious about this question so have created a web application to test it. At a pre-scheduled time, thousands of people from around the world will go on to the app and enter a chat interface. There is a 50-50 chance that they are matched to another visitor or GPT3. Through messaging back and forth, they have to figure out who is on the other side, Ai or human.&lt;/p&gt;\n\n&lt;p&gt;What do you think the results will be?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.theaicore.com/imitationgame?utm_source=reddit""&gt;The Imitation Game project&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;A key consideration is that rather than limiting it just to skilled interrogators, this project is more about if GPT3 can fool the general population so it differs from the classic Turing Test in that way. Another difference is that when matched with a human, they are both the &amp;quot;interrogator&amp;quot; instead of just one person interrogating and the other trying to prove they are not a computer.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;UPDATE: Even though I have access to GPT3, they did not approve me using it in this application to am using a different chatbot technology.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': 'confidence', 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '98a7a21e-9280-11ea-b427-0eca3434291d', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2qhfb', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#94e044', 'id': 'lncumk', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'theaicore', 'discussion_type': None, 'num_comments': 48, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/artificial/comments/lncumk/do_you_think_openais_gpt3_is_good_enough_to_pass/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/artificial/comments/lncumk/do_you_think_openais_gpt3_is_good_enough_to_pass/', 'subreddit_subscribers': 142718, 'created_utc': 1613730923.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_lncumk,,,0
705,,LanguageTechnology,"I am new to the NLP game and exploring the available options. I have stumbled across both Spacy and Hugging Face Transformers as python packages that seem applicable to my use cases. However, I am having a surprisingly hard time differentiation between the two packages. I would like to hear your input on the differences between Spacy and Hugging Face and perhaps some use cases in which you would prefer on over the other.

A second question relates to the fine-tuning of the models. It is my understanding that both Spacy and Hugging Face typically require fine-tuning before reasonable accuracy can be expected on domain-specific use cases. Could anyone give an estimate of the number of labeled text files one should expect to need for fine-tuning a model? Again, I am having a hard time finding an estimate for these numbers as most blogs use pre-existing datasets with large amounts of data.

Finally, a third question relates to the Wav2Vec 2 model, which can transcribe audio into text. It is my understanding that this model was trained on multiple languages. However, on [huggingface.co/models](https://huggingface.co/models), I am only finding english models at the moment. Is there some way in which I could use Wav2Vec (preferably with the hugging face package) to transcribe for example French texts?

I would very much appreciate it if you could share your expertise and help me to navigate the woods here.",t2_8rj5xf4h,False,,0,False,"Some questions about Spacy vs Hugging face transformers, fine-tuning and wav2vec.",[],r/LanguageTechnology,False,6,,0,,False,t3_lnca2q,False,dark,1.0,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,False,,False,,[],{},,True,,1613757602.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am new to the NLP game and exploring the available options. I have stumbled across both Spacy and Hugging Face Transformers as python packages that seem applicable to my use cases. However, I am having a surprisingly hard time differentiation between the two packages. I would like to hear your input on the differences between Spacy and Hugging Face and perhaps some use cases in which you would prefer on over the other.&lt;/p&gt;

&lt;p&gt;A second question relates to the fine-tuning of the models. It is my understanding that both Spacy and Hugging Face typically require fine-tuning before reasonable accuracy can be expected on domain-specific use cases. Could anyone give an estimate of the number of labeled text files one should expect to need for fine-tuning a model? Again, I am having a hard time finding an estimate for these numbers as most blogs use pre-existing datasets with large amounts of data.&lt;/p&gt;

&lt;p&gt;Finally, a third question relates to the Wav2Vec 2 model, which can transcribe audio into text. It is my understanding that this model was trained on multiple languages. However, on &lt;a href=""https://huggingface.co/models""&gt;huggingface.co/models&lt;/a&gt;, I am only finding english models at the moment. Is there some way in which I could use Wav2Vec (preferably with the hugging face package) to transcribe for example French texts?&lt;/p&gt;

&lt;p&gt;I would very much appreciate it if you could share your expertise and help me to navigate the woods here.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lnca2q,True,,VarietyElderberry,,11,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lnca2q/some_questions_about_spacy_vs_hugging_face/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lnca2q/some_questions_about_spacy_vs_hugging_face/,30199,1613728802.0,0,,False,,,,,,1422
706,,LanguageTechnology," 

Hey Reddit community, 

I am a recent graduate from Columbia University with a masters in financial engineering and a minor in machine learning and I'm looking for advice as I am sort of in a dilemma. I have been actively involved in data science and machine learning since 2017 and have loved working in the space and applying concepts to real-world problems (Worked Full time 2018-2019). In order to build on my knowledge and attain expertise applying ML to financial problems, I took up a masters in financial engineering hoping to leverage the program to work in FinTech roles and startup in the US. Through the course of my masters, I actively took up courses from the CS &amp; Data Science department taking a total of 5 courses (NLP, Applied Deep Learning, Cloud Computing &amp; Big Data, Artificial Intelligence and Recommender Systems) and realized that more than the financial engineering degree wasn't exactly what I wanted to do as I was in love with the field of NLP and the possible applications it can have across the spectrum rather than just limiting myself to applications of AI in finance. 

Unfortunately due to the course load and relatively short term course (3 semesters with 36 credits = 12 courses over 3 semesters) as well as the impact of COVID with classes going virtual since March 2020 I never really got the opportunity to take up roles in research during my masters despite the fact that I was extremely keen on it. The usual process would be to take a class with a professor, excel in the class, approach them for research and work with them after. However, due to the structure of my program, this never panned out. 

Currently, I feel capable enough to take up roles in the industry and work as a data scientist or machine learning engineer but have this burning desire to continue exploring NLP and topics in the space. I am extremely passionate about Abstractive Summarization, Search and Information Retrieval, and also keen to explore Speech Recognition systems. I want to be involved in both developing better algorithms and systems in these areas as well as finding robust inter-disciplinary applications for current systems. As I don't have any research experience I am highly doubtful of my chances as a PhD student. I would love to take up a program like the CMU MLT which gives you the chance to start out as a masters student and then transition to a full-time PhD student. 

I would be grateful for any advice you could provide to me. Would it be better to work for a year and try to be involved in projects in NLP before applying for a PhD and how can an individual with my profile successfully get into a PhD program? (Hopefully Top 10 eventually but more than anything I want to work with a driven like-minded advisor who can guide me in my development)",t2_aglc1lc5,False,,0,False,Can I apply for a PhD after a Master's if I don't have Research Experience? Help!,[],r/LanguageTechnology,False,6,,0,,False,t3_lnr82k,False,dark,0.4,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1613799979.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey Reddit community, &lt;/p&gt;

&lt;p&gt;I am a recent graduate from Columbia University with a masters in financial engineering and a minor in machine learning and I&amp;#39;m looking for advice as I am sort of in a dilemma. I have been actively involved in data science and machine learning since 2017 and have loved working in the space and applying concepts to real-world problems (Worked Full time 2018-2019). In order to build on my knowledge and attain expertise applying ML to financial problems, I took up a masters in financial engineering hoping to leverage the program to work in FinTech roles and startup in the US. Through the course of my masters, I actively took up courses from the CS &amp;amp; Data Science department taking a total of 5 courses (NLP, Applied Deep Learning, Cloud Computing &amp;amp; Big Data, Artificial Intelligence and Recommender Systems) and realized that more than the financial engineering degree wasn&amp;#39;t exactly what I wanted to do as I was in love with the field of NLP and the possible applications it can have across the spectrum rather than just limiting myself to applications of AI in finance. &lt;/p&gt;

&lt;p&gt;Unfortunately due to the course load and relatively short term course (3 semesters with 36 credits = 12 courses over 3 semesters) as well as the impact of COVID with classes going virtual since March 2020 I never really got the opportunity to take up roles in research during my masters despite the fact that I was extremely keen on it. The usual process would be to take a class with a professor, excel in the class, approach them for research and work with them after. However, due to the structure of my program, this never panned out. &lt;/p&gt;

&lt;p&gt;Currently, I feel capable enough to take up roles in the industry and work as a data scientist or machine learning engineer but have this burning desire to continue exploring NLP and topics in the space. I am extremely passionate about Abstractive Summarization, Search and Information Retrieval, and also keen to explore Speech Recognition systems. I want to be involved in both developing better algorithms and systems in these areas as well as finding robust inter-disciplinary applications for current systems. As I don&amp;#39;t have any research experience I am highly doubtful of my chances as a PhD student. I would love to take up a program like the CMU MLT which gives you the chance to start out as a masters student and then transition to a full-time PhD student. &lt;/p&gt;

&lt;p&gt;I would be grateful for any advice you could provide to me. Would it be better to work for a year and try to be involved in projects in NLP before applying for a PhD and how can an individual with my profile successfully get into a PhD program? (Hopefully Top 10 eventually but more than anything I want to work with a driven like-minded advisor who can guide me in my development)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lnr82k,True,,NLPexplorer,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lnr82k/can_i_apply_for_a_phd_after_a_masters_if_i_dont/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lnr82k/can_i_apply_for_a_phd_after_a_masters_if_i_dont/,30199,1613771179.0,0,,False,,,,,,2806
707,,LanguageTechnology,"The most pertinent use-case I have found for Paraphrase Generation models deals with data-augmentation and adversarial example generation. However, I am looking for other potential applications and would be interested to dive deep into one topic.  Broadly I am also interested in open problems in Controlled Text Generation. Any leads would be helpful.",t2_a1hqdli,False,,0,False,What are some good open problems in/applications of Paraphrase Generation?,[],r/LanguageTechnology,False,6,,0,,False,t3_ln52j2,False,dark,0.81,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1613732815.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The most pertinent use-case I have found for Paraphrase Generation models deals with data-augmentation and adversarial example generation. However, I am looking for other potential applications and would be interested to dive deep into one topic.  Broadly I am also interested in open problems in Controlled Text Generation. Any leads would be helpful.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ln52j2,True,,flerakml,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ln52j2/what_are_some_good_open_problems_inapplications/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ln52j2/what_are_some_good_open_problems_inapplications/,30199,1613704015.0,0,,False,,,,,,352
708,,LanguageTechnology,"Hey, I know this is more of a devops thing, but as more and more people are asking questions about how to deploy their NLP models to production and which kind of infrastructure they should set up, I thought I would share 2 articles I wrote about that recently:

Container orchestration with Docker Swarm: [https://juliensalinas.com/en/container-orchestration-docker-swarm-nlpcloud/](https://juliensalinas.com/en/container-orchestration-docker-swarm-nlpcloud/)

Routing requests to the right NLP model with Traefik: [https://juliensalinas.com/en/traefik-reverse-proxy-docker-compose-docker-swarm-nlpcloud/](https://juliensalinas.com/en/traefik-reverse-proxy-docker-compose-docker-swarm-nlpcloud/)

I'm basically talking about how we're doing things behind the hood at [NLP Cloud](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=2fc10c28-ab0d-11eb-bcbc-0242ac130002), where each spaCy NLP model is running inside its own container.

I hope some of you will find these posts useful.",t2_4z4m2qcs,False,,0,False,"NLP Infrastructure with Docker Swarm, Docker Compose, and Traefik",[],r/LanguageTechnology,False,6,,0,,False,t3_lmo3jk,False,dark,1.0,,public,40,0,{},,False,[],,False,False,,{},,False,40,,False,False,,1619937396.0,,[],{},,True,,1613687694.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, I know this is more of a devops thing, but as more and more people are asking questions about how to deploy their NLP models to production and which kind of infrastructure they should set up, I thought I would share 2 articles I wrote about that recently:&lt;/p&gt;

&lt;p&gt;Container orchestration with Docker Swarm: &lt;a href=""https://juliensalinas.com/en/container-orchestration-docker-swarm-nlpcloud/""&gt;https://juliensalinas.com/en/container-orchestration-docker-swarm-nlpcloud/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Routing requests to the right NLP model with Traefik: &lt;a href=""https://juliensalinas.com/en/traefik-reverse-proxy-docker-compose-docker-swarm-nlpcloud/""&gt;https://juliensalinas.com/en/traefik-reverse-proxy-docker-compose-docker-swarm-nlpcloud/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m basically talking about how we&amp;#39;re doing things behind the hood at &lt;a href=""https://nlpcloud.io/?utm_source=reddit&amp;amp;utm_campaign=2fc10c28-ab0d-11eb-bcbc-0242ac130002""&gt;NLP Cloud&lt;/a&gt;, where each spaCy NLP model is running inside its own container.&lt;/p&gt;

&lt;p&gt;I hope some of you will find these posts useful.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lmo3jk,True,,juliensalinas,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lmo3jk/nlp_infrastructure_with_docker_swarm_docker/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lmo3jk/nlp_infrastructure_with_docker_swarm_docker/,30199,1613658894.0,0,,False,,,,,,987
709,,LanguageTechnology,"I need a good text normalization algorithm. Every single thing I've looked up on the subject just has a bunch of ad hoc rules that are blanket regex-replacements and they're frankly horrible. I want human-corrected text to a normalized format, using HUMAN INTELLIGENCE to normalize the text, and then I want to use it to actually train a normalizer. Example of how garbage rule-based normalization is is here:

The team is 7-0 and took a 7-0 lead in the first quarter.

What the normalization SHOULD be (if done with human intelligence and full knowledge of context):

The team is seven and O and took a seven nothing lead in the first quarter.

What most garbage rule-based normalizers would do with this sentence:

The team is seven minus zero and took a seven minus zero lead in the first quarter.

So obviously, you can see why I need human intelligence to do this properly, and if I do it by machine, I need it TRAINED on normalizations done with human intelligence. The issue is I have no idea how to do that, does anyone know how this might be done? What library, algorithm etc. is best for this? I REFUSE to use a rule-based model to do this, I've just proven how stupid it is to do that.",t2_agi81vd2,False,,0,False,How do you make a text normalizer NOT based on rules but based on TRAINING DATA?,[],r/LanguageTechnology,False,6,,0,,False,t3_lnnne0,False,dark,0.2,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1613790169.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I need a good text normalization algorithm. Every single thing I&amp;#39;ve looked up on the subject just has a bunch of ad hoc rules that are blanket regex-replacements and they&amp;#39;re frankly horrible. I want human-corrected text to a normalized format, using HUMAN INTELLIGENCE to normalize the text, and then I want to use it to actually train a normalizer. Example of how garbage rule-based normalization is is here:&lt;/p&gt;

&lt;p&gt;The team is 7-0 and took a 7-0 lead in the first quarter.&lt;/p&gt;

&lt;p&gt;What the normalization SHOULD be (if done with human intelligence and full knowledge of context):&lt;/p&gt;

&lt;p&gt;The team is seven and O and took a seven nothing lead in the first quarter.&lt;/p&gt;

&lt;p&gt;What most garbage rule-based normalizers would do with this sentence:&lt;/p&gt;

&lt;p&gt;The team is seven minus zero and took a seven minus zero lead in the first quarter.&lt;/p&gt;

&lt;p&gt;So obviously, you can see why I need human intelligence to do this properly, and if I do it by machine, I need it TRAINED on normalizations done with human intelligence. The issue is I have no idea how to do that, does anyone know how this might be done? What library, algorithm etc. is best for this? I REFUSE to use a rule-based model to do this, I&amp;#39;ve just proven how stupid it is to do that.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lnnne0,True,,lfoehhwfo,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lnnne0/how_do_you_make_a_text_normalizer_not_based_on/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lnnne0/how_do_you_make_a_text_normalizer_not_based_on/,30199,1613761369.0,0,,False,,,,,,1196
710,,LanguageTechnology,"Hi all,

As an attorney &amp; ML enthusiast alarmed by the prospect of losing my profession to AI, I ran a few experiments of legal summaries using GPT-3 that I thought the community might find interesting. My comments follow. I would love to hear your thoughts, particularly anyone who has had success using GPT-3 for high-accuracy summarization.

[http://www.davidvictorrodriguez.com/uploads/2/6/4/2/26420847/gpt-3\_esq\_-\_evaluating\_ai\_legal\_summaries.pdf](http://www.davidvictorrodriguez.com/uploads/2/6/4/2/26420847/gpt-3_esq_-_evaluating_ai_legal_summaries.pdf)",t2_2ngmu1cw,False,,0,False,"GPT-3, Esq? Evaluating AI Legal Summaries",[],r/LanguageTechnology,False,6,,0,,False,t3_lmsxxq,False,dark,1.0,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1613700325.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;As an attorney &amp;amp; ML enthusiast alarmed by the prospect of losing my profession to AI, I ran a few experiments of legal summaries using GPT-3 that I thought the community might find interesting. My comments follow. I would love to hear your thoughts, particularly anyone who has had success using GPT-3 for high-accuracy summarization.&lt;/p&gt;

&lt;p&gt;&lt;a href=""http://www.davidvictorrodriguez.com/uploads/2/6/4/2/26420847/gpt-3_esq_-_evaluating_ai_legal_summaries.pdf""&gt;http://www.davidvictorrodriguez.com/uploads/2/6/4/2/26420847/gpt-3_esq_-_evaluating_ai_legal_summaries.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lmsxxq,True,,EyesEarsNose,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lmsxxq/gpt3_esq_evaluating_ai_legal_summaries/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lmsxxq/gpt3_esq_evaluating_ai_legal_summaries/,30199,1613671525.0,0,,False,,,,,,571
711,,LanguageTechnology,"I'm new to NLP and I'd like to get some of your input on a random idea I had. 

I'd like to know if there's any use to the idea of using word embeddings to predict types of words. Let's say I download 50k words and convert them to some set of labels (POS tags, LIWC categories, etc) and then I use the N-dimensional vectors to classify those labels using a standard ML algorithm.  If this is successful then I could point a subset of those features/dimensions and show they have strong correlations with certain types of words. And I could use my model to appraise texts based on the selected categories.

Please let me know if this makes sense or if this is a totally nonsensical idea. 

Thanks!!",t2_8iobl,False,,0,False,Is classifying word categories with word embeddings a thing?,[],r/LanguageTechnology,False,6,,0,,False,t3_ln4435,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1613729961.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m new to NLP and I&amp;#39;d like to get some of your input on a random idea I had. &lt;/p&gt;

&lt;p&gt;I&amp;#39;d like to know if there&amp;#39;s any use to the idea of using word embeddings to predict types of words. Let&amp;#39;s say I download 50k words and convert them to some set of labels (POS tags, LIWC categories, etc) and then I use the N-dimensional vectors to classify those labels using a standard ML algorithm.  If this is successful then I could point a subset of those features/dimensions and show they have strong correlations with certain types of words. And I could use my model to appraise texts based on the selected categories.&lt;/p&gt;

&lt;p&gt;Please let me know if this makes sense or if this is a totally nonsensical idea. &lt;/p&gt;

&lt;p&gt;Thanks!!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ln4435,True,,Cheesebro69,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ln4435/is_classifying_word_categories_with_word/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ln4435/is_classifying_word_categories_with_word/,30199,1613701161.0,0,,False,,,,,,697
712,,LanguageTechnology,"Hey all,

I graduated from uni with a master's in linguistics and worked as a PhD researcher in NLP (had some knowledge of ML beforehand). After a few years I decided research wasn't for me and quit. Now I find myself in the (European) job market with decent experience as a researcher, but little to no development experience -- my coding work was all academic. I've done a number of interviews for NLP engineer roles and was always passed over for my lack of development/company experience. I love coding, though, and so I'd like to transition to more of a DevOps role.

Trouble is, I don't know where to start, and the companies I found aren't willing to take on a junior right now. Not even sure what extra skills I need. How do I get into DevOps? Are there resources floating around for that? Good courses that focus on the scaling &amp; production side of NLP work? Any personal experience you can share?",t2_8lzizum7,False,,0,False,Going into DevOps?,[],r/LanguageTechnology,False,6,,0,,False,t3_lmrti0,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1613697419.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all,&lt;/p&gt;

&lt;p&gt;I graduated from uni with a master&amp;#39;s in linguistics and worked as a PhD researcher in NLP (had some knowledge of ML beforehand). After a few years I decided research wasn&amp;#39;t for me and quit. Now I find myself in the (European) job market with decent experience as a researcher, but little to no development experience -- my coding work was all academic. I&amp;#39;ve done a number of interviews for NLP engineer roles and was always passed over for my lack of development/company experience. I love coding, though, and so I&amp;#39;d like to transition to more of a DevOps role.&lt;/p&gt;

&lt;p&gt;Trouble is, I don&amp;#39;t know where to start, and the companies I found aren&amp;#39;t willing to take on a junior right now. Not even sure what extra skills I need. How do I get into DevOps? Are there resources floating around for that? Good courses that focus on the scaling &amp;amp; production side of NLP work? Any personal experience you can share?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lmrti0,True,,bowdance,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lmrti0/going_into_devops/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lmrti0/going_into_devops/,30199,1613668619.0,0,,False,,,,,,910
713,,LanguageTechnology,"My grandma has a Canadian pen pal but she doesn’t speak English so I translate for her. She recently received magazines from her pen pal and asked me if I could translate them for her. She really enjoys the images and the overall magazine style so I wanted to keep that format. I figured converting it would do the trick but all online converters just scramble and mess up the resulting word document. Here is a sample of the scanned pdf file of the magazine [here](https://imgur.com/a/7DcIAIo)
Thanks for any help you can give!

P.S: I would really prefer to have a free converter since I don’t normally use them for such a hard task.",t2_847xizoc,False,,0,False,Best OCR converter for magazine images?,[],r/LanguageTechnology,False,6,,0,,False,t3_lmmu7z,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1613683901.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My grandma has a Canadian pen pal but she doesn’t speak English so I translate for her. She recently received magazines from her pen pal and asked me if I could translate them for her. She really enjoys the images and the overall magazine style so I wanted to keep that format. I figured converting it would do the trick but all online converters just scramble and mess up the resulting word document. Here is a sample of the scanned pdf file of the magazine &lt;a href=""https://imgur.com/a/7DcIAIo""&gt;here&lt;/a&gt;
Thanks for any help you can give!&lt;/p&gt;

&lt;p&gt;P.S: I would really prefer to have a free converter since I don’t normally use them for such a hard task.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lmmu7z,True,,Benediikt_J,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lmmu7z/best_ocr_converter_for_magazine_images/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lmmu7z/best_ocr_converter_for_magazine_images/,30199,1613655101.0,0,,False,,,,,,635
714,,LanguageTechnology,"Hi y’all, I have a large volume of searchable PDFs. Based on their structure (and text) I’d like to cluster or classify them into 3 categories: A, B and other (C). I 

What is best practice for this task? I’ve experimented with extracting the structure and the text content as hOCR and plain text respectively. However, I’m not sure how to use this information efficiently. 

I could simply do a text-based model using a tf-idf matrix but I’m interested in how I can use the structure to classify or cluster. 

Any suggestions? Thanks 

Btw, I love this subreddit! It seems I bookmark every other post.",t2_26n50qtm,False,,0,False,Best pratice for pdf classification/clustering,[],r/LanguageTechnology,False,6,,0,,False,t3_lmllo4,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1613679812.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi y’all, I have a large volume of searchable PDFs. Based on their structure (and text) I’d like to cluster or classify them into 3 categories: A, B and other (C). I &lt;/p&gt;

&lt;p&gt;What is best practice for this task? I’ve experimented with extracting the structure and the text content as hOCR and plain text respectively. However, I’m not sure how to use this information efficiently. &lt;/p&gt;

&lt;p&gt;I could simply do a text-based model using a tf-idf matrix but I’m interested in how I can use the structure to classify or cluster. &lt;/p&gt;

&lt;p&gt;Any suggestions? Thanks &lt;/p&gt;

&lt;p&gt;Btw, I love this subreddit! It seems I bookmark every other post.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lmllo4,True,,Academy-,,14,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lmllo4/best_pratice_for_pdf_classificationclustering/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lmllo4/best_pratice_for_pdf_classificationclustering/,30199,1613651012.0,0,,False,,,,,,602
715,,LanguageTechnology,I have a long audio message and the text. I want to split the sentence in the sentence bases on full stop and also split the relevant audio path. Can anyone recommend the relevant source,t2_5owk7j7,False,,0,False,Split audio based on text tokenization,[],r/LanguageTechnology,False,6,,0,,False,t3_lmpcst,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1613691165.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a long audio message and the text. I want to split the sentence in the sentence bases on full stop and also split the relevant audio path. Can anyone recommend the relevant source&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lmpcst,True,,mrtac96,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lmpcst/split_audio_based_on_text_tokenization/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lmpcst/split_audio_based_on_text_tokenization/,30199,1613662365.0,0,,False,,,,,,186
716,,LanguageTechnology,"Hi,

Does somebody have first-hand experience from bootcamps/short courses (strongly preferred in-person)  to get into the NLP space?",t2_5w9jnbsn,False,,0,False,Europe Boot camp/Courses on Natural Language Processing?,[],r/LanguageTechnology,False,6,,0,,False,t3_lmo8d2,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1613688065.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;Does somebody have first-hand experience from bootcamps/short courses (strongly preferred in-person)  to get into the NLP space?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lmo8d2,True,,BenXavier,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lmo8d2/europe_boot_campcourses_on_natural_language/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lmo8d2/europe_boot_campcourses_on_natural_language/,30199,1613659265.0,1,,False,,,,,,133
717,,LanguageTechnology,"I was consulting the following paper about cross lingual language models (XLMs): https://arxiv.org/pdf/1901.07291.pdf

I am having a bit of trouble understanding the cross lingual aspect in the MLM and CLM objectives. I have read the paper quite a few times, but I don’t see how they use any cross lingual signal in the monolingual case. In the TLM+MLM objective it’s pretty clear to me that the encoder is a cross lingual space but in the others I don’t see where the cross lingual aspect is taking place. I hope someone can clear this for me :) thanks!!",t2_13s4mru,False,,0,False,Question about cross lingual pretraining,[],r/LanguageTechnology,False,6,,0,,False,t3_lm7mgt,False,dark,1.0,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,1613604218.0,,[],{},,True,,1613632679.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was consulting the following paper about cross lingual language models (XLMs): &lt;a href=""https://arxiv.org/pdf/1901.07291.pdf""&gt;https://arxiv.org/pdf/1901.07291.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I am having a bit of trouble understanding the cross lingual aspect in the MLM and CLM objectives. I have read the paper quite a few times, but I don’t see how they use any cross lingual signal in the monolingual case. In the TLM+MLM objective it’s pretty clear to me that the encoder is a cross lingual space but in the others I don’t see where the cross lingual aspect is taking place. I hope someone can clear this for me :) thanks!!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lm7mgt,True,,hayis4horses1,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lm7mgt/question_about_cross_lingual_pretraining/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lm7mgt/question_about_cross_lingual_pretraining/,30199,1613603879.0,0,,False,,,,,,555
718,,LanguageTechnology,,t2_5f25dvzg,False,,0,False,Anyone know of a Farsi TTS?,[],r/LanguageTechnology,False,6,,0,,False,t3_llsmlh,False,dark,1.0,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,False,,1613590862.0,text,6,,,text,self.farsi,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,llsmlh,True,,ganzzahl,,0,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/llsmlh/anyone_know_of_a_farsi_tts/,all_ads,False,/r/farsi/comments/llfnsv/looking_for_a_good_speech_synthesizer_to_read_out/,30199,1613562062.0,0,,False,/r/farsi/comments/llfnsv/looking_for_a_good_speech_synthesizer_to_read_out/,"[{'approved_at_utc': None, 'subreddit': 'farsi', 'selftext': ""Hi everyone,\n\nI hope one of you might help me. \n\nOne of my friends is from Afghanistan and speaks Dari but he doesn't know how to read or write it. He's now learning French and he's doing great in this language (speak/read/write). But sometimes it can be tough to understand some specific or more abstract words for him. \n\nWhat I'm looking for, is a speech synthesizer (an application or a website) where he can enter words written in farsi (which he has previously translated from French) that will be read out loud to him so he can understand it. \n\nThis would be also useful the other way around. A software where he can dictate a word or a sentence in Dari that would be written in Persian alphabet and then he could copy and paste the text to be translated to French. \n\nAs you maybe noticed, Google translate doesn't include a speech synthesizer or dictation for farsi. I'm looking for anything possibly free but I will consider paying for a good app. \n\nLet me know if I wasn't clear 🙃\n\nThank you very much for your help 😊\n\nEdit : spelling"", 'author_fullname': 't2_6ekdv5k7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Looking for a good Speech synthesizer to read out loud farsi/dari', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/farsi', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_llfnsv', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': 1613554693.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1613544099.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.farsi', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I hope one of you might help me. &lt;/p&gt;\n\n&lt;p&gt;One of my friends is from Afghanistan and speaks Dari but he doesn&amp;#39;t know how to read or write it. He&amp;#39;s now learning French and he&amp;#39;s doing great in this language (speak/read/write). But sometimes it can be tough to understand some specific or more abstract words for him. &lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m looking for, is a speech synthesizer (an application or a website) where he can enter words written in farsi (which he has previously translated from French) that will be read out loud to him so he can understand it. &lt;/p&gt;\n\n&lt;p&gt;This would be also useful the other way around. A software where he can dictate a word or a sentence in Dari that would be written in Persian alphabet and then he could copy and paste the text to be translated to French. &lt;/p&gt;\n\n&lt;p&gt;As you maybe noticed, Google translate doesn&amp;#39;t include a speech synthesizer or dictation for farsi. I&amp;#39;m looking for anything possibly free but I will consider paying for a good app. &lt;/p&gt;\n\n&lt;p&gt;Let me know if I wasn&amp;#39;t clear 🙃&lt;/p&gt;\n\n&lt;p&gt;Thank you very much for your help 😊&lt;/p&gt;\n\n&lt;p&gt;Edit : spelling&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2t22i', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'llfnsv', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Charly_Ngals', 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/farsi/comments/llfnsv/looking_for_a_good_speech_synthesizer_to_read_out/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/farsi/comments/llfnsv/looking_for_a_good_speech_synthesizer_to_read_out/', 'subreddit_subscribers': 4773, 'created_utc': 1613515299.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_llfnsv,,,0
719,,LanguageTechnology,"Hi,

Are there any available libraries for the task of word dehyphenation (removing unwanted hyphens within words, when they are a result of document hyphenation)? This occurs especially when converting PDFs to TXTs.

Thank you!

&amp;#x200B;

EDIT: I'm not working with PDFs; i just have plain text which is already in the hyphenated format (without newline or space-newline after the hyphen). Therefore words like ""bag-of-words"" and ""fu-ture"" are not differentiated in any way. I've now just tried a vocabulary-based approach, which is basically: given a hyphenated word in the ""x-y"" form, if it is more frequent in the corpus in its ""xy"" form, dehyphenate to ""xy"". Seems to work. Any other suggestions?",t2_82pcwuv6,False,,0,False,Dehyphenation,[],r/LanguageTechnology,False,6,,0,,False,t3_lm3gpa,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,1613660061.0,,[],{},,True,,1613621524.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;Are there any available libraries for the task of word dehyphenation (removing unwanted hyphens within words, when they are a result of document hyphenation)? This occurs especially when converting PDFs to TXTs.&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;EDIT: I&amp;#39;m not working with PDFs; i just have plain text which is already in the hyphenated format (without newline or space-newline after the hyphen). Therefore words like &amp;quot;bag-of-words&amp;quot; and &amp;quot;fu-ture&amp;quot; are not differentiated in any way. I&amp;#39;ve now just tried a vocabulary-based approach, which is basically: given a hyphenated word in the &amp;quot;x-y&amp;quot; form, if it is more frequent in the corpus in its &amp;quot;xy&amp;quot; form, dehyphenate to &amp;quot;xy&amp;quot;. Seems to work. Any other suggestions?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lm3gpa,True,,AntCont__,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lm3gpa/dehyphenation/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lm3gpa/dehyphenation/,30199,1613592724.0,0,,False,,,,,,705
720,,LanguageTechnology,"Hi,

is it okay to use multiple frameworks/libraries in the same project? 

i am currently writing my bachelorthesis about analyzing online lectures and would like to use gensim for it. But i already used spacy for data preperation like filtering stopwords e.g. 

Is it okay to mix these tools or better use the same tool over the span of the project?",t2_5eyplvnc,False,,0,False,Multiple Frameworks in the same projekt?,[],r/LanguageTechnology,False,6,,0,,False,t3_lluwlh,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1613598640.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;is it okay to use multiple frameworks/libraries in the same project? &lt;/p&gt;

&lt;p&gt;i am currently writing my bachelorthesis about analyzing online lectures and would like to use gensim for it. But i already used spacy for data preperation like filtering stopwords e.g. &lt;/p&gt;

&lt;p&gt;Is it okay to mix these tools or better use the same tool over the span of the project?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lluwlh,True,,p-dog1,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lluwlh/multiple_frameworks_in_the_same_projekt/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lluwlh/multiple_frameworks_in_the_same_projekt/,30199,1613569840.0,0,,False,,,,,,351
721,,LanguageTechnology,"Hi,

I am working on NLP projects where I usually extract context embeddings for some given tokens. As an example, when I am performing a filling-mask task using hugging face transformers, I always wonder how to properly aggregate context embedding vectors for each \[MASK\] token in the same sentence. Usual methods to proceed are :

\- Averaging them all (e.g using np.mean())

\- Summing over embedding space dimensions

\- Concatenates them (and probably apply a PCA to get a more compact representation)

&amp;#x200B;

Is there any better way to perform multidimensional vector aggregation while preserving most of the information?",t2_83w0n37q,False,,0,False,What proper method to aggregate embeddings?,[],r/LanguageTechnology,False,6,,0,,False,t3_llgeoa,False,dark,1.0,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,False,,1613517731.0,,[],{},,True,,1613546304.0,text,6,,,text,self.LanguageTechnology,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I am working on NLP projects where I usually extract context embeddings for some given tokens. As an example, when I am performing a filling-mask task using hugging face transformers, I always wonder how to properly aggregate context embedding vectors for each [MASK] token in the same sentence. Usual methods to proceed are :&lt;/p&gt;

&lt;p&gt;- Averaging them all (e.g using np.mean())&lt;/p&gt;

&lt;p&gt;- Summing over embedding space dimensions&lt;/p&gt;

&lt;p&gt;- Concatenates them (and probably apply a PCA to get a more compact representation)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Is there any better way to perform multidimensional vector aggregation while preserving most of the information?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,llgeoa,True,,motynel,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/llgeoa/what_proper_method_to_aggregate_embeddings/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/llgeoa/what_proper_method_to_aggregate_embeddings/,30199,1613517504.0,0,,False,,,,,,636
722,,LanguageTechnology,"This paper digs into a new algorithm called SpAtten, an efficient algorithm-architecture co-design that leverages token sparsity, head sparsity, and quantization opportunities to reduce attention computation and memory access. 

\[[Paper Video Presentation](https://crossminds.ai/video/602b30c3d8d2f96ed18b0d23/)\] \[[arXiv Link](https://arxiv.org/abs/2012.09852)\]

&amp;#x200B;

**Abstract:** The attention mechanism is becoming increasingly popular in Natural Language Processing (NLP) applications, showing superior performance than convolutional and recurrent architectures. However, general-purpose platforms such as CPUs and GPUs are inefficient when performing attention inference due to complicated data movement and low arithmetic intensity. Moreover, existing NN accelerators mainly focus on optimizing convolutional or recurrent models, and cannot efficiently support attention. In this paper, we present SpAtten, an efficient algorithm-architecture co-design that leverages token sparsity, head sparsity, and quantization opportunities to reduce the attention computation and memory access. Inspired by the high redundancy of human languages, we propose the novel cascade token pruning to prune away unimportant tokens in the sentence. We also propose cascade head pruning to remove unessential heads. Cascade pruning is fundamentally different from weight pruning since there is no trainable weight in the attention mechanism, and the pruned tokens and heads are selected on the fly. To efficiently support them on hardware, we design a novel top-k engine to rank token and head importance scores with high throughput. Furthermore, we propose progressive quantization that first fetches MSBs only and performs the computation; if the confidence is low, it fetches LSBs and recomputes the attention outputs, trading computation for memory reduction. Extensive experiments on 30 benchmarks show that, on average, SpAtten reduces DRAM access by 10.0x with no accuracy loss, and achieves 1.6x, 3.0x, 162x, 347x speedup, and 1,4x, 3.2x, 1193x, 4059x energy savings over A3 accelerator, MNNFast accelerator, TITAN Xp GPU, Xeon CPU, respectively.

&amp;#x200B;

Authors: Hanrui Wang, Zhekai Zhang, Song Han (MIT)",t2_1uoh1xj8,False,,0,False,[R] SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning,[],r/LanguageTechnology,False,6,,0,,False,t3_llkerj,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1613559438.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This paper digs into a new algorithm called SpAtten, an efficient algorithm-architecture co-design that leverages token sparsity, head sparsity, and quantization opportunities to reduce attention computation and memory access. &lt;/p&gt;

&lt;p&gt;[&lt;a href=""https://crossminds.ai/video/602b30c3d8d2f96ed18b0d23/""&gt;Paper Video Presentation&lt;/a&gt;] [&lt;a href=""https://arxiv.org/abs/2012.09852""&gt;arXiv Link&lt;/a&gt;]&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; The attention mechanism is becoming increasingly popular in Natural Language Processing (NLP) applications, showing superior performance than convolutional and recurrent architectures. However, general-purpose platforms such as CPUs and GPUs are inefficient when performing attention inference due to complicated data movement and low arithmetic intensity. Moreover, existing NN accelerators mainly focus on optimizing convolutional or recurrent models, and cannot efficiently support attention. In this paper, we present SpAtten, an efficient algorithm-architecture co-design that leverages token sparsity, head sparsity, and quantization opportunities to reduce the attention computation and memory access. Inspired by the high redundancy of human languages, we propose the novel cascade token pruning to prune away unimportant tokens in the sentence. We also propose cascade head pruning to remove unessential heads. Cascade pruning is fundamentally different from weight pruning since there is no trainable weight in the attention mechanism, and the pruned tokens and heads are selected on the fly. To efficiently support them on hardware, we design a novel top-k engine to rank token and head importance scores with high throughput. Furthermore, we propose progressive quantization that first fetches MSBs only and performs the computation; if the confidence is low, it fetches LSBs and recomputes the attention outputs, trading computation for memory reduction. Extensive experiments on 30 benchmarks show that, on average, SpAtten reduces DRAM access by 10.0x with no accuracy loss, and achieves 1.6x, 3.0x, 162x, 347x speedup, and 1,4x, 3.2x, 1193x, 4059x energy savings over A3 accelerator, MNNFast accelerator, TITAN Xp GPU, Xeon CPU, respectively.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Authors: Hanrui Wang, Zhekai Zhang, Song Han (MIT)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,llkerj,True,,m1900kang2,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/llkerj/r_spatten_efficient_sparse_attention_architecture/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/llkerj/r_spatten_efficient_sparse_attention_architecture/,30199,1613530638.0,0,,False,,,,,,2219
723,,LanguageTechnology,"Apologies if this is a broken record post-- I tried looking around and couldn't find anything on this. 

I've heard in here multiple times that Speech &amp; Language Processing is sort of the Bible for NLP stuff. I've been going through it, doing the exercises, and trying to implement the algorithms it talks about where possible. I'm also trying to follow along with Relevant Papers for each chapter (like the original Word2Vec &amp; Negative Sampling papers for Word Vectors, etc)

That said, I'm not certain I'm really holding onto information in any manner where I'm actually going to implement it in my own projects. Are there any other supplements to this? Or is it sort of just drill this until you have it down?",t2_cc1kr,False,,0,False,Anything to be doing alongside Jurafsky &amp; Martin SLP?,[],r/LanguageTechnology,False,6,,0,,False,t3_lla9md,False,dark,1.0,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,True,,1613529151.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Apologies if this is a broken record post-- I tried looking around and couldn&amp;#39;t find anything on this. &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve heard in here multiple times that Speech &amp;amp; Language Processing is sort of the Bible for NLP stuff. I&amp;#39;ve been going through it, doing the exercises, and trying to implement the algorithms it talks about where possible. I&amp;#39;m also trying to follow along with Relevant Papers for each chapter (like the original Word2Vec &amp;amp; Negative Sampling papers for Word Vectors, etc)&lt;/p&gt;

&lt;p&gt;That said, I&amp;#39;m not certain I&amp;#39;m really holding onto information in any manner where I&amp;#39;m actually going to implement it in my own projects. Are there any other supplements to this? Or is it sort of just drill this until you have it down?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lla9md,True,,ryan516,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lla9md/anything_to_be_doing_alongside_jurafsky_martin_slp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lla9md/anything_to_be_doing_alongside_jurafsky_martin_slp/,30199,1613500351.0,0,,False,,,,,,720
724,,LanguageTechnology,,t2_aea54h4d,False,,0,False,Paper: 65 Million Probably-Asked Questions and What You Can Do With Them,[],r/LanguageTechnology,False,6,,0,,False,t3_llcqjw,False,dark,1.0,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,False,,1613535880.0,text,6,,,text,arxiv.org,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,llcqjw,True,,bert4QA,,0,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/llcqjw/paper_65_million_probablyasked_questions_and_what/,all_ads,False,https://arxiv.org/abs/2102.07033,30199,1613507080.0,1,,False,https://arxiv.org/abs/2102.07033,,,,,0
725,,LanguageTechnology,,t2_5alofkdd,False,,0,False,"Paper: ""Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm""",[],r/LanguageTechnology,False,6,,0,,False,t3_llb807,False,dark,0.88,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,False,,1613531716.0,text,6,,,text,arxiv.org,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,llb807,True,,Wiskkey,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/llb807/paper_prompt_programming_for_large_language/,all_ads,False,https://arxiv.org/abs/2102.07350,30199,1613502916.0,0,,False,https://arxiv.org/abs/2102.07350,,,,,0
726,,LanguageTechnology,"Hello. I'm reading the paper titled [_Neural Architectures for Named Entity Recognition (Lample et al., 2016)_](https://www.aclweb.org/anthology/N16-1030/) and had a question regarding some details of the paper.

In section 2.3 Parameterization and Training, the paper claims:

&gt; These representations (i.e., the hidden representations $h$ for left- and right-going LSTM) are concatenated and linearly projected onto a layer **whose size is equal to the number of distinct tags**. Instead of using the softmax output from this layer, we use a CRF as previously described to take into account neighboring tags, yielding the final predictions for every word.

The part that's confusing me is in bold. My understanding is that if we have an input sequence of length _n_, then the output of a CRF should also be of size _n_ since we're essentially predicting the labels for the entire sequence. If we linearly project the hidden representations onto a layer whose size is the number of distinct tags (say, _k_) then wouldn't there be a size mismatch?",t2_m8kccne,False,,0,False,Question Regarding LSTM-CRF architecture for NER,[],r/LanguageTechnology,False,6,,0,,False,t3_lljnlj,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1613556959.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello. I&amp;#39;m reading the paper titled &lt;a href=""https://www.aclweb.org/anthology/N16-1030/""&gt;&lt;em&gt;Neural Architectures for Named Entity Recognition (Lample et al., 2016)&lt;/em&gt;&lt;/a&gt; and had a question regarding some details of the paper.&lt;/p&gt;

&lt;p&gt;In section 2.3 Parameterization and Training, the paper claims:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;These representations (i.e., the hidden representations $h$ for left- and right-going LSTM) are concatenated and linearly projected onto a layer &lt;strong&gt;whose size is equal to the number of distinct tags&lt;/strong&gt;. Instead of using the softmax output from this layer, we use a CRF as previously described to take into account neighboring tags, yielding the final predictions for every word.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The part that&amp;#39;s confusing me is in bold. My understanding is that if we have an input sequence of length &lt;em&gt;n&lt;/em&gt;, then the output of a CRF should also be of size &lt;em&gt;n&lt;/em&gt; since we&amp;#39;re essentially predicting the labels for the entire sequence. If we linearly project the hidden representations onto a layer whose size is the number of distinct tags (say, &lt;em&gt;k&lt;/em&gt;) then wouldn&amp;#39;t there be a size mismatch?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lljnlj,True,,Seankala,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lljnlj/question_regarding_lstmcrf_architecture_for_ner/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lljnlj/question_regarding_lstmcrf_architecture_for_ner/,30199,1613528159.0,0,,False,,,,,,1049
727,,LanguageTechnology,,t2_aej9we1h,False,,0,False,CLIP - Learning Transferable Visual Models From Natural Language Supervision,[],r/LanguageTechnology,False,6,,0,,False,t3_llgw3n,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,False,,1613547849.0,text,6,,,text,lamaai.io,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,llgw3n,True,,lamaai_io,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/llgw3n/clip_learning_transferable_visual_models_from/,all_ads,False,https://www.lamaai.io/posts/clip-learning-transferable-visual-models-from-natural-language-supervision,30199,1613519049.0,0,,False,https://www.lamaai.io/posts/clip-learning-transferable-visual-models-from-natural-language-supervision,,,,,0
728,,LanguageTechnology,,t2_5s89k712,False,,0,False,Shortformer: Better Language Modeling using Shorter Inputs (Paper Explained),[],r/LanguageTechnology,False,6,,0,,False,t3_lkjakf,False,dark,0.92,,public,24,0,"{'content': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/WuwR5WTMteM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 267, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Shortformer: Better Language Modeling using Shorter Inputs (Paper Explained)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 267, 'html': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/WuwR5WTMteM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Deep Learning Explainer', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/WuwR5WTMteM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC9aifsrhLEt4cL4mbPiehMg'}}",False,False,,"{'content': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/WuwR5WTMteM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 267, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lkjakf', 'height': 200}",,False,24,,False,False,,False,,[],{},,False,,1613441389.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lkjakf,True,,deeplearningperson,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lkjakf/shortformer_better_language_modeling_using/,all_ads,False,https://youtu.be/WuwR5WTMteM,30199,1613412589.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Shortformer: Better Language Modeling using Shorter Inputs (Paper Explained)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 267, 'html': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/WuwR5WTMteM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Deep Learning Explainer', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/WuwR5WTMteM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC9aifsrhLEt4cL4mbPiehMg'}}",False,https://youtu.be/WuwR5WTMteM,,,,,0
729,,LanguageTechnology,"I've seen some posts on this in the far past (half a decade or more ago). Looking to see if anyone has any up to date ideas on the subject. Process of simplifying a single sentence. I know there are a lot of tools and models out there for summarizing multi-sentence corpora but I'm looking for one that can operate on a single sentence. Bonus points if it doesn't require training, domain-specific knowledge, or tuning.",t2_a6ibzdi5,False,,0,False,Single Sentence Simplification,[],r/LanguageTechnology,False,6,,0,,False,t3_lksosf,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1613468451.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve seen some posts on this in the far past (half a decade or more ago). Looking to see if anyone has any up to date ideas on the subject. Process of simplifying a single sentence. I know there are a lot of tools and models out there for summarizing multi-sentence corpora but I&amp;#39;m looking for one that can operate on a single sentence. Bonus points if it doesn&amp;#39;t require training, domain-specific knowledge, or tuning.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lksosf,True,,tanto_von_scumbag,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lksosf/single_sentence_simplification/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lksosf/single_sentence_simplification/,30199,1613439651.0,0,,False,,,,,,419
730,,LanguageTechnology,I would just like to know more about what the student experience is like at those universities for CL.,t2_8wrpuekd,False,,0,False,Has anyone here studied Computational Linguistics at Tübingen University (Germany) or at UAM university ( Poland) ?,[],r/LanguageTechnology,False,6,,0,,False,t3_lkka02,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1613444114.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I would just like to know more about what the student experience is like at those universities for CL.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lkka02,True,,Downtown-Cup-7458,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lkka02/has_anyone_here_studied_computational_linguistics/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lkka02/has_anyone_here_studied_computational_linguistics/,30199,1613415314.0,0,,False,,,,,,102
731,,LanguageTechnology,"struggling to choose Saarland or Stuttgart university.
How about work opportunity after graduating from both universities?",t2_8hn6n8gg,False,,0,False,Computational linguistics in Stuttgart university and Saarland university,[],r/LanguageTechnology,False,6,,0,,False,t3_lks7r3,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1613466907.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;struggling to choose Saarland or Stuttgart university.
How about work opportunity after graduating from both universities?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lks7r3,True,,Able-Ad1213,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lks7r3/computational_linguistics_in_stuttgart_university/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lks7r3/computational_linguistics_in_stuttgart_university/,30199,1613438107.0,1,,False,,,,,,122
732,,LanguageTechnology,"Hi all, I am looking to source some support to do some text summarization and sentiment analysis from some transcripts of interviews. 

I have a pretty small budget but I was wondering if this community could guide me to somewhere or someone who could take this on?",t2_9pqlz1g,False,,0,False,How to source support for a project,[],r/LanguageTechnology,False,6,,0,,False,t3_lki3rv,False,dark,0.81,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1613438107.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, I am looking to source some support to do some text summarization and sentiment analysis from some transcripts of interviews. &lt;/p&gt;

&lt;p&gt;I have a pretty small budget but I was wondering if this community could guide me to somewhere or someone who could take this on?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lki3rv,True,,zebedee1800,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lki3rv/how_to_source_support_for_a_project/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lki3rv/how_to_source_support_for_a_project/,30199,1613409307.0,0,,False,,,,,,265
733,,LanguageTechnology,"I work in a company and was recently assigned to an NLP team (it's a startup). Recently, my superior asked me to research technology that can be used to solve a particular problem he described that sounds a lot like a QnA chat bot.   


For example if the system is meant to be used by tenants to ask question about the building they live in, and the building has an automated locking system that uses keycards (and RFID) to get in, one use case may be: 

  
Tenant: I can't unlock the door

Bot: Is there a green light on the lock? 

Tenant: Yes

Bot: Is there any video feed on the screen? 

Tenant: No

Bot: Solution 1

&amp;#x200B;

I work in a company and was recently assigned to an NLP team (it's a startup). Recently, my superior asked me to research technology that can be used to solve a particular problem he described that sounds a lot like a QnA chatbot.   
 

Is AIML still used to build systems like these?",t2_3weuh9mf,False,,0,False,Is AIML worth considering in 2021?,[],r/LanguageTechnology,False,6,,0,,False,t3_lk5kf5,False,dark,0.89,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1613391792.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I work in a company and was recently assigned to an NLP team (it&amp;#39;s a startup). Recently, my superior asked me to research technology that can be used to solve a particular problem he described that sounds a lot like a QnA chat bot.   &lt;/p&gt;

&lt;p&gt;For example if the system is meant to be used by tenants to ask question about the building they live in, and the building has an automated locking system that uses keycards (and RFID) to get in, one use case may be: &lt;/p&gt;

&lt;p&gt;Tenant: I can&amp;#39;t unlock the door&lt;/p&gt;

&lt;p&gt;Bot: Is there a green light on the lock? &lt;/p&gt;

&lt;p&gt;Tenant: Yes&lt;/p&gt;

&lt;p&gt;Bot: Is there any video feed on the screen? &lt;/p&gt;

&lt;p&gt;Tenant: No&lt;/p&gt;

&lt;p&gt;Bot: Solution 1&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I work in a company and was recently assigned to an NLP team (it&amp;#39;s a startup). Recently, my superior asked me to research technology that can be used to solve a particular problem he described that sounds a lot like a QnA chatbot.   &lt;/p&gt;

&lt;p&gt;Is AIML still used to build systems like these?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lk5kf5,True,,radioactive-poop,,15,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lk5kf5/is_aiml_worth_considering_in_2021/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lk5kf5/is_aiml_worth_considering_in_2021/,30199,1613362992.0,0,,False,,,,,,921
734,,LanguageTechnology,"Or are those audio snippets collection from host characters real videos

If not what technology is used to create them.",t2_2n3nfxca,False,,0,False,How do fake news video have audio matching almost perfectly to chosen hosts voice?,[],r/LanguageTechnology,False,6,,0,,False,t3_lkey2a,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1613428853.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Or are those audio snippets collection from host characters real videos&lt;/p&gt;

&lt;p&gt;If not what technology is used to create them.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lkey2a,True,,vijaykurhade,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lkey2a/how_do_fake_news_video_have_audio_matching_almost/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lkey2a/how_do_fake_news_video_have_audio_matching_almost/,30199,1613400053.0,0,,False,,,,,,119
735,,LanguageTechnology,"*edit:* formatting

Hello everyone. I'm working my way through *Natural Language Processing with Python and spaCy* by Yuli Vasilev and have hit an error in chapter 2.

The chapter wants me to add a special case to set the `LEMMA` of ""Frisco"" to ""San Francisco."" I have written the following code:

    import spacy
    from spacy.symbols import ORTH, LEMMA
    
    nlp = spacy.load('en_core_web_sm')
    doc = nlp(u'I am flying to Frisco')
    print([w.text for w in doc])
    
    special_case = [{ORTH: u'Frisco', LEMMA: u'San Francisco'}]
    nlp.tokenizer.add_special_case(u'Frisco', special_case)
    print([w.lemma_ for w in nlp(u'I am flying to Frisco')])

but when I run this, I get the following exception:

    Traceback (most recent call last):
      File "".\lemmatization2.py"", line 6, in &lt;module&gt;
        nlp.tokenizer.add_special_case(u'Frisco', special_case)
      File ""spacy\tokenizer.pyx"", line 601, in spacy.tokenizer.Tokenizer.add_special_case
      File ""spacy\tokenizer.pyx"", line 589, in spacy.tokenizer.Tokenizer._validate_special_case
    ValueError: [E1005] Unable to set attribute 'LEMMA' in tokenizer exception for 'Frisco'. Tokenizer exceptions are only allowed to specify ORTH and NORM.

I've tried to look up how to change the `LEMMA` value of a token and there's some Stack Overflow answers where people are doing the same thing the book is telling me to do. I can't find any information on this outside of GitHub issues that seem related at first but then aren't.

So I figure maybe when this book was written (looks like 2020?), spaCy let you add exceptions for `LEMMA`, but now it doesn't. Is that accurate? Should I use `NORM` instead? Is there a different problem that I might be overlooking?",t2_42tyvo8,False,,0,False,Did spaCy Used to Allow LEMMA exceptions?,[],r/LanguageTechnology,False,6,,0,,False,t3_lk56p2,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1613390459.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;em&gt;edit:&lt;/em&gt; formatting&lt;/p&gt;

&lt;p&gt;Hello everyone. I&amp;#39;m working my way through &lt;em&gt;Natural Language Processing with Python and spaCy&lt;/em&gt; by Yuli Vasilev and have hit an error in chapter 2.&lt;/p&gt;

&lt;p&gt;The chapter wants me to add a special case to set the &lt;code&gt;LEMMA&lt;/code&gt; of &amp;quot;Frisco&amp;quot; to &amp;quot;San Francisco.&amp;quot; I have written the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import spacy
from spacy.symbols import ORTH, LEMMA

nlp = spacy.load(&amp;#39;en_core_web_sm&amp;#39;)
doc = nlp(u&amp;#39;I am flying to Frisco&amp;#39;)
print([w.text for w in doc])

special_case = [{ORTH: u&amp;#39;Frisco&amp;#39;, LEMMA: u&amp;#39;San Francisco&amp;#39;}]
nlp.tokenizer.add_special_case(u&amp;#39;Frisco&amp;#39;, special_case)
print([w.lemma_ for w in nlp(u&amp;#39;I am flying to Frisco&amp;#39;)])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;but when I run this, I get the following exception:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):
  File &amp;quot;.\lemmatization2.py&amp;quot;, line 6, in &amp;lt;module&amp;gt;
    nlp.tokenizer.add_special_case(u&amp;#39;Frisco&amp;#39;, special_case)
  File &amp;quot;spacy\tokenizer.pyx&amp;quot;, line 601, in spacy.tokenizer.Tokenizer.add_special_case
  File &amp;quot;spacy\tokenizer.pyx&amp;quot;, line 589, in spacy.tokenizer.Tokenizer._validate_special_case
ValueError: [E1005] Unable to set attribute &amp;#39;LEMMA&amp;#39; in tokenizer exception for &amp;#39;Frisco&amp;#39;. Tokenizer exceptions are only allowed to specify ORTH and NORM.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&amp;#39;ve tried to look up how to change the &lt;code&gt;LEMMA&lt;/code&gt; value of a token and there&amp;#39;s some Stack Overflow answers where people are doing the same thing the book is telling me to do. I can&amp;#39;t find any information on this outside of GitHub issues that seem related at first but then aren&amp;#39;t.&lt;/p&gt;

&lt;p&gt;So I figure maybe when this book was written (looks like 2020?), spaCy let you add exceptions for &lt;code&gt;LEMMA&lt;/code&gt;, but now it doesn&amp;#39;t. Is that accurate? Should I use &lt;code&gt;NORM&lt;/code&gt; instead? Is there a different problem that I might be overlooking?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lk56p2,True,,5awaja,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lk56p2/did_spacy_used_to_allow_lemma_exceptions/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lk56p2/did_spacy_used_to_allow_lemma_exceptions/,30199,1613361659.0,0,,False,,,,,,1736
736,,LanguageTechnology,,t2_5fsp2x6v,False,,0,False,How we have been evaluating Knowledge Graph Completion models INACCURATELY | Research Papers Summary 008,[],r/LanguageTechnology,False,6,,0,,False,t3_ljuuux,False,dark,0.93,,public,12,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/uGRyWQRcyjY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'A Re-evaluation of Knowledge Graph Completion Methods | Research Papers Summary 008', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/uGRyWQRcyjY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/uGRyWQRcyjY/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/uGRyWQRcyjY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ljuuux', 'height': 200}",,False,12,,False,False,,False,,[],{},,False,,1613358299.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ljuuux,True,,RyanAI100,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ljuuux/how_we_have_been_evaluating_knowledge_graph/,all_ads,False,https://youtu.be/uGRyWQRcyjY,30199,1613329499.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'A Re-evaluation of Knowledge Graph Completion Methods | Research Papers Summary 008', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/uGRyWQRcyjY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/uGRyWQRcyjY/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,https://youtu.be/uGRyWQRcyjY,,,,,0
737,,LanguageTechnology,[https://www.paperdigest.org/2021/02/most-influential-acl-papers/](https://www.paperdigest.org/2021/02/most-influential-acl-papers/),t2_2niqx8mz,False,,0,False,Most influential ACL papers by year (1980-2020),[],r/LanguageTechnology,False,6,,0,,False,t3_ljr9ld,False,dark,0.89,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1613348069.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.paperdigest.org/2021/02/most-influential-acl-papers/""&gt;https://www.paperdigest.org/2021/02/most-influential-acl-papers/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ljr9ld,True,,biandangou,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ljr9ld/most_influential_acl_papers_by_year_19802020/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ljr9ld/most_influential_acl_papers_by_year_19802020/,30199,1613319269.0,0,,False,,,,,,132
738,,LanguageTechnology,,t2_t1g8u,False,,0,False,"Happy Valentine's, everyone! This year, I used NLP to solve me being single by programming an autocomplete program using Markov Chains and RNNs trained on romantic movies! The results were REALLY unexpected so I turned that whole experience into a video. I hope y'all like it!",[],r/LanguageTechnology,False,6,,0,,False,t3_ljogla,False,dark,0.8,,public,13,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/QdsEeFmbOyw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': ""Using Artificial Intelligence to complete my sentences | Valentine's Day Special ❤"", 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/QdsEeFmbOyw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'AI does AI', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/QdsEeFmbOyw/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCALnBYThNO4_b77zJDY4mpQ'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/QdsEeFmbOyw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ljogla', 'height': 200}",,False,13,,False,False,,False,,[],{},,False,,1613338413.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ljogla,True,,arhumishi,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ljogla/happy_valentines_everyone_this_year_i_used_nlp_to/,all_ads,False,https://youtu.be/QdsEeFmbOyw,30199,1613309613.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': ""Using Artificial Intelligence to complete my sentences | Valentine's Day Special ❤"", 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/QdsEeFmbOyw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'AI does AI', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/QdsEeFmbOyw/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCALnBYThNO4_b77zJDY4mpQ'}}",False,https://youtu.be/QdsEeFmbOyw,,,,,0
739,,LanguageTechnology," Q1: where is self supervision implemented in this code ([https://github.com/Shen-Lab/SS-GCNs/blob/master/SS-GCNs/main.py](https://github.com/Shen-Lab/SS-GCNs/blob/master/SS-GCNs/main.py)) . please mention the line numbers and give a small explanation

Q2:Also how to implement self super vision if i want to implement in this code [https://github.com/tkipf/pygcn](https://github.com/tkipf/pygcn)",t2_2k1ykjv9,False,,0,False,How to implement self supervision in GCN [D],[],r/LanguageTechnology,False,6,,0,,False,t3_lk2rv7,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1613382371.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Q1: where is self supervision implemented in this code (&lt;a href=""https://github.com/Shen-Lab/SS-GCNs/blob/master/SS-GCNs/main.py""&gt;https://github.com/Shen-Lab/SS-GCNs/blob/master/SS-GCNs/main.py&lt;/a&gt;) . please mention the line numbers and give a small explanation&lt;/p&gt;

&lt;p&gt;Q2:Also how to implement self super vision if i want to implement in this code &lt;a href=""https://github.com/tkipf/pygcn""&gt;https://github.com/tkipf/pygcn&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lk2rv7,True,,ajithvallabai,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lk2rv7/how_to_implement_self_supervision_in_gcn_d/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lk2rv7/how_to_implement_self_supervision_in_gcn_d/,30199,1613353571.0,0,,False,,,,,,396
740,,LanguageTechnology,"I've been playing around with Mallet and I know how to discover the per-document topic distributions, per-topic word distributions, and how to find similar documents within an existing corpus.

&amp;#x200B;

But what I'm not clear on and can't find anywhere is how to deal with NEW documents without running the entire topic model again.

&amp;#x200B;

So given a new document, discovering what topics are within in and also similar documents within the base corpus.

&amp;#x200B;

Can anyone offer some light on this?",t2_9g160,False,,0,False,Discovering topics in NEW document using Mallet,[],r/LanguageTechnology,False,6,,0,,False,t3_ljy7zs,False,dark,0.38,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1613368091.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been playing around with Mallet and I know how to discover the per-document topic distributions, per-topic word distributions, and how to find similar documents within an existing corpus.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;But what I&amp;#39;m not clear on and can&amp;#39;t find anywhere is how to deal with NEW documents without running the entire topic model again.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So given a new document, discovering what topics are within in and also similar documents within the base corpus.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Can anyone offer some light on this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ljy7zs,True,,linklater2012,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ljy7zs/discovering_topics_in_new_document_using_mallet/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ljy7zs/discovering_topics_in_new_document_using_mallet/,30199,1613339291.0,0,,False,,,,,,518
741,,LanguageTechnology,"Hi, so if you've seen a previous post of mine, I mentioned a class project where we were designing a new model. The problem here is, that our advisory board wants us to show a comparison between the results of this model and existing models in the field.

The Transformer and the Reformer are two such models, among others. The problem is that I assume it would be impossible to train them for an extended period of time, on a dataset on the scale of enwik9 or so. My laptop specs are 8GB RAM with 256 GB SSD and I'm not really sure about the GPU but I think there is some sort of NVidia GeForce.

Colab crashed the last time I tried to train the model. And we can't really afford to shell out a lot for cloud instances, being undergrads on a budget.

Does anybody have any suggestions about what to do?",t2_9vs8qz4o,False,,0,False,How to train large models on a normal laptop?,[],r/LanguageTechnology,False,6,,0,,False,t3_ljsyrt,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1613352903.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, so if you&amp;#39;ve seen a previous post of mine, I mentioned a class project where we were designing a new model. The problem here is, that our advisory board wants us to show a comparison between the results of this model and existing models in the field.&lt;/p&gt;

&lt;p&gt;The Transformer and the Reformer are two such models, among others. The problem is that I assume it would be impossible to train them for an extended period of time, on a dataset on the scale of enwik9 or so. My laptop specs are 8GB RAM with 256 GB SSD and I&amp;#39;m not really sure about the GPU but I think there is some sort of NVidia GeForce.&lt;/p&gt;

&lt;p&gt;Colab crashed the last time I tried to train the model. And we can&amp;#39;t really afford to shell out a lot for cloud instances, being undergrads on a budget.&lt;/p&gt;

&lt;p&gt;Does anybody have any suggestions about what to do?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ljsyrt,True,,exwordsmythe,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ljsyrt/how_to_train_large_models_on_a_normal_laptop/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ljsyrt/how_to_train_large_models_on_a_normal_laptop/,30199,1613324103.0,0,,False,,,,,,803
742,,LanguageTechnology,"I have been exploring some options like Gothenburg and Saarbrucken. Wanted to hear some thoughts from current or past students, about these and other such programs.

If it helps, I am quite interested in common sense reasoning and explainability so if the Unis have research labs focused on such topics will be an added bonus.",t2_j6dy6,False,,0,False,Best Masters/PhD programs in Computational Linguistics in Europe,[],r/LanguageTechnology,False,6,,0,,False,t3_ljhkjs,False,dark,0.86,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1613306836.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have been exploring some options like Gothenburg and Saarbrucken. Wanted to hear some thoughts from current or past students, about these and other such programs.&lt;/p&gt;

&lt;p&gt;If it helps, I am quite interested in common sense reasoning and explainability so if the Unis have research labs focused on such topics will be an added bonus.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ljhkjs,True,,ribhu97,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ljhkjs/best_mastersphd_programs_in_computational/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ljhkjs/best_mastersphd_programs_in_computational/,30199,1613278036.0,0,,False,,,,,,326
743,,LanguageTechnology,"Has anyone here done the MS in Computational linguistics at Montclair? Or is currently in the program? I’m considering applying but I would like to know if anyone would recommend it/what they think about it. 
Thanks!",,False,,0,False,MS in CL at Montclair?,[],r/LanguageTechnology,False,6,,0,,False,t3_ljcdju,False,dark,0.75,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,,,False,,,{},,True,,1613288706.0,text,6,,,,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Has anyone here done the MS in Computational linguistics at Montclair? Or is currently in the program? I’m considering applying but I would like to know if anyone would recommend it/what they think about it. 
Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ljcdju,True,,[deleted],,3,True,all_ads,False,[],,dark,/r/LanguageTechnology/comments/ljcdju/ms_in_cl_at_montclair/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ljcdju/ms_in_cl_at_montclair/,30199,1613259906.0,1,,False,,,,,,216
744,,LanguageTechnology,"Hi guys! I'm new here and I have to create a model for the Question Answering task using only as training data the SQuAD dataset. It's very difficult find something that match my requests, cause all the model used in this task are pretained model like BERT or LUKE and I want to trained the model by myself. 

I would be really grateful to any of you who know how or where to find tutorials or resources.",t2_1w2gnf9b,False,,0,False,How to create a model for Question Answering on the SQuAD Dataset?,[],r/LanguageTechnology,False,6,,0,,False,t3_lj29ww,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1613258002.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys! I&amp;#39;m new here and I have to create a model for the Question Answering task using only as training data the SQuAD dataset. It&amp;#39;s very difficult find something that match my requests, cause all the model used in this task are pretained model like BERT or LUKE and I want to trained the model by myself. &lt;/p&gt;

&lt;p&gt;I would be really grateful to any of you who know how or where to find tutorials or resources.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lj29ww,True,,NickBoomZoom,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lj29ww/how_to_create_a_model_for_question_answering_on/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lj29ww/how_to_create_a_model_for_question_answering_on/,30199,1613229202.0,0,,False,,,,,,404
745,,LanguageTechnology,,t2_86zivv0h,False,,0,False,Any standard textbooks on Bangla Natural Language Processing or other Indian languages? --Where data preprocessing is nicely explained.,[],r/LanguageTechnology,False,6,,0,,False,t3_lj5358,False,dark,0.85,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1613266768.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lj5358,True,,hafizcse031,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lj5358/any_standard_textbooks_on_bangla_natural_language/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lj5358/any_standard_textbooks_on_bangla_natural_language/,30199,1613237968.0,0,,False,,,,,,0
746,,LanguageTechnology,"# John Snow Labs NLU 1.1.1 : New multilingual models, Spark 2.3 support, new tutorials and more! 

## NLU 1.1.1 Release Notes
We are very excited to release NLU 1.1.1!
This release features 3 new tutorial notebooks for Open/Closed book question answering with Google's T5, Intent classification, and Aspect Based NER.
In Addition, NLU 1.1.0 comes with  25+ pre-trained models and pipelines in Amharic, Bengali, Bhojpuri, Japanese, and Korean languages from the [amazing Spark2.7.2 release](https://github.com/JohnSnowLabs/spark-nlp/releases/tag/2.7.2)
Finally, NLU now supports running on Spark 2.3 clusters.


### NLU 1.1.0 New Non-English Models
|Language | nlu.load() reference | Spark NLP Model reference | Type |
|---------|---------------------|----------------------------|------|
|Arabic | [ar.ner](https://nlp.johnsnowlabs.com/2020/12/05/aner_cc_300d_ar.html) |[arabic_w2v_cc_300d](https://nlp.johnsnowlabs.com/2020/12/05/aner_cc_300d_ar.html) | Named Entity Recognizer                    |
|Arabic | [ar.embed.aner](https://nlp.johnsnowlabs.com/2020/12/05/aner_cc_300d_ar.html) |[aner_cc_300d](https://nlp.johnsnowlabs.com/2020/12/05/aner_cc_300d_ar.html) | Word Embedding                    |
|Arabic | [ar.embed.aner.300d](https://nlp.johnsnowlabs.com/2020/12/05/aner_cc_300d_ar.html) |[aner_cc_300d](https://nlp.johnsnowlabs.com/2020/12/05/aner_cc_300d_ar.html) | Word Embedding (Alias)                    |
|Bengali | [bn.stopwords](https://nlp.johnsnowlabs.com/2020/07/14/stopwords_bn.html) |[stopwords_bn](https://nlp.johnsnowlabs.com/2020/07/14/stopwords_bn.html) | Stopwords Cleaner                    |
|Bengali | [bn.pos](https://nlp.johnsnowlabs.com/2021/01/20/pos_msri_bn.html) |[pos_msri](https://nlp.johnsnowlabs.com/2021/01/20/pos_msri_bn.html) | Part of Speech                    |
|Thai | [th.segment_words](https://nlp.johnsnowlabs.com/2021/01/11/ner_lst20_glove_840B_300d_th.html) |[wordseg_best](https://nlp.johnsnowlabs.com/2021/01/11/ner_lst20_glove_840B_300d_th.html) | Word Segmenter                    |
|Thai | [th.pos](https://nlp.johnsnowlabs.com/2021/01/13/pos_lst20_th.html) |[pos_lst20](https://nlp.johnsnowlabs.com/2021/01/13/pos_lst20_th.html) | Part of Speech                    |
|Thai |   [th.sentiment](https://nlp.johnsnowlabs.com/2021/01/14/sentiment_jager_use_th.html) |[sentiment_jager_use](https://nlp.johnsnowlabs.com/2021/01/14/sentiment_jager_use_th.html) | Sentiment Classifier                     |
|Thai |    [th.classify.sentiment](https://nlp.johnsnowlabs.com/2021/01/14/sentiment_jager_use_th.html) |[sentiment_jager_use](https://nlp.johnsnowlabs.com/2021/01/14/sentiment_jager_use_th.html) | Sentiment Classifier (Alias)                    |
|Chinese | [zh.pos.ud_gsd_trad](https://nlp.johnsnowlabs.com/2021/01/25/pos_ud_gsd_trad_zh.html) |[pos_ud_gsd_trad](https://nlp.johnsnowlabs.com/2021/01/25/pos_ud_gsd_trad_zh.html) | Part of Speech                    |
|Chinese | [zh.segment_words.gsd](https://nlp.johnsnowlabs.com/2021/01/25/wordseg_gsd_ud_trad_zh.html) |[wordseg_gsd_ud_trad](https://nlp.johnsnowlabs.com/2021/01/25/wordseg_gsd_ud_trad_zh.html) | Word Segmenter                    |
|Bihari | [bh.pos](https://nlp.johnsnowlabs.com/2021/01/18/pos_ud_bhtb_bh.html) |[pos_ud_bhtb](https://nlp.johnsnowlabs.com/2021/01/18/pos_ud_bhtb_bh.html) | Part of Speech                    |
|Amharic | [am.pos](https://nlp.johnsnowlabs.com/2021/01/20/pos_ud_att_am.html) |[pos_ud_att](https://nlp.johnsnowlabs.com/2021/01/20/pos_ud_att_am.html) | Part of Speech                    |



### NLU 1.1.1 New English Models and Pipelines
|Language | nlu.load() reference | Spark NLP Model reference | Type |
|---------|---------------------|----------------------------|------|
| English | [en.sentiment.glove](https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html) |[analyze_sentimentdl_glove_imdb](https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html)     | Sentiment Classifier |
| English | [en.sentiment.glove.imdb](https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html) |[analyze_sentimentdl_glove_imdb](https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html)     | Sentiment Classifier (Alias) |
| English | [en.classify.sentiment.glove.imdb](https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html) |[analyze_sentimentdl_glove_imdb](https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html)     | Sentiment Classifier (Alias) |
| English | [en.classify.sentiment.glove](https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html) |[analyze_sentimentdl_glove_imdb](https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html)     | Sentiment Classifier (Alias) |
| English | [en.classify.trec50.pipe](https://nlp.johnsnowlabs.com/2021/01/08/classifierdl_use_trec50_pipeline_en.html) |[classifierdl_use_trec50_pipeline](https://nlp.johnsnowlabs.com/2021/01/08/classifierdl_use_trec50_pipeline_en.html)     | Language Classifier |
| English | [en.ner.onto.large](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_large_en.html) |[onto_recognize_entities_electra_large](https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_large_en.html)     | Named Entity Recognizer |
| English | [en.classify.questions.atis](https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html) |[classifierdl_use_atis](https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html)     | Intent Classifier |
| English  | [en.classify.questions.airline](https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html) |[classifierdl_use_atis](https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html)     | Intent Classifier (Alias) |
| English | [en.classify.intent.atis](https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html) |[classifierdl_use_atis](https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html)     | Intent Classifier (Alias) |
| English | [en.classify.intent.airline](https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html) |[classifierdl_use_atis](https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html)     | Intent Classifier (Alias) |
| English | [en.ner.atis](https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html) |[nerdl_atis_840b_300d](https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html)     | Aspect based NER |
| English | [en.ner.airline](https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html) |[nerdl_atis_840b_300d](https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html)     | Aspect based NER (Alias) |
| English | [en.ner.aspect.airline](https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html) |[nerdl_atis_840b_300d](https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html)     | Aspect based NER (Alias) |
| English | [en.ner.aspect.atis](https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html) |[nerdl_atis_840b_300d](https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html)     | Aspect based NER (Alias) |

### New Easy NLU 1-liner Examples : 

#### Extract aspects and entities from airline questions (ATIS dataset)
```python
	
nlu.load(""en.ner.atis"").predict(""i want to fly from baltimore to dallas round trip"")
output:  [""baltimore"","" dallas"", ""round trip""]
```



#### Intent Classification for Airline Traffic Information System queries (ATIS dataset)


```python

nlu.load(""en.classify.questions.atis"").predict(""what is the price of flight from newyork to washington"")
output:  ""atis_airfare""	
```



#### Recognize Entities OntoNotes - ELECTRA Large


```python

nlu.load(""en.ner.onto.large"").predict(""Johnson first entered politics when elected in 2001 as a member of Parliament. He then served eight years as the mayor of London."")	
output:  [""Johnson"", ""first"", ""2001"", ""eight years"", ""London""]	
```

#### Question classification of open-domain and fact-based questions Pipeline - TREC50


```python
nlu.load(""en.classify.trec50.pipe"").predict(""When did the construction of stone circles begin in the UK? "")
output:  LOC_other
```

#### Traditional Chinese Word Segmentation

```python
# 'However, this treatment also creates some problems' in Chinese
nlu.load(""zh.segment_words.gsd"").predict(""然而，這樣的處理也衍生了一些問題。"")
output:  [""然而"","","",""這樣"",""的"",""處理"",""也"",""衍生"",""了"",""一些"",""問題"",""。""]

```


#### Part of Speech for Traditional Chinese

```python
# 'However, this treatment also creates some problems' in Chinese
nlu.load(""zh.pos.ud_gsd_trad"").predict(""然而，這樣的處理也衍生了一些問題。"")
```
Output:

|Token |  POS   |
| ----- | ----- |
| 然而  | ADV   |
| ，    | PUNCT |
| 這樣  | PRON  |
| 的    | PART  |
| 處理  | NOUN  |
| 也    | ADV   |
| 衍生  | VERB  |
| 了    | PART  |
| 一些  | ADJ   |
| 問題  | NOUN  |
| 。    | PUNCT |

#### Thai Word Segment Recognition


```python
# 'Mona Lisa is a 16th-century oil painting created by Leonardo held at the Louvre in Paris' in Thai
nlu.loadnlu.load(""th.segment_words"").predict(""Mona Lisa เป็นภาพวาดสีน้ำมันในศตวรรษที่ 16 ที่สร้างโดย Leonardo จัดขึ้นที่พิพิธภัณฑ์ลูฟร์ในปารีส"")

```
Output:

| token |
| --------- |
| M         |
| o         |
| n         |
| a         |
| Lisa      |
| เป็น       |
| ภาพ       |
| ว         |
| า         |
| ด         |
| สีน้ำ       |
| มัน        |
| ใน        |
| ศตวรรษ    |
| ที่         |
| 16        |
| ที่         |
| สร้าง      |
| โ         |
| ด         |
| ย         |
| L         |
| e         |
| o         |
| n         |
| a         |
| r         |
| d         |
| o         |
| จัด        |
| ขึ้น        |
| ที่         |
| พิพิธภัณฑ์    |
| ลูฟร์       |
| ใน        |
| ปารีส      |

#### Part of Speech for Bengali (POS)

```python
# 'The village is also called 'Mod' in Tora language' in Bengali 
nlu.load(""bn.pos"").predict(""বাসস্থান-ঘরগৃহস্থালি তোড়া ভাষায় গ্রামকেও বলে ` মোদ ' ৷"")
```
Output:

| token             | pos  |
| ----------------- | ---- |
| বাসস্থান-ঘরগৃহস্থালি | NN   |
| তোড়া              | NNP  |
| ভাষায়             | NN   |
| গ্রামকেও           | NN   |
| বলে               | VM   |
| `                 | SYM  |
| মোদ               | NN   |
| '                 | SYM  |
| ৷                 | SYM  |



#### Stop Words Cleaner for Bengali


```python
# 'This language is not enough' in Bengali 
df = nlu.load(""bn.stopwords"").predict(""এই ভাষা যথেষ্ট নয়"")

```
Output:

| cleanTokens | token |
| :---------- | :---- |
| ভাষা        | এই    |
| যথেষ্ট       | ভাষা  |
| নয়          | যথেষ্ট |
| None        | নয়    |


#### Part of Speech for Bengali
```python

# 'The people of Ohu know that the foundation of Bhojpuri was shaken' in Bengali
nlu.load('bh.pos').predict(""ओहु लोग के मालूम बा कि श्लील होखते भोजपुरी के नींव हिल जाई"")
```
Output:

| pos   | token   |
| :---- | :------ |
| DET   | ओहु     |
| NOUN  | लोग     |
| ADP   | के      |
| NOUN  | मालूम   |
| VERB  | बा      |
| SCONJ | कि      |
| ADJ   | श्लील   |
| VERB  | होखते   |
| PROPN | भोजपुरी |
| ADP   | के      |
| NOUN  | नींव    |
| VERB  | हिल     |
| AUX   | जाई     |


#### Amharic Part of Speech (POS)
```python
# ' ""Son, finish the job,"" he said.' in Amharic
nlu.load('am.pos').predict('ልጅ ኡ ን ሥራ ው ን አስጨርስ ኧው ኣል ኧሁ ።""')
```

Output:

| pos   | token   |
|:------|:--------|
| NOUN  | ልጅ      |
| DET   | ኡ       |
| PART  | ን       |
| NOUN  | ሥራ      |
| DET   | ው       |
| PART  | ን       |
| VERB  | አስጨርስ   |
| PRON  | ኧው      |
| AUX   | ኣል      |
| PRON  | ኧሁ      |
| PUNCT | ።       |
| NOUN  | ""       |


#### Thai Sentiment Classification
```python
#  'I love peanut butter and jelly!' in thai
nlu.load('th.classify.sentiment').predict('ฉันชอบเนยถั่วและเยลลี่!')[['sentiment','sentiment_confidence']]
```

Output:

| sentiment   |   sentiment_confidence |
|:------------|-----------------------:|
| positive    |               0.999998 |


#### Arabic Named Entity Recognition (NER)
```python
# 'In 1918, the forces of the Arab Revolt liberated Damascus with the help of the British' in Arabic
nlu.load('ar.ner').predict('في عام 1918 حررت قوات الثورة العربية دمشق بمساعدة من الإنكليز',output_level='chunk')[['entities_confidence','ner_confidence','entities']]
```

Output:

| entity_class   | ner_confidence                                                                                                                                                                  | entities            |
|:----------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------|
| ORG                   | [1.0, 1.0, 1.0, 0.9997000098228455, 0.9840999841690063, 0.9987999796867371, 0.9990000128746033, 0.9998999834060669, 0.9998999834060669, 0.9993000030517578, 0.9998999834060669] | قوات الثورة العربية |
| LOC                   | [1.0, 1.0, 1.0, 0.9997000098228455, 0.9840999841690063, 0.9987999796867371, 0.9990000128746033, 0.9998999834060669, 0.9998999834060669, 0.9993000030517578, 0.9998999834060669] | دمشق                |
| PER                   | [1.0, 1.0, 1.0, 0.9997000098228455, 0.9840999841690063, 0.9987999796867371, 0.9990000128746033, 0.9998999834060669, 0.9998999834060669, 0.9993000030517578, 0.9998999834060669] | الإنكليز            |



### NLU 1.1.0 Enhancements : 
-  Spark 2.3 compatibility

### New NLU Notebooks and Tutorials 
- [Open and Closed book question Ansering](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_question_answering.ipynb)
- [Aspect based NER for Airline ATIS]New multilingual models, Spark 2.3 support, new tutorials for Bengali, Bhojpuri, Japanese, T5  and more in 1 line of Python code with NLU 1.1.1!(https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/classifiers/intent_classification_airlines_ATIS.ipynb)
- [Intent Classification for Airline emssages ATIS](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/named_entity_recognition_(NER)/NER_aspect_airline_ATIS.ipynb)

### Installation

```bash
# PyPi
!pip install nlu pyspark==2.4.7
#Conda
# Install NLU from Anaconda/Conda
conda install -c johnsnowlabs nlu
```

### Additional NLU ressources
- [NLU Website](https://nlu.johnsnowlabs.com/)
- [All NLU Tutorial Notebooks](https://nlu.johnsnowlabs.com/docs/en/notebooks)
- [NLU Videos and Blogposts on NLU](https://nlp.johnsnowlabs.com/learn#pythons-nlu-library)
- [NLU on Github](https://github.com/JohnSnowLabs/nlu)",t2_53n73cus,False,,0,False,"New multilingual models, Spark 2.3 support, new tutorials for Bengali, Bhojpuri, Japanese, T5, and more in 1 line of Python code with NLU 1.1.1!",[],r/LanguageTechnology,False,6,,0,,False,t3_liv1mj,False,dark,1.0,,public,19,0,{},,False,[],,False,False,,{},,False,19,,False,False,,False,,[],{},,True,,1613226595.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;h1&gt;John Snow Labs NLU 1.1.1 : New multilingual models, Spark 2.3 support, new tutorials and more!&lt;/h1&gt;

&lt;h2&gt;NLU 1.1.1 Release Notes&lt;/h2&gt;

&lt;p&gt;We are very excited to release NLU 1.1.1!
This release features 3 new tutorial notebooks for Open/Closed book question answering with Google&amp;#39;s T5, Intent classification, and Aspect Based NER.
In Addition, NLU 1.1.0 comes with  25+ pre-trained models and pipelines in Amharic, Bengali, Bhojpuri, Japanese, and Korean languages from the &lt;a href=""https://github.com/JohnSnowLabs/spark-nlp/releases/tag/2.7.2""&gt;amazing Spark2.7.2 release&lt;/a&gt;
Finally, NLU now supports running on Spark 2.3 clusters.&lt;/p&gt;

&lt;h3&gt;NLU 1.1.0 New Non-English Models&lt;/h3&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Arabic&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/aner_cc_300d_ar.html""&gt;ar.ner&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/aner_cc_300d_ar.html""&gt;arabic_w2v_cc_300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Named Entity Recognizer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Arabic&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/aner_cc_300d_ar.html""&gt;ar.embed.aner&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/aner_cc_300d_ar.html""&gt;aner_cc_300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Word Embedding&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Arabic&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/aner_cc_300d_ar.html""&gt;ar.embed.aner.300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/05/aner_cc_300d_ar.html""&gt;aner_cc_300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Word Embedding (Alias)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Bengali&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/14/stopwords_bn.html""&gt;bn.stopwords&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/14/stopwords_bn.html""&gt;stopwords_bn&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Stopwords Cleaner&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Bengali&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/20/pos_msri_bn.html""&gt;bn.pos&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/20/pos_msri_bn.html""&gt;pos_msri&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Part of Speech&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Thai&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/11/ner_lst20_glove_840B_300d_th.html""&gt;th.segment_words&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/11/ner_lst20_glove_840B_300d_th.html""&gt;wordseg_best&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Word Segmenter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Thai&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/13/pos_lst20_th.html""&gt;th.pos&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/13/pos_lst20_th.html""&gt;pos_lst20&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Part of Speech&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Thai&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/14/sentiment_jager_use_th.html""&gt;th.sentiment&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/14/sentiment_jager_use_th.html""&gt;sentiment_jager_use&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Sentiment Classifier&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Thai&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/14/sentiment_jager_use_th.html""&gt;th.classify.sentiment&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/14/sentiment_jager_use_th.html""&gt;sentiment_jager_use&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Sentiment Classifier (Alias)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chinese&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/25/pos_ud_gsd_trad_zh.html""&gt;zh.pos.ud_gsd_trad&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/25/pos_ud_gsd_trad_zh.html""&gt;pos_ud_gsd_trad&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Part of Speech&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chinese&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/25/wordseg_gsd_ud_trad_zh.html""&gt;zh.segment_words.gsd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/25/wordseg_gsd_ud_trad_zh.html""&gt;wordseg_gsd_ud_trad&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Word Segmenter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Bihari&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/18/pos_ud_bhtb_bh.html""&gt;bh.pos&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/18/pos_ud_bhtb_bh.html""&gt;pos_ud_bhtb&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Part of Speech&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Amharic&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/20/pos_ud_att_am.html""&gt;am.pos&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/20/pos_ud_att_am.html""&gt;pos_ud_att&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Part of Speech&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h3&gt;NLU 1.1.1 New English Models and Pipelines&lt;/h3&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html""&gt;en.sentiment.glove&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html""&gt;analyze_sentimentdl_glove_imdb&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Sentiment Classifier&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html""&gt;en.sentiment.glove.imdb&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html""&gt;analyze_sentimentdl_glove_imdb&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Sentiment Classifier (Alias)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html""&gt;en.classify.sentiment.glove.imdb&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html""&gt;analyze_sentimentdl_glove_imdb&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Sentiment Classifier (Alias)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html""&gt;en.classify.sentiment.glove&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/15/analyze_sentimentdl_glove_imdb_en.html""&gt;analyze_sentimentdl_glove_imdb&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Sentiment Classifier (Alias)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/08/classifierdl_use_trec50_pipeline_en.html""&gt;en.classify.trec50.pipe&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/08/classifierdl_use_trec50_pipeline_en.html""&gt;classifierdl_use_trec50_pipeline&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Language Classifier&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_large_en.html""&gt;en.ner.onto.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/09/onto_recognize_entities_electra_large_en.html""&gt;onto_recognize_entities_electra_large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Named Entity Recognizer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html""&gt;en.classify.questions.atis&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html""&gt;classifierdl_use_atis&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Intent Classifier&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html""&gt;en.classify.questions.airline&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html""&gt;classifierdl_use_atis&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Intent Classifier (Alias)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html""&gt;en.classify.intent.atis&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html""&gt;classifierdl_use_atis&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Intent Classifier (Alias)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html""&gt;en.classify.intent.airline&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/25/classifierdl_use_atis_en.html""&gt;classifierdl_use_atis&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Intent Classifier (Alias)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html""&gt;en.ner.atis&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html""&gt;nerdl_atis_840b_300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Aspect based NER&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html""&gt;en.ner.airline&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html""&gt;nerdl_atis_840b_300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Aspect based NER (Alias)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html""&gt;en.ner.aspect.airline&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html""&gt;nerdl_atis_840b_300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Aspect based NER (Alias)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html""&gt;en.ner.aspect.atis&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/25/nerdl_atis_840b_300d_en.html""&gt;nerdl_atis_840b_300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Aspect based NER (Alias)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h3&gt;New Easy NLU 1-liner Examples :&lt;/h3&gt;

&lt;h4&gt;Extract aspects and entities from airline questions (ATIS dataset)&lt;/h4&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;p&gt;nlu.load(&amp;quot;en.ner.atis&amp;quot;).predict(&amp;quot;i want to fly from baltimore to dallas round trip&amp;quot;)
output:  [&amp;quot;baltimore&amp;quot;,&amp;quot; dallas&amp;quot;, &amp;quot;round trip&amp;quot;]
```&lt;/p&gt;

&lt;h4&gt;Intent Classification for Airline Traffic Information System queries (ATIS dataset)&lt;/h4&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;p&gt;nlu.load(&amp;quot;en.classify.questions.atis&amp;quot;).predict(&amp;quot;what is the price of flight from newyork to washington&amp;quot;)
output:  &amp;quot;atis_airfare&amp;quot; 
```&lt;/p&gt;

&lt;h4&gt;Recognize Entities OntoNotes - ELECTRA Large&lt;/h4&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;p&gt;nlu.load(&amp;quot;en.ner.onto.large&amp;quot;).predict(&amp;quot;Johnson first entered politics when elected in 2001 as a member of Parliament. He then served eight years as the mayor of London.&amp;quot;)&lt;br/&gt;
output:  [&amp;quot;Johnson&amp;quot;, &amp;quot;first&amp;quot;, &amp;quot;2001&amp;quot;, &amp;quot;eight years&amp;quot;, &amp;quot;London&amp;quot;]&lt;br/&gt;
```&lt;/p&gt;

&lt;h4&gt;Question classification of open-domain and fact-based questions Pipeline - TREC50&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;python
nlu.load(&amp;quot;en.classify.trec50.pipe&amp;quot;).predict(&amp;quot;When did the construction of stone circles begin in the UK? &amp;quot;)
output:  LOC_other
&lt;/code&gt;&lt;/p&gt;

&lt;h4&gt;Traditional Chinese Word Segmentation&lt;/h4&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;&amp;#39;However, this treatment also creates some problems&amp;#39; in Chinese&lt;/h1&gt;

&lt;p&gt;nlu.load(&amp;quot;zh.segment_words.gsd&amp;quot;).predict(&amp;quot;然而，這樣的處理也衍生了一些問題。&amp;quot;)
output:  [&amp;quot;然而&amp;quot;,&amp;quot;,&amp;quot;,&amp;quot;這樣&amp;quot;,&amp;quot;的&amp;quot;,&amp;quot;處理&amp;quot;,&amp;quot;也&amp;quot;,&amp;quot;衍生&amp;quot;,&amp;quot;了&amp;quot;,&amp;quot;一些&amp;quot;,&amp;quot;問題&amp;quot;,&amp;quot;。&amp;quot;]&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;h4&gt;Part of Speech for Traditional Chinese&lt;/h4&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;&amp;#39;However, this treatment also creates some problems&amp;#39; in Chinese&lt;/h1&gt;

&lt;p&gt;nlu.load(&amp;quot;zh.pos.ud_gsd_trad&amp;quot;).predict(&amp;quot;然而，這樣的處理也衍生了一些問題。&amp;quot;)
```
Output:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Token&lt;/th&gt;
&lt;th&gt;POS&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;然而&lt;/td&gt;
&lt;td&gt;ADV&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;，&lt;/td&gt;
&lt;td&gt;PUNCT&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;這樣&lt;/td&gt;
&lt;td&gt;PRON&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;的&lt;/td&gt;
&lt;td&gt;PART&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;處理&lt;/td&gt;
&lt;td&gt;NOUN&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;也&lt;/td&gt;
&lt;td&gt;ADV&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;衍生&lt;/td&gt;
&lt;td&gt;VERB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;td&gt;PART&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;一些&lt;/td&gt;
&lt;td&gt;ADJ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;問題&lt;/td&gt;
&lt;td&gt;NOUN&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;。&lt;/td&gt;
&lt;td&gt;PUNCT&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;Thai Word Segment Recognition&lt;/h4&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;&amp;#39;Mona Lisa is a 16th-century oil painting created by Leonardo held at the Louvre in Paris&amp;#39; in Thai&lt;/h1&gt;

&lt;p&gt;nlu.loadnlu.load(&amp;quot;th.segment_words&amp;quot;).predict(&amp;quot;Mona Lisa เป็นภาพวาดสีน้ำมันในศตวรรษที่ 16 ที่สร้างโดย Leonardo จัดขึ้นที่พิพิธภัณฑ์ลูฟร์ในปารีส&amp;quot;)&lt;/p&gt;

&lt;p&gt;```
Output:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;token&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;n&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lisa&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;เป็น&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ภาพ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ว&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;า&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ด&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;สีน้ำ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;มัน&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ใน&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ศตวรรษ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ที่&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ที่&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;สร้าง&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;โ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ด&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ย&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;L&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;e&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;n&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;r&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;d&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;o&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;จัด&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ขึ้น&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ที่&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;พิพิธภัณฑ์&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ลูฟร์&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ใน&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ปารีส&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;Part of Speech for Bengali (POS)&lt;/h4&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;&amp;#39;The village is also called &amp;#39;Mod&amp;#39; in Tora language&amp;#39; in Bengali&lt;/h1&gt;

&lt;p&gt;nlu.load(&amp;quot;bn.pos&amp;quot;).predict(&amp;quot;বাসস্থান-ঘরগৃহস্থালি তোড়া ভাষায় গ্রামকেও বলে &lt;code&gt;মোদ &amp;#39; ৷&amp;quot;)
&lt;/code&gt;``
Output:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;token&lt;/th&gt;
&lt;th&gt;pos&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;বাসস্থান-ঘরগৃহস্থালি&lt;/td&gt;
&lt;td&gt;NN&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;তোড়া&lt;/td&gt;
&lt;td&gt;NNP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ভাষায়&lt;/td&gt;
&lt;td&gt;NN&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;গ্রামকেও&lt;/td&gt;
&lt;td&gt;NN&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;বলে&lt;/td&gt;
&lt;td&gt;VM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;`&lt;/td&gt;
&lt;td&gt;SYM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;মোদ&lt;/td&gt;
&lt;td&gt;NN&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;#39;&lt;/td&gt;
&lt;td&gt;SYM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;৷&lt;/td&gt;
&lt;td&gt;SYM&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;Stop Words Cleaner for Bengali&lt;/h4&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;&amp;#39;This language is not enough&amp;#39; in Bengali&lt;/h1&gt;

&lt;p&gt;df = nlu.load(&amp;quot;bn.stopwords&amp;quot;).predict(&amp;quot;এই ভাষা যথেষ্ট নয়&amp;quot;)&lt;/p&gt;

&lt;p&gt;```
Output:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""left""&gt;cleanTokens&lt;/th&gt;
&lt;th align=""left""&gt;token&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;ভাষা&lt;/td&gt;
&lt;td align=""left""&gt;এই&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;যথেষ্ট&lt;/td&gt;
&lt;td align=""left""&gt;ভাষা&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;নয়&lt;/td&gt;
&lt;td align=""left""&gt;যথেষ্ট&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;None&lt;/td&gt;
&lt;td align=""left""&gt;নয়&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;Part of Speech for Bengali&lt;/h4&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;&amp;#39;The people of Ohu know that the foundation of Bhojpuri was shaken&amp;#39; in Bengali&lt;/h1&gt;

&lt;p&gt;nlu.load(&amp;#39;bh.pos&amp;#39;).predict(&amp;quot;ओहु लोग के मालूम बा कि श्लील होखते भोजपुरी के नींव हिल जाई&amp;quot;)
```
Output:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""left""&gt;pos&lt;/th&gt;
&lt;th align=""left""&gt;token&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;DET&lt;/td&gt;
&lt;td align=""left""&gt;ओहु&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;NOUN&lt;/td&gt;
&lt;td align=""left""&gt;लोग&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;ADP&lt;/td&gt;
&lt;td align=""left""&gt;के&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;NOUN&lt;/td&gt;
&lt;td align=""left""&gt;मालूम&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;VERB&lt;/td&gt;
&lt;td align=""left""&gt;बा&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;SCONJ&lt;/td&gt;
&lt;td align=""left""&gt;कि&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;ADJ&lt;/td&gt;
&lt;td align=""left""&gt;श्लील&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;VERB&lt;/td&gt;
&lt;td align=""left""&gt;होखते&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;PROPN&lt;/td&gt;
&lt;td align=""left""&gt;भोजपुरी&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;ADP&lt;/td&gt;
&lt;td align=""left""&gt;के&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;NOUN&lt;/td&gt;
&lt;td align=""left""&gt;नींव&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;VERB&lt;/td&gt;
&lt;td align=""left""&gt;हिल&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;AUX&lt;/td&gt;
&lt;td align=""left""&gt;जाई&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;Amharic Part of Speech (POS)&lt;/h4&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;&amp;#39; &amp;quot;Son, finish the job,&amp;quot; he said.&amp;#39; in Amharic&lt;/h1&gt;

&lt;p&gt;nlu.load(&amp;#39;am.pos&amp;#39;).predict(&amp;#39;ልጅ ኡ ን ሥራ ው ን አስጨርስ ኧው ኣል ኧሁ ።&amp;quot;&amp;#39;)
```&lt;/p&gt;

&lt;p&gt;Output:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""left""&gt;pos&lt;/th&gt;
&lt;th align=""left""&gt;token&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;NOUN&lt;/td&gt;
&lt;td align=""left""&gt;ልጅ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;DET&lt;/td&gt;
&lt;td align=""left""&gt;ኡ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;PART&lt;/td&gt;
&lt;td align=""left""&gt;ን&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;NOUN&lt;/td&gt;
&lt;td align=""left""&gt;ሥራ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;DET&lt;/td&gt;
&lt;td align=""left""&gt;ው&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;PART&lt;/td&gt;
&lt;td align=""left""&gt;ን&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;VERB&lt;/td&gt;
&lt;td align=""left""&gt;አስጨርስ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;PRON&lt;/td&gt;
&lt;td align=""left""&gt;ኧው&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;AUX&lt;/td&gt;
&lt;td align=""left""&gt;ኣል&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;PRON&lt;/td&gt;
&lt;td align=""left""&gt;ኧሁ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;PUNCT&lt;/td&gt;
&lt;td align=""left""&gt;።&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;NOUN&lt;/td&gt;
&lt;td align=""left""&gt;&amp;quot;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;Thai Sentiment Classification&lt;/h4&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;&amp;#39;I love peanut butter and jelly!&amp;#39; in thai&lt;/h1&gt;

&lt;p&gt;nlu.load(&amp;#39;th.classify.sentiment&amp;#39;).predict(&amp;#39;ฉันชอบเนยถั่วและเยลลี่!&amp;#39;)[[&amp;#39;sentiment&amp;#39;,&amp;#39;sentiment_confidence&amp;#39;]]
```&lt;/p&gt;

&lt;p&gt;Output:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""left""&gt;sentiment&lt;/th&gt;
&lt;th align=""right""&gt;sentiment_confidence&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;positive&lt;/td&gt;
&lt;td align=""right""&gt;0.999998&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h4&gt;Arabic Named Entity Recognition (NER)&lt;/h4&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;&amp;#39;In 1918, the forces of the Arab Revolt liberated Damascus with the help of the British&amp;#39; in Arabic&lt;/h1&gt;

&lt;p&gt;nlu.load(&amp;#39;ar.ner&amp;#39;).predict(&amp;#39;في عام 1918 حررت قوات الثورة العربية دمشق بمساعدة من الإنكليز&amp;#39;,output_level=&amp;#39;chunk&amp;#39;)[[&amp;#39;entities_confidence&amp;#39;,&amp;#39;ner_confidence&amp;#39;,&amp;#39;entities&amp;#39;]]
```&lt;/p&gt;

&lt;p&gt;Output:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th align=""left""&gt;entity_class&lt;/th&gt;
&lt;th align=""left""&gt;ner_confidence&lt;/th&gt;
&lt;th align=""left""&gt;entities&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;ORG&lt;/td&gt;
&lt;td align=""left""&gt;[1.0, 1.0, 1.0, 0.9997000098228455, 0.9840999841690063, 0.9987999796867371, 0.9990000128746033, 0.9998999834060669, 0.9998999834060669, 0.9993000030517578, 0.9998999834060669]&lt;/td&gt;
&lt;td align=""left""&gt;قوات الثورة العربية&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;LOC&lt;/td&gt;
&lt;td align=""left""&gt;[1.0, 1.0, 1.0, 0.9997000098228455, 0.9840999841690063, 0.9987999796867371, 0.9990000128746033, 0.9998999834060669, 0.9998999834060669, 0.9993000030517578, 0.9998999834060669]&lt;/td&gt;
&lt;td align=""left""&gt;دمشق&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""left""&gt;PER&lt;/td&gt;
&lt;td align=""left""&gt;[1.0, 1.0, 1.0, 0.9997000098228455, 0.9840999841690063, 0.9987999796867371, 0.9990000128746033, 0.9998999834060669, 0.9998999834060669, 0.9993000030517578, 0.9998999834060669]&lt;/td&gt;
&lt;td align=""left""&gt;الإنكليز&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h3&gt;NLU 1.1.0 Enhancements :&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt; Spark 2.3 compatibility&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;New NLU Notebooks and Tutorials&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_question_answering.ipynb""&gt;Open and Closed book question Ansering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[Aspect based NER for Airline ATIS]New multilingual models, Spark 2.3 support, new tutorials for Bengali, Bhojpuri, Japanese, T5  and more in 1 line of Python code with NLU 1.1.1!(&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/classifiers/intent_classification_airlines_ATIS.ipynb""&gt;https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/classifiers/intent_classification_airlines_ATIS.ipynb&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/named_entity_recognition_(NER""&gt;Intent Classification for Airline emssages ATIS&lt;/a&gt;/NER_aspect_airline_ATIS.ipynb)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Installation&lt;/h3&gt;

&lt;p&gt;```bash&lt;/p&gt;

&lt;h1&gt;PyPi&lt;/h1&gt;

&lt;p&gt;!pip install nlu pyspark==2.4.7&lt;/p&gt;

&lt;h1&gt;Conda&lt;/h1&gt;

&lt;h1&gt;Install NLU from Anaconda/Conda&lt;/h1&gt;

&lt;p&gt;conda install -c johnsnowlabs nlu
```&lt;/p&gt;

&lt;h3&gt;Additional NLU ressources&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/""&gt;NLU Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/docs/en/notebooks""&gt;All NLU Tutorial Notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlp.johnsnowlabs.com/learn#pythons-nlu-library""&gt;NLU Videos and Blogposts on NLU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu""&gt;NLU on Github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,liv1mj,True,,CKL-IT,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/liv1mj/new_multilingual_models_spark_23_support_new/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/liv1mj/new_multilingual_models_spark_23_support_new/,30199,1613197795.0,0,,False,,,,,,14632
747,,LanguageTechnology,"Could someone recommend some updated NLP learning material? I am looking specifically for NLG and text summarisation. 

One of the most recent books I found is: [Hands-On Python Natural Language Processing: Explore tools and techniques to analyze and process text with a view to building real-world NLP applications](https://www.goodreads.com/book/show/54412967-hands-on-python-natural-language-processing)

by [Aman Kedia](https://www.goodreads.com/author/show/20480128.Aman_Kedia), [Mayank Rasu](https://www.goodreads.com/author/show/20480129.Mayank_Rasu)

but couldn't find any reviews or opinions.. 

Looking forward to your recommendations.",t2_abxnlmni,False,,0,False,Any recommendations for NLP learning material in 2021?,[],r/LanguageTechnology,False,6,,0,,False,t3_liwfkz,False,dark,1.0,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,1613245544.0,,[],{},,True,,1613232598.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Could someone recommend some updated NLP learning material? I am looking specifically for NLG and text summarisation. &lt;/p&gt;

&lt;p&gt;One of the most recent books I found is: &lt;a href=""https://www.goodreads.com/book/show/54412967-hands-on-python-natural-language-processing""&gt;Hands-On Python Natural Language Processing: Explore tools and techniques to analyze and process text with a view to building real-world NLP applications&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;by &lt;a href=""https://www.goodreads.com/author/show/20480128.Aman_Kedia""&gt;Aman Kedia&lt;/a&gt;, &lt;a href=""https://www.goodreads.com/author/show/20480129.Mayank_Rasu""&gt;Mayank Rasu&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;but couldn&amp;#39;t find any reviews or opinions.. &lt;/p&gt;

&lt;p&gt;Looking forward to your recommendations.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,liwfkz,True,,drinnova,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/liwfkz/any_recommendations_for_nlp_learning_material_in/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/liwfkz/any_recommendations_for_nlp_learning_material_in/,30199,1613203798.0,0,,False,,,,,,645
748,,LanguageTechnology,"Searching repositories of existing source code for code snippets is a key task in software engineering. Earlier techniques like Neural Code Search(NCS), takes in a natural language query and outputs relevant code snippets, often suffers incase of short queries or query that have vague intent. Researchers propose NQE for expanding search queries to improve results. 🤠

Blog- https://link.medium.com/uFDypem4Pdb",t2_hkv9s,False,,0,False,Neural Query Expansion for Code Search,[],r/LanguageTechnology,False,6,,0,,False,t3_lix441,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1613235868.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Searching repositories of existing source code for code snippets is a key task in software engineering. Earlier techniques like Neural Code Search(NCS), takes in a natural language query and outputs relevant code snippets, often suffers incase of short queries or query that have vague intent. Researchers propose NQE for expanding search queries to improve results. 🤠&lt;/p&gt;

&lt;p&gt;Blog- &lt;a href=""https://link.medium.com/uFDypem4Pdb""&gt;https://link.medium.com/uFDypem4Pdb&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lix441,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lix441/neural_query_expansion_for_code_search/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lix441/neural_query_expansion_for_code_search/,30199,1613207068.0,0,,False,,,,,,411
749,,LanguageTechnology,"Hi! I will be starting grad school to get my MS in CS this fall and am planning on specializing in NLP and working as an NLP engineer afterwards. I have always been very interested in politics + government work as well and I imagine there's need for NLP work in intelligence. Does anyone in this sub have any experience in this field? Or knows how much opportunity there is, or anything else worth sharing?

Also a bit more generally, how prevalent are CS/NLP jobs related to the humanities? For example, working in a technical role on investigative work for a news company. So roles working in politics/social/advocacy/news (esp if they require writing and communication alongside the CS). Maybe this is more applicable to leadership positions? But is it plausible that I could get a job in these fields/roles after grad school?",t2_5fagzxz4,False,,0,False,NLP work in politics/intelligence,[],r/LanguageTechnology,False,6,,0,,False,t3_lidu7y,False,dark,1.0,,public,18,0,{},,False,[],,False,False,,{},,False,18,,False,False,,False,,[],{},,True,,1613174063.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi! I will be starting grad school to get my MS in CS this fall and am planning on specializing in NLP and working as an NLP engineer afterwards. I have always been very interested in politics + government work as well and I imagine there&amp;#39;s need for NLP work in intelligence. Does anyone in this sub have any experience in this field? Or knows how much opportunity there is, or anything else worth sharing?&lt;/p&gt;

&lt;p&gt;Also a bit more generally, how prevalent are CS/NLP jobs related to the humanities? For example, working in a technical role on investigative work for a news company. So roles working in politics/social/advocacy/news (esp if they require writing and communication alongside the CS). Maybe this is more applicable to leadership positions? But is it plausible that I could get a job in these fields/roles after grad school?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lidu7y,True,,WholePhotograph,,18,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lidu7y/nlp_work_in_politicsintelligence/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lidu7y/nlp_work_in_politicsintelligence/,30199,1613145263.0,0,,False,,,,,,829
750,,LanguageTechnology,,t2_711g5z9b,False,,0,False,Semantic Viz: a semantic distance visualizer written in Haskell - let me know what you think!,[],r/LanguageTechnology,False,6,,0,,False,t3_lip7mb,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,False,,1613205798.0,text,6,,,text,github.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lip7mb,True,,DareInformal3077,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lip7mb/semantic_viz_a_semantic_distance_visualizer/,all_ads,False,https://github.com/malwaredllc/semantic-viz,30199,1613176998.0,0,,False,https://github.com/malwaredllc/semantic-viz,,,,,0
751,,LanguageTechnology,I would like to take the abstractive approach. Eager to learn more about this.,t2_6s740nk,False,,0,False,How should I go about creating an API to summarize text automatically?,[],r/LanguageTechnology,False,6,,0,,False,t3_lio1l9,False,dark,0.6,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1613202278.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I would like to take the abstractive approach. Eager to learn more about this.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lio1l9,True,,Wufi,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lio1l9/how_should_i_go_about_creating_an_api_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lio1l9/how_should_i_go_about_creating_an_api_to/,30199,1613173478.0,0,,False,,,,,,78
752,,LanguageTechnology,"I'm looking for some papers applying NLP to politics.

It doesn't matter if it is new or old, groundbreaking or not, as long as you enjoyed it, I'd like to know about it!",t2_20ttpzmv,False,,0,False,Which are some good papers on political science applications of NLP?,[],r/LanguageTechnology,False,6,,0,,False,t3_li7q77,False,dark,0.99,,public,13,0,{},,False,[],,False,False,,{},,False,13,,False,False,,False,,[],{},,True,,1613151225.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m looking for some papers applying NLP to politics.&lt;/p&gt;

&lt;p&gt;It doesn&amp;#39;t matter if it is new or old, groundbreaking or not, as long as you enjoyed it, I&amp;#39;d like to know about it!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,li7q77,True,,diogene01,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/li7q77/which_are_some_good_papers_on_political_science/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/li7q77/which_are_some_good_papers_on_political_science/,30199,1613122425.0,0,,False,,,,,,170
753,,LanguageTechnology,"Hello everyone! I would like to ask you all for recommendations on some interesting NLP projects that are running at the moment and need contributors. I do not have extensive experience, but I'd love a starting point. **Feel free to advertise your open source project here**!",t2_3kon0s39,False,,0,False,Recommendations for OpenSource NLP Projects to contribute at,[],r/LanguageTechnology,False,6,,0,,False,t3_li7hal,False,dark,0.99,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,False,,False,,[],{},,True,,1613150040.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone! I would like to ask you all for recommendations on some interesting NLP projects that are running at the moment and need contributors. I do not have extensive experience, but I&amp;#39;d love a starting point. &lt;strong&gt;Feel free to advertise your open source project here&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,li7hal,True,,konoikon,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/li7hal/recommendations_for_opensource_nlp_projects_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/li7hal/recommendations_for_opensource_nlp_projects_to/,30199,1613121240.0,0,,False,,,,,,275
754,,LanguageTechnology,"Hello everybody,

I've been working with a deep learning model (a parser) for some time to make dependency parsing.

In brief, the model is composed of several embeddings that are passed to a BiLSTM and finally to two classifiers (one for the labels and one for the positions). An idea to improve the model is to integrate a self-attention mechanism to BiLSTM. In particular, the attempts I have made are:

* add self-attention to BiLSTM using this [gist](https://gist.github.com/cbaziotis/94e53bdd6e4852756e0395560ff38aa4)
* add multi head (self) attention using the [function](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) provided by pytorch

The results obtained are not very comforting ... with self-attention I have a very slight improvement in performance, while with multi head (self) attention I have no improvements.

Being self-attention mechanisms, in the multi head the vectors corresponding to Q (query), K (key) and V (value) are the same and correspond to the BiLSTM output. What I did was to vary the number of heads (2, 4, 6, 8, 10, 12: no improvement). 

The only other attempt I can think of is to use a [BatchNormalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) layer after the BiLSTM and before the attention or after the attention, but honestly I doubt it will do anything ...

Obviously, adding an attention mechanism does not mean that the model necessarily improves. But I don't know exactly how to justify it. Anyone have any ideas? Or, does anyone have any ideas on how I might try to insert self attention / multi head self attention into BiLSTM?

If you have read this far, thank you very much!",t2_12c30ms7,False,,0,False,Try to improve a DL model with Attention,[],r/LanguageTechnology,False,6,,0,,False,t3_liacim,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1613162829.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everybody,&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been working with a deep learning model (a parser) for some time to make dependency parsing.&lt;/p&gt;

&lt;p&gt;In brief, the model is composed of several embeddings that are passed to a BiLSTM and finally to two classifiers (one for the labels and one for the positions). An idea to improve the model is to integrate a self-attention mechanism to BiLSTM. In particular, the attempts I have made are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;add self-attention to BiLSTM using this &lt;a href=""https://gist.github.com/cbaziotis/94e53bdd6e4852756e0395560ff38aa4""&gt;gist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;add multi head (self) attention using the &lt;a href=""https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html""&gt;function&lt;/a&gt; provided by pytorch&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The results obtained are not very comforting ... with self-attention I have a very slight improvement in performance, while with multi head (self) attention I have no improvements.&lt;/p&gt;

&lt;p&gt;Being self-attention mechanisms, in the multi head the vectors corresponding to Q (query), K (key) and V (value) are the same and correspond to the BiLSTM output. What I did was to vary the number of heads (2, 4, 6, 8, 10, 12: no improvement). &lt;/p&gt;

&lt;p&gt;The only other attempt I can think of is to use a &lt;a href=""https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html""&gt;BatchNormalization&lt;/a&gt; layer after the BiLSTM and before the attention or after the attention, but honestly I doubt it will do anything ...&lt;/p&gt;

&lt;p&gt;Obviously, adding an attention mechanism does not mean that the model necessarily improves. But I don&amp;#39;t know exactly how to justify it. Anyone have any ideas? Or, does anyone have any ideas on how I might try to insert self attention / multi head self attention into BiLSTM?&lt;/p&gt;

&lt;p&gt;If you have read this far, thank you very much!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,liacim,True,,Elidor00,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/liacim/try_to_improve_a_dl_model_with_attention/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/liacim/try_to_improve_a_dl_model_with_attention/,30199,1613134029.0,0,,False,,,,,,1685
755,,LanguageTechnology,,t2_16gftjf,False,,0,False,[N] ICMI 2020 Best Paper | Gesticulator: A framework for semantically-aware speech-driven gesture generation,[],r/LanguageTechnology,False,6,,0,,False,t3_liepa3,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1613176498.0,text,6,,,text,self.MachineLearning,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,liepa3,True,,Svito-zar,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/liepa3/n_icmi_2020_best_paper_gesticulator_a_framework/,all_ads,False,/r/MachineLearning/comments/li16oy/n_icmi_2020_best_paper_gesticulator_a_framework/,30199,1613147698.0,0,,False,/r/MachineLearning/comments/li16oy/n_icmi_2020_best_paper_gesticulator_a_framework/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'Human communication is, to no small extent, non-verbal. While talking, people spontaneously gesticulate, which plays a crucial role in conveying information. Think about the hand, arm, and body motions we make when we talk. Our research is on machine learning models for non-verbal behavior generation, such as hand gestures and facial expressions. We mainly focus on hand gestures generation. We develop machine learning methods that enable virtual agents (such as avatars from a computer game) to communicate non-verbally.\n\nHere is a quick read: Gesticulator: [A framework for semantically-aware speech-driven gesture generation](https://syncedreview.com/2021/02/10/icmi-2020-best-paper-gesticulator-a-framework-for-semantically-aware-speech-driven-gesture-generation/)\n\nThe paper *Gesticulator: A framework for semantically-aware speech-driven gesture generation* is on [arXiv.](https://arxiv.org/pdf/2001.09326.pdf)', 'author_fullname': 't2_2fv4yodo', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[N] ICMI 2020 Best Paper | Gesticulator: A framework for semantically-aware speech-driven gesture generation', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'two', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_li16oy', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.87, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'News', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1613124739.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Human communication is, to no small extent, non-verbal. While talking, people spontaneously gesticulate, which plays a crucial role in conveying information. Think about the hand, arm, and body motions we make when we talk. Our research is on machine learning models for non-verbal behavior generation, such as hand gestures and facial expressions. We mainly focus on hand gestures generation. We develop machine learning methods that enable virtual agents (such as avatars from a computer game) to communicate non-verbally.&lt;/p&gt;\n\n&lt;p&gt;Here is a quick read: Gesticulator: &lt;a href=""https://syncedreview.com/2021/02/10/icmi-2020-best-paper-gesticulator-a-framework-for-semantically-aware-speech-driven-gesture-generation/""&gt;A framework for semantically-aware speech-driven gesture generation&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The paper &lt;em&gt;Gesticulator: A framework for semantically-aware speech-driven gesture generation&lt;/em&gt; is on &lt;a href=""https://arxiv.org/pdf/2001.09326.pdf""&gt;arXiv.&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'li16oy', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Yuqing7', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/li16oy/n_icmi_2020_best_paper_gesticulator_a_framework/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/li16oy/n_icmi_2020_best_paper_gesticulator_a_framework/', 'subreddit_subscribers': 1930720, 'created_utc': 1613095939.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_li16oy,,,0
756,,LanguageTechnology,"Hello all. Im a data science graduate student and NLP research assistant looking to write my thesis about generative language models. 

As I understand it, the current field is largely concerned with large scale pre-trained transformer-based models such as the GPT and BERT projects. 

It seems that the scale and computational cost of such models sets very high barriers of entry for researchers aspiring to challenge state-of-the-art or do interesting research in this sub-field. 

So, I’m curious as to what you may consider interesting frontiers within text-generation/generative models. In your opinion, is there anything that would be worth exploring in this subfield, that do not require access to GPT-level resources?

So far I have considered a comparative study of traditional models (word2vec etc) and these pre-trained models on the new Allen Institute [GENIE testbank](https://leaderboard.allenai.org/genie-anlg/submissions/get-started). However, I’d also very much like to build my own model somehow. 

In relation, are there any other NLP frontiers that you might suggest for a thesis project? 

Thanks for reading and for you time. Any inputs would be appreciated and I would be happy to discuss with further with anyone interested. Naturally, I don’t mind sharing credit either.",t2_26n50qtm,False,,0,False,Text-generation frontiers,[],r/LanguageTechnology,False,6,,0,,False,t3_lidvu7,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1613174197.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all. Im a data science graduate student and NLP research assistant looking to write my thesis about generative language models. &lt;/p&gt;

&lt;p&gt;As I understand it, the current field is largely concerned with large scale pre-trained transformer-based models such as the GPT and BERT projects. &lt;/p&gt;

&lt;p&gt;It seems that the scale and computational cost of such models sets very high barriers of entry for researchers aspiring to challenge state-of-the-art or do interesting research in this sub-field. &lt;/p&gt;

&lt;p&gt;So, I’m curious as to what you may consider interesting frontiers within text-generation/generative models. In your opinion, is there anything that would be worth exploring in this subfield, that do not require access to GPT-level resources?&lt;/p&gt;

&lt;p&gt;So far I have considered a comparative study of traditional models (word2vec etc) and these pre-trained models on the new Allen Institute &lt;a href=""https://leaderboard.allenai.org/genie-anlg/submissions/get-started""&gt;GENIE testbank&lt;/a&gt;. However, I’d also very much like to build my own model somehow. &lt;/p&gt;

&lt;p&gt;In relation, are there any other NLP frontiers that you might suggest for a thesis project? &lt;/p&gt;

&lt;p&gt;Thanks for reading and for you time. Any inputs would be appreciated and I would be happy to discuss with further with anyone interested. Naturally, I don’t mind sharing credit either.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lidvu7,True,,Academy-,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lidvu7/textgeneration_frontiers/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lidvu7/textgeneration_frontiers/,30199,1613145397.0,0,,False,,,,,,1295
757,,LanguageTechnology,,t2_21qaqh1p,False,,0,False,English Speech-to-Text Transcript with Hugging Face,[],r/LanguageTechnology,False,6,,0,,False,t3_lhphom,False,dark,0.94,,public,26,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/dJAoK5zK36M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'English Audio Speech-to-Text Transcript with Hugging Face | Python NLP', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/dJAoK5zK36M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/dJAoK5zK36M/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/dJAoK5zK36M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lhphom', 'height': 200}",,False,26,,False,False,,False,,[],{},,False,,1613092753.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lhphom,True,,dulldata,,1,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/lhphom/english_speechtotext_transcript_with_hugging_face/,all_ads,False,https://youtu.be/dJAoK5zK36M,30199,1613063953.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'English Audio Speech-to-Text Transcript with Hugging Face | Python NLP', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/dJAoK5zK36M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/dJAoK5zK36M/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,https://youtu.be/dJAoK5zK36M,,,,,0
758,,LanguageTechnology,"
I want to get a job as a freelancer

Interests 

 - deeplearning
  - NLP
  - Recommender System

Additionally, i am also looking for teammate to do any project with deeplearning.",t2_xwx2tg2,False,,0,False,Looking for a job!!,[],r/LanguageTechnology,False,6,,0,,False,t3_libjyz,False,dark,0.29,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,1613142117.0,,[],{},,True,,1613167180.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to get a job as a freelancer&lt;/p&gt;

&lt;p&gt;Interests &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;deeplearning

&lt;ul&gt;
&lt;li&gt;NLP&lt;/li&gt;
&lt;li&gt;Recommender System&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additionally, i am also looking for teammate to do any project with deeplearning.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,libjyz,True,,jaekr,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/libjyz/looking_for_a_job/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/libjyz/looking_for_a_job/,30199,1613138380.0,0,,False,,,,,,179
759,,LanguageTechnology,,t2_84htu,False,,0,False,"Rebuilding the spellchecker: Well, akchualy...",[],r/LanguageTechnology,False,6,,0,,False,t3_lhrb23,False,dark,0.92,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,False,,1613097504.0,text,6,,,text,zverok.github.io,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lhrb23,True,,zverok_kha,,1,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/lhrb23/rebuilding_the_spellchecker_well_akchualy/,all_ads,False,https://zverok.github.io/blog/2021-02-10-spellchecker-6.html,30199,1613068704.0,0,,False,https://zverok.github.io/blog/2021-02-10-spellchecker-6.html,,,,,0
760,,LanguageTechnology,"Saarland nlp program = language science and technology
Stuttgart’s nlp program= computational linguistics.

I have got admission in stuttgart nlp program and have applied in saarland’s and will also probably get an admit from there too. So i will be deciding between them for masters.

Both programs “look” great (by module book)  but i need review of real experience of studying there. I guess since saarland is more like a university town hence there will be less work opportunities in the CS or NLP field outside university, compared to stuttgart. But on the other hand saarland is cheaper than stuttgart.

Any other thing i should know about before making the decision. Also, note that i think first 1 or 2 semesters will be online because of covid.

TIA.


Edit: I am unable to find people from stuttgart university in this program (not just on this post, but generally). so if you know someone from this program, can you kindly ask them to help me out here. thanks",t2_5pmdgm1l,False,,0,False,Need Reviews Of Saarland and Stuttgart’s Masters NLP Program,[],r/LanguageTechnology,False,6,,0,,False,t3_lhgvsz,False,dark,1.0,,public,16,0,{},,False,[],,False,False,,{},,False,16,,False,False,,1613069364.0,,[],{},,True,,1613063397.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Saarland nlp program = language science and technology
Stuttgart’s nlp program= computational linguistics.&lt;/p&gt;

&lt;p&gt;I have got admission in stuttgart nlp program and have applied in saarland’s and will also probably get an admit from there too. So i will be deciding between them for masters.&lt;/p&gt;

&lt;p&gt;Both programs “look” great (by module book)  but i need review of real experience of studying there. I guess since saarland is more like a university town hence there will be less work opportunities in the CS or NLP field outside university, compared to stuttgart. But on the other hand saarland is cheaper than stuttgart.&lt;/p&gt;

&lt;p&gt;Any other thing i should know about before making the decision. Also, note that i think first 1 or 2 semesters will be online because of covid.&lt;/p&gt;

&lt;p&gt;TIA.&lt;/p&gt;

&lt;p&gt;Edit: I am unable to find people from stuttgart university in this program (not just on this post, but generally). so if you know someone from this program, can you kindly ask them to help me out here. thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lhgvsz,True,,inopico3,,20,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lhgvsz/need_reviews_of_saarland_and_stuttgarts_masters/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lhgvsz/need_reviews_of_saarland_and_stuttgarts_masters/,30199,1613034597.0,1,,False,,,,,,970
761,,LanguageTechnology,"I have a semantic searcher that retrieves the most similar documents, and then the similarity for each segment (which are sentences). I have been using FAISS for the KNN search over the documents, then I retrieve the sentences to do a dot product operation and highlight the most similar ones.

I also need to keep my data either pickled, in hdf5, or in json, to be able to build the searcher, but after a while, it takes too long to rebuild. I need to rebuild the searcher because I don't have the option to neither update nor delete from a FAISS index, and I don't think I can save the FAISS index either - can't pickle because it yields an error related to using C++ objects.

I also have to have the searcher object in memory (plus the embedding model itself, which is unavoidable). If you have tried Whoosh (probably is similar to Elastic\_Search), it creates an index file which opened/closed per search/data\_manipulation. I can update and delete documents easily, and besides being fast, doesn't have concurrency problems either, I'd love an option like this.

EDIT: another issue I have with FAISS (and other KNN I have tried at the time), is that they don't handle IDs, instead, they retrieve positions and I need to keep a mapping to the original data.",t2_33ls9eif,False,,0,False,Are there more practical tools for KNN searches and storing documents/embeddings?,[],r/LanguageTechnology,False,6,,0,,False,t3_lhhmec,False,dark,0.7,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,1613038635.0,,[],{},,True,,1613066533.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a semantic searcher that retrieves the most similar documents, and then the similarity for each segment (which are sentences). I have been using FAISS for the KNN search over the documents, then I retrieve the sentences to do a dot product operation and highlight the most similar ones.&lt;/p&gt;

&lt;p&gt;I also need to keep my data either pickled, in hdf5, or in json, to be able to build the searcher, but after a while, it takes too long to rebuild. I need to rebuild the searcher because I don&amp;#39;t have the option to neither update nor delete from a FAISS index, and I don&amp;#39;t think I can save the FAISS index either - can&amp;#39;t pickle because it yields an error related to using C++ objects.&lt;/p&gt;

&lt;p&gt;I also have to have the searcher object in memory (plus the embedding model itself, which is unavoidable). If you have tried Whoosh (probably is similar to Elastic_Search), it creates an index file which opened/closed per search/data_manipulation. I can update and delete documents easily, and besides being fast, doesn&amp;#39;t have concurrency problems either, I&amp;#39;d love an option like this.&lt;/p&gt;

&lt;p&gt;EDIT: another issue I have with FAISS (and other KNN I have tried at the time), is that they don&amp;#39;t handle IDs, instead, they retrieve positions and I need to keep a mapping to the original data.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lhhmec,True,,SagaciousRaven,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lhhmec/are_there_more_practical_tools_for_knn_searches/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lhhmec/are_there_more_practical_tools_for_knn_searches/,30199,1613037733.0,0,,False,,,,,,1263
762,,LanguageTechnology,"The researchers at the University of Sheffield, Beihang University, and Open University’s Knowledge Media Institute have introduced historical text summarization task, where documents in historical forms of a language are summarised in the corresponding modern language.

The process of text summarization is a fundamentally important routine to historians and digital humanities researchers. Historical text summarization is regarded as a particular case of cross-lingual summarization. However, summarizing and interpreting historical documents can cost a lot of time and effort, even for experts. This is due to the limited historical and modern language corpora and cultural and linguistic variations over time.

Paper summary: https://www.marktechpost.com/2021/02/10/researchers-from-the-university-of-sheffield-beihang-university-introduce-a-new-approach-based-on-transfer-learning-to-automate-historical-text-summarization/

Paper: https://arxiv.org/pdf/2101.10759.pdf

Github: https://github.com/Pzoom522/HistSumm",t2_2wsvqwhg,False,,0,False,Researchers from the University of Sheffield &amp; Beihang University Introduce a New Approach Based on Transfer Learning to Automate Historical Text Summarization,[],r/LanguageTechnology,False,6,,0,,False,t3_lhb1x7,False,dark,0.89,,public,15,0,{},,False,[],,False,False,,{},,False,15,,False,False,,False,,[],{},,True,,1613040425.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The researchers at the University of Sheffield, Beihang University, and Open University’s Knowledge Media Institute have introduced historical text summarization task, where documents in historical forms of a language are summarised in the corresponding modern language.&lt;/p&gt;

&lt;p&gt;The process of text summarization is a fundamentally important routine to historians and digital humanities researchers. Historical text summarization is regarded as a particular case of cross-lingual summarization. However, summarizing and interpreting historical documents can cost a lot of time and effort, even for experts. This is due to the limited historical and modern language corpora and cultural and linguistic variations over time.&lt;/p&gt;

&lt;p&gt;Paper summary: &lt;a href=""https://www.marktechpost.com/2021/02/10/researchers-from-the-university-of-sheffield-beihang-university-introduce-a-new-approach-based-on-transfer-learning-to-automate-historical-text-summarization/""&gt;https://www.marktechpost.com/2021/02/10/researchers-from-the-university-of-sheffield-beihang-university-introduce-a-new-approach-based-on-transfer-learning-to-automate-historical-text-summarization/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/pdf/2101.10759.pdf""&gt;https://arxiv.org/pdf/2101.10759.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Github: &lt;a href=""https://github.com/Pzoom522/HistSumm""&gt;https://github.com/Pzoom522/HistSumm&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lhb1x7,True,,ai-lover,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lhb1x7/researchers_from_the_university_of_sheffield/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lhb1x7/researchers_from_the_university_of_sheffield/,30199,1613011625.0,0,,False,,,,,,1021
763,,LanguageTechnology,,t2_kxps2,False,,0,False,Large archive of papers on machine translation starting from the beginning of the field,[],r/LanguageTechnology,False,6,,0,,False,t3_lhal82,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,False,,1613038980.0,text,6,,,text,mt-archive.info,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lhal82,True,,Terpomo11,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lhal82/large_archive_of_papers_on_machine_translation/,all_ads,False,http://mt-archive.info,30199,1613010180.0,1,,False,http://mt-archive.info,,,,,0
764,,LanguageTechnology,"So many ML projects are failing because teams don't have the skills to deploy their new model to production... [NLP Cloud](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=7fc10c28-ab0d-11eb-bcbc-0242ac130002) wants to solve this problem by providing spaCy users with an easy way to deploy their models to production.

More and more companies are using [spaCy](https://spacy.io/) for their NLP projects as it's a modern and production-ready framework for NLP.

Successfully  serving spaCy NLP models in production is a tough job that has nothing  to do with data science, but that is nevertheless critical to the  success of a project. The goal of [https://nlpcloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=7fc10c28-ab0d-11eb-bcbc-0242ac130002)  is to take care of this DevOps task: developers or data scientists only  have to upload their spaCy models to NLP Cloud and their models are  then served through a RESTful API. NLP Cloud ensures high availability  of the models by automatically scaling the instances, adding redundancy,  managing memory consumption, etc.

All the pre-trained spaCy models are also available and small pre-trained models are actually completely free.

For more details here is the API documentation: [https://docs.nlpcloud.io](https://docs.nlpcloud.io/)

If any question or feedback, don't hesitate to answer this post!",t2_4z4m2qcs,False,,0,False,NLP Cloud for spaCy NLP models in production,[],r/LanguageTechnology,False,6,,0,,False,t3_lgum9p,False,dark,0.92,,public,22,0,{},,False,[],,False,False,,{},,False,22,,False,False,,1619937699.0,,[],{},,True,,1612995709.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So many ML projects are failing because teams don&amp;#39;t have the skills to deploy their new model to production... &lt;a href=""https://nlpcloud.io/?utm_source=reddit&amp;amp;utm_campaign=7fc10c28-ab0d-11eb-bcbc-0242ac130002""&gt;NLP Cloud&lt;/a&gt; wants to solve this problem by providing spaCy users with an easy way to deploy their models to production.&lt;/p&gt;

&lt;p&gt;More and more companies are using &lt;a href=""https://spacy.io/""&gt;spaCy&lt;/a&gt; for their NLP projects as it&amp;#39;s a modern and production-ready framework for NLP.&lt;/p&gt;

&lt;p&gt;Successfully  serving spaCy NLP models in production is a tough job that has nothing  to do with data science, but that is nevertheless critical to the  success of a project. The goal of &lt;a href=""https://nlpcloud.io/?utm_source=reddit&amp;amp;utm_campaign=7fc10c28-ab0d-11eb-bcbc-0242ac130002""&gt;https://nlpcloud.io&lt;/a&gt;  is to take care of this DevOps task: developers or data scientists only  have to upload their spaCy models to NLP Cloud and their models are  then served through a RESTful API. NLP Cloud ensures high availability  of the models by automatically scaling the instances, adding redundancy,  managing memory consumption, etc.&lt;/p&gt;

&lt;p&gt;All the pre-trained spaCy models are also available and small pre-trained models are actually completely free.&lt;/p&gt;

&lt;p&gt;For more details here is the API documentation: &lt;a href=""https://docs.nlpcloud.io/""&gt;https://docs.nlpcloud.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If any question or feedback, don&amp;#39;t hesitate to answer this post!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lgum9p,True,,juliensalinas,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lgum9p/nlp_cloud_for_spacy_nlp_models_in_production/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lgum9p/nlp_cloud_for_spacy_nlp_models_in_production/,30199,1612966909.0,0,,False,,,,,,1374
765,,LanguageTechnology,I actually tried this and the representation seems to perform worse than using just one of these. Is there a theoretical reason why this might be so?,t2_10efjmjx,False,,0,False,"I see mean pool and max pool output of transformer models as representations of passages of text, why not concatenate the two?",[],r/LanguageTechnology,False,6,,0,,False,t3_lh16bv,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1613013111.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I actually tried this and the representation seems to perform worse than using just one of these. Is there a theoretical reason why this might be so?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lh16bv,True,,BatmantoshReturns,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lh16bv/i_see_mean_pool_and_max_pool_output_of/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lh16bv/i_see_mean_pool_and_max_pool_output_of/,30199,1612984311.0,0,,False,,,,,,149
766,,LanguageTechnology,I tried using parser that are readily available but for some domain specific tasks the parsers do not work well. Could you please recommend me a way to create my own parser or fine tune available ones with some supervision? How do I jump start?,t2_2y9toqyt,False,,0,False,Creating a domain specific part of speech tagger,[],r/LanguageTechnology,False,6,,0,,False,t3_lgx71d,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1613002898.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I tried using parser that are readily available but for some domain specific tasks the parsers do not work well. Could you please recommend me a way to create my own parser or fine tune available ones with some supervision? How do I jump start?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lgx71d,True,,rbnjade1,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lgx71d/creating_a_domain_specific_part_of_speech_tagger/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lgx71d/creating_a_domain_specific_part_of_speech_tagger/,30199,1612974098.0,0,,False,,,,,,244
767,,LanguageTechnology,,t2_5cvo57qv,False,,0,False,LibreTranslate - Free and Open Source Machine Translation API,[],r/LanguageTechnology,False,6,,0,,False,t3_lgijcu,False,dark,0.95,,public,14,0,{},,False,[],,False,False,,{},,False,14,,False,False,,False,,[],{},,False,,1612948589.0,text,6,,,text,libretranslate.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lgijcu,True,,argosopentech,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lgijcu/libretranslate_free_and_open_source_machine/,all_ads,False,https://libretranslate.com/,30199,1612919789.0,1,,False,https://libretranslate.com/,,,,,0
768,,LanguageTechnology,,t2_2zyh29i,False,,0,False,"Intro to Embodied AI: How to combine NLP, CV, and RL",[],r/LanguageTechnology,False,6,,0,,False,t3_lgfc7h,False,dark,0.91,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,False,,1612938770.0,text,6,,,text,medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lgfc7h,True,,beluis3d,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lgfc7h/intro_to_embodied_ai_how_to_combine_nlp_cv_and_rl/,all_ads,False,https://medium.com/machinevision/overview-of-embodied-artificial-intelligence-b7f19d18022,30199,1612909970.0,0,,False,https://medium.com/machinevision/overview-of-embodied-artificial-intelligence-b7f19d18022,,,,,0
769,,LanguageTechnology,"Attention is all the rage (at least, it gets a lot of press) in NLP DL. I have been trying to wrap my head around how and why it works so well. I have embarked on a journey of reading papers and watching YouTube videos (e.g., [https://distill.pub/2016/augmented-rnns/](https://distill.pub/2016/augmented-rnns/) and [https://www.youtube.com/watch?v=KN3ZL65Dze0](https://www.youtube.com/watch?v=KN3ZL65Dze0))

I have one nagging question. One of the key ideas in attention is that the model learns a weight from each item in the input sequence to each item in the output sequence. And I can see that for a single input/output pair (e.g., the input sequence ""The agreement on the European Economic Area was signed in August 1992."" and the corresponding output sequence ""l' accord sur la zone economique europeene a ete signe en aout 1992.""), how the weights are very helpful and important. In this example, the 5th input token ""European"" should/will have a high weight to the 7th output token ""europeene"". Paper figures indeed show nice examples of the weights for single input/output pairs, and those figures make sense to me. But my understanding is in these attention models, there is only *one* set of weights to be learned (and later used) for all input/output pairs. So my question is: how does that work, given the enormous variations possible in sentence structure? For example, does the model expect that there should always be a strong weight between the 5th input token and the 7th output token?  It seems like the weights should be drastically different depending on the specific input/output pair, and that a single, global set of weights won't work well.

Any help or guidance would be much appreciated.",t2_wiugmg,False,,0,False,Simple question about attention in NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_lg3zko,False,dark,0.92,,public,19,0,{},,False,[],,False,False,,{},,False,19,,False,False,,False,,[],{},,True,,1612908308.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Attention is all the rage (at least, it gets a lot of press) in NLP DL. I have been trying to wrap my head around how and why it works so well. I have embarked on a journey of reading papers and watching YouTube videos (e.g., &lt;a href=""https://distill.pub/2016/augmented-rnns/""&gt;https://distill.pub/2016/augmented-rnns/&lt;/a&gt; and &lt;a href=""https://www.youtube.com/watch?v=KN3ZL65Dze0""&gt;https://www.youtube.com/watch?v=KN3ZL65Dze0&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;I have one nagging question. One of the key ideas in attention is that the model learns a weight from each item in the input sequence to each item in the output sequence. And I can see that for a single input/output pair (e.g., the input sequence &amp;quot;The agreement on the European Economic Area was signed in August 1992.&amp;quot; and the corresponding output sequence &amp;quot;l&amp;#39; accord sur la zone economique europeene a ete signe en aout 1992.&amp;quot;), how the weights are very helpful and important. In this example, the 5th input token &amp;quot;European&amp;quot; should/will have a high weight to the 7th output token &amp;quot;europeene&amp;quot;. Paper figures indeed show nice examples of the weights for single input/output pairs, and those figures make sense to me. But my understanding is in these attention models, there is only &lt;em&gt;one&lt;/em&gt; set of weights to be learned (and later used) for all input/output pairs. So my question is: how does that work, given the enormous variations possible in sentence structure? For example, does the model expect that there should always be a strong weight between the 5th input token and the 7th output token?  It seems like the weights should be drastically different depending on the specific input/output pair, and that a single, global set of weights won&amp;#39;t work well.&lt;/p&gt;

&lt;p&gt;Any help or guidance would be much appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lg3zko,True,,stepthom,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lg3zko/simple_question_about_attention_in_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lg3zko/simple_question_about_attention_in_nlp/,30199,1612879508.0,0,,False,,,,,,1714
770,,LanguageTechnology,,t2_a7i59xms,False,,0,False,Unit testing neural networks (BERT example),[],r/LanguageTechnology,False,6,,0,,False,t3_lgat6y,False,dark,1.0,,public,8,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/_KVV9jXSzvo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Mocking neural networks', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/_KVV9jXSzvo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'mildlyoverfitted', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/_KVV9jXSzvo/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCYBSjwkGTK06NnDnFsOcR7g'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/_KVV9jXSzvo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lgat6y', 'height': 200}",,False,8,,False,False,,False,,[],{},,False,,1612926569.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lgat6y,True,,mildlyoverfitted,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lgat6y/unit_testing_neural_networks_bert_example/,all_ads,False,https://youtu.be/_KVV9jXSzvo,30199,1612897769.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Mocking neural networks', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/_KVV9jXSzvo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'mildlyoverfitted', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/_KVV9jXSzvo/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCYBSjwkGTK06NnDnFsOcR7g'}}",False,https://youtu.be/_KVV9jXSzvo,,,,,0
771,,LanguageTechnology,"Hello there,

&amp;#x200B;

Long time lurking, never posting, but I'll break the habit here. I saw every now and then there's a post about NLP-focused jobs in the real world, and I wanted to get a bit of discussion going about the future of DS/ML jobs that focus on NLP.

&amp;#x200B;

I'm mainly interested as that is a future I wish to build for myself. Worked in academia in scientific research for some time (physics and psychology) but been a data scientist for the past 2 years-ish, although since Covid started I pretty much became a BI engineer as my firm needed that gap to be filled. I guess the skills of building and monitoring ETL processes, containerisation, as well as plugging in analytics to integrate with other tools or BI frontend can be useful in the long run, so that's why I didn't mind the slight change. 

&amp;#x200B;

Ever since I have worked as a data scientist, I really enjoyed reading and studying about NLP (thank you Dan Jurafsky and Chris Manning for the courses!) , I feel like it hits close to my kinds of challenges. Even though I have not really used it in many projects at work, apart from occasional named entity recognition and lots of regex usage and stuff like that, it still excites me more than other tasks I had to tackle before. I am doing some basic projects on my own every now and then (news for sentiment analysis for trading, decomposing TedX transcripts for some basic insights) but would hardly call that a portfolio. 

&amp;#x200B;

So kind of as a bundle of questions, I was wondering what's a general consensus on someone entering the NLP field without specific research background, ML engineering experience, or computational linguistics studies. To get to a stage where most of my work is with NLP, is it wiser to apply for junior NLP roles given I have experience in data science and some engineering too, or build a stronger NLP portfolio and apply for mid-level role and demonstrate experience and skills in language tech through that rather than work? Plus, projecting for the next 2-3-5 years, what trends should we envisage happening in the NLP workforce/community? 

&amp;#x200B;

Thanks, and take care!",t2_8s8dx4gr,False,,0,False,Commercial data scientist to NLP-specialist?,[],r/LanguageTechnology,False,6,,0,,False,t3_lge15e,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1612935095.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello there,&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Long time lurking, never posting, but I&amp;#39;ll break the habit here. I saw every now and then there&amp;#39;s a post about NLP-focused jobs in the real world, and I wanted to get a bit of discussion going about the future of DS/ML jobs that focus on NLP.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m mainly interested as that is a future I wish to build for myself. Worked in academia in scientific research for some time (physics and psychology) but been a data scientist for the past 2 years-ish, although since Covid started I pretty much became a BI engineer as my firm needed that gap to be filled. I guess the skills of building and monitoring ETL processes, containerisation, as well as plugging in analytics to integrate with other tools or BI frontend can be useful in the long run, so that&amp;#39;s why I didn&amp;#39;t mind the slight change. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Ever since I have worked as a data scientist, I really enjoyed reading and studying about NLP (thank you Dan Jurafsky and Chris Manning for the courses!) , I feel like it hits close to my kinds of challenges. Even though I have not really used it in many projects at work, apart from occasional named entity recognition and lots of regex usage and stuff like that, it still excites me more than other tasks I had to tackle before. I am doing some basic projects on my own every now and then (news for sentiment analysis for trading, decomposing TedX transcripts for some basic insights) but would hardly call that a portfolio. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So kind of as a bundle of questions, I was wondering what&amp;#39;s a general consensus on someone entering the NLP field without specific research background, ML engineering experience, or computational linguistics studies. To get to a stage where most of my work is with NLP, is it wiser to apply for junior NLP roles given I have experience in data science and some engineering too, or build a stronger NLP portfolio and apply for mid-level role and demonstrate experience and skills in language tech through that rather than work? Plus, projecting for the next 2-3-5 years, what trends should we envisage happening in the NLP workforce/community? &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks, and take care!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lge15e,True,,JimboPeanut,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lge15e/commercial_data_scientist_to_nlpspecialist/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lge15e/commercial_data_scientist_to_nlpspecialist/,30199,1612906295.0,0,,False,,,,,,2169
772,,LanguageTechnology,"My introduction to NLP was from the Stanford lectures on **NLP with Deep Learning** (available freely online - see [here](http://web.stanford.edu/class/cs224n/) for links to the course pages from different years).

I was wondering how much is missed by having jumped straight into neural methods and if I should supplement this with something more classical (e.g. the book **Natural Language Processing with Python**). I also saw there's an older [course](https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html) from Stanford (also available freely online) focusing on more classical methods.",t2_146mu3,False,,0,False,How important is classical (non-Neural) ML for NLP?,[],r/LanguageTechnology,False,6,,0,,False,t3_lgawtg,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,1612898314.0,,[],{},,True,,1612926821.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My introduction to NLP was from the Stanford lectures on &lt;strong&gt;NLP with Deep Learning&lt;/strong&gt; (available freely online - see &lt;a href=""http://web.stanford.edu/class/cs224n/""&gt;here&lt;/a&gt; for links to the course pages from different years).&lt;/p&gt;

&lt;p&gt;I was wondering how much is missed by having jumped straight into neural methods and if I should supplement this with something more classical (e.g. the book &lt;strong&gt;Natural Language Processing with Python&lt;/strong&gt;). I also saw there&amp;#39;s an older &lt;a href=""https://web.stanford.edu/%7Ejurafsky/NLPCourseraSlides.html""&gt;course&lt;/a&gt; from Stanford (also available freely online) focusing on more classical methods.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lgawtg,True,,thequirkynerdy1,,10,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lgawtg/how_important_is_classical_nonneural_ml_for_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lgawtg/how_important_is_classical_nonneural_ml_for_nlp/,30199,1612898021.0,0,,False,,,,,,595
773,,LanguageTechnology,,t2_mr1oj,False,,0,False,INTERLINGO: Mi nueva APP para aprender idiomas,[],r/LanguageTechnology,False,6,,0,,False,t3_lgky9k,False,dark,0.5,,public,0,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/uF3zA60BbXk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'INTERLINGO: Mi nueva APP para aprender idiomas', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/uF3zA60BbXk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Aprender Idiomas y Cultura General con Rodrigo', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/uF3zA60BbXk/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCR6K-ZudJ0CNJjNVDqeMYBA'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/uF3zA60BbXk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lgky9k', 'height': 200}",,False,0,,False,False,,False,,[],{},,False,,1612956739.0,text,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lgky9k,True,,roferre,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lgky9k/interlingo_mi_nueva_app_para_aprender_idiomas/,all_ads,False,https://youtube.com/watch?v=uF3zA60BbXk&amp;feature=share,30199,1612927939.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'INTERLINGO: Mi nueva APP para aprender idiomas', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/uF3zA60BbXk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Aprender Idiomas y Cultura General con Rodrigo', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/uF3zA60BbXk/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCR6K-ZudJ0CNJjNVDqeMYBA'}}",False,https://youtube.com/watch?v=uF3zA60BbXk&amp;feature=share,,,,,0
774,,LanguageTechnology,"Hi, I'm taking a course on NLP and the class' final project is about character prediction. The task is to predict the next character given a sequence of characters.

I'm trying to find an interesting model I can use. The default answer would probably be LSTMs, but I want to try using something more interesting.

Any suggestions for a model architecture I could use that's more interesting than LSTMs but still doable by an undergrad without much experience?",t2_3n3j5x00,False,,0,False,Interesting models for character prediction?,[],r/LanguageTechnology,False,6,,0,,False,t3_lgbrcp,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1612929029.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m taking a course on NLP and the class&amp;#39; final project is about character prediction. The task is to predict the next character given a sequence of characters.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m trying to find an interesting model I can use. The default answer would probably be LSTMs, but I want to try using something more interesting.&lt;/p&gt;

&lt;p&gt;Any suggestions for a model architecture I could use that&amp;#39;s more interesting than LSTMs but still doable by an undergrad without much experience?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lgbrcp,True,,liquid-kickass,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lgbrcp/interesting_models_for_character_prediction/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lgbrcp/interesting_models_for_character_prediction/,30199,1612900229.0,0,,False,,,,,,459
775,,LanguageTechnology,,t2_trjf2,False,,0,False,Microsoft spelling correction for 100 languages,[],r/LanguageTechnology,False,6,,0,,False,t3_lfm30k,False,dark,0.99,,public,40,0,{},,False,[],,False,False,,{},,False,40,,False,False,,False,,[],{},,False,,1612847775.0,text,6,,,text,microsoft.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lfm30k,True,,adammathias,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lfm30k/microsoft_spelling_correction_for_100_languages/,all_ads,False,https://www.microsoft.com/en-us/research/blog/speller100-zero-shot-spelling-correction-at-scale-for-100-plus-languages/,30199,1612818975.0,0,,False,https://www.microsoft.com/en-us/research/blog/speller100-zero-shot-spelling-correction-at-scale-for-100-plus-languages/,,,,,0
776,,LanguageTechnology,,t2_8l6cewm2,False,,0,False,Detect hate speech using a Transformer with only a few lines of code.,[],r/LanguageTechnology,False,6,,0,,False,t3_lffvyi,False,dark,0.73,,public,8,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/jti2sPQYzeQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'NLP: Hate Speech Detection With Transformers Made Easy', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/jti2sPQYzeQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Vennify AI', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/jti2sPQYzeQ/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC7-EWrr8YdcQgPPk76OiUVw'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/jti2sPQYzeQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lffvyi', 'height': 200}",,False,8,,False,False,,False,,[],{},,False,,1612831605.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lffvyi,True,,VennifyAI,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lffvyi/detect_hate_speech_using_a_transformer_with_only/,all_ads,False,https://youtu.be/jti2sPQYzeQ,30199,1612802805.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'NLP: Hate Speech Detection With Transformers Made Easy', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/jti2sPQYzeQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Vennify AI', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/jti2sPQYzeQ/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC7-EWrr8YdcQgPPk76OiUVw'}}",False,https://youtu.be/jti2sPQYzeQ,,,,,0
777,,LanguageTechnology,"For example, if I have a text classification problem that's trying to determine if a document belongs to ""politics"" or ""sports"", is there a way that I could ""pull out"" a list of the tokens and/or n-grams in ""politics"" that set it apart from ""sports"" as a category? I assume it would be some process of comparing the difference in word frequency across each corpus, and then finding the terms that have the biggest difference in word frequency, but I'm wondering if there is a more efficient or smarter way to do this. Thanks.",t2_asi5u,False,,0,False,Are there any methods for extracting tokens that make a category unique in a dataset?,[],r/LanguageTechnology,False,6,,0,,False,t3_lfp5qz,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1612856304.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For example, if I have a text classification problem that&amp;#39;s trying to determine if a document belongs to &amp;quot;politics&amp;quot; or &amp;quot;sports&amp;quot;, is there a way that I could &amp;quot;pull out&amp;quot; a list of the tokens and/or n-grams in &amp;quot;politics&amp;quot; that set it apart from &amp;quot;sports&amp;quot; as a category? I assume it would be some process of comparing the difference in word frequency across each corpus, and then finding the terms that have the biggest difference in word frequency, but I&amp;#39;m wondering if there is a more efficient or smarter way to do this. Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lfp5qz,True,,TheBuddhist,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lfp5qz/are_there_any_methods_for_extracting_tokens_that/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lfp5qz/are_there_any_methods_for_extracting_tokens_that/,30199,1612827504.0,0,,False,,,,,,525
778,,LanguageTechnology,"Does anyone know of publicly available data which includes labels for levels of empathy/kindness/compassion in text?

I am trying to reproduce old results with new data.",t2_2a4e9yjh,False,,0,False,Looking for dataset with empathic language,[],r/LanguageTechnology,False,6,,0,,False,t3_lfkbzf,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1612843223.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Does anyone know of publicly available data which includes labels for levels of empathy/kindness/compassion in text?&lt;/p&gt;

&lt;p&gt;I am trying to reproduce old results with new data.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lfkbzf,True,,computerResearcher,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lfkbzf/looking_for_dataset_with_empathic_language/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lfkbzf/looking_for_dataset_with_empathic_language/,30199,1612814423.0,0,,False,,,,,,169
779,,LanguageTechnology,"Hey all! Im new to ML and Im working on an iOS project that I am needing a little advice on. I am trying to extract information from advertisements and categorize them. On top of that, I need to associate certain categories with each other so that when they are processed by my computer vision algorithm, they are associated together and presented as a single item. I am kind of stumped on how to approach this problem. I think I need to use some kind of vision algorithm to recognize the relevant parts of the advertisement and then either a vision or NLP algorithm to associate the particular categories within each ad together correctly. Any advice would be greatly, greatly appreciated!",t2_y9ex0,False,,0,False,Help with Named Entity Recognition (I think?) project,[],r/LanguageTechnology,False,6,,0,,False,t3_lfjyts,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1612842270.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all! Im new to ML and Im working on an iOS project that I am needing a little advice on. I am trying to extract information from advertisements and categorize them. On top of that, I need to associate certain categories with each other so that when they are processed by my computer vision algorithm, they are associated together and presented as a single item. I am kind of stumped on how to approach this problem. I think I need to use some kind of vision algorithm to recognize the relevant parts of the advertisement and then either a vision or NLP algorithm to associate the particular categories within each ad together correctly. Any advice would be greatly, greatly appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lfjyts,True,,cmcrumley,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lfjyts/help_with_named_entity_recognition_i_think_project/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lfjyts/help_with_named_entity_recognition_i_think_project/,30199,1612813470.0,0,,False,,,,,,690
780,,LanguageTechnology,,t2_mr1oj,False,,0,False,INTERLINGO: Tu nueva APP para aprender INGLES,[],r/LanguageTechnology,False,6,,0,,False,t3_lfdvwj,False,dark,0.44,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,False,,1612826129.0,text,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lfdvwj,True,,roferre,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lfdvwj/interlingo_tu_nueva_app_para_aprender_ingles/,all_ads,False,https://youtube.com/watch?v=wgFudaFZQpo&amp;feature=share,30199,1612797329.0,0,,False,https://youtube.com/watch?v=wgFudaFZQpo&amp;feature=share,,,,,0
781,,LanguageTechnology,"Hi, I'd like to learn NLP by using python3. What should I do to learn? What do I have to know before start? Should I firstly check AI? It's related? Could you share documents or complete course videos. Thanks. 😊",t2_37a131mc,False,,0,False,Where should I start?,[],r/LanguageTechnology,False,6,,0,,False,t3_lf97qy,False,dark,0.67,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1612809540.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;d like to learn NLP by using python3. What should I do to learn? What do I have to know before start? Should I firstly check AI? It&amp;#39;s related? Could you share documents or complete course videos. Thanks. 😊&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lf97qy,True,,phyex,,15,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lf97qy/where_should_i_start/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lf97qy/where_should_i_start/,30199,1612780740.0,0,,False,,,,,,211
782,,LanguageTechnology,,t2_5fsp2x6v,False,,0,False,A Robust and Domain-Adaptive Approach for Low-Resource NER | Research Papers Summary 007,[],r/LanguageTechnology,False,6,,0,,False,t3_letc3i,False,dark,1.0,,public,29,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/cPngQMYV_CE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'A Robust and Domain-Adaptive Approach for Low-Resource NER | Research Papers Summary 007', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/cPngQMYV_CE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/cPngQMYV_CE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/cPngQMYV_CE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/letc3i', 'height': 200}",,False,29,,False,False,,False,,[],{},,False,,1612754471.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,letc3i,True,,RyanAI100,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/letc3i/a_robust_and_domainadaptive_approach_for/,all_ads,False,https://youtu.be/cPngQMYV_CE,30199,1612725671.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'A Robust and Domain-Adaptive Approach for Low-Resource NER | Research Papers Summary 007', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/cPngQMYV_CE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/cPngQMYV_CE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,https://youtu.be/cPngQMYV_CE,,,,,0
783,,LanguageTechnology,"Hello every one 

I am new in NLP , and I want to perform emotion analysis especially (worry) emotion. I have collected tweets related to COVID-19  to classify into 2 classes with deep learning. Someone said you have to make a linguistic study related to the Worry concept to use it as linguistic features and get good results. Does anyone know any resources that can help me or any linguistic resources? Thanks",t2_tx187ox,False,,0,False,Studying linguistic features in emotion analysis with deep learning,[],r/LanguageTechnology,False,6,,0,,False,t3_lesb4q,False,dark,0.82,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1612751601.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello every one &lt;/p&gt;

&lt;p&gt;I am new in NLP , and I want to perform emotion analysis especially (worry) emotion. I have collected tweets related to COVID-19  to classify into 2 classes with deep learning. Someone said you have to make a linguistic study related to the Worry concept to use it as linguistic features and get good results. Does anyone know any resources that can help me or any linguistic resources? Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lesb4q,True,,Tahani_cs,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lesb4q/studying_linguistic_features_in_emotion_analysis/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lesb4q/studying_linguistic_features_in_emotion_analysis/,30199,1612722801.0,0,,False,,,,,,411
784,,LanguageTechnology,"I need to retrieve information from papers and I found few tools, but still I don't know how a workflow should be. Can anyone post resources from data preprocessing to model training for information retrieval? Any pre-trained models that I can use (I checked pegasus for summarization, T5 and few others) Help is appreciated.",t2_3ogh75n7,False,,0,False,I'm looking for nice resources on information retrieval,[],r/LanguageTechnology,False,6,,0,,False,t3_lepaak,False,dark,1.0,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1612742860.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I need to retrieve information from papers and I found few tools, but still I don&amp;#39;t know how a workflow should be. Can anyone post resources from data preprocessing to model training for information retrieval? Any pre-trained models that I can use (I checked pegasus for summarization, T5 and few others) Help is appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lepaak,True,,unofficialmerve,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lepaak/im_looking_for_nice_resources_on_information/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lepaak/im_looking_for_nice_resources_on_information/,30199,1612714060.0,0,,False,,,,,,325
785,,LanguageTechnology,"Alexa team is hiring ""Data Linguists"", are these literal manual annotation roles? Is Alexa literally hand-tuned based manual decision trees?

&gt; Sample JD Description

&gt; Amazon is seeking a Data Linguist-II to join our Amazon Devices Team. This role focuses on language data, primarily in the areas of text annotation and general data analysis deliverables.

&gt;The Data Linguist-II must have a passion for data, efficiency, accuracy, and should be capable of:

&gt;• Handling unique data analysis requests from a range of data customers

&gt;• Providing data quality expertise to other team members and coaching improvements

&gt;• Delivering high quality work under aggressive deadlines

&gt;• Working autonomously with minimum direction

&gt;• Building a thorough understanding of conventions and providing support to global sites

&gt;• Understanding changes to conventions deployed in response to customers’ requests and modifying workflows accordingly

&gt;• Contributing to process improvements to reduce handling time and improve output

&gt;• Improving software tools by identifying bugs and suggesting enhancements

&gt;• Diving deep into issues and implementing solutions independently

&gt;• Proactively addressing issues and problems

&gt;• Keeping up with changing project conventions and priorities

&gt;Basic Qualifications

&gt;• Bachelor’s degree

&gt;• 2+ years of experience working with written language data, including experience with annotation and other forms of data markup

&gt;• 1+ year(s) of experience working with command line interfaces and basic UNIX commands

&gt;• Near-native level fluency in one or more non-English language

&gt;• Business level fluency in English

&gt;• Working knowledge of Microsoft Office products (e.g. Word, Excel, Access, etc.)


&gt;Preferred Qualifications

&gt;• Bachelor’s degree in Linguistics or Computational Linguistics

&gt;• Interest in semantics and related areas

&gt;• Comfortable working with text from various languages and dialects

&gt;• Ability to quickly grasp technical concepts and learn in-house data processing tools

&gt;• Strong analytical, problem-solving, and critical-thinking skills

&gt;• Practical knowledge of data processing needs and trade-offs

&gt;• Detail-oriented with excellent communication and strong organizational skills

&gt;• Team player with exceptional interpersonal skills and a solution-oriented attitude

&gt;• Comfortable working in a fast-paced, highly-collaborative, dynamic work environment

&gt;• Willingness to support several projects at one time and to accept reprioritization as necessary",t2_a3vhl,False,,0,False,Amazon Alexa Team -- Data Linguist Roles,[],r/LanguageTechnology,False,6,,0,,False,t3_lew97t,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,1612750582.0,,[],{},,True,,1612762757.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Alexa team is hiring &amp;quot;Data Linguists&amp;quot;, are these literal manual annotation roles? Is Alexa literally hand-tuned based manual decision trees?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Sample JD Description&lt;/p&gt;

&lt;p&gt;Amazon is seeking a Data Linguist-II to join our Amazon Devices Team. This role focuses on language data, primarily in the areas of text annotation and general data analysis deliverables.&lt;/p&gt;

&lt;p&gt;The Data Linguist-II must have a passion for data, efficiency, accuracy, and should be capable of:&lt;/p&gt;

&lt;p&gt;• Handling unique data analysis requests from a range of data customers&lt;/p&gt;

&lt;p&gt;• Providing data quality expertise to other team members and coaching improvements&lt;/p&gt;

&lt;p&gt;• Delivering high quality work under aggressive deadlines&lt;/p&gt;

&lt;p&gt;• Working autonomously with minimum direction&lt;/p&gt;

&lt;p&gt;• Building a thorough understanding of conventions and providing support to global sites&lt;/p&gt;

&lt;p&gt;• Understanding changes to conventions deployed in response to customers’ requests and modifying workflows accordingly&lt;/p&gt;

&lt;p&gt;• Contributing to process improvements to reduce handling time and improve output&lt;/p&gt;

&lt;p&gt;• Improving software tools by identifying bugs and suggesting enhancements&lt;/p&gt;

&lt;p&gt;• Diving deep into issues and implementing solutions independently&lt;/p&gt;

&lt;p&gt;• Proactively addressing issues and problems&lt;/p&gt;

&lt;p&gt;• Keeping up with changing project conventions and priorities&lt;/p&gt;

&lt;p&gt;Basic Qualifications&lt;/p&gt;

&lt;p&gt;• Bachelor’s degree&lt;/p&gt;

&lt;p&gt;• 2+ years of experience working with written language data, including experience with annotation and other forms of data markup&lt;/p&gt;

&lt;p&gt;• 1+ year(s) of experience working with command line interfaces and basic UNIX commands&lt;/p&gt;

&lt;p&gt;• Near-native level fluency in one or more non-English language&lt;/p&gt;

&lt;p&gt;• Business level fluency in English&lt;/p&gt;

&lt;p&gt;• Working knowledge of Microsoft Office products (e.g. Word, Excel, Access, etc.)&lt;/p&gt;

&lt;p&gt;Preferred Qualifications&lt;/p&gt;

&lt;p&gt;• Bachelor’s degree in Linguistics or Computational Linguistics&lt;/p&gt;

&lt;p&gt;• Interest in semantics and related areas&lt;/p&gt;

&lt;p&gt;• Comfortable working with text from various languages and dialects&lt;/p&gt;

&lt;p&gt;• Ability to quickly grasp technical concepts and learn in-house data processing tools&lt;/p&gt;

&lt;p&gt;• Strong analytical, problem-solving, and critical-thinking skills&lt;/p&gt;

&lt;p&gt;• Practical knowledge of data processing needs and trade-offs&lt;/p&gt;

&lt;p&gt;• Detail-oriented with excellent communication and strong organizational skills&lt;/p&gt;

&lt;p&gt;• Team player with exceptional interpersonal skills and a solution-oriented attitude&lt;/p&gt;

&lt;p&gt;• Comfortable working in a fast-paced, highly-collaborative, dynamic work environment&lt;/p&gt;

&lt;p&gt;• Willingness to support several projects at one time and to accept reprioritization as necessary&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lew97t,True,,htrp,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lew97t/amazon_alexa_team_data_linguist_roles/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lew97t/amazon_alexa_team_data_linguist_roles/,30199,1612733957.0,0,,False,,,,,,2614
786,,LanguageTechnology,,t2_iw5gz,False,,0,False,How can I look up for early-stage NLP companies?,[],r/LanguageTechnology,False,6,,0,,False,t3_leks8f,False,dark,1.0,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,False,,False,,[],{},,True,,1612726090.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,leks8f,True,,Wronglen,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/leks8f/how_can_i_look_up_for_earlystage_nlp_companies/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/leks8f/how_can_i_look_up_for_earlystage_nlp_companies/,30199,1612697290.0,0,,False,,,,,,0
787,,LanguageTechnology,"I created an open source tool to collect textual information from various sources (Social media, app stores and Reddit), then applying NLP models (Sentiment, NER and Classification)  on it and later report them back to various places (Slack, ES, Jira etc). My main aim to add many integrations and use existing opensource NLP models (instead of reinvent the wheel) so user can create practical workflow as fast as possible.

Please try it and gives feedback. [Obsei repo](https://github.com/lalitpagaria/obsei)

![Image](https://raw.githubusercontent.com/lalitpagaria/obsei/master/images/Obsei-flow-diagram.png)",t2_a2abtuov,False,,0,False,Obsei: OBserve SEgment and Inform,[],r/LanguageTechnology,False,6,,0,,False,t3_levvxk,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,1612735752.0,,[],{},,True,,1612761704.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I created an open source tool to collect textual information from various sources (Social media, app stores and Reddit), then applying NLP models (Sentiment, NER and Classification)  on it and later report them back to various places (Slack, ES, Jira etc). My main aim to add many integrations and use existing opensource NLP models (instead of reinvent the wheel) so user can create practical workflow as fast as possible.&lt;/p&gt;

&lt;p&gt;Please try it and gives feedback. &lt;a href=""https://github.com/lalitpagaria/obsei""&gt;Obsei repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;![Image](&lt;a href=""https://raw.githubusercontent.com/lalitpagaria/obsei/master/images/Obsei-flow-diagram.png""&gt;https://raw.githubusercontent.com/lalitpagaria/obsei/master/images/Obsei-flow-diagram.png&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,levvxk,True,,FitStatistician7378,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/levvxk/obsei_observe_segment_and_inform/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/levvxk/obsei_observe_segment_and_inform/,30199,1612732904.0,0,,False,,,,,,611
788,,LanguageTechnology,"Hi.

I have a very specific dataset with around 20K sentences, that I want to use for a neural text generation pipeline. I'm looking for really lightweight models, which can train fast and give decent (not at all perfect) result. I'm currently using Markov chain models, but while they are decent, they aren't that original, so I'm trying to experiment with NN models.

I've tested textgenrnn, it's good but the model I think is pretty heavy still, not as heavy as XLNet or other transformer models, but still pretty heavy. Any suggestions would be greatly appreciated.",t2_kpq8t,False,,0,False,Small Neural Text Generator Models,[],r/LanguageTechnology,False,6,,0,,False,t3_lemmr8,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1612734023.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi.&lt;/p&gt;

&lt;p&gt;I have a very specific dataset with around 20K sentences, that I want to use for a neural text generation pipeline. I&amp;#39;m looking for really lightweight models, which can train fast and give decent (not at all perfect) result. I&amp;#39;m currently using Markov chain models, but while they are decent, they aren&amp;#39;t that original, so I&amp;#39;m trying to experiment with NN models.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve tested textgenrnn, it&amp;#39;s good but the model I think is pretty heavy still, not as heavy as XLNet or other transformer models, but still pretty heavy. Any suggestions would be greatly appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lemmr8,True,,quit_daedalus,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lemmr8/small_neural_text_generator_models/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lemmr8/small_neural_text_generator_models/,30199,1612705223.0,0,,False,,,,,,569
789,,LanguageTechnology,"Hey everyone! 

I am currently trying to do a sentiment analysis of different text documents. I am using pyhton 3 and jupyter Notebook. The underlying issue is that I am completely new to programming and have probably taking too much on. I was wondering if someone could explain how I can read a .csv file into [textblob.de](https://textblob.de) and actually get a sentiment analysis 

&amp;#x200B;

PS: sorry for all the terrible simplifications but I am a total noob in this field",t2_knfmrvk,False,,0,False,NLP model in Python,[],r/LanguageTechnology,False,6,,0,,False,t3_lel26z,False,dark,0.75,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1612727376.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone! &lt;/p&gt;

&lt;p&gt;I am currently trying to do a sentiment analysis of different text documents. I am using pyhton 3 and jupyter Notebook. The underlying issue is that I am completely new to programming and have probably taking too much on. I was wondering if someone could explain how I can read a .csv file into &lt;a href=""https://textblob.de""&gt;textblob.de&lt;/a&gt; and actually get a sentiment analysis &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;PS: sorry for all the terrible simplifications but I am a total noob in this field&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lel26z,True,,adolfketler,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lel26z/nlp_model_in_python/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lel26z/nlp_model_in_python/,30199,1612698576.0,0,,False,,,,,,482
790,,LanguageTechnology,"Hello, I have experimented with an Natural Language Understanding task on a real short story.  The original text is as follows:

&amp;#x200B;

&gt;""The Doctor's Word"", Short Story From ""Maguldi Days"", by R.K. Narayan (1972):  
&gt;  
&gt;People came to him when the patient was on his last legs. Dr Raman often burst out, 'Why couldn't you have come a day earlier?' The reason was obvious - visiting fee twenty-five rupees, and more than that, people liked to shirk the fact that the time had come to call in Dr Raman; for them there was something ominous in the very association. As a result, when the big man came on the scene it was always a quick decision one way or another. There was no scope or time for any kind of wavering or whitewashing. Long years of practice of this kind had bred in the doctor a certain curt truthfulness; for that very reason his opinion was valued; he was not a mere doctor expressing an opinion but a judge pronouncing a verdict. The patient's life hung on his words. This never unduly worried Dr Raman. He never believed that agreeable words ever saved lives. He did not think it was any of his business to provide comforting lies when as a matter of course nature would tell them the truth in a few hours. However, when he glimpsed the faintest sign of hope, he rolled up his sleeve and stepped into the arena: it might be hours or days, but he never withdrew till he wrested the prize from Yama's hands.  
&gt;  
&gt;Today, standing over a bed, the doctor felt that he himself needed someone to tell him soothing lies. He mopped his brow with his kerchief and sat down in the chair beside the bed. On the bed lay his dearest friend in the world: Gopal. They had known each other for forty years now, starting with their kindergarten days. They could not, of course, meet as much as they wanted, each being wrapped in his own family and profession. Occasionally, on a Sunday, Gopal would walk into the consulting room and wait patiently in a corner till the doctor was free. And then they would dine together, see a picture and talk of each other's life and activities. It was a classic friendship, which endured untouched by changing times, circumstances and activities.  
&gt;  
&gt;In his busy round of work, Dr Raman had not noticed that Gopal had not called in for over three months now. He only remembered it when he saw Gopal's son sitting on a bench in the consulting hall one crowded morning.  Dr Raman could not talk to him for over an hour. When he got up and was about to pass on to the operating room, he called up the young man and asked, 'What brings you here, sir?' The youth was nervous and shy. 'Mother sent me here.'  
&gt;  
&gt;'What can I do for you?'  
&gt;  
&gt;'Father is ill...'  
&gt;  
&gt;It was an operation day and he was not free till three in the afternoon. He rushed off straight from the clinic to his friend's house, in Lawley Extension.  
&gt;  
&gt;Gopal lay in bed as if in sleep. The doctor stood over him and asked Gopal's wife, 'How long has he been in bed?'  
&gt;  
&gt;'A month and a half, Doctor.'  
&gt;  
&gt;'Who is attending him?'  
&gt;  
&gt;'A doctor in the next street. He comes down once in three days and gives him medicine.'  
&gt;  
&gt;'What is his name?' He had never heard of him. 'Someone I don't know, but I wish he had had the goodness to tell me about it. Why, why couldn't you have sent me word earlier?'  
&gt;  
&gt;'We thought you would be busy and did not wish to trouble you unnecessarily.' They were apologetic and miserable. There was hardly any time to be lost. He took off his coat and opened his bag. He took out an injection tube, the needle sizzled over the stove. The sick man's wife whimpered in a corner and essayed to ask questions.  
&gt;  
&gt;'Please don't ask questions,' snapped the doctor. He looked at the children, who were watching the sterilizer, and said, 'Send them all away somewhere, except the eldest.'  
&gt;  
&gt;He shot in the drug, sat back in his chair and gazed at the patient's face for over an hour. The patient still remained motionless. The doctor's face gleamed with perspiration, and his eyelids drooped with fatigue. The sick man's wife stood in a corner and watched silently. She asked timidly, 'Doctor, shall I make some coffee for you?' 'No,' he replied, although he felt famished, having missed his midday meal. He got up and said, 'I will be back in a few minutes. Don't disturb him on any account.' He picked up his bag and went to his car. In a quarter of an hour he was back, followed by an assistant and a nurse. The doctor told the lady of the house, ' I have to perform an operation.'  
&gt;  
&gt;'Why, why? Why?' she asked faintly.  
&gt;  
&gt;'I will tell you all that soon. Will you leave your son here to help us, and go over to the next house and stay there till I call you?'  
&gt;  
&gt;The lady felt giddy and sank down on the floor, unable to bear the strain. The nurse attended to her and led her out.  
&gt;  
&gt;At about eight in the evening the patient opened his eyes and stirred slightly in bed. The assistant was overjoyed. He exclaimed enthusiastically, 'Sir, he will pull through.' The doctor looked at him coldly and whispered, ' I would give anything to see him pull through but, but the heart...'  
&gt;  
&gt;'The pulse has improved, sir.'  
&gt;  
&gt;'Well, well,' replied the doctor. 'Don't trust it. It is only a false flash-up, very common in these cases.' He ruminated for a while and added, 'If the pulse keeps up till eight in the morning it will go on for the next forty years, but I doubt very much if we shall see anything of it at all after two tonight.'  
&gt;  
&gt;He sent away the assistant and sat beside the patient. At about eleven the patient opened his eyes and smiled at his friend. He showed a slight improvement, he was able to take in a little food. A great feeling of relief and joy went through the household. They swarmed around the doctor and poured out their gratitude. He sat in his seat beside the bed, gazing sternly at the patient's face, hardly showing any signs of hearing what they were saying to him. The sick man's wife asked, 'Is he now out of danger?' Without turning his head the doctor said, 'Give glucose and brandy every forty minutes; just a couple of spoons will do.' The lady went away to the kitchen. She felt restless. She felt she must know the truth whatever it was. Why was the great man so evasive? The suspense was unbearable.. Perhaps he could not speak so near the patient's bed. She beckoned to him from the kitchen doorway. The doctor rose and went over. She asked, 'What about him now? How is he?' The doctor bit his lips and replied, looking at the floor, 'Don't get excited. Unless you must know about it, don't ask now.' Her eyes opened wide in terror. She clasped her hands together and implored, 'Tell me the truth.' The doctor replied, 'I would rather not talk to you now.' He turned round and went back to his chair. A terrible wailing shot through the still house; the patient stirred and looked about in bewilderment. The doctor got up again, went over to the kitchen door, drew it in securely and shut off the wail.  
&gt;  
&gt;When the doctor resumed his seat the patient asked in the faintest whisper possible, 'Is that someone crying?' The doctor advised, 'Don't exert yourself. You mustn't talk.' He felt the pulse. It was already agitated by the exertion. The patient asked, 'Am I going? Don't hide it from me.' The doctor made a deprecating noise and sat back in his chair. He had never faced a situation like this. It was not in his nature to whitewash. People attached great value to his word because of that. He stole a look at the other. The patient motioned a finger to draw him nearer and whispered, 'I must know how long I am going to last. I must sign the will. It is all ready. Ask my wife for the despatch box. You must sign as a witness.'  
&gt;  
&gt;'Oh!' the doctor exclaimed. 'You are exerting yourself too much. You must be quieter.' He felt idiotic to be repeating it. 'How fine it would be,' he reflected, 'to drop the whole business and run away somewhere without answering anybody any question!' The patient clutched the doctor's wrist with his weak fingers and said, 'Ramu, it is my good fortune that you are here at this moment. I can trust your word. I can't leave my property unsettled. That will mean endless misery for my wife and children. You know all about Subbiah and his gang. Let me sign before it is too late. Tell me...'  
&gt;  
&gt;'Yes, presently,' replied the doctor. He walked off to his car, sat in the back seat and reflected. He looked at his watch. Midnight. If the will was to be signed, it must be done within the next two hours, or never. He could not be responsible for a mess there; he knew the family affairs too well and about those wolves, Subbiah and his gang. But what could he do? If he asked him to sign the will, it would virtually mean a death sentence and destroy the thousandth part of a chance that the patient had of survival. he got down from the car and went in. He resumed his seat in the chair. The patient was staring at him appealingly. The doctor said to himself, 'If my word can save his life, he shall not die. The will be damned.' He called, 'Gopal, listen.' This was the first time he was going to do a piece of acting before a patient, simulate a feeling and conceal his judgement. He stooped over the patient and said, with deliberate emphasis, 'Don't worry about the will now. You are going to live. Your heart is absolutely sound.' A new glow suffused the patient's face as he heard it. He asked in a tone of relief, 'Do you say so? If it comes from your lips it must be true...' The doctor said, 'Quite right. You are improving every second. Sleep in peace. You must not exert yourself on any account. You must sleep very soundly. I will see you in the morning.' The patient looked at him gratefully for a moment and then closed his eyes. The doctor picked up his bag and went out, shutting the door softly behind him.  
&gt;  
&gt;On his way home he stopped for a moment at his hospital, called out his assistant and said, 'That Lawley Extension case. You might expect the collapse any second now. Go there with a tube of --- in hand, and give it in case the struggle is too hard at the end. Hurry up.'  
&gt;  
&gt;Next morning he was back at Lawley Extension at ten. From his car he made a dash for the sick bed. The patient was awake and looked very well. The assistant reported satisfactory pulse. The doctor put his tube to his heart, listened for a while and told the sick man's wife, 'Don't look so unhappy, lady. Your husband will live to be ninety.' When they were going back to the hospital, the assistant sitting beside him in the car asked, 'Is he going to live, sir?'  
&gt;  
&gt;'I will bet on it. He will live to be ninety. He has turned the corner. How he has survived this attack will be a puzzle to me all my life,' replied the doctor.

&amp;#x200B;

Next, I have prepped the data by removing punctuation marks and capital letters etc, resulting in a cleaned up version like this:

&amp;#x200B;

&gt;people came to him when the patient was on his last legs dr raman often burst out why couldnt you have come a day earlier the reason was obvious - visiting fee twenty-five rupees and more than that people liked to shirk the fact that the time had come to call in dr raman for them there was something ominous in the very association as a result when the big man came on the scene it was always a quick decision one way or another there was no scope or time for any kind of wavering or whitewashing long years of practice of this kind had bred in the doctor a certain curt truthfulness for that very reason his opinion was valued he was not a mere doctor expressing an opinion but a judge pronouncing a verdict the patients life hung on his words this never unduly worried dr raman he never believed that agreeable words ever saved lives he did not think it was any of his business to provide comforting lies when as a matter of course nature would tell them the truth in a few hours however when he glimpsed the faintest sign of hope he rolled up his sleeve and stepped into the arena: it might be hours or days but he never withdrew till he wrested the prize from yamas hands today standing over a bed the doctor felt that he himself needed someone to tell him soothing lies he mopped his brow with his kerchief and sat down in the chair beside the bed on the bed lay his dearest friend in the world: gopal they had known each other for forty years now starting with their kindergarten days they could not of course meet as much as they wanted each being wrapped in his own family and profession occasionally on a sunday gopal would walk into the consulting room and wait patiently in a corner till the doctor was free and then they would dine together see a picture and talk of each others life and activities it was a classic friendship which endured untouched by changing times circumstances and activities in his busy round of work dr raman had not noticed that gopal had not called in for over three months now he only remembered it when he saw gopals son sitting on a bench in the consulting hall one crowded morning dr raman could not talk to him for over an hour when he got up and was about to pass on to the operating room he called up the young man and asked what brings you here sir the youth was nervous and shy mother sent me here what can i do for you father is ill it was an operation day and he was not free till three in the afternoon he rushed off straight from the clinic to his friends house in lawley extension gopal lay in bed as if in sleep the doctor stood over him and asked gopals wife how long has he been in bed a month and a half doctor who is attending him a doctor in the next street he comes down once in three days and gives him medicine what is his name he had never heard of him someone i dont know but i wish he had had the goodness to tell me about it why why couldnt you have sent me word earlier we thought you would be busy and did not wish to trouble you unnecessarily they were apologetic and miserable there was hardly any time to be lost he took off his coat and opened his bag he took out an injection tube the needle sizzled over the stove the sick mans wife whimpered in a corner and essayed to ask questions please dont ask questions snapped the doctor he looked at the children who were watching the sterilizer and said send them all away somewhere except the eldest he shot in the drug sat back in his chair and gazed at the patients face for over an hour the patient still remained motionless the doctors face gleamed with perspiration and his eyelids drooped with fatigue the sick mans wife stood in a corner and watched silently she asked timidly doctor shall i make some coffee for you no he replied although he felt famished having missed his midday meal he got up and said i will be back in a few minutes dont disturb him on any account he picked up his bag and went to his car in a quarter of an hour he was back followed by an assistant and a nurse the doctor told the lady of the house i have to perform an operation why why why she asked faintly i will tell you all that soon will you leave your son here to help us and go over to the next house and stay there till i call you the lady felt giddy and sank down on the floor unable to bear the strain the nurse attended to her and led her out at about eight in the evening the patient opened his eyes and stirred slightly in bed the assistant was overjoyed he exclaimed enthusiastically sir he will pull through the doctor looked at him coldly and whispered i would give anything to see him pull through but but the heart the pulse has improved sir well well replied the doctor dont trust it it is only a false flash-up very common in these cases he ruminated for a while and added if the pulse keeps up till eight in the morning it will go on for the next forty years but i doubt very much if we shall see anything of it at all after two tonight he sent away the assistant and sat beside the patient at about eleven the patient opened his eyes and smiled at his friend he showed a slight improvement he was able to take in a little food a great feeling of relief and joy went through the household they swarmed around the doctor and poured out their gratitude he sat in his seat beside the bed gazing sternly at the patients face hardly showing any signs of hearing what they were saying to him the sick mans wife asked is he now out of danger without turning his head the doctor said give glucose and brandy every forty minutes just a couple of spoons will do the lady went away to the kitchen she felt restless she felt she must know the truth whatever it was why was the great man so evasive the suspense was unbearable perhaps he could not speak so near the patients bed she beckoned to him from the kitchen doorway the doctor rose and went over she asked what about him now how is he the doctor bit his lips and replied looking at the floor dont get excited unless you must know about it dont ask now her eyes opened wide in terror she clasped her hands together and implored tell me the truth the doctor replied i would rather not talk to you now he turned round and went back to his chair a terrible wailing shot through the still house the patient stirred and looked about in bewilderment the doctor got up again went over to the kitchen door drew it in securely and shut off the wail when the doctor resumed his seat the patient asked in the faintest whisper possible is that someone crying the doctor advised dont exert yourself you mustnt talk he felt the pulse it was already agitated by the exertion the patient asked am i going dont hide it from me the doctor made a deprecating noise and sat back in his chair he had never faced a situation like this it was not in his nature to whitewash people attached great value to his word because of that he stole a look at the other the patient motioned a finger to draw him nearer and whispered i must know how long i am going to last i must sign the will it is all ready ask my wife for the despatch box you must sign as a witness oh the doctor exclaimed you are exerting yourself too much you must be quieter he felt idiotic to be repeating it how fine it would be he reflected to drop the whole business and run away somewhere without answering anybody any question the patient clutched the doctors wrist with his weak fingers and said ramu it is my good fortune that you are here at this moment i can trust your word i cant leave my property unsettled that will mean endless misery for my wife and children you know all about subbiah and his gang let me sign before it is too late tell me yes presently replied the doctor he walked off to his car sat in the back seat and reflected he looked at his watch midnight if the will was to be signed it must be done within the next two hours or never he could not be responsible for a mess there he knew the family affairs too well and about those wolves subbiah and his gang but what could he do if he asked him to sign the will it would virtually mean a death sentence and destroy the thousandth part of a chance that the patient had of survival he got down from the car and went in he resumed his seat in the chair the patient was staring at him appealingly the doctor said to himself if my word can save his life he shall not die the will be damned he called gopal listen this was the first time he was going to do a piece of acting before a patient simulate a feeling and conceal his judgement he stooped over the patient and said with deliberate emphasis dont worry about the will now you are going to live your heart is absolutely sound a new glow suffused the patients face as he heard it he asked in a tone of relief do you say so if it comes from your lips it must be true the doctor said quite right you are improving every second sleep in peace you must not exert yourself on any account you must sleep very soundly i will see you in the morning the patient looked at him gratefully for a moment and then closed his eyes the doctor picked up his bag and went out shutting the door softly behind him on his way home he stopped for a moment at his hospital called out his assistant and said that lawley extension case you might expect the collapse any second now go there with a tube of --- in hand and give it in case the struggle is too hard at the end hurry up next morning he was back at lawley extension at ten from his car he made a dash for the sick bed the patient was awake and looked very well the assistant reported satisfactory pulse the doctor put his tube to his heart listened for a while and told the sick mans wife dont look so unhappy lady your husband will live to be ninety when they were going back to the hospital the assistant sitting beside him in the car asked is he going to live sir i will bet on it he will live to be ninety he has turned the corner how he has survived this attack will be a puzzle to me all my life replied the doctor

&amp;#x200B;

Next, i extracted a list of unique words used in the text, of which there are a total of 653 unique words in this short story:

&amp;#x200B;

&gt;people came to him when the patient was on his last legs dr raman often burst out why couldnt you have come a day earlier reason obvious - visiting fee twenty-five rupees and more than that liked shirk fact time had call in for them there something ominous very association as result big man scene it always quick decision one way or another no scope any kind of wavering whitewashing long years practice this bred doctor certain curt truthfulness opinion valued he not mere expressing an but judge pronouncing verdict patients life hung words never unduly worried believed agreeable ever saved lives did think business provide comforting lies matter course nature would tell truth few hours however glimpsed faintest sign hope rolled up sleeve stepped into arena: might be days withdrew till wrested prize from yamas hands today standing over bed felt himself needed someone soothing mopped brow with kerchief sat down chair beside lay dearest friend world: gopal they known each other forty now starting their kindergarten could meet much wanted being wrapped own family profession occasionally sunday walk consulting room wait patiently corner free then dine together see picture talk others activities classic friendship which endured untouched by changing times circumstances busy round work noticed called three months only remembered saw gopals son sitting bench hall crowded morning hour got about pass operating young asked what brings here sir youth nervous shy mother sent me can i do father is ill operation afternoon rushed off straight clinic friends house lawley extension if sleep stood wife how has been month half who attending next street comes once gives medicine name heard dont know wish goodness word we thought trouble unnecessarily were apologetic miserable hardly lost took coat opened bag injection tube needle sizzled stove sick mans whimpered

&amp;#x200B;

Then, I have searched for all co-occuring pair of words, neighboring incidents of two-words and three-words phrases, and listed out the most common of those:

&amp;#x200B;

&gt;Index \[2, 5\] Co-occurence of phrase ' to the ' = 5  
Index \[2, 9\] Co-occurence of phrase ' to his ' = 6  
Index \[2, 210\] Co-occurence of phrase ' to be ' = 5  
Index \[5, 6\] Co-occurence of phrase ' the patient ' = 15  
Index \[5, 109\] Co-occurence of phrase ' the doctor ' = 23  
Index \[5, 138\] Co-occurence of phrase ' the patients ' = 5  
Index \[5, 623\] Co-occurence of phrase ' the sick ' = 5  
Index \[12, 13\] Co-occurence of phrase ' dr raman ' = 5  
Index \[51, 5\] Co-occurence of phrase ' in the ' = 15  
Index \[51, 9\] Co-occurence of phrase ' in his ' = 6  
Index \[51, 22\] Co-occurence of phrase ' in a ' = 8  
Index \[54, 22\] Co-occurence of phrase ' for a ' = 5  
Index \[75, 7\] Co-occurence of phrase ' it was ' = 7  
Index \[122, 7\] Co-occurence of phrase ' he was ' = 6  
  
&gt;  
&gt;Index \[5, 6, 7\] Co-occurence of phrase ' the patient was ' = 3  
Index \[5, 6, 433\] Co-occurence of phrase ' the patient asked ' = 2  
Index \[5, 6, 607\] Co-occurence of phrase ' the patient opened ' = 2  
Index \[5, 109, 122\] Co-occurence of phrase ' the doctor he ' = 2  
Index \[5, 623, 624\] Co-occurence of phrase ' the sick mans ' = 4  
Index \[6, 607, 9\] Co-occurence of phrase ' patient opened his ' = 2  
Index \[9, 609, 34\] Co-occurence of phrase ' his bag and ' = 2

&amp;#x200B;

My question is that I feel stuck at a dead end here, because those co-occurrences do not seem to provide any meaningful data by which we can gain insight into the story at hand.

For example, taking out filler words like 'to', 'in' and so forth, the most common occurring word-pair is ""The doctor"" at 23 times. The most common word-triples are only used 3 or 4 times, and not very interesting.

What other tools in the Natural Language Processing toolkit can allow us to gain deeper insights into a short story such as this?

Thank you beforehand!

Cheers!",t2_9ht2ap0i,False,,0,False,Need help with NLU task of getting insights into a real short story,[],r/LanguageTechnology,False,6,,0,,False,t3_leo34k,False,dark,0.66,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1612739051.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I have experimented with an Natural Language Understanding task on a real short story.  The original text is as follows:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;quot;The Doctor&amp;#39;s Word&amp;quot;, Short Story From &amp;quot;Maguldi Days&amp;quot;, by R.K. Narayan (1972):  &lt;/p&gt;

&lt;p&gt;People came to him when the patient was on his last legs. Dr Raman often burst out, &amp;#39;Why couldn&amp;#39;t you have come a day earlier?&amp;#39; The reason was obvious - visiting fee twenty-five rupees, and more than that, people liked to shirk the fact that the time had come to call in Dr Raman; for them there was something ominous in the very association. As a result, when the big man came on the scene it was always a quick decision one way or another. There was no scope or time for any kind of wavering or whitewashing. Long years of practice of this kind had bred in the doctor a certain curt truthfulness; for that very reason his opinion was valued; he was not a mere doctor expressing an opinion but a judge pronouncing a verdict. The patient&amp;#39;s life hung on his words. This never unduly worried Dr Raman. He never believed that agreeable words ever saved lives. He did not think it was any of his business to provide comforting lies when as a matter of course nature would tell them the truth in a few hours. However, when he glimpsed the faintest sign of hope, he rolled up his sleeve and stepped into the arena: it might be hours or days, but he never withdrew till he wrested the prize from Yama&amp;#39;s hands.  &lt;/p&gt;

&lt;p&gt;Today, standing over a bed, the doctor felt that he himself needed someone to tell him soothing lies. He mopped his brow with his kerchief and sat down in the chair beside the bed. On the bed lay his dearest friend in the world: Gopal. They had known each other for forty years now, starting with their kindergarten days. They could not, of course, meet as much as they wanted, each being wrapped in his own family and profession. Occasionally, on a Sunday, Gopal would walk into the consulting room and wait patiently in a corner till the doctor was free. And then they would dine together, see a picture and talk of each other&amp;#39;s life and activities. It was a classic friendship, which endured untouched by changing times, circumstances and activities.  &lt;/p&gt;

&lt;p&gt;In his busy round of work, Dr Raman had not noticed that Gopal had not called in for over three months now. He only remembered it when he saw Gopal&amp;#39;s son sitting on a bench in the consulting hall one crowded morning.  Dr Raman could not talk to him for over an hour. When he got up and was about to pass on to the operating room, he called up the young man and asked, &amp;#39;What brings you here, sir?&amp;#39; The youth was nervous and shy. &amp;#39;Mother sent me here.&amp;#39;  &lt;/p&gt;

&lt;p&gt;&amp;#39;What can I do for you?&amp;#39;  &lt;/p&gt;

&lt;p&gt;&amp;#39;Father is ill...&amp;#39;  &lt;/p&gt;

&lt;p&gt;It was an operation day and he was not free till three in the afternoon. He rushed off straight from the clinic to his friend&amp;#39;s house, in Lawley Extension.  &lt;/p&gt;

&lt;p&gt;Gopal lay in bed as if in sleep. The doctor stood over him and asked Gopal&amp;#39;s wife, &amp;#39;How long has he been in bed?&amp;#39;  &lt;/p&gt;

&lt;p&gt;&amp;#39;A month and a half, Doctor.&amp;#39;  &lt;/p&gt;

&lt;p&gt;&amp;#39;Who is attending him?&amp;#39;  &lt;/p&gt;

&lt;p&gt;&amp;#39;A doctor in the next street. He comes down once in three days and gives him medicine.&amp;#39;  &lt;/p&gt;

&lt;p&gt;&amp;#39;What is his name?&amp;#39; He had never heard of him. &amp;#39;Someone I don&amp;#39;t know, but I wish he had had the goodness to tell me about it. Why, why couldn&amp;#39;t you have sent me word earlier?&amp;#39;  &lt;/p&gt;

&lt;p&gt;&amp;#39;We thought you would be busy and did not wish to trouble you unnecessarily.&amp;#39; They were apologetic and miserable. There was hardly any time to be lost. He took off his coat and opened his bag. He took out an injection tube, the needle sizzled over the stove. The sick man&amp;#39;s wife whimpered in a corner and essayed to ask questions.  &lt;/p&gt;

&lt;p&gt;&amp;#39;Please don&amp;#39;t ask questions,&amp;#39; snapped the doctor. He looked at the children, who were watching the sterilizer, and said, &amp;#39;Send them all away somewhere, except the eldest.&amp;#39;  &lt;/p&gt;

&lt;p&gt;He shot in the drug, sat back in his chair and gazed at the patient&amp;#39;s face for over an hour. The patient still remained motionless. The doctor&amp;#39;s face gleamed with perspiration, and his eyelids drooped with fatigue. The sick man&amp;#39;s wife stood in a corner and watched silently. She asked timidly, &amp;#39;Doctor, shall I make some coffee for you?&amp;#39; &amp;#39;No,&amp;#39; he replied, although he felt famished, having missed his midday meal. He got up and said, &amp;#39;I will be back in a few minutes. Don&amp;#39;t disturb him on any account.&amp;#39; He picked up his bag and went to his car. In a quarter of an hour he was back, followed by an assistant and a nurse. The doctor told the lady of the house, &amp;#39; I have to perform an operation.&amp;#39;  &lt;/p&gt;

&lt;p&gt;&amp;#39;Why, why? Why?&amp;#39; she asked faintly.  &lt;/p&gt;

&lt;p&gt;&amp;#39;I will tell you all that soon. Will you leave your son here to help us, and go over to the next house and stay there till I call you?&amp;#39;  &lt;/p&gt;

&lt;p&gt;The lady felt giddy and sank down on the floor, unable to bear the strain. The nurse attended to her and led her out.  &lt;/p&gt;

&lt;p&gt;At about eight in the evening the patient opened his eyes and stirred slightly in bed. The assistant was overjoyed. He exclaimed enthusiastically, &amp;#39;Sir, he will pull through.&amp;#39; The doctor looked at him coldly and whispered, &amp;#39; I would give anything to see him pull through but, but the heart...&amp;#39;  &lt;/p&gt;

&lt;p&gt;&amp;#39;The pulse has improved, sir.&amp;#39;  &lt;/p&gt;

&lt;p&gt;&amp;#39;Well, well,&amp;#39; replied the doctor. &amp;#39;Don&amp;#39;t trust it. It is only a false flash-up, very common in these cases.&amp;#39; He ruminated for a while and added, &amp;#39;If the pulse keeps up till eight in the morning it will go on for the next forty years, but I doubt very much if we shall see anything of it at all after two tonight.&amp;#39;  &lt;/p&gt;

&lt;p&gt;He sent away the assistant and sat beside the patient. At about eleven the patient opened his eyes and smiled at his friend. He showed a slight improvement, he was able to take in a little food. A great feeling of relief and joy went through the household. They swarmed around the doctor and poured out their gratitude. He sat in his seat beside the bed, gazing sternly at the patient&amp;#39;s face, hardly showing any signs of hearing what they were saying to him. The sick man&amp;#39;s wife asked, &amp;#39;Is he now out of danger?&amp;#39; Without turning his head the doctor said, &amp;#39;Give glucose and brandy every forty minutes; just a couple of spoons will do.&amp;#39; The lady went away to the kitchen. She felt restless. She felt she must know the truth whatever it was. Why was the great man so evasive? The suspense was unbearable.. Perhaps he could not speak so near the patient&amp;#39;s bed. She beckoned to him from the kitchen doorway. The doctor rose and went over. She asked, &amp;#39;What about him now? How is he?&amp;#39; The doctor bit his lips and replied, looking at the floor, &amp;#39;Don&amp;#39;t get excited. Unless you must know about it, don&amp;#39;t ask now.&amp;#39; Her eyes opened wide in terror. She clasped her hands together and implored, &amp;#39;Tell me the truth.&amp;#39; The doctor replied, &amp;#39;I would rather not talk to you now.&amp;#39; He turned round and went back to his chair. A terrible wailing shot through the still house; the patient stirred and looked about in bewilderment. The doctor got up again, went over to the kitchen door, drew it in securely and shut off the wail.  &lt;/p&gt;

&lt;p&gt;When the doctor resumed his seat the patient asked in the faintest whisper possible, &amp;#39;Is that someone crying?&amp;#39; The doctor advised, &amp;#39;Don&amp;#39;t exert yourself. You mustn&amp;#39;t talk.&amp;#39; He felt the pulse. It was already agitated by the exertion. The patient asked, &amp;#39;Am I going? Don&amp;#39;t hide it from me.&amp;#39; The doctor made a deprecating noise and sat back in his chair. He had never faced a situation like this. It was not in his nature to whitewash. People attached great value to his word because of that. He stole a look at the other. The patient motioned a finger to draw him nearer and whispered, &amp;#39;I must know how long I am going to last. I must sign the will. It is all ready. Ask my wife for the despatch box. You must sign as a witness.&amp;#39;  &lt;/p&gt;

&lt;p&gt;&amp;#39;Oh!&amp;#39; the doctor exclaimed. &amp;#39;You are exerting yourself too much. You must be quieter.&amp;#39; He felt idiotic to be repeating it. &amp;#39;How fine it would be,&amp;#39; he reflected, &amp;#39;to drop the whole business and run away somewhere without answering anybody any question!&amp;#39; The patient clutched the doctor&amp;#39;s wrist with his weak fingers and said, &amp;#39;Ramu, it is my good fortune that you are here at this moment. I can trust your word. I can&amp;#39;t leave my property unsettled. That will mean endless misery for my wife and children. You know all about Subbiah and his gang. Let me sign before it is too late. Tell me...&amp;#39;  &lt;/p&gt;

&lt;p&gt;&amp;#39;Yes, presently,&amp;#39; replied the doctor. He walked off to his car, sat in the back seat and reflected. He looked at his watch. Midnight. If the will was to be signed, it must be done within the next two hours, or never. He could not be responsible for a mess there; he knew the family affairs too well and about those wolves, Subbiah and his gang. But what could he do? If he asked him to sign the will, it would virtually mean a death sentence and destroy the thousandth part of a chance that the patient had of survival. he got down from the car and went in. He resumed his seat in the chair. The patient was staring at him appealingly. The doctor said to himself, &amp;#39;If my word can save his life, he shall not die. The will be damned.&amp;#39; He called, &amp;#39;Gopal, listen.&amp;#39; This was the first time he was going to do a piece of acting before a patient, simulate a feeling and conceal his judgement. He stooped over the patient and said, with deliberate emphasis, &amp;#39;Don&amp;#39;t worry about the will now. You are going to live. Your heart is absolutely sound.&amp;#39; A new glow suffused the patient&amp;#39;s face as he heard it. He asked in a tone of relief, &amp;#39;Do you say so? If it comes from your lips it must be true...&amp;#39; The doctor said, &amp;#39;Quite right. You are improving every second. Sleep in peace. You must not exert yourself on any account. You must sleep very soundly. I will see you in the morning.&amp;#39; The patient looked at him gratefully for a moment and then closed his eyes. The doctor picked up his bag and went out, shutting the door softly behind him.  &lt;/p&gt;

&lt;p&gt;On his way home he stopped for a moment at his hospital, called out his assistant and said, &amp;#39;That Lawley Extension case. You might expect the collapse any second now. Go there with a tube of --- in hand, and give it in case the struggle is too hard at the end. Hurry up.&amp;#39;  &lt;/p&gt;

&lt;p&gt;Next morning he was back at Lawley Extension at ten. From his car he made a dash for the sick bed. The patient was awake and looked very well. The assistant reported satisfactory pulse. The doctor put his tube to his heart, listened for a while and told the sick man&amp;#39;s wife, &amp;#39;Don&amp;#39;t look so unhappy, lady. Your husband will live to be ninety.&amp;#39; When they were going back to the hospital, the assistant sitting beside him in the car asked, &amp;#39;Is he going to live, sir?&amp;#39;  &lt;/p&gt;

&lt;p&gt;&amp;#39;I will bet on it. He will live to be ninety. He has turned the corner. How he has survived this attack will be a puzzle to me all my life,&amp;#39; replied the doctor.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Next, I have prepped the data by removing punctuation marks and capital letters etc, resulting in a cleaned up version like this:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;people came to him when the patient was on his last legs dr raman often burst out why couldnt you have come a day earlier the reason was obvious - visiting fee twenty-five rupees and more than that people liked to shirk the fact that the time had come to call in dr raman for them there was something ominous in the very association as a result when the big man came on the scene it was always a quick decision one way or another there was no scope or time for any kind of wavering or whitewashing long years of practice of this kind had bred in the doctor a certain curt truthfulness for that very reason his opinion was valued he was not a mere doctor expressing an opinion but a judge pronouncing a verdict the patients life hung on his words this never unduly worried dr raman he never believed that agreeable words ever saved lives he did not think it was any of his business to provide comforting lies when as a matter of course nature would tell them the truth in a few hours however when he glimpsed the faintest sign of hope he rolled up his sleeve and stepped into the arena: it might be hours or days but he never withdrew till he wrested the prize from yamas hands today standing over a bed the doctor felt that he himself needed someone to tell him soothing lies he mopped his brow with his kerchief and sat down in the chair beside the bed on the bed lay his dearest friend in the world: gopal they had known each other for forty years now starting with their kindergarten days they could not of course meet as much as they wanted each being wrapped in his own family and profession occasionally on a sunday gopal would walk into the consulting room and wait patiently in a corner till the doctor was free and then they would dine together see a picture and talk of each others life and activities it was a classic friendship which endured untouched by changing times circumstances and activities in his busy round of work dr raman had not noticed that gopal had not called in for over three months now he only remembered it when he saw gopals son sitting on a bench in the consulting hall one crowded morning dr raman could not talk to him for over an hour when he got up and was about to pass on to the operating room he called up the young man and asked what brings you here sir the youth was nervous and shy mother sent me here what can i do for you father is ill it was an operation day and he was not free till three in the afternoon he rushed off straight from the clinic to his friends house in lawley extension gopal lay in bed as if in sleep the doctor stood over him and asked gopals wife how long has he been in bed a month and a half doctor who is attending him a doctor in the next street he comes down once in three days and gives him medicine what is his name he had never heard of him someone i dont know but i wish he had had the goodness to tell me about it why why couldnt you have sent me word earlier we thought you would be busy and did not wish to trouble you unnecessarily they were apologetic and miserable there was hardly any time to be lost he took off his coat and opened his bag he took out an injection tube the needle sizzled over the stove the sick mans wife whimpered in a corner and essayed to ask questions please dont ask questions snapped the doctor he looked at the children who were watching the sterilizer and said send them all away somewhere except the eldest he shot in the drug sat back in his chair and gazed at the patients face for over an hour the patient still remained motionless the doctors face gleamed with perspiration and his eyelids drooped with fatigue the sick mans wife stood in a corner and watched silently she asked timidly doctor shall i make some coffee for you no he replied although he felt famished having missed his midday meal he got up and said i will be back in a few minutes dont disturb him on any account he picked up his bag and went to his car in a quarter of an hour he was back followed by an assistant and a nurse the doctor told the lady of the house i have to perform an operation why why why she asked faintly i will tell you all that soon will you leave your son here to help us and go over to the next house and stay there till i call you the lady felt giddy and sank down on the floor unable to bear the strain the nurse attended to her and led her out at about eight in the evening the patient opened his eyes and stirred slightly in bed the assistant was overjoyed he exclaimed enthusiastically sir he will pull through the doctor looked at him coldly and whispered i would give anything to see him pull through but but the heart the pulse has improved sir well well replied the doctor dont trust it it is only a false flash-up very common in these cases he ruminated for a while and added if the pulse keeps up till eight in the morning it will go on for the next forty years but i doubt very much if we shall see anything of it at all after two tonight he sent away the assistant and sat beside the patient at about eleven the patient opened his eyes and smiled at his friend he showed a slight improvement he was able to take in a little food a great feeling of relief and joy went through the household they swarmed around the doctor and poured out their gratitude he sat in his seat beside the bed gazing sternly at the patients face hardly showing any signs of hearing what they were saying to him the sick mans wife asked is he now out of danger without turning his head the doctor said give glucose and brandy every forty minutes just a couple of spoons will do the lady went away to the kitchen she felt restless she felt she must know the truth whatever it was why was the great man so evasive the suspense was unbearable perhaps he could not speak so near the patients bed she beckoned to him from the kitchen doorway the doctor rose and went over she asked what about him now how is he the doctor bit his lips and replied looking at the floor dont get excited unless you must know about it dont ask now her eyes opened wide in terror she clasped her hands together and implored tell me the truth the doctor replied i would rather not talk to you now he turned round and went back to his chair a terrible wailing shot through the still house the patient stirred and looked about in bewilderment the doctor got up again went over to the kitchen door drew it in securely and shut off the wail when the doctor resumed his seat the patient asked in the faintest whisper possible is that someone crying the doctor advised dont exert yourself you mustnt talk he felt the pulse it was already agitated by the exertion the patient asked am i going dont hide it from me the doctor made a deprecating noise and sat back in his chair he had never faced a situation like this it was not in his nature to whitewash people attached great value to his word because of that he stole a look at the other the patient motioned a finger to draw him nearer and whispered i must know how long i am going to last i must sign the will it is all ready ask my wife for the despatch box you must sign as a witness oh the doctor exclaimed you are exerting yourself too much you must be quieter he felt idiotic to be repeating it how fine it would be he reflected to drop the whole business and run away somewhere without answering anybody any question the patient clutched the doctors wrist with his weak fingers and said ramu it is my good fortune that you are here at this moment i can trust your word i cant leave my property unsettled that will mean endless misery for my wife and children you know all about subbiah and his gang let me sign before it is too late tell me yes presently replied the doctor he walked off to his car sat in the back seat and reflected he looked at his watch midnight if the will was to be signed it must be done within the next two hours or never he could not be responsible for a mess there he knew the family affairs too well and about those wolves subbiah and his gang but what could he do if he asked him to sign the will it would virtually mean a death sentence and destroy the thousandth part of a chance that the patient had of survival he got down from the car and went in he resumed his seat in the chair the patient was staring at him appealingly the doctor said to himself if my word can save his life he shall not die the will be damned he called gopal listen this was the first time he was going to do a piece of acting before a patient simulate a feeling and conceal his judgement he stooped over the patient and said with deliberate emphasis dont worry about the will now you are going to live your heart is absolutely sound a new glow suffused the patients face as he heard it he asked in a tone of relief do you say so if it comes from your lips it must be true the doctor said quite right you are improving every second sleep in peace you must not exert yourself on any account you must sleep very soundly i will see you in the morning the patient looked at him gratefully for a moment and then closed his eyes the doctor picked up his bag and went out shutting the door softly behind him on his way home he stopped for a moment at his hospital called out his assistant and said that lawley extension case you might expect the collapse any second now go there with a tube of --- in hand and give it in case the struggle is too hard at the end hurry up next morning he was back at lawley extension at ten from his car he made a dash for the sick bed the patient was awake and looked very well the assistant reported satisfactory pulse the doctor put his tube to his heart listened for a while and told the sick mans wife dont look so unhappy lady your husband will live to be ninety when they were going back to the hospital the assistant sitting beside him in the car asked is he going to live sir i will bet on it he will live to be ninety he has turned the corner how he has survived this attack will be a puzzle to me all my life replied the doctor&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Next, i extracted a list of unique words used in the text, of which there are a total of 653 unique words in this short story:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;people came to him when the patient was on his last legs dr raman often burst out why couldnt you have come a day earlier reason obvious - visiting fee twenty-five rupees and more than that liked shirk fact time had call in for them there something ominous very association as result big man scene it always quick decision one way or another no scope any kind of wavering whitewashing long years practice this bred doctor certain curt truthfulness opinion valued he not mere expressing an but judge pronouncing verdict patients life hung words never unduly worried believed agreeable ever saved lives did think business provide comforting lies matter course nature would tell truth few hours however glimpsed faintest sign hope rolled up sleeve stepped into arena: might be days withdrew till wrested prize from yamas hands today standing over bed felt himself needed someone soothing mopped brow with kerchief sat down chair beside lay dearest friend world: gopal they known each other forty now starting their kindergarten could meet much wanted being wrapped own family profession occasionally sunday walk consulting room wait patiently corner free then dine together see picture talk others activities classic friendship which endured untouched by changing times circumstances busy round work noticed called three months only remembered saw gopals son sitting bench hall crowded morning hour got about pass operating young asked what brings here sir youth nervous shy mother sent me can i do father is ill operation afternoon rushed off straight clinic friends house lawley extension if sleep stood wife how has been month half who attending next street comes once gives medicine name heard dont know wish goodness word we thought trouble unnecessarily were apologetic miserable hardly lost took coat opened bag injection tube needle sizzled stove sick mans whimpered&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Then, I have searched for all co-occuring pair of words, neighboring incidents of two-words and three-words phrases, and listed out the most common of those:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Index [2, 5] Co-occurence of phrase &amp;#39; to the &amp;#39; = 5&lt;br/&gt;
Index [2, 9] Co-occurence of phrase &amp;#39; to his &amp;#39; = 6&lt;br/&gt;
Index [2, 210] Co-occurence of phrase &amp;#39; to be &amp;#39; = 5&lt;br/&gt;
Index [5, 6] Co-occurence of phrase &amp;#39; the patient &amp;#39; = 15&lt;br/&gt;
Index [5, 109] Co-occurence of phrase &amp;#39; the doctor &amp;#39; = 23&lt;br/&gt;
Index [5, 138] Co-occurence of phrase &amp;#39; the patients &amp;#39; = 5&lt;br/&gt;
Index [5, 623] Co-occurence of phrase &amp;#39; the sick &amp;#39; = 5&lt;br/&gt;
Index [12, 13] Co-occurence of phrase &amp;#39; dr raman &amp;#39; = 5&lt;br/&gt;
Index [51, 5] Co-occurence of phrase &amp;#39; in the &amp;#39; = 15&lt;br/&gt;
Index [51, 9] Co-occurence of phrase &amp;#39; in his &amp;#39; = 6&lt;br/&gt;
Index [51, 22] Co-occurence of phrase &amp;#39; in a &amp;#39; = 8&lt;br/&gt;
Index [54, 22] Co-occurence of phrase &amp;#39; for a &amp;#39; = 5&lt;br/&gt;
Index [75, 7] Co-occurence of phrase &amp;#39; it was &amp;#39; = 7&lt;br/&gt;
Index [122, 7] Co-occurence of phrase &amp;#39; he was &amp;#39; = 6  &lt;/p&gt;

&lt;p&gt;Index [5, 6, 7] Co-occurence of phrase &amp;#39; the patient was &amp;#39; = 3&lt;br/&gt;
Index [5, 6, 433] Co-occurence of phrase &amp;#39; the patient asked &amp;#39; = 2&lt;br/&gt;
Index [5, 6, 607] Co-occurence of phrase &amp;#39; the patient opened &amp;#39; = 2&lt;br/&gt;
Index [5, 109, 122] Co-occurence of phrase &amp;#39; the doctor he &amp;#39; = 2&lt;br/&gt;
Index [5, 623, 624] Co-occurence of phrase &amp;#39; the sick mans &amp;#39; = 4&lt;br/&gt;
Index [6, 607, 9] Co-occurence of phrase &amp;#39; patient opened his &amp;#39; = 2&lt;br/&gt;
Index [9, 609, 34] Co-occurence of phrase &amp;#39; his bag and &amp;#39; = 2&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;My question is that I feel stuck at a dead end here, because those co-occurrences do not seem to provide any meaningful data by which we can gain insight into the story at hand.&lt;/p&gt;

&lt;p&gt;For example, taking out filler words like &amp;#39;to&amp;#39;, &amp;#39;in&amp;#39; and so forth, the most common occurring word-pair is &amp;quot;The doctor&amp;quot; at 23 times. The most common word-triples are only used 3 or 4 times, and not very interesting.&lt;/p&gt;

&lt;p&gt;What other tools in the Natural Language Processing toolkit can allow us to gain deeper insights into a short story such as this?&lt;/p&gt;

&lt;p&gt;Thank you beforehand!&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,leo34k,True,,Agreeable_3573,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/leo34k/need_help_with_nlu_task_of_getting_insights_into/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/leo34k/need_help_with_nlu_task_of_getting_insights_into/,30199,1612710251.0,0,,False,,,,,,25249
791,,LanguageTechnology,,t2_a549lysx,False,,0,False,How does working on NLP job looks like?,[],r/LanguageTechnology,False,6,,0,,False,t3_le717k,False,dark,0.92,,public,21,0,{},,False,[],,False,False,,{},,False,21,,False,False,,False,,[],{},,True,,1612674927.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,le717k,True,,ZeroCoooooooooool,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/le717k/how_does_working_on_nlp_job_looks_like/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/le717k/how_does_working_on_nlp_job_looks_like/,30199,1612646127.0,0,,False,,,,,,0
792,,LanguageTechnology,"Hi there,

I am completely new to nlp but already fascinated - especially by the concept of word embeddings. 

Since I am novel to programming in general, can someone recommend a book that gives an overview into different nlp models/techniques without getting to technical? I have no idea whether this even exists or if books are the go to way to acquire knowledge in machine learning. Alternatively if someone could explain good websites/youtube channels/podcasts I highly appreciate.

Fyi, I do have a thorough understanding of linear algebra including matrix-operations so if it gets very detailed on the mathematical layer I am happy to explore. 

Thanks in advance",t2_1aw1jcwc,False,,0,False,Book recommendations,[],r/LanguageTechnology,False,6,,0,,False,t3_lelmaa,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1612729892.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there,&lt;/p&gt;

&lt;p&gt;I am completely new to nlp but already fascinated - especially by the concept of word embeddings. &lt;/p&gt;

&lt;p&gt;Since I am novel to programming in general, can someone recommend a book that gives an overview into different nlp models/techniques without getting to technical? I have no idea whether this even exists or if books are the go to way to acquire knowledge in machine learning. Alternatively if someone could explain good websites/youtube channels/podcasts I highly appreciate.&lt;/p&gt;

&lt;p&gt;Fyi, I do have a thorough understanding of linear algebra including matrix-operations so if it gets very detailed on the mathematical layer I am happy to explore. &lt;/p&gt;

&lt;p&gt;Thanks in advance&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lelmaa,True,,random9ness,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lelmaa/book_recommendations/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lelmaa/book_recommendations/,30199,1612701092.0,0,,False,,,,,,669
793,,LanguageTechnology,"Often in Natural Language Processing(NLP), we see some kind of accuracy measure as the proof of our model’s correctness. But clearly, such automated objective evaluations end-up resulting in overestimating the performance.  This paper exactly talks about that and proposes new framework for evaluation called CheckList. 🎉 

This paper won Best Paper Award at ACL 2020. 🥇 


Blog - https://lnkd.in/dhgFKRE",t2_hkv9s,False,,0,False,Beyond Accuracy: Behavioral Testing of NLP models with CheckList,[],r/LanguageTechnology,False,6,,0,,False,t3_ldsuun,False,dark,0.97,,public,29,0,{},,False,[],,False,False,,{},,False,29,,False,False,,False,,[],{},,True,,1612627369.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Often in Natural Language Processing(NLP), we see some kind of accuracy measure as the proof of our model’s correctness. But clearly, such automated objective evaluations end-up resulting in overestimating the performance.  This paper exactly talks about that and proposes new framework for evaluation called CheckList. 🎉 &lt;/p&gt;

&lt;p&gt;This paper won Best Paper Award at ACL 2020. 🥇 &lt;/p&gt;

&lt;p&gt;Blog - &lt;a href=""https://lnkd.in/dhgFKRE""&gt;https://lnkd.in/dhgFKRE&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ldsuun,True,,prakhar21,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ldsuun/beyond_accuracy_behavioral_testing_of_nlp_models/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ldsuun/beyond_accuracy_behavioral_testing_of_nlp_models/,30199,1612598569.0,0,,False,,,,,,404
794,,LanguageTechnology,,t2_mr1oj,False,,0,False,Interlingo - App de Ingles para MÓVIL (próximo lanzamiento),[],r/LanguageTechnology,False,6,,0,,False,t3_le94f1,False,dark,0.5,,public,0,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/JVMC1QRK6rE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Interlingo - App de Ingles para MÓVIL (próximo lanzamiento)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/JVMC1QRK6rE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Aprender Idiomas y Cultura General con Rodrigo', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/JVMC1QRK6rE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/CursoDeInglesGratisFreeSpanishLessons'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/JVMC1QRK6rE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/le94f1', 'height': 200}",,False,0,,False,False,,False,,[],{},,False,,1612681105.0,text,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,le94f1,True,,roferre,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/le94f1/interlingo_app_de_ingles_para_móvil_próximo/,all_ads,False,https://youtube.com/watch?v=JVMC1QRK6rE&amp;feature=share,30199,1612652305.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Interlingo - App de Ingles para MÓVIL (próximo lanzamiento)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/JVMC1QRK6rE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Aprender Idiomas y Cultura General con Rodrigo', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/JVMC1QRK6rE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/CursoDeInglesGratisFreeSpanishLessons'}}",False,https://youtube.com/watch?v=JVMC1QRK6rE&amp;feature=share,,,,,0
795,,LanguageTechnology,"I've been a professional web developer for a couple of years, but never took on a project that uses any kind of ML. Now I have a problem that I'd like to solve with ML, but am having a hard time figuring out what kind of technology is fit for this purpose.

What I want to do is to tag articles by country, based on the headline. A few examples of expected mappings:

""Interview with Martin Jacques on BBC Coverage of China "" =&gt; ""China""  
""The Senate says no to $15"" =&gt; ""United States""  
""Why Memes Will Never Be Monetized "" =&gt; ""International""  
""Along the Thames "" =&gt; ""United Kingdom""  
""Rising Tides: Diving Into Mumbai’s Flooding Challenges "" =&gt; ""India""  
""US Election: What’s at stake for Brazil "" =&gt; ""United States, Brazil""  


What's a reasonable way to solve this problem? Preferably something I can run myself, without using a paid service",t2_anlzl,False,,0,False,How to match headlines with countries?,[],r/LanguageTechnology,False,6,,0,,False,t3_ldzyxa,False,dark,0.76,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1612654811.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been a professional web developer for a couple of years, but never took on a project that uses any kind of ML. Now I have a problem that I&amp;#39;d like to solve with ML, but am having a hard time figuring out what kind of technology is fit for this purpose.&lt;/p&gt;

&lt;p&gt;What I want to do is to tag articles by country, based on the headline. A few examples of expected mappings:&lt;/p&gt;

&lt;p&gt;&amp;quot;Interview with Martin Jacques on BBC Coverage of China &amp;quot; =&amp;gt; &amp;quot;China&amp;quot;&lt;br/&gt;
&amp;quot;The Senate says no to $15&amp;quot; =&amp;gt; &amp;quot;United States&amp;quot;&lt;br/&gt;
&amp;quot;Why Memes Will Never Be Monetized &amp;quot; =&amp;gt; &amp;quot;International&amp;quot;&lt;br/&gt;
&amp;quot;Along the Thames &amp;quot; =&amp;gt; &amp;quot;United Kingdom&amp;quot;&lt;br/&gt;
&amp;quot;Rising Tides: Diving Into Mumbai’s Flooding Challenges &amp;quot; =&amp;gt; &amp;quot;India&amp;quot;&lt;br/&gt;
&amp;quot;US Election: What’s at stake for Brazil &amp;quot; =&amp;gt; &amp;quot;United States, Brazil&amp;quot;  &lt;/p&gt;

&lt;p&gt;What&amp;#39;s a reasonable way to solve this problem? Preferably something I can run myself, without using a paid service&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ldzyxa,True,,Bomull,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ldzyxa/how_to_match_headlines_with_countries/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ldzyxa/how_to_match_headlines_with_countries/,30199,1612626011.0,0,,False,,,,,,865
796,,LanguageTechnology,"I would like to train a Transformer to perform a next-word prediction task, using a basic corpus (e.g., a native corpus from Python nltk). Is this doable with the resources I have?

&amp;#x200B;

**Processor**: Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz 3.19 GHz

**RAM:** 16.0 GB (15.8 GB usable)

**GPU:** My Device Manager indicates that I have two: NVIDIA GeForce GTX 1070 &amp;  Intel UHD Graphics 630",t2_128h0c,False,,0,False,[question] What kind of resources are required to train a decoder-only Transformer end-to-end?,[],r/LanguageTechnology,False,6,,0,,False,t3_ldngxd,False,dark,0.75,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1612606203.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I would like to train a Transformer to perform a next-word prediction task, using a basic corpus (e.g., a native corpus from Python nltk). Is this doable with the resources I have?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Processor&lt;/strong&gt;: Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz 3.19 GHz&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 16.0 GB (15.8 GB usable)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GPU:&lt;/strong&gt; My Device Manager indicates that I have two: NVIDIA GeForce GTX 1070 &amp;amp;  Intel UHD Graphics 630&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ldngxd,True,,synysterbates,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ldngxd/question_what_kind_of_resources_are_required_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ldngxd/question_what_kind_of_resources_are_required_to/,30199,1612577403.0,0,,False,,,,,,403
797,,LanguageTechnology,"Hey guys. Working on a document-level relation extraction (DocRE) task and I'm wondering what kind of datasets there may be.

In the general domain I've only been able to come across DocRED, and there are also the BC5CDR and GDA datasets in the biomedical domain. I'm wondering what else may be out there that I've missed.

Thanks!",t2_m8kccne,False,,0,False,What are some document-level relation extraction datasets I may have missed?,[],r/LanguageTechnology,False,6,,0,,False,t3_ldnm43,False,dark,0.76,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1612606650.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys. Working on a document-level relation extraction (DocRE) task and I&amp;#39;m wondering what kind of datasets there may be.&lt;/p&gt;

&lt;p&gt;In the general domain I&amp;#39;ve only been able to come across DocRED, and there are also the BC5CDR and GDA datasets in the biomedical domain. I&amp;#39;m wondering what else may be out there that I&amp;#39;ve missed.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ldnm43,True,,Seankala,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ldnm43/what_are_some_documentlevel_relation_extraction/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ldnm43/what_are_some_documentlevel_relation_extraction/,30199,1612577850.0,0,,False,,,,,,331
798,,LanguageTechnology,"Hello everyone!
I'm doing this project where, I'll be taking in consumer complaints. 
The basic aim of my project is to highlight most common issues faced by consumers by using either keyword extraction or document clustering.

Kindly suggest some techniques/tools using which I could implement this.",t2_8n5d56qx,False,,0,False,I want to find out popular issues faced by consumers,[],r/LanguageTechnology,False,6,,0,,False,t3_lde6ci,False,dark,0.88,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1612579778.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone!
I&amp;#39;m doing this project where, I&amp;#39;ll be taking in consumer complaints. 
The basic aim of my project is to highlight most common issues faced by consumers by using either keyword extraction or document clustering.&lt;/p&gt;

&lt;p&gt;Kindly suggest some techniques/tools using which I could implement this.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lde6ci,True,,ChandlerBingggggggg,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lde6ci/i_want_to_find_out_popular_issues_faced_by/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lde6ci/i_want_to_find_out_popular_issues_faced_by/,30199,1612550978.0,0,,False,,,,,,300
799,,LanguageTechnology,"First two words are the seeds

* your wife didn't marry you because we've understood all along . Deep Fucking Value . Salute !
* gamestop is still a 300% green day . Hold the line , the funds most likely rebought
* the retard you must not be earth shattering dump . Don’t worry everyone always wipes their first
* the retard strength in this nonsense lmao ! Priceless . Good shit Edit: In discovery we could
* stonks can only place I can warm my tendies . Can someone explain how this movie 2
* your wife and her boyfriend extra close tonight . J . Simpson to do another paper trading
* your wife may have covered the shorts to have joined . I can’t buy more shares
* stonks can only hope for the word ""tendies"" . # HOLD HOLD HOLD ! 💎🙌 . Guys
* gamestop is going to $100 Million . DO NOT SELL 💎🙌🏻 , BUY ! ! ! ! Available shares just got bodied ",t2_8l0cb,False,,0,False,"Trained a Markov Chain on a bunch of r/WSB posts and comments. Only 2-word conditional probabilities but honestly, that's all that's necessary 🚀🚀",[],r/LanguageTechnology,False,6,,0,,False,t3_lcn2ee,False,dark,1.0,,public,81,0,{},,False,[],,False,False,,{},,False,81,,False,False,,1612465468.0,,[],{},,True,,1612493981.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;First two words are the seeds&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;your wife didn&amp;#39;t marry you because we&amp;#39;ve understood all along . Deep Fucking Value . Salute !&lt;/li&gt;
&lt;li&gt;gamestop is still a 300% green day . Hold the line , the funds most likely rebought&lt;/li&gt;
&lt;li&gt;the retard you must not be earth shattering dump . Don’t worry everyone always wipes their first&lt;/li&gt;
&lt;li&gt;the retard strength in this nonsense lmao ! Priceless . Good shit Edit: In discovery we could&lt;/li&gt;
&lt;li&gt;stonks can only place I can warm my tendies . Can someone explain how this movie 2&lt;/li&gt;
&lt;li&gt;your wife and her boyfriend extra close tonight . J . Simpson to do another paper trading&lt;/li&gt;
&lt;li&gt;your wife may have covered the shorts to have joined . I can’t buy more shares&lt;/li&gt;
&lt;li&gt;stonks can only hope for the word &amp;quot;tendies&amp;quot; . # HOLD HOLD HOLD ! 💎🙌 . Guys&lt;/li&gt;
&lt;li&gt;gamestop is going to $100 Million . DO NOT SELL 💎🙌🏻 , BUY ! ! ! ! Available shares just got bodied &lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lcn2ee,True,,japooki,,13,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lcn2ee/trained_a_markov_chain_on_a_bunch_of_rwsb_posts/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lcn2ee/trained_a_markov_chain_on_a_bunch_of_rwsb_posts/,30199,1612465181.0,0,,False,,,,,,844
800,,LanguageTechnology,"This blog reflects my learnings from a recent project that I wrapped up as a part of my last semester course. I hope this will be helpful to many of you looking forward to solving similar problems. The task is to come up with an unsupervised technique for automatic extraction of glossary and their respective definitions from some input text (could be the book, chapter, etc)

Blog Link: https://link.medium.com/hjgUMtjECdb",t2_hkv9s,False,,0,False,Automatic Glossary Creation and Definition Extraction from Text in Unsupervised way,[],r/LanguageTechnology,False,6,,0,,False,t3_ld25oz,False,dark,0.8,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1612539260.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This blog reflects my learnings from a recent project that I wrapped up as a part of my last semester course. I hope this will be helpful to many of you looking forward to solving similar problems. The task is to come up with an unsupervised technique for automatic extraction of glossary and their respective definitions from some input text (could be the book, chapter, etc)&lt;/p&gt;

&lt;p&gt;Blog Link: &lt;a href=""https://link.medium.com/hjgUMtjECdb""&gt;https://link.medium.com/hjgUMtjECdb&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ld25oz,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ld25oz/automatic_glossary_creation_and_definition/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ld25oz/automatic_glossary_creation_and_definition/,30199,1612510460.0,0,,False,,,,,,424
801,,LanguageTechnology,"NLP enthusiasts - we all know POS tagging is an important task for so many applications. I made a mastery-based assignment to help you practice and learn the Penn Treebank POS tags. Please feel free to share any feedback with me on the assignment. Thank you!

[https://open.openclass.ai/resource/assignment-601c5ae255950e64a8c0d581?code=94s5gGSI1PZKSg](https://open.openclass.ai/resource/assignment-601c5ae255950e64a8c0d581?code=94s5gGSI1PZKSg)",t2_2uzfmau,False,,0,False,POS tagging practice,[],r/LanguageTechnology,False,6,,0,,False,t3_lcw7ld,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1612518166.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;NLP enthusiasts - we all know POS tagging is an important task for so many applications. I made a mastery-based assignment to help you practice and learn the Penn Treebank POS tags. Please feel free to share any feedback with me on the assignment. Thank you!&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://open.openclass.ai/resource/assignment-601c5ae255950e64a8c0d581?code=94s5gGSI1PZKSg""&gt;https://open.openclass.ai/resource/assignment-601c5ae255950e64a8c0d581?code=94s5gGSI1PZKSg&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lcw7ld,True,,galalalal,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lcw7ld/pos_tagging_practice/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lcw7ld/pos_tagging_practice/,30199,1612489366.0,0,,False,,,,,,444
802,,LanguageTechnology,"In this article you’ll see how to structure work on deep learning projects — from the inception to deployment, and everything in between. You will learn:

- About the lifecycle of the project.
- Importance of defining an objective or goal of the project.
- Collecting data based on the requirements of the project.
- Model training and results exploration including:
    - Establishing baselines for better results.
    -Adopting techniques and approaches from the existing open-source state-of-the-art models research papers and code repositories.
    - Experiment tracking and management management 
- Model refinement techniques to avoid underfitting and overfitting like:
    - Controlling hyperparameters
    - Regularisation
    - Pruning
- Testing and evaluating your project before deployment.
- Model deployment
- Project maintenance

[Structuring deep learning projects](https://neptune.ai/blog/how-to-organize-deep-learning-projects-best-practices?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=blog-how-to-organize-deep-learning-projects-best-practices&amp;utm_content=languagetechnology)",t2_5hfacnnv,False,,0,False,[Best Practices] on how to organize deep learning projects,[],r/LanguageTechnology,False,6,,0,,False,t3_lcj064,False,dark,1.0,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1612483744.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In this article you’ll see how to structure work on deep learning projects — from the inception to deployment, and everything in between. You will learn:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;About the lifecycle of the project.&lt;/li&gt;
&lt;li&gt;Importance of defining an objective or goal of the project.&lt;/li&gt;
&lt;li&gt;Collecting data based on the requirements of the project.&lt;/li&gt;
&lt;li&gt;Model training and results exploration including:

&lt;ul&gt;
&lt;li&gt;Establishing baselines for better results.
-Adopting techniques and approaches from the existing open-source state-of-the-art models research papers and code repositories.&lt;/li&gt;
&lt;li&gt;Experiment tracking and management management &lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Model refinement techniques to avoid underfitting and overfitting like:

&lt;ul&gt;
&lt;li&gt;Controlling hyperparameters&lt;/li&gt;
&lt;li&gt;Regularisation&lt;/li&gt;
&lt;li&gt;Pruning&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Testing and evaluating your project before deployment.&lt;/li&gt;
&lt;li&gt;Model deployment&lt;/li&gt;
&lt;li&gt;Project maintenance&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=""https://neptune.ai/blog/how-to-organize-deep-learning-projects-best-practices?utm_source=reddit&amp;amp;utm_medium=post&amp;amp;utm_campaign=blog-how-to-organize-deep-learning-projects-best-practices&amp;amp;utm_content=languagetechnology""&gt;Structuring deep learning projects&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lcj064,True,,kk_ai,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lcj064/best_practices_on_how_to_organize_deep_learning/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lcj064/best_practices_on_how_to_organize_deep_learning/,30199,1612454944.0,0,,False,,,,,,1108
803,,LanguageTechnology,,t2_a1cdt4it,False,,0,False,The most Spoken Languages in the World - 1900/2021,[],r/LanguageTechnology,False,6,,0,,False,t3_lcg3pb,False,dark,0.84,,public,13,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/XMf-XhqLlbo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'The most Spoken Languages in the World - 1900/2021', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/XMf-XhqLlbo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Statistics and data', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/XMf-XhqLlbo/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/Statisticsanddata'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/XMf-XhqLlbo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lcg3pb', 'height': 200}",,False,13,,False,True,,False,,[],{},,False,,1612475636.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lcg3pb,True,,cuffia_azzurra_2,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lcg3pb/the_most_spoken_languages_in_the_world_19002021/,all_ads,False,https://youtu.be/XMf-XhqLlbo,30199,1612446836.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'The most Spoken Languages in the World - 1900/2021', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/XMf-XhqLlbo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Statistics and data', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/XMf-XhqLlbo/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/Statisticsanddata'}}",False,https://youtu.be/XMf-XhqLlbo,,,,,0
804,,LanguageTechnology,"First approach for recognizing logical document structures like texts, sentences, segments, words, chars and sentence/segment depth based on recurrent neural network grammars.

The model is able to recognizing the followig logical document structures

* (t - text start
* (s - sentence start
* (seg - segment start
* (w - word start
* (c - char start
* )- end of logical document structure
* Ti - sentence/segment depth will be measured recursive

 

The sentence

Georg Bendemann, ein junger Kaufmann, saß in seinem Privatzimmer im ersten Stock eines der niedrigen leichtgebauten Häuser, die entlang des Flusses in einer langen Reihe, fast nur in der Höhe und Färbung unterschieden, sich hinzogen.

should be predicted as

(s (seg (w Georg) (w Bendemann) (c ,)) (seg (w ein) (w junger) (w Kaufmann) (c ,)) (seg (w saß) (w in) (w seinem) (w Privatzimmer) (w im) (w ersten) (w Stock) (w eines) (w der) (w niedrigen) (c ,)) (seg (w leichtgebauten) (w Häuser) (c ,)) (seg (w die) (w entlang) (w des) (w Flusses) (w in) (w einer) (w langen) (w Reihe) (c,)) (seg (w fast) (w nur) (w in) (w der) (w Höhe) (w und) (w Färbung) (w unterschieden) (c ,)) (seg (w sich) (w hinzogen) (c .)))

Check out my Github for the full source Code: [https://github.com/Psarpei/Recognition-of-logical-document-structures](https://github.com/Psarpei/Recognition-of-logical-document-structures)",t2_4aflqwk0,False,,0,False,Recognition of logical document structures,[],r/LanguageTechnology,False,6,,0,,False,t3_lcqft8,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1612502276.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;First approach for recognizing logical document structures like texts, sentences, segments, words, chars and sentence/segment depth based on recurrent neural network grammars.&lt;/p&gt;

&lt;p&gt;The model is able to recognizing the followig logical document structures&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(t - text start&lt;/li&gt;
&lt;li&gt;(s - sentence start&lt;/li&gt;
&lt;li&gt;(seg - segment start&lt;/li&gt;
&lt;li&gt;(w - word start&lt;/li&gt;
&lt;li&gt;(c - char start&lt;/li&gt;
&lt;li&gt;)- end of logical document structure&lt;/li&gt;
&lt;li&gt;Ti - sentence/segment depth will be measured recursive&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The sentence&lt;/p&gt;

&lt;p&gt;Georg Bendemann, ein junger Kaufmann, saß in seinem Privatzimmer im ersten Stock eines der niedrigen leichtgebauten Häuser, die entlang des Flusses in einer langen Reihe, fast nur in der Höhe und Färbung unterschieden, sich hinzogen.&lt;/p&gt;

&lt;p&gt;should be predicted as&lt;/p&gt;

&lt;p&gt;(s (seg (w Georg) (w Bendemann) (c ,)) (seg (w ein) (w junger) (w Kaufmann) (c ,)) (seg (w saß) (w in) (w seinem) (w Privatzimmer) (w im) (w ersten) (w Stock) (w eines) (w der) (w niedrigen) (c ,)) (seg (w leichtgebauten) (w Häuser) (c ,)) (seg (w die) (w entlang) (w des) (w Flusses) (w in) (w einer) (w langen) (w Reihe) (c,)) (seg (w fast) (w nur) (w in) (w der) (w Höhe) (w und) (w Färbung) (w unterschieden) (c ,)) (seg (w sich) (w hinzogen) (c .)))&lt;/p&gt;

&lt;p&gt;Check out my Github for the full source Code: &lt;a href=""https://github.com/Psarpei/Recognition-of-logical-document-structures""&gt;https://github.com/Psarpei/Recognition-of-logical-document-structures&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lcqft8,True,,psarpei,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lcqft8/recognition_of_logical_document_structures/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lcqft8/recognition_of_logical_document_structures/,30199,1612473476.0,0,,False,,,,,,1368
805,,LanguageTechnology,"I'm looking for a way to avoid removing ending s when s isn't a suffix. In order to do that, I first check if a word exists in my index, if it does, I don't remove the ending s but If it doesn't, I go on and remove the ending s and add it to the index. But the problem is what to do when starting to build the index.

Imagine we encounter ""books"", I remove s and add ""book"" to my index. On the other hand, I may encounter  ""dangerous"" for the first time, since it doesn't exists in my index yet, I remove s and add ""dangerou"" which is obviously wrong. What should I do?

Specifically I'm looking for ways to properly detect if suffixes and prefixes are indeed one or part of the original word.

P.S: I'm not working on English docs. Therefore I'm looking for general ideas about these situations not ready to use libraries.",t2_40ofuvyl,False,,0,False,How to stem plural words properly?,[],r/LanguageTechnology,False,6,,0,,False,t3_lces4e,False,dark,0.75,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1612471210.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m looking for a way to avoid removing ending s when s isn&amp;#39;t a suffix. In order to do that, I first check if a word exists in my index, if it does, I don&amp;#39;t remove the ending s but If it doesn&amp;#39;t, I go on and remove the ending s and add it to the index. But the problem is what to do when starting to build the index.&lt;/p&gt;

&lt;p&gt;Imagine we encounter &amp;quot;books&amp;quot;, I remove s and add &amp;quot;book&amp;quot; to my index. On the other hand, I may encounter  &amp;quot;dangerous&amp;quot; for the first time, since it doesn&amp;#39;t exists in my index yet, I remove s and add &amp;quot;dangerou&amp;quot; which is obviously wrong. What should I do?&lt;/p&gt;

&lt;p&gt;Specifically I&amp;#39;m looking for ways to properly detect if suffixes and prefixes are indeed one or part of the original word.&lt;/p&gt;

&lt;p&gt;P.S: I&amp;#39;m not working on English docs. Therefore I&amp;#39;m looking for general ideas about these situations not ready to use libraries.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lces4e,True,,fiveMop,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lces4e/how_to_stem_plural_words_properly/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lces4e/how_to_stem_plural_words_properly/,30199,1612442410.0,0,,False,,,,,,823
806,,LanguageTechnology,,t2_6zjod2ey,False,,0,False,Folks'Talks human-computer interaction test 11,[],r/LanguageTechnology,False,6,,0,,False,t3_lc8g6b,False,dark,0.82,,public,7,0,"{'content': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/fl-a-8LEJfU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 267, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': ""Folks'Talks human-computer interaction test 11"", 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 267, 'html': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/fl-a-8LEJfU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Toddler Talk', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/fl-a-8LEJfU/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCrK81rqceTtPBW9nZbszOKA'}}",False,False,,"{'content': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/fl-a-8LEJfU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 267, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/lc8g6b', 'height': 200}",,False,7,,False,False,,False,,[],{},,False,,1612444933.0,text,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lc8g6b,True,,FolksTalksGame,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lc8g6b/folkstalks_humancomputer_interaction_test_11/,all_ads,False,https://youtube.com/watch?v=fl-a-8LEJfU&amp;feature=share,30199,1612416133.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': ""Folks'Talks human-computer interaction test 11"", 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 267, 'html': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/fl-a-8LEJfU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Toddler Talk', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/fl-a-8LEJfU/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCrK81rqceTtPBW9nZbszOKA'}}",False,https://youtube.com/watch?v=fl-a-8LEJfU&amp;feature=share,,,,,0
807,,LanguageTechnology,"So I'm going through [TensorFlows NLP Zero to Hero](https://www.youtube.com/watch?v=fNxaJsNG3-s) video playlist as an introduction to NLP.

The host shows the word LISTEN and talks about how it may be encoded letter by letter with ASCII.

He then proceeds to show the word SILENT and claims that because it contains the same letters and numbers, it is hard for us to understand the sentiment of the word.

Am I stupid or is it easy as hell to read something in order?",t2_829yysxk,False,,0,False,I have a question,[],r/LanguageTechnology,False,6,,0,,False,t3_lbvm4u,False,dark,0.84,,public,13,0,{},,False,[],,False,False,,{},,False,13,,False,False,,False,,[],{},,True,,1612408645.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I&amp;#39;m going through &lt;a href=""https://www.youtube.com/watch?v=fNxaJsNG3-s""&gt;TensorFlows NLP Zero to Hero&lt;/a&gt; video playlist as an introduction to NLP.&lt;/p&gt;

&lt;p&gt;The host shows the word LISTEN and talks about how it may be encoded letter by letter with ASCII.&lt;/p&gt;

&lt;p&gt;He then proceeds to show the word SILENT and claims that because it contains the same letters and numbers, it is hard for us to understand the sentiment of the word.&lt;/p&gt;

&lt;p&gt;Am I stupid or is it easy as hell to read something in order?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lbvm4u,True,,rcadephantom,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lbvm4u/i_have_a_question/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lbvm4u/i_have_a_question/,30199,1612379845.0,0,,False,,,,,,467
808,,LanguageTechnology,"Levenshtein woud be ideal, but it's not computationally feasible to compare hundreds of pages long documents with it. Hamming and other edit-based metrics look good but are also way too time consuming.

Cosine similarity is very fast, but it doesn't take into account the order of appearance of tokens, which matters to me. Same goes for Jaccard and other token-based similarity metrics I've ran across.

Is there a fast algorithm out there which takes into account the order of the sequence?

I don't care about semantic stuff btw, I need to compare texts superficially.",t2_duss2,False,,0,False,"How to get an accurate text similarity score between very large documents (dozens or even hundreds of pages of text), when token order matters?",[],r/LanguageTechnology,False,6,,0,,False,t3_lbl7wk,False,dark,1.0,,public,19,2,{},,False,[],,False,False,,{},,False,19,,False,False,,False,,[],{},,True,,1612378995.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Levenshtein woud be ideal, but it&amp;#39;s not computationally feasible to compare hundreds of pages long documents with it. Hamming and other edit-based metrics look good but are also way too time consuming.&lt;/p&gt;

&lt;p&gt;Cosine similarity is very fast, but it doesn&amp;#39;t take into account the order of appearance of tokens, which matters to me. Same goes for Jaccard and other token-based similarity metrics I&amp;#39;ve ran across.&lt;/p&gt;

&lt;p&gt;Is there a fast algorithm out there which takes into account the order of the sequence?&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t care about semantic stuff btw, I need to compare texts superficially.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lbl7wk,True,,bolaft,,18,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lbl7wk/how_to_get_an_accurate_text_similarity_score/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lbl7wk/how_to_get_an_accurate_text_similarity_score/,30199,1612350195.0,0,,False,,,,,,571
809,,LanguageTechnology,"Hello everyone! 
I’m from the US. Recently graduated with BA in Spanish with a minor in Linguistics (3.75 GPA). I have coursework in Spanish, phonology, phonetics, syntax, language and technology and machine learning (only one class).

I’m looking for Master’s in NLP or CL in Europe that are open to applicants from a non CS background. I have intermediate level knowledge of Python and I am taking Python classes through Coursera. I have already completed a few. Are there any programs that are willing to consider applicants from a strictly a language/linguistics background?

Thank you!",,False,,0,False,Master’s in NLP or CompLing in Europe (non CS background),[],r/LanguageTechnology,False,6,,0,,False,t3_lbg5qv,False,dark,0.94,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,,,False,,,{},,True,,1612357488.0,text,6,,,,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone! 
I’m from the US. Recently graduated with BA in Spanish with a minor in Linguistics (3.75 GPA). I have coursework in Spanish, phonology, phonetics, syntax, language and technology and machine learning (only one class).&lt;/p&gt;

&lt;p&gt;I’m looking for Master’s in NLP or CL in Europe that are open to applicants from a non CS background. I have intermediate level knowledge of Python and I am taking Python classes through Coursera. I have already completed a few. Are there any programs that are willing to consider applicants from a strictly a language/linguistics background?&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lbg5qv,True,,[deleted],,12,True,all_ads,False,[],,dark,/r/LanguageTechnology/comments/lbg5qv/masters_in_nlp_or_compling_in_europe_non_cs/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lbg5qv/masters_in_nlp_or_compling_in_europe_non_cs/,30199,1612328688.0,0,,False,,,,,,590
810,,LanguageTechnology,Just a quick question.... Are wordnet synsets linked to opendata in any way like wikidata or dbpedia ?,t2_5io2qgmw,False,,0,False,Connection between WIKIDATA and WORDNETS,[],r/LanguageTechnology,False,6,,0,,False,t3_lbkz8e,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1612377959.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Just a quick question.... Are wordnet synsets linked to opendata in any way like wikidata or dbpedia ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lbkz8e,True,,AdIntelligent7264,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lbkz8e/connection_between_wikidata_and_wordnets/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lbkz8e/connection_between_wikidata_and_wordnets/,30199,1612349159.0,0,,False,,,,,,102
811,,LanguageTechnology,"Hi there NLP friends,
Im pretty new to python and NLP. Please help point me in the right direction.

Im curious about potential causes to an effect. Casual inference but maybe not just directly cause and effect, and even more like potential causes that may lead to effect.
I would want to be able to type in an effect and get a collection of all the potential causes and vice versa.
As well the negative or nullifying forces that may potentially oppose the effect.
My project needs me to type in a sentence not just a bag of words. And would return phrases and words that meet the criteria.

Eg. Forest Fire.
 Potential causes and correlations: cigarette, spark, negligence, firepit, lightening strike, fireworks, spontaneous combustion of mulch, dry weather, dry plant matter.
Potentially causes: burn down homes, fear, 
Potential opposing elements: rain, firefighters, fire extinguisher, dump water on camp fire, careful, etc.

Preferably trained on a blend of General academic subjects, psychology, physics, phrases and quotes, and large web commons like wiki, google, etc.",t2_4xwr92ro,False,,0,False,Best module for my project? Please help,[],r/LanguageTechnology,False,6,,0,,False,t3_lbrctb,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1612398367.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there NLP friends,
Im pretty new to python and NLP. Please help point me in the right direction.&lt;/p&gt;

&lt;p&gt;Im curious about potential causes to an effect. Casual inference but maybe not just directly cause and effect, and even more like potential causes that may lead to effect.
I would want to be able to type in an effect and get a collection of all the potential causes and vice versa.
As well the negative or nullifying forces that may potentially oppose the effect.
My project needs me to type in a sentence not just a bag of words. And would return phrases and words that meet the criteria.&lt;/p&gt;

&lt;p&gt;Eg. Forest Fire.
 Potential causes and correlations: cigarette, spark, negligence, firepit, lightening strike, fireworks, spontaneous combustion of mulch, dry weather, dry plant matter.
Potentially causes: burn down homes, fear, 
Potential opposing elements: rain, firefighters, fire extinguisher, dump water on camp fire, careful, etc.&lt;/p&gt;

&lt;p&gt;Preferably trained on a blend of General academic subjects, psychology, physics, phrases and quotes, and large web commons like wiki, google, etc.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lbrctb,True,,metal88heart,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lbrctb/best_module_for_my_project_please_help/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lbrctb/best_module_for_my_project_please_help/,30199,1612369567.0,0,,False,,,,,,1076
812,,LanguageTechnology, [https://drive.google.com/file/d/1cWNqnOyowoAr3d\_DJZbdIpKuvlAYu7eu/view?usp=sharing](https://drive.google.com/file/d/1cWNqnOyowoAr3d_DJZbdIpKuvlAYu7eu/view?usp=sharing),t2_6zjod2ey,False,,0,False,Language acquisition by virtual agent (The Folks’Talks game project),[],r/LanguageTechnology,False,6,,0,,False,t3_lbqzvm,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1612397481.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://drive.google.com/file/d/1cWNqnOyowoAr3d_DJZbdIpKuvlAYu7eu/view?usp=sharing""&gt;https://drive.google.com/file/d/1cWNqnOyowoAr3d_DJZbdIpKuvlAYu7eu/view?usp=sharing&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lbqzvm,True,,FolksTalksGame,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lbqzvm/language_acquisition_by_virtual_agent_the/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lbqzvm/language_acquisition_by_virtual_agent_the/,30199,1612368681.0,0,,False,,,,,,170
813,,LanguageTechnology,"Pororo: A Deep Learning based Multilingual Natural Language Processing Library

Hello, we at Kakao Brain made [Pororo](https://github.com/kakaobrain/pororo) open source based Natural Language and Speech Processing library

Pororo is a Python library that implements more than 30 natural language processing models in various languages ​​such as English, Korean, Chinese, and Japanese.

 Even if you don't know anything about artificial intelligence or natural language processing, you can easily perform various tasks such as name entity recognition, machine reading comprehension, machine translation, summarization, and sentiment classification by writing 3 to 4 lines of code.  For example, NER can be performed with the code below.

 If you install it with pip install pororo, you can use it immediately, and we plan to expand models and tasks by language, so please use and feedback a lot.
 More details can be found at https://github.com/kakaobrain/pororo.  Thank you.",t2_3vcebtqk,False,,0,False,Pororo: A Deep Learning based Multilingual Natural Language Processing Library,[],r/LanguageTechnology,False,6,,0,,False,t3_lawow3,False,dark,0.98,,public,28,1,{},,False,[],,False,False,,{},,False,28,,False,False,,1612276597.0,,[],{'gid_1': 1},,True,,1612304939.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Pororo: A Deep Learning based Multilingual Natural Language Processing Library&lt;/p&gt;

&lt;p&gt;Hello, we at Kakao Brain made &lt;a href=""https://github.com/kakaobrain/pororo""&gt;Pororo&lt;/a&gt; open source based Natural Language and Speech Processing library&lt;/p&gt;

&lt;p&gt;Pororo is a Python library that implements more than 30 natural language processing models in various languages ​​such as English, Korean, Chinese, and Japanese.&lt;/p&gt;

&lt;p&gt;Even if you don&amp;#39;t know anything about artificial intelligence or natural language processing, you can easily perform various tasks such as name entity recognition, machine reading comprehension, machine translation, summarization, and sentiment classification by writing 3 to 4 lines of code.  For example, NER can be performed with the code below.&lt;/p&gt;

&lt;p&gt;If you install it with pip install pororo, you can use it immediately, and we plan to expand models and tasks by language, so please use and feedback a lot.
 More details can be found at &lt;a href=""https://github.com/kakaobrain/pororo""&gt;https://github.com/kakaobrain/pororo&lt;/a&gt;.  Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lawow3,True,,huffonism,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lawow3/pororo_a_deep_learning_based_multilingual_natural/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lawow3/pororo_a_deep_learning_based_multilingual_natural/,30199,1612276139.0,0,,False,,,,,,974
814,,LanguageTechnology,"I'm a marketing, college student learning about AI and NLP. I have noticed that when I try to explain NLP to my non-technical friends, they have a hard time understanding. I know this group talks about more technical stuff (and I cannot understand 90% of the conversations), but does anyone have any suggestions on how I can explain NLP to someone who's never heard of NLP?",t2_9ady8b0d,False,,0,False,How do you explain NLP to someone who's never heard of NLP?,[],r/LanguageTechnology,False,6,,0,,False,t3_lb4lg5,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1612324115.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a marketing, college student learning about AI and NLP. I have noticed that when I try to explain NLP to my non-technical friends, they have a hard time understanding. I know this group talks about more technical stuff (and I cannot understand 90% of the conversations), but does anyone have any suggestions on how I can explain NLP to someone who&amp;#39;s never heard of NLP?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lb4lg5,True,,spacestation2021,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lb4lg5/how_do_you_explain_nlp_to_someone_whos_never/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lb4lg5/how_do_you_explain_nlp_to_someone_whos_never/,30199,1612295315.0,0,,False,,,,,,373
815,,LanguageTechnology,"So we are doing a class project, which requires use to use NLP. We have chosen to design a model akin to the Transformer. The faculty evaluating us is a tad inexperienced with language modeling, so we would like to demonstrate the performance gains and competency of our model using a simple task, like article generation, for example.
What other tasks would you recommend, especially those that have comprehensive datasets available?",t2_9vs8qz4o,False,,0,False,What is a good task to demonstrate the power of a new language modeling architecture?,[],r/LanguageTechnology,False,6,,0,,False,t3_lasuz2,False,dark,0.81,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1612291303.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So we are doing a class project, which requires use to use NLP. We have chosen to design a model akin to the Transformer. The faculty evaluating us is a tad inexperienced with language modeling, so we would like to demonstrate the performance gains and competency of our model using a simple task, like article generation, for example.
What other tasks would you recommend, especially those that have comprehensive datasets available?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lasuz2,True,,exwordsmythe,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lasuz2/what_is_a_good_task_to_demonstrate_the_power_of_a/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lasuz2/what_is_a_good_task_to_demonstrate_the_power_of_a/,30199,1612262503.0,0,,False,,,,,,434
816,,LanguageTechnology,"This is a long-context, anonymized, clean, multi-turn and single-turn conversational dataset based on discord data scraped from a large variety of severs, big and small.

The goal is to use this data to pretrain a small conversational model on a big variety of data.

The raw data for this version contained 51,826,268 messages5103788 (regex) + 696161 (toxic)/51826268, or 0.11% of the messages were removed**The dataset's final size is 46,026,319 messages across 456810 conversations**, which is reduced from 33.06 GB of raw json data to 968.87 MB

[https://www.kaggle.com/jef1056/discord-data](https://www.kaggle.com/jef1056/discord-data)",t2_1ehr2on6,False,,0,False,[DATASET] Massive multi-turn conversational dataset based on cleaned discord data,[],r/LanguageTechnology,False,6,,0,,False,t3_lap9r9,False,dark,1.0,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,1612485262.0,,[],{},,True,,1612275649.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is a long-context, anonymized, clean, multi-turn and single-turn conversational dataset based on discord data scraped from a large variety of severs, big and small.&lt;/p&gt;

&lt;p&gt;The goal is to use this data to pretrain a small conversational model on a big variety of data.&lt;/p&gt;

&lt;p&gt;The raw data for this version contained 51,826,268 messages5103788 (regex) + 696161 (toxic)/51826268, or 0.11% of the messages were removed&lt;strong&gt;The dataset&amp;#39;s final size is 46,026,319 messages across 456810 conversations&lt;/strong&gt;, which is reduced from 33.06 GB of raw json data to 968.87 MB&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.kaggle.com/jef1056/discord-data""&gt;https://www.kaggle.com/jef1056/discord-data&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lap9r9,True,,QTE1056,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lap9r9/dataset_massive_multiturn_conversational_dataset/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lap9r9/dataset_massive_multiturn_conversational_dataset/,30199,1612246849.0,0,,False,,,,,,640
817,,LanguageTechnology,"Title explains itself. I’m a sophomore studying linguistics with a concentration in computational linguistics. If you feel so inclined the include salary, I would not mind either!",t2_2lwjk7kg,False,,0,False,"What do you do at work? I’m a student in computational linguistics, wondering what the actuality of the degree is",[],r/LanguageTechnology,False,6,,0,,False,t3_lah4yg,False,dark,1.0,,public,19,0,{},,False,[],,False,False,,{},,False,19,,False,False,,False,,[],{},,True,,1612249912.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Title explains itself. I’m a sophomore studying linguistics with a concentration in computational linguistics. If you feel so inclined the include salary, I would not mind either!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lah4yg,True,,dmoses815,,12,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lah4yg/what_do_you_do_at_work_im_a_student_in/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lah4yg/what_do_you_do_at_work_im_a_student_in/,30199,1612221112.0,0,,False,,,,,,179
818,,LanguageTechnology,We fine-tune the work representations in BERT to learn information from the task but we don't touch the tokenizer. Why don't we learn better subword representations from the task too?,t2_291zpj19,False,,0,False,Why don't we fine-tune BERT tokenizer?,[],r/LanguageTechnology,False,6,,0,,False,t3_lap79p,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1612275379.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We fine-tune the work representations in BERT to learn information from the task but we don&amp;#39;t touch the tokenizer. Why don&amp;#39;t we learn better subword representations from the task too?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lap79p,True,,i_likebrains,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lap79p/why_dont_we_finetune_bert_tokenizer/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lap79p/why_dont_we_finetune_bert_tokenizer/,30199,1612246579.0,0,,False,,,,,,183
819,,LanguageTechnology,"I would like to look into and train a word decompounder for Nordic languages. However, I have never done it, so I am not really familiar with which methods work best. Would anyone have any advice?",t2_gqjhd,False,,0,False,How should I go with training a word decompounder?,[],r/LanguageTechnology,False,6,,0,,False,t3_lau3dm,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1612296423.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I would like to look into and train a word decompounder for Nordic languages. However, I have never done it, so I am not really familiar with which methods work best. Would anyone have any advice?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lau3dm,True,,itsmegeorge,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lau3dm/how_should_i_go_with_training_a_word_decompounder/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lau3dm/how_should_i_go_with_training_a_word_decompounder/,30199,1612267623.0,0,,False,,,,,,196
820,,LanguageTechnology,"I have a problem where I need to classify chunks of text as to whether they pertain to the pharmaceutical industry or not. It will be a supervised learning task but I will need to go through the process of manually labeling the text to come up with the training data.

What are the current state-of-the-art text classifiers and are they feasible to use given my constraints?",t2_43zfw6dw,False,,0,False,Current state of the art for document classification?,[],r/LanguageTechnology,False,6,,0,,False,t3_laliqu,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1612262733.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a problem where I need to classify chunks of text as to whether they pertain to the pharmaceutical industry or not. It will be a supervised learning task but I will need to go through the process of manually labeling the text to come up with the training data.&lt;/p&gt;

&lt;p&gt;What are the current state-of-the-art text classifiers and are they feasible to use given my constraints?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,laliqu,True,,carusGOAT,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/laliqu/current_state_of_the_art_for_document/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/laliqu/current_state_of_the_art_for_document/,30199,1612233933.0,0,,False,,,,,,374
821,,LanguageTechnology,"I have been fortunate enough to be accepted into three schools so far for an MS degree and am waiting on others. Therefore, I have begun searching for internships for this upcoming summer. I have found that most research internships only want Ph.D. students. I have tried cold-emailing, connecting, asking research scientists for a ""coffee chat,"" etc. Unfortunately, I have had no success. Is there any place I should be looking? Is it rare for people in my position not to get internships?

I have almost two years of research experience working and leading NLP research projects. However, I do not have any publications. I am also located in the US if that makes a difference.",t2_gfum0,False,,0,False,Do companies offer NLP Research Internships for the summer before starting grad school?,[],r/LanguageTechnology,False,6,,0,,False,t3_lakttx,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1612260490.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have been fortunate enough to be accepted into three schools so far for an MS degree and am waiting on others. Therefore, I have begun searching for internships for this upcoming summer. I have found that most research internships only want Ph.D. students. I have tried cold-emailing, connecting, asking research scientists for a &amp;quot;coffee chat,&amp;quot; etc. Unfortunately, I have had no success. Is there any place I should be looking? Is it rare for people in my position not to get internships?&lt;/p&gt;

&lt;p&gt;I have almost two years of research experience working and leading NLP research projects. However, I do not have any publications. I am also located in the US if that makes a difference.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lakttx,True,,letsgodevils123,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lakttx/do_companies_offer_nlp_research_internships_for/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/lakttx/do_companies_offer_nlp_research_internships_for/,30199,1612231690.0,0,,False,,,,,,678
822,,LanguageTechnology,"I thought sentence and document embeddings are different. I thought one is one that embeds sentences whereas another is one that embeds more then a sentence such as paragraph or whole document text. However, some research paper got me confusing.

[LASER](https://arxiv.org/pdf/1812.10464.pdf) paper describes it as sentence embeddings whereas [T-LASER](https://arxiv.org/pdf/2008.08567.pdf) metions LASER as document embedding. In LASER paper, text classification experiment is done, as well as multiple text classification experiments are carrying using sentence embeddings. If there would be sentence classification, I wouldn't be confused. But don't text usually means a document and instead would be efficient to use document embeddings instead?",t2_3oyyv5,False,,0,False,Is there any difference between sentence embedding and document embedding?,[],r/LanguageTechnology,False,6,,0,,False,t3_la8yd5,False,dark,0.91,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1612229927.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I thought sentence and document embeddings are different. I thought one is one that embeds sentences whereas another is one that embeds more then a sentence such as paragraph or whole document text. However, some research paper got me confusing.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://arxiv.org/pdf/1812.10464.pdf""&gt;LASER&lt;/a&gt; paper describes it as sentence embeddings whereas &lt;a href=""https://arxiv.org/pdf/2008.08567.pdf""&gt;T-LASER&lt;/a&gt; metions LASER as document embedding. In LASER paper, text classification experiment is done, as well as multiple text classification experiments are carrying using sentence embeddings. If there would be sentence classification, I wouldn&amp;#39;t be confused. But don&amp;#39;t text usually means a document and instead would be efficient to use document embeddings instead?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,la8yd5,True,,manirai91,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/la8yd5/is_there_any_difference_between_sentence/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/la8yd5/is_there_any_difference_between_sentence/,30199,1612201127.0,0,,False,,,,,,749
823,,LanguageTechnology,,t2_cu8vpox,False,,0,False,What is topic modeling?,[],r/LanguageTechnology,False,6,,0,,False,t3_la1es9,False,dark,0.93,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,False,,False,,[],{},,False,,1612209128.0,text,6,,,text,shyambhu20.blogspot.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,la1es9,True,,shyamcody,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/la1es9/what_is_topic_modeling/,all_ads,False,https://shyambhu20.blogspot.com/2021/01/what-is-topic-modeling.html,30199,1612180328.0,0,,False,https://shyambhu20.blogspot.com/2021/01/what-is-topic-modeling.html,,,,,0
824,,LanguageTechnology,,t2_9g160,False,,0,False,What's a good dataset to demonstrate LDA?,[],r/LanguageTechnology,False,6,,0,,False,t3_lahvoj,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,False,,1612251942.0,text,6,,,text,self.textdatamining,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,lahvoj,True,,linklater2012,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/lahvoj/whats_a_good_dataset_to_demonstrate_lda/,all_ads,False,/r/textdatamining/comments/lac4jk/whats_a_good_dataset_to_demonstrate_lda/,30199,1612223142.0,0,,False,/r/textdatamining/comments/lac4jk/whats_a_good_dataset_to_demonstrate_lda/,"[{'approved_at_utc': None, 'subreddit': 'textdatamining', 'selftext': 'I need something that can help get the point across while running in decent time in a Colab notebook. Any recommendations?', 'author_fullname': 't2_9g160', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ""What's a good dataset to demonstrate LDA?"", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/textdatamining', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_lac4jk', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1612237489.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.textdatamining', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I need something that can help get the point across while running in decent time in a Colab notebook. Any recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_31xmi', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'lac4jk', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'linklater2012', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/textdatamining/comments/lac4jk/whats_a_good_dataset_to_demonstrate_lda/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/textdatamining/comments/lac4jk/whats_a_good_dataset_to_demonstrate_lda/', 'subreddit_subscribers': 4543, 'created_utc': 1612208689.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_lac4jk,,,0
825,,LanguageTechnology,I would be really interested if anyone a list of links/reading resources in this area as it of particular interest for a project i am working on.,t2_1zkgq,False,,0,False,What is the latest research on NLU and decision making?,[],r/LanguageTechnology,False,6,,0,,False,t3_la1b4m,False,dark,0.91,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1612208713.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I would be really interested if anyone a list of links/reading resources in this area as it of particular interest for a project i am working on.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,la1b4m,True,,salkhan,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/la1b4m/what_is_the_latest_research_on_nlu_and_decision/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/la1b4m/what_is_the_latest_research_on_nlu_and_decision/,30199,1612179913.0,0,,False,,,,,,145
826,,LanguageTechnology,"Hi, I have been reading that the state of the art for summarizing, and probably other NLP tasks are done with supervised training, with huge data sets and complex architectures like transformers with pre training. 

That is awesome, but I don't have the GPU power nor access to the datasets to explore that part. Also, I'm OK achieving acceptable results for a start.

That's why I was thinking on unsupervised ways to summarize.

Could you recommend what are the state of the art techniques to do it? Where should I start to dig in?

Thanks!

**Edit for clarification**: I have already read about the abstractive and extractive approaches. What I would like to learn is how to do unsupervised abstractive summarization for a generic text corpora, and the concepts behind this task.",t2_5m0mxfui,False,,0,False,How can I summarize text with an unsupervised technique these days?,[],r/LanguageTechnology,False,6,,0,,False,t3_la9bz6,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,1612217095.0,,[],{},,True,,1612230813.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I have been reading that the state of the art for summarizing, and probably other NLP tasks are done with supervised training, with huge data sets and complex architectures like transformers with pre training. &lt;/p&gt;

&lt;p&gt;That is awesome, but I don&amp;#39;t have the GPU power nor access to the datasets to explore that part. Also, I&amp;#39;m OK achieving acceptable results for a start.&lt;/p&gt;

&lt;p&gt;That&amp;#39;s why I was thinking on unsupervised ways to summarize.&lt;/p&gt;

&lt;p&gt;Could you recommend what are the state of the art techniques to do it? Where should I start to dig in?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Edit for clarification&lt;/strong&gt;: I have already read about the abstractive and extractive approaches. What I would like to learn is how to do unsupervised abstractive summarization for a generic text corpora, and the concepts behind this task.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,la9bz6,True,,iblysa,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/la9bz6/how_can_i_summarize_text_with_an_unsupervised/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/la9bz6/how_can_i_summarize_text_with_an_unsupervised/,30199,1612202013.0,0,,False,,,,,,782
827,,LanguageTechnology,,t2_ucmad,False,,0,False,How To Profit From Alexa Skill Development Using Node JS,[],r/LanguageTechnology,False,6,,0,,False,t3_la4zbo,False,dark,0.5,,public,0,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/m35DBGM2ncE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'How To Profit From Alexa Skill Development Using Node JS', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/m35DBGM2ncE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Refactored', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/m35DBGM2ncE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCiO0K2xt5irIN6x13FNzYTg'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/m35DBGM2ncE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/la4zbo', 'height': 200}",,False,0,,False,False,,False,,[],{},,False,,1612220468.0,text,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,la4zbo,True,,tim_macgyver,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/la4zbo/how_to_profit_from_alexa_skill_development_using/,all_ads,False,https://www.youtube.com/watch?v=m35DBGM2ncE,30199,1612191668.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'How To Profit From Alexa Skill Development Using Node JS', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/m35DBGM2ncE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Refactored', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/m35DBGM2ncE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCiO0K2xt5irIN6x13FNzYTg'}}",False,https://www.youtube.com/watch?v=m35DBGM2ncE,,,,,0
828,,LanguageTechnology,,t2_3p2y3dl,False,,0,False,Do distributed semantic word embeddings such as word2vec have a theoretical foundation in linguistics?,[],r/LanguageTechnology,False,6,,0,,False,t3_l9qz90,False,dark,1.0,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,False,,False,,[],{},,False,,1612170425.0,text,6,,,text,self.linguistics,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l9qz90,True,,actualsnek,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l9qz90/do_distributed_semantic_word_embeddings_such_as/,all_ads,False,/r/linguistics/comments/l9q6eg/do_distributed_semantic_word_embeddings_such_as/,30199,1612141625.0,0,,False,/r/linguistics/comments/l9q6eg/do_distributed_semantic_word_embeddings_such_as/,"[{'approved_at_utc': None, 'subreddit': 'linguistics', 'selftext': ""My understanding is that they evolved out of the desire to model n-grams or word sequences as evidenced by much of the background presented in [this seminal paper](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf). \n\nResearchers eventually realized that these distributed representations which are trained on the context of words turn out to be pretty good proxies for the actual meaning of a word. My question is, has there been any formal research from the linguistics/semantics side into whether these are valid models of word meaning?\n\nI know that many computational linguists still use WordNet and other semantic graphs for modeling meaning, but vectors are much more apt for machine learning which is why you don't see those graphs catching on in modern statistical NLP. If there isn't any formal semantics research into word embeddings, why does modeling the representation of a word based on its context turn out to be so good at predicting its meaning?\n\nWhen I really think about it, it's still kind of crazy to me that 'king' - 'man' + 'woman' = 'queen', when each of their vectors are just based on their respective contexts in a huge corpus of English text. \n\nBonus points for answers involving Derrida :)"", 'author_fullname': 't2_3p2y3dl', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Do distributed semantic word embeddings such as word2vec have a theoretical foundation in linguistics?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/linguistics', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_l9q6eg', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 15, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 15, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1612167964.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.linguistics', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My understanding is that they evolved out of the desire to model n-grams or word sequences as evidenced by much of the background presented in &lt;a href=""https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf""&gt;this seminal paper&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;Researchers eventually realized that these distributed representations which are trained on the context of words turn out to be pretty good proxies for the actual meaning of a word. My question is, has there been any formal research from the linguistics/semantics side into whether these are valid models of word meaning?&lt;/p&gt;\n\n&lt;p&gt;I know that many computational linguists still use WordNet and other semantic graphs for modeling meaning, but vectors are much more apt for machine learning which is why you don&amp;#39;t see those graphs catching on in modern statistical NLP. If there isn&amp;#39;t any formal semantics research into word embeddings, why does modeling the representation of a word based on its context turn out to be so good at predicting its meaning?&lt;/p&gt;\n\n&lt;p&gt;When I really think about it, it&amp;#39;s still kind of crazy to me that &amp;#39;king&amp;#39; - &amp;#39;man&amp;#39; + &amp;#39;woman&amp;#39; = &amp;#39;queen&amp;#39;, when each of their vectors are just based on their respective contexts in a huge corpus of English text. &lt;/p&gt;\n\n&lt;p&gt;Bonus points for answers involving Derrida :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2qhos', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'l9q6eg', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'actualsnek', 'discussion_type': None, 'num_comments': 20, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/linguistics/comments/l9q6eg/do_distributed_semantic_word_embeddings_such_as/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/linguistics/comments/l9q6eg/do_distributed_semantic_word_embeddings_such_as/', 'subreddit_subscribers': 249863, 'created_utc': 1612139164.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_l9q6eg,,,0
829,,LanguageTechnology,"Aside from the traditional inverted-index searches, and nearest-neighbor searches (with document/sentence embeddings), I mean.

I am having trouble mixing the two as well, because the ML models fail when it comes to proper nouns, I need an inverted-index search. Sometimes the nearest-neighbor searchs are all over the place too, with tangentially related sentences, and it's hard to adjust a similarity threshold.

Furthermore, I'd like to know how align (highlight) related parts of the text to the query (like Google does). I suspect this is done with expanded queries on inverted-index search, because it would be impractical to store embeddings for each word/wordpiece.

If that's the case, then how am I supposed to expand the searches?",t2_33ls9eif,False,,0,False,Exactly *how* is the new technology supposed to help improving search engines?,[],r/LanguageTechnology,False,6,,0,,False,t3_la3hbs,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1612216284.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Aside from the traditional inverted-index searches, and nearest-neighbor searches (with document/sentence embeddings), I mean.&lt;/p&gt;

&lt;p&gt;I am having trouble mixing the two as well, because the ML models fail when it comes to proper nouns, I need an inverted-index search. Sometimes the nearest-neighbor searchs are all over the place too, with tangentially related sentences, and it&amp;#39;s hard to adjust a similarity threshold.&lt;/p&gt;

&lt;p&gt;Furthermore, I&amp;#39;d like to know how align (highlight) related parts of the text to the query (like Google does). I suspect this is done with expanded queries on inverted-index search, because it would be impractical to store embeddings for each word/wordpiece.&lt;/p&gt;

&lt;p&gt;If that&amp;#39;s the case, then how am I supposed to expand the searches?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,la3hbs,True,,SagaciousRaven,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/la3hbs/exactly_how_is_the_new_technology_supposed_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/la3hbs/exactly_how_is_the_new_technology_supposed_to/,30199,1612187484.0,0,,False,,,,,,742
830,,LanguageTechnology,"This paper extends BERT for doing long document classification in nlp. They propose BERT variations RoBERT and ToBERT as hierarchical enchancements for the same. They obtained a significant improvement over the baseline models.

Paper Walkthrough: https://youtu.be/3IOl5d9PZeM

Paper Link: https://arxiv.org/abs/1910.10781",t2_hkv9s,False,,0,False,Hierarchical Transformers for Long Document Classification (Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_l9cylk,False,dark,1.0,,public,21,0,{},,False,[],,False,False,,{},,False,21,,False,False,,False,,[],{},,True,,1612130907.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This paper extends BERT for doing long document classification in nlp. They propose BERT variations RoBERT and ToBERT as hierarchical enchancements for the same. They obtained a significant improvement over the baseline models.&lt;/p&gt;

&lt;p&gt;Paper Walkthrough: &lt;a href=""https://youtu.be/3IOl5d9PZeM""&gt;https://youtu.be/3IOl5d9PZeM&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Paper Link: &lt;a href=""https://arxiv.org/abs/1910.10781""&gt;https://arxiv.org/abs/1910.10781&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l9cylk,True,,prakhar21,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l9cylk/hierarchical_transformers_for_long_document/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l9cylk/hierarchical_transformers_for_long_document/,30199,1612102107.0,0,,False,,,,,,322
831,,LanguageTechnology,"If I have a set of synthetic text/articles generated by language models, what metrics can I use to measure the text's quality, in terms of how coherent the context is and how close to human writing? Thank you!",t2_11f1xg,False,,0,False,Metrics to analyze the quality of generated text?,[],r/LanguageTechnology,False,6,,0,,False,t3_l9w7du,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1612187598.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If I have a set of synthetic text/articles generated by language models, what metrics can I use to measure the text&amp;#39;s quality, in terms of how coherent the context is and how close to human writing? Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l9w7du,True,,nancywhr,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l9w7du/metrics_to_analyze_the_quality_of_generated_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l9w7du/metrics_to_analyze_the_quality_of_generated_text/,30199,1612158798.0,0,,False,,,,,,209
832,,LanguageTechnology,,t2_5fsp2x6v,False,,0,False,TriggerNER: Learning with Entity Triggers as Explanations | Research Papers Summary 006,[],r/LanguageTechnology,False,6,,0,,False,t3_l9jdrb,False,dark,0.86,,public,5,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/AyvOOeFP2i4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'TriggerNER: Learning with Entity Triggers as Explanations | Research Papers Summary 006', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/AyvOOeFP2i4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/AyvOOeFP2i4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/AyvOOeFP2i4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/l9jdrb', 'height': 200}",,False,5,,False,False,,False,,[],{},,False,,1612148948.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l9jdrb,True,,RyanAI100,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l9jdrb/triggerner_learning_with_entity_triggers_as/,all_ads,False,https://youtu.be/AyvOOeFP2i4,30199,1612120148.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'TriggerNER: Learning with Entity Triggers as Explanations | Research Papers Summary 006', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/AyvOOeFP2i4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/AyvOOeFP2i4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,https://youtu.be/AyvOOeFP2i4,,,,,0
833,,LanguageTechnology,"In the pytorch official tutorial for language translation ([https://pytorch.org/tutorials/beginner/torchtext\_translation\_tutorial.html](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html)), tokenizer in torchtext is used. Parts of code are

    from torchtext.data.utils import get_tokenizer
    de_tokenizer = get_tokenizer('spacy', language='de')
    en_tokenizer = get_tokenizer('spacy', language='en')

However, I have difficulties in downloading and installing spacy package. Are there other substitutes for the tokenizer? Thanks.",t2_55wji4cd,False,,0,False,substitute for tokenizer in torchtext,[],r/LanguageTechnology,False,6,,0,,False,t3_l9cd9g,False,dark,0.8,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1612128803.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In the pytorch official tutorial for language translation (&lt;a href=""https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html""&gt;https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html&lt;/a&gt;), tokenizer in torchtext is used. Parts of code are&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from torchtext.data.utils import get_tokenizer
de_tokenizer = get_tokenizer(&amp;#39;spacy&amp;#39;, language=&amp;#39;de&amp;#39;)
en_tokenizer = get_tokenizer(&amp;#39;spacy&amp;#39;, language=&amp;#39;en&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, I have difficulties in downloading and installing spacy package. Are there other substitutes for the tokenizer? Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l9cd9g,True,,AndyLeeeeee,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l9cd9g/substitute_for_tokenizer_in_torchtext/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l9cd9g/substitute_for_tokenizer_in_torchtext/,30199,1612100003.0,0,,False,,,,,,561
834,,LanguageTechnology,,t2_12peby,False,,0,False,"[P] An interactive history of natural language processing, starting a long time ago",[],r/LanguageTechnology,False,6,,0,,False,t3_l8so1a,False,dark,1.0,,public,42,0,{},,False,[],,False,False,,{},,False,42,,False,False,,False,,[],{},,False,,1612061628.0,text,6,,,text,nlphistory.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l8so1a,True,,flotothemoon,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l8so1a/p_an_interactive_history_of_natural_language/,all_ads,False,https://www.nlphistory.com,30199,1612032828.0,0,,False,https://www.nlphistory.com,,,,,0
835,,LanguageTechnology,"Hi all, I finished my master in nlp two years ago (worked on summarization) and I'm thinking of starting a PhD program.
The thing is, I'm not sure what topics I should work on. 

What I know is that I'd like to work on applicable topics, so nothing too theoretical. Also, leaderboard climbing using Bert in different ways is not that interesting imo.

So I'm basically asking what nlp domains you find interesting and you think should be great research topics.
Thanks!!",t2_a2xd38rm,False,,0,False,Research topics,[],r/LanguageTechnology,False,6,,0,,False,t3_l8wlpr,False,dark,0.88,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1612072068.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, I finished my master in nlp two years ago (worked on summarization) and I&amp;#39;m thinking of starting a PhD program.
The thing is, I&amp;#39;m not sure what topics I should work on. &lt;/p&gt;

&lt;p&gt;What I know is that I&amp;#39;d like to work on applicable topics, so nothing too theoretical. Also, leaderboard climbing using Bert in different ways is not that interesting imo.&lt;/p&gt;

&lt;p&gt;So I&amp;#39;m basically asking what nlp domains you find interesting and you think should be great research topics.
Thanks!!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l8wlpr,True,,vissinibass,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l8wlpr/research_topics/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l8wlpr/research_topics/,30199,1612043268.0,0,,False,,,,,,469
836,,LanguageTechnology,"Hello everyone, hope you doing well. I just wanted share the discord server for the people who search for learning partners. You can join server to find a partner for learning different programming languages or any topics you are interested in.
Here is the link for the server:

https://discord.gg/ayeGrsaSG2",t2_82w9styv,False,,0,False,Partner Up for Learning,[],r/LanguageTechnology,False,6,,0,,False,t3_l8zkyl,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1612080548.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone, hope you doing well. I just wanted share the discord server for the people who search for learning partners. You can join server to find a partner for learning different programming languages or any topics you are interested in.
Here is the link for the server:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://discord.gg/ayeGrsaSG2""&gt;https://discord.gg/ayeGrsaSG2&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l8zkyl,True,,heisenbug403,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l8zkyl/partner_up_for_learning/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l8zkyl/partner_up_for_learning/,30199,1612051748.0,0,,False,,,,,,308
837,,LanguageTechnology,"I am new to ML and starting off with what I think is an easy project. My first foray into ML was to predict the number of syllables in a word given its pronunciation, or phonetic transcription. That was pretty straightforward using graph convolutional networks to solve a classification problem. Having succeeded at that, my next goal is to predict the the location of syllable breaks in a word. I have written code to enumerate all possible combinations of *N* syllables for a word, but I don't know which is correct. The core of my problem seems to me to be how to accurately break a graph into subgraphs. This is where I'm running up against my own ignorance about ML approaches. How would I go about doing that? My first thought was link prediction, but I'm open to other ideas. Thanks.",t2_gr2i2,False,,0,False,How to break a word into syllables?,[],r/LanguageTechnology,False,6,,0,,False,t3_l8p62a,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1612052716.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am new to ML and starting off with what I think is an easy project. My first foray into ML was to predict the number of syllables in a word given its pronunciation, or phonetic transcription. That was pretty straightforward using graph convolutional networks to solve a classification problem. Having succeeded at that, my next goal is to predict the the location of syllable breaks in a word. I have written code to enumerate all possible combinations of &lt;em&gt;N&lt;/em&gt; syllables for a word, but I don&amp;#39;t know which is correct. The core of my problem seems to me to be how to accurately break a graph into subgraphs. This is where I&amp;#39;m running up against my own ignorance about ML approaches. How would I go about doing that? My first thought was link prediction, but I&amp;#39;m open to other ideas. Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l8p62a,True,,ghostoftheuniverse,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l8p62a/how_to_break_a_word_into_syllables/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l8p62a/how_to_break_a_word_into_syllables/,30199,1612023916.0,0,,False,,,,,,790
838,,LanguageTechnology,"I am trying to implement the explanation for attention in this video using pytorch (https://www.youtube.com/watch?v=yGTUuEx3GkA) . The main idea can be seen on the slide at 12:38

Focusing on just the word ""bank"":

The idea is to get glove embeddings for the words ""Bank of a river"", and then take a dot product of the word embedding of bank with itself and every other word, to get a tensor of 4 weights. Then normalize the weights and use them to get a new word embedding for the word ""bank"" which is now the sum of all the word embeddings scaled by their corresponding normalized weights.

Before, I tried to find the cosine similarity between the words ""bank"" and ""river"" and got 0.33 and after the reweighing I got 0.59 i.e it worked.

I also checked that the similarity between ""water"" and ""bank"" was 0.49 and became 0.57 with the new embeddings.

BUT, the similarity between ""money"" and ""bank"" was 0.57 and before and became 0.64 after! So the ""bank"" vector came closet to ""money"" after I provided context.

So while what I did to the original ""bank"" vector got it closer to things like ""water"" (which is what I was trying to do), it didn't get away from ""money"" as I was expecting.

Why is the change happening in only one direction?

You can have a look at the code and the outputs here https://github.com/VishakBharadwaj94/transformers/blob/master/transformers_tmp.ipynb",t2_14x7uo,False,,0,False,using self attention on Word embeddings improved context in only one direction,[],r/LanguageTechnology,False,6,,0,,False,t3_l8e57t,False,dark,0.92,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1612011008.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to implement the explanation for attention in this video using pytorch (&lt;a href=""https://www.youtube.com/watch?v=yGTUuEx3GkA""&gt;https://www.youtube.com/watch?v=yGTUuEx3GkA&lt;/a&gt;) . The main idea can be seen on the slide at 12:38&lt;/p&gt;

&lt;p&gt;Focusing on just the word &amp;quot;bank&amp;quot;:&lt;/p&gt;

&lt;p&gt;The idea is to get glove embeddings for the words &amp;quot;Bank of a river&amp;quot;, and then take a dot product of the word embedding of bank with itself and every other word, to get a tensor of 4 weights. Then normalize the weights and use them to get a new word embedding for the word &amp;quot;bank&amp;quot; which is now the sum of all the word embeddings scaled by their corresponding normalized weights.&lt;/p&gt;

&lt;p&gt;Before, I tried to find the cosine similarity between the words &amp;quot;bank&amp;quot; and &amp;quot;river&amp;quot; and got 0.33 and after the reweighing I got 0.59 i.e it worked.&lt;/p&gt;

&lt;p&gt;I also checked that the similarity between &amp;quot;water&amp;quot; and &amp;quot;bank&amp;quot; was 0.49 and became 0.57 with the new embeddings.&lt;/p&gt;

&lt;p&gt;BUT, the similarity between &amp;quot;money&amp;quot; and &amp;quot;bank&amp;quot; was 0.57 and before and became 0.64 after! So the &amp;quot;bank&amp;quot; vector came closet to &amp;quot;money&amp;quot; after I provided context.&lt;/p&gt;

&lt;p&gt;So while what I did to the original &amp;quot;bank&amp;quot; vector got it closer to things like &amp;quot;water&amp;quot; (which is what I was trying to do), it didn&amp;#39;t get away from &amp;quot;money&amp;quot; as I was expecting.&lt;/p&gt;

&lt;p&gt;Why is the change happening in only one direction?&lt;/p&gt;

&lt;p&gt;You can have a look at the code and the outputs here &lt;a href=""https://github.com/VishakBharadwaj94/transformers/blob/master/transformers_tmp.ipynb""&gt;https://github.com/VishakBharadwaj94/transformers/blob/master/transformers_tmp.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l8e57t,True,,Integral_humanist,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l8e57t/using_self_attention_on_word_embeddings_improved/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l8e57t/using_self_attention_on_word_embeddings_improved/,30199,1611982208.0,0,,False,,,,,,1380
839,,LanguageTechnology,"I want to build a question answering model that take the context (the reading paragraphs), the question and 4 choices as inputs and output the best answer for that question given that context. I’m new to NLP but willing to learn more so any recommendations will help a lot. Thanks",t2_10b2sjxx,False,,0,False,Which model should I use to pick the best answer for the TOEIC reading test?,[],r/LanguageTechnology,False,6,,0,,False,t3_l8pfvl,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1612053403.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to build a question answering model that take the context (the reading paragraphs), the question and 4 choices as inputs and output the best answer for that question given that context. I’m new to NLP but willing to learn more so any recommendations will help a lot. Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l8pfvl,True,,SEMClovesYOU,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l8pfvl/which_model_should_i_use_to_pick_the_best_answer/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l8pfvl/which_model_should_i_use_to_pick_the_best_answer/,30199,1612024603.0,0,,False,,,,,,280
840,,LanguageTechnology,,t2_84htu,False,,0,False,Rebuilding the spellchecker: Hunspell and the order of edits,[],r/LanguageTechnology,False,6,,0,,False,t3_l7yz1s,False,dark,0.93,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,False,,1611971323.0,text,6,,,text,zverok.github.io,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l7yz1s,True,,zverok_kha,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l7yz1s/rebuilding_the_spellchecker_hunspell_and_the/,all_ads,False,https://zverok.github.io/blog/2021-01-28-spellchecker-5.html,30199,1611942523.0,0,,False,https://zverok.github.io/blog/2021-01-28-spellchecker-5.html,,,,,0
841,,LanguageTechnology,"Last year, we set out to create a master entity resolution tool. We wanted to identify and resolve multiple occurrences of a single entity to get a clearer picture of the information within data... but in a practical and scaleable way.

Our blog post, **Entity Resolution for Master Data Management** explains the why and how behind er², ThinkData's entity resolution tool.

If you're interested, check it out here: [https://blog.thinkdataworks.com/entity-resolution-for-master-data-management](https://blog.thinkdataworks.com/entity-resolution-for-master-data-management)",t2_48mdxbb9,False,,0,False,Entity Resolution for Master Data Management,[],r/LanguageTechnology,False,6,,0,,False,t3_l85gyu,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1611986202.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Last year, we set out to create a master entity resolution tool. We wanted to identify and resolve multiple occurrences of a single entity to get a clearer picture of the information within data... but in a practical and scaleable way.&lt;/p&gt;

&lt;p&gt;Our blog post, &lt;strong&gt;Entity Resolution for Master Data Management&lt;/strong&gt; explains the why and how behind er², ThinkData&amp;#39;s entity resolution tool.&lt;/p&gt;

&lt;p&gt;If you&amp;#39;re interested, check it out here: &lt;a href=""https://blog.thinkdataworks.com/entity-resolution-for-master-data-management""&gt;https://blog.thinkdataworks.com/entity-resolution-for-master-data-management&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l85gyu,True,,ThinkDataWorks,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l85gyu/entity_resolution_for_master_data_management/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l85gyu/entity_resolution_for_master_data_management/,30199,1611957402.0,0,,False,,,,,,572
842,,LanguageTechnology,,t2_odgsxsj,False,,0,False,An unexpected use case of word embeddings,[],r/LanguageTechnology,False,6,,0,,False,t3_l7zu86,False,dark,0.67,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,False,,1611973235.0,text,6,,,text,paulbricman.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l7zu86,True,,paubric,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l7zu86/an_unexpected_use_case_of_word_embeddings/,all_ads,False,https://paulbricman.com/docs/tools/semantica/,30199,1611944435.0,0,,False,https://paulbricman.com/docs/tools/semantica/,"[{'approved_at_utc': None, 'subreddit': 'AugmentedMind', 'selftext': '', 'author_fullname': 't2_odgsxsj', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Semantica: Extending Conceptual Thinking Through Semantic Embeddings', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/AugmentedMind', 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_l7zrbl', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Project', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1611973052.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'paulbricman.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://paulbricman.com/docs/tools/semantica/', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '9fe4ba28-5902-11eb-a87c-0e798300c8ef', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_3qob4p', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#cc8b00', 'id': 'l7zrbl', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'paubric', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/AugmentedMind/comments/l7zrbl/semantica_extending_conceptual_thinking_through/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://paulbricman.com/docs/tools/semantica/', 'subreddit_subscribers': 177, 'created_utc': 1611944252.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_l7zrbl,,,0
843,,LanguageTechnology,,t2_2n3nfxca,False,,0,False,"Which are top APIs for Indian languages mainly VR, OCR, Speech - Text - Speech?",[],r/LanguageTechnology,False,6,,0,,False,t3_l7ufe9,False,dark,0.67,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1611960848.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l7ufe9,True,,vijaykurhade,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l7ufe9/which_are_top_apis_for_indian_languages_mainly_vr/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l7ufe9/which_are_top_apis_for_indian_languages_mainly_vr/,30199,1611932048.0,0,,False,,,,,,0
844,,LanguageTechnology,"Say I have a two layers of labels with a label and sublabel:

    Fruit.Apple
    Fruit.Orange
    Fruit.Banana

    Vegetable:Cucumber
    Vegetable:Squash
    Vegetable:Cabbage

    Meat:Cow
    Meat:Chicken
    Meat:Fish

Then suppose I build a dataset of ""meals"" which are combinations of the above labels:

    [Vegetable:Squash,Meat:Cow]
    [Fruit.Apple,Fruit.Orange]
    [Meat:Fish,Fruit.Orange]

Really each ""meal"" is a document with a category and subcategory. If I were to train a bert classifier for guessing the ingredients in a given meal, would it be better to:

* train 1 classifier with 9 labels
* or build a classier with 3 labels (fruits, vegetables,meats) then build 3 additional classifier to sort the finer sublabels (fruits -&gt; (apple, orange, banana), vegetable -&gt;(cucumber, squash, cabbage), etc)

Now I'd imagine if you had a large enough database, the first would probably be fine. But if you had say 20 labels each with 10 sublabels, then the second option starts to make more sense.",t2_hj9oy,False,,0,False,Chaining BERT multilabel classifiers.,[],r/LanguageTechnology,False,6,,0,,False,t3_l7hm0b,False,dark,1.0,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,1611897318.0,,[],{},,True,,1611919013.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Say I have a two layers of labels with a label and sublabel:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Fruit.Apple
Fruit.Orange
Fruit.Banana

Vegetable:Cucumber
Vegetable:Squash
Vegetable:Cabbage

Meat:Cow
Meat:Chicken
Meat:Fish
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then suppose I build a dataset of &amp;quot;meals&amp;quot; which are combinations of the above labels:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Vegetable:Squash,Meat:Cow]
[Fruit.Apple,Fruit.Orange]
[Meat:Fish,Fruit.Orange]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Really each &amp;quot;meal&amp;quot; is a document with a category and subcategory. If I were to train a bert classifier for guessing the ingredients in a given meal, would it be better to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;train 1 classifier with 9 labels&lt;/li&gt;
&lt;li&gt;or build a classier with 3 labels (fruits, vegetables,meats) then build 3 additional classifier to sort the finer sublabels (fruits -&amp;gt; (apple, orange, banana), vegetable -&amp;gt;(cucumber, squash, cabbage), etc)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now I&amp;#39;d imagine if you had a large enough database, the first would probably be fine. But if you had say 20 labels each with 10 sublabels, then the second option starts to make more sense.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l7hm0b,True,,spidermonkey12345,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l7hm0b/chaining_bert_multilabel_classifiers/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l7hm0b/chaining_bert_multilabel_classifiers/,30199,1611890213.0,0,,False,,,,,,1016
845,,LanguageTechnology,"Hey everybody,

I'm new to NLP and looking into training tensorflow using sales conversations, i.e. e-mail conversations. The problem of course is getting data... I have not been able to come up with a good possible source yet. Any ideas are appreciated :-)",t2_4qn5j,False,,0,False,Sales Conversations Datasets,[],r/LanguageTechnology,False,6,,0,,False,t3_l7b50r,False,dark,0.72,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1611902216.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everybody,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m new to NLP and looking into training tensorflow using sales conversations, i.e. e-mail conversations. The problem of course is getting data... I have not been able to come up with a good possible source yet. Any ideas are appreciated :-)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l7b50r,True,,absolem,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l7b50r/sales_conversations_datasets/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l7b50r/sales_conversations_datasets/,30199,1611873416.0,0,,False,,,,,,257
846,,LanguageTechnology,"I've written a library for doing forensic stylometry (identifying who wrote an unknown text)

pip install faststylometry

Here's a tutorial [https://freelancedatascientist.net/fast-stylometry-tutorial/](https://freelancedatascientist.net/fast-stylometry-tutorial/)

And here's the library [https://pypi.org/project/faststylometry](https://pypi.org/project/faststylometry)

There are a few cool things I'd like to try with it, such as checking if it could identify Miles Taylor as the staffer who publicly criticised Trump's administration",t2_o0ro6,False,,0,False,Stylometry library in Python,[],r/LanguageTechnology,False,6,,0,,False,t3_l6ud6c,False,dark,0.93,,public,23,0,{},,False,[],,False,False,,{},,False,23,,False,False,,False,,[],{},,True,,1611861774.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve written a library for doing forensic stylometry (identifying who wrote an unknown text)&lt;/p&gt;

&lt;p&gt;pip install faststylometry&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s a tutorial &lt;a href=""https://freelancedatascientist.net/fast-stylometry-tutorial/""&gt;https://freelancedatascientist.net/fast-stylometry-tutorial/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And here&amp;#39;s the library &lt;a href=""https://pypi.org/project/faststylometry""&gt;https://pypi.org/project/faststylometry&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;There are a few cool things I&amp;#39;d like to try with it, such as checking if it could identify Miles Taylor as the staffer who publicly criticised Trump&amp;#39;s administration&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l6ud6c,True,,niujin,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l6ud6c/stylometry_library_in_python/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l6ud6c/stylometry_library_in_python/,30199,1611832974.0,0,,False,,,,,,538
847,,LanguageTechnology,"Title is the question. I'm working on a relation extraction research project and had a shower thought of whether models that are good at summarization (although this expression is also highly controversial) are also good at extracting relationships between two entities.

Intuitively, it makes sense. However, if I think about the details making the transfer between two tasks seems very difficult (e.g., how to make the transition between structured to unstructured output spaces).

I'd appreciate it if anyone would let me know if they know anything. Thanks!",t2_m8kccne,False,,0,False,Has anyone seen work investigating the relationship between neural text summarization and relation extraction?,[],r/LanguageTechnology,False,6,,0,,False,t3_l6vlqw,False,dark,1.0,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1611866348.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Title is the question. I&amp;#39;m working on a relation extraction research project and had a shower thought of whether models that are good at summarization (although this expression is also highly controversial) are also good at extracting relationships between two entities.&lt;/p&gt;

&lt;p&gt;Intuitively, it makes sense. However, if I think about the details making the transfer between two tasks seems very difficult (e.g., how to make the transition between structured to unstructured output spaces).&lt;/p&gt;

&lt;p&gt;I&amp;#39;d appreciate it if anyone would let me know if they know anything. Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l6vlqw,True,,Seankala,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l6vlqw/has_anyone_seen_work_investigating_the/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l6vlqw/has_anyone_seen_work_investigating_the/,30199,1611837548.0,0,,False,,,,,,560
848,,LanguageTechnology,,t2_2p20wymg,False,,0,False,Towards Automated Fact-Checking,[],r/LanguageTechnology,False,6,,0,,False,t3_l6i4xa,False,dark,0.95,,public,31,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Wy2E9Bg8hxk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Towards automated fact checking with Andreas Vlachos', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Wy2E9Bg8hxk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'de Dicto', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Wy2E9Bg8hxk/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCTr2nEASDDfgCA1xjkB7x9g'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Wy2E9Bg8hxk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/l6i4xa', 'height': 200}",,False,31,,False,False,,False,,[],{},,False,,1611820271.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l6i4xa,True,,linguistInAPoncho,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l6i4xa/towards_automated_factchecking/,all_ads,False,https://www.youtube.com/watch?v=Wy2E9Bg8hxk,30199,1611791471.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Towards automated fact checking with Andreas Vlachos', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Wy2E9Bg8hxk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'de Dicto', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Wy2E9Bg8hxk/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCTr2nEASDDfgCA1xjkB7x9g'}}",False,https://www.youtube.com/watch?v=Wy2E9Bg8hxk,,,,,0
849,,LanguageTechnology,,t2_bcevk,False,,0,False,"Hi all! For my side project, I made an AI-based program that predicts what a user will search next for an Online Dictionary. Here is a short article that I wrote about the project. Any thoughts or feedback are greatly appreciate. Thank you!",[],r/LanguageTechnology,False,6,,0,,False,t3_l6unfh,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1611862886.0,text,6,,,text,aiplusinfo.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l6unfh,True,,sharewithme,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l6unfh/hi_all_for_my_side_project_i_made_an_aibased/,all_ads,False,https://www.aiplusinfo.com/blog/ai-search-prediction-for-online-dictionaries/,30199,1611834086.0,0,,False,https://www.aiplusinfo.com/blog/ai-search-prediction-for-online-dictionaries/,,,,,0
850,,LanguageTechnology,"Hi r/LanguageTechnology readers!

We have created a [labelling tool](https://humanlambdas.com/solutions/data-labelling) that can be customized to display all sorts of data models and tasks. Here are a couple of [examples](https://www.humanlambdas.com/templates/nlp-news-article-annotation) of [types](https://www.humanlambdas.com/templates/sec-filing-data-extraction) of complex tasks one can set up.

I hope some of you will find this useful, and if you have any thoughts I would love to hear your feedback!",t2_bk9jh,False,,0,False,Tool for Complex Data Labelling Tasks,[],r/LanguageTechnology,False,6,,0,,False,t3_l6tbrp,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1611857639.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi &lt;a href=""/r/LanguageTechnology""&gt;r/LanguageTechnology&lt;/a&gt; readers!&lt;/p&gt;

&lt;p&gt;We have created a &lt;a href=""https://humanlambdas.com/solutions/data-labelling""&gt;labelling tool&lt;/a&gt; that can be customized to display all sorts of data models and tasks. Here are a couple of &lt;a href=""https://www.humanlambdas.com/templates/nlp-news-article-annotation""&gt;examples&lt;/a&gt; of &lt;a href=""https://www.humanlambdas.com/templates/sec-filing-data-extraction""&gt;types&lt;/a&gt; of complex tasks one can set up.&lt;/p&gt;

&lt;p&gt;I hope some of you will find this useful, and if you have any thoughts I would love to hear your feedback!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l6tbrp,True,,bernatfp,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l6tbrp/tool_for_complex_data_labelling_tasks/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l6tbrp/tool_for_complex_data_labelling_tasks/,30199,1611828839.0,0,,False,,,,,,508
851,,LanguageTechnology,"RECCON is a dataset for causal reasoning of emotions in conversations. It has subtasks of textual entailment aka Natural Language Inference (NLI) and QA.  


**Abstract.** Recognizing the cause behind emotions in text is a fundamental yet under-explored area of research in NLP. Advances in this area hold the potential to improve interpretability and performance in affect-based models. Identifying emotion causes at the utterance level in conversations is particularly challenging due to the intermingling dynamic among the interlocutors. To this end, we introduce the task of recognizing emotion cause in conversations with an accompanying dataset named RECCON. Furthermore, we define different cause types based on the source of the causes and establish strong transformer-based baselines to address two different sub-tasks of RECCON: 1) Causal Span Extraction and 2) Causal Emotion Entailment. The dataset is available at [https://github.com/declare-lab/RECCON](https://github.com/declare-lab/RECCON).

**Paper:** [**https://arxiv.org/pdf/2012.11820.pdf**](https://arxiv.org/pdf/2012.11820.pdf)

**Code:** [**https://github.com/declare-lab/RECCON**](https://github.com/declare-lab/RECCON)

**PwC:** [**https://paperswithcode.com/paper/recognizing-emotion-cause-in-conversations**](https://paperswithcode.com/paper/recognizing-emotion-cause-in-conversations)",t2_75zqs5pq,False,,0,False,[R] RECCON: A Dataset for Recognizing Emotion Cause in Conversations,[],r/LanguageTechnology,False,6,,0,,False,t3_l6o73d,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1611837734.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;RECCON is a dataset for causal reasoning of emotions in conversations. It has subtasks of textual entailment aka Natural Language Inference (NLI) and QA.  &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstract.&lt;/strong&gt; Recognizing the cause behind emotions in text is a fundamental yet under-explored area of research in NLP. Advances in this area hold the potential to improve interpretability and performance in affect-based models. Identifying emotion causes at the utterance level in conversations is particularly challenging due to the intermingling dynamic among the interlocutors. To this end, we introduce the task of recognizing emotion cause in conversations with an accompanying dataset named RECCON. Furthermore, we define different cause types based on the source of the causes and establish strong transformer-based baselines to address two different sub-tasks of RECCON: 1) Causal Span Extraction and 2) Causal Emotion Entailment. The dataset is available at &lt;a href=""https://github.com/declare-lab/RECCON""&gt;https://github.com/declare-lab/RECCON&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt; &lt;a href=""https://arxiv.org/pdf/2012.11820.pdf""&gt;&lt;strong&gt;https://arxiv.org/pdf/2012.11820.pdf&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt; &lt;a href=""https://github.com/declare-lab/RECCON""&gt;&lt;strong&gt;https://github.com/declare-lab/RECCON&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PwC:&lt;/strong&gt; &lt;a href=""https://paperswithcode.com/paper/recognizing-emotion-cause-in-conversations""&gt;&lt;strong&gt;https://paperswithcode.com/paper/recognizing-emotion-cause-in-conversations&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l6o73d,True,,Snoo-88012,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l6o73d/r_reccon_a_dataset_for_recognizing_emotion_cause/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l6o73d/r_reccon_a_dataset_for_recognizing_emotion_cause/,30199,1611808934.0,0,,False,,,,,,1362
852,,LanguageTechnology,,t2_9fiq3zbb,False,,0,False,Does anyone have any experience in Japanese text classification? Or is there anyone who can help me with that? I’ll be really thankful!,[],r/LanguageTechnology,False,6,,0,,False,t3_l6rjgs,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1611850294.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l6rjgs,True,,Busyreadingg,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l6rjgs/does_anyone_have_any_experience_in_japanese_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l6rjgs/does_anyone_have_any_experience_in_japanese_text/,30199,1611821494.0,0,,False,,,,,,0
853,,LanguageTechnology,"basically we want to help users find similar questions

I've been using https://github.com/hanxiao/bert-as-service#building-a-qa-semantic-search-engine-in-3-minutes but am not very happy with the result. 

Are there any low hanging fruits for it?


Why am I not so happy?

Given this input:
`Someone got stuck in the middle of a donation`


We received these matches:
```
18.805347 How the fiscal sponsorship works
18.803146 What fees are involved in running a campaign on MyApp?
18.800673 Do I have to provide contribution levels in my campaign?
18.682882 Someone could not donate, what is wrong?
```

well it found the match on position 4, but this is a suuuuper small example size to match again, we only had a total of 14 possible questions that it could match. So not very impressed. Are there any better easy approaches?",t2_2sx0nrzp,False,,0,False,Match questions to similar questions?,[],r/LanguageTechnology,False,6,,0,,False,t3_l68q6u,False,dark,0.83,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1611795005.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;basically we want to help users find similar questions&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been using &lt;a href=""https://github.com/hanxiao/bert-as-service#building-a-qa-semantic-search-engine-in-3-minutes""&gt;https://github.com/hanxiao/bert-as-service#building-a-qa-semantic-search-engine-in-3-minutes&lt;/a&gt; but am not very happy with the result. &lt;/p&gt;

&lt;p&gt;Are there any low hanging fruits for it?&lt;/p&gt;

&lt;p&gt;Why am I not so happy?&lt;/p&gt;

&lt;p&gt;Given this input:
&lt;code&gt;Someone got stuck in the middle of a donation&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We received these matches:
&lt;code&gt;
18.805347 How the fiscal sponsorship works
18.803146 What fees are involved in running a campaign on MyApp?
18.800673 Do I have to provide contribution levels in my campaign?
18.682882 Someone could not donate, what is wrong?
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;well it found the match on position 4, but this is a suuuuper small example size to match again, we only had a total of 14 possible questions that it could match. So not very impressed. Are there any better easy approaches?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l68q6u,True,,Iossi_84,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l68q6u/match_questions_to_similar_questions/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l68q6u/match_questions_to_similar_questions/,30199,1611766205.0,0,,False,,,,,,826
854,,LanguageTechnology,"Hi there!

For videos on YouTube or podcasts on other platforms, I want to find all the brand(s) that are sponsoring that video or that podcast. Right now we are using fuzzy queries on description and transcript texts. But that does not cover all the cases. 

Often content creators use terms like “this video is sponsored/brought to you by {brand name}” but not always. So we want to use nlp. 

Please suggest any approach/idea/library or any direction to further investigation. Any help is appreciated.",t2_7q8isez1,False,,0,False,Find sponsors from transcripts (yt/podcast),[],r/LanguageTechnology,False,6,,0,,False,t3_l6dmtx,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,1611817388.0,,[],{},,True,,1611808127.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there!&lt;/p&gt;

&lt;p&gt;For videos on YouTube or podcasts on other platforms, I want to find all the brand(s) that are sponsoring that video or that podcast. Right now we are using fuzzy queries on description and transcript texts. But that does not cover all the cases. &lt;/p&gt;

&lt;p&gt;Often content creators use terms like “this video is sponsored/brought to you by {brand name}” but not always. So we want to use nlp. &lt;/p&gt;

&lt;p&gt;Please suggest any approach/idea/library or any direction to further investigation. Any help is appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l6dmtx,True,,CaptainOld90,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l6dmtx/find_sponsors_from_transcripts_ytpodcast/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l6dmtx/find_sponsors_from_transcripts_ytpodcast/,30199,1611779327.0,0,,False,,,,,,504
855,,LanguageTechnology,"For example we use a float value to represent whether Alice is helping Bob:

* In **Alice gives Bob $100**, Alice is helping Bob; (+0.5)
* In **Alice gives Bob a slap**, Bob is hurt by Alice; (-0.5)
* In **Alice gives Bob a slap to keep him from falling sleep in the snow**, Though Bob is hurt, Alice is saving Bob's life so it's still good for him; (+0.7)

So, my idea is to make a corpus based on [FrameNet](https://framenet.icsi.berkeley.edu/fndrupal/WhatIsFrameNet), giving *beneficial* value for frame elements. (Assuming my input is already in the frame data structure) 

Is there any better idea or existing solution for this problem?",t2_h9s1s,False,,0,False,Is there a way to analyze a sentence to calculate whether A is beneficial to B?,[],r/LanguageTechnology,False,6,,0,,False,t3_l5xlvd,False,dark,0.94,,public,13,0,{},,False,[],,False,False,,{},,False,13,,False,False,,False,,[],{},,True,,1611755254.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For example we use a float value to represent whether Alice is helping Bob:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In &lt;strong&gt;Alice gives Bob $100&lt;/strong&gt;, Alice is helping Bob; (+0.5)&lt;/li&gt;
&lt;li&gt;In &lt;strong&gt;Alice gives Bob a slap&lt;/strong&gt;, Bob is hurt by Alice; (-0.5)&lt;/li&gt;
&lt;li&gt;In &lt;strong&gt;Alice gives Bob a slap to keep him from falling sleep in the snow&lt;/strong&gt;, Though Bob is hurt, Alice is saving Bob&amp;#39;s life so it&amp;#39;s still good for him; (+0.7)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, my idea is to make a corpus based on &lt;a href=""https://framenet.icsi.berkeley.edu/fndrupal/WhatIsFrameNet""&gt;FrameNet&lt;/a&gt;, giving &lt;em&gt;beneficial&lt;/em&gt; value for frame elements. (Assuming my input is already in the frame data structure) &lt;/p&gt;

&lt;p&gt;Is there any better idea or existing solution for this problem?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l5xlvd,True,,tmpxyz,,9,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/l5xlvd/is_there_a_way_to_analyze_a_sentence_to_calculate/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l5xlvd/is_there_a_way_to_analyze_a_sentence_to_calculate/,30199,1611726454.0,0,,False,,,,,,641
856,,LanguageTechnology,"I’m working on enhancing low resource translation using cross lingual signals from related languages and I was wondering if it’s somehow possible to train a decoder stack in a seq2seq task using just monolingual data. For example if I have A -&gt; B translation with A having good monolingual data but little or no parallel data with B thus is low resource, my current methodology uses a monolingual embedding space of A to first encode the input sentence, then uses a cross lingual map to map this encoded sentence into a pivot language, and lastly I use a joint cross lingual map of pivot and B to decode the sentence. Now I’m think of employing this method is B -&gt; A direction. The issue is, it’s not possible to learn a joint cross lingual map between A and B (spaces are not isomorphic) so I was thinking of learning a decoder stack for A using monolingual data available. I can clarify more if needed! Apologies if the questions is a bit uninformed haha",t2_13s4mru,False,,0,False,Is it possible to train a decoder using monolingual data only?,[],r/LanguageTechnology,False,6,,0,,False,t3_l64huo,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1611782985.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m working on enhancing low resource translation using cross lingual signals from related languages and I was wondering if it’s somehow possible to train a decoder stack in a seq2seq task using just monolingual data. For example if I have A -&amp;gt; B translation with A having good monolingual data but little or no parallel data with B thus is low resource, my current methodology uses a monolingual embedding space of A to first encode the input sentence, then uses a cross lingual map to map this encoded sentence into a pivot language, and lastly I use a joint cross lingual map of pivot and B to decode the sentence. Now I’m think of employing this method is B -&amp;gt; A direction. The issue is, it’s not possible to learn a joint cross lingual map between A and B (spaces are not isomorphic) so I was thinking of learning a decoder stack for A using monolingual data available. I can clarify more if needed! Apologies if the questions is a bit uninformed haha&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l64huo,True,,hayis4horses1,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l64huo/is_it_possible_to_train_a_decoder_using/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l64huo/is_it_possible_to_train_a_decoder_using/,30199,1611754185.0,1,,False,,,,,,962
857,,LanguageTechnology,"Hello people, so I’m experimenting with an RNN language model and normal back-propagation appears to be performing better than bptt even when it’s been truncated to only look back two or five steps (we’ve tested both). Can anyone give me some intuition about why this might be going on? Does BPTT require more training data to perform better? I would have thought the truncation would mean that vanishing gradients shouldn’t be a problem. If anyone knows any articles or blogs I could read to try and understand this better it would be really useful :)",t2_y1zunf6,False,,0,False,Backprop through Time vs Normal for RNN language model,[],r/LanguageTechnology,False,6,,0,,False,t3_l624ej,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1611773825.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello people, so I’m experimenting with an RNN language model and normal back-propagation appears to be performing better than bptt even when it’s been truncated to only look back two or five steps (we’ve tested both). Can anyone give me some intuition about why this might be going on? Does BPTT require more training data to perform better? I would have thought the truncation would mean that vanishing gradients shouldn’t be a problem. If anyone knows any articles or blogs I could read to try and understand this better it would be really useful :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l624ej,True,,idkwhatever1337,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l624ej/backprop_through_time_vs_normal_for_rnn_language/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l624ej/backprop_through_time_vs_normal_for_rnn_language/,30199,1611745025.0,0,,False,,,,,,552
858,,LanguageTechnology,"Hi all,

I can't seem to form an intuition for the following settings : I would like to perform a LDA on a group of text but some document are many, many time longer than others. 

The average is at 80 tokens, median 40 but there are document reaching up to 6000 tokens. 

My strategy was to discard document with too few words (lower than the 25% quantile ) as they are unlikely to satisfy the assumption that ""documents are a mixture of topics"". 

But I'm not sure how to deal with the outlier with too many words. Would the LDA be robust enough to deal with the imbalance? Or should I split large documents into smaller chunks and randomly select one chunk.

Thanks in advance.",t2_85xh7nac,False,,0,False,Unbalanced document length &amp; topic modeling (lda) [help],[],r/LanguageTechnology,False,6,,0,,False,t3_l68ab8,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1611793912.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I can&amp;#39;t seem to form an intuition for the following settings : I would like to perform a LDA on a group of text but some document are many, many time longer than others. &lt;/p&gt;

&lt;p&gt;The average is at 80 tokens, median 40 but there are document reaching up to 6000 tokens. &lt;/p&gt;

&lt;p&gt;My strategy was to discard document with too few words (lower than the 25% quantile ) as they are unlikely to satisfy the assumption that &amp;quot;documents are a mixture of topics&amp;quot;. &lt;/p&gt;

&lt;p&gt;But I&amp;#39;m not sure how to deal with the outlier with too many words. Would the LDA be robust enough to deal with the imbalance? Or should I split large documents into smaller chunks and randomly select one chunk.&lt;/p&gt;

&lt;p&gt;Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l68ab8,True,,pastels_sounds,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l68ab8/unbalanced_document_length_topic_modeling_lda_help/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l68ab8/unbalanced_document_length_topic_modeling_lda_help/,30199,1611765112.0,2,,False,,,,,,680
859,,LanguageTechnology,"Dear Reddit-Fam

Can anyone tell me whether the NLP Specialization track on coursera is a good starting point for someone who is studying Data Science (Master's) and want's to go deeper into NLP?

Greeets",t2_7cejly7g,False,,0,False,NLP Specialization course on coursera worth it?,[],r/LanguageTechnology,False,6,,0,,False,t3_l5fodl,False,dark,0.93,,public,18,0,{},,False,[],,False,False,,{},,False,18,,False,False,,False,,[],{},,True,,1611702019.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Dear Reddit-Fam&lt;/p&gt;

&lt;p&gt;Can anyone tell me whether the NLP Specialization track on coursera is a good starting point for someone who is studying Data Science (Master&amp;#39;s) and want&amp;#39;s to go deeper into NLP?&lt;/p&gt;

&lt;p&gt;Greeets&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l5fodl,True,,oxygeneticai,,14,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l5fodl/nlp_specialization_course_on_coursera_worth_it/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l5fodl/nlp_specialization_course_on_coursera_worth_it/,30199,1611673219.0,0,,False,,,,,,204
860,,LanguageTechnology,"I'm trying to figure out how to fine-tune the largest version of T5 for a language generation task, but I can't seem to find any notebooks or resources to do so. Any great places to start?",t2_8xraz5pp,False,,0,False,Fine-Tune 11B T5 for Generation Task,[],r/LanguageTechnology,False,6,,0,,False,t3_l5u8mz,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1611743586.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to figure out how to fine-tune the largest version of T5 for a language generation task, but I can&amp;#39;t seem to find any notebooks or resources to do so. Any great places to start?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l5u8mz,True,,Effective_Sea_9367,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l5u8mz/finetune_11b_t5_for_generation_task/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l5u8mz/finetune_11b_t5_for_generation_task/,30199,1611714786.0,0,,False,,,,,,188
861,,LanguageTechnology,Can someone point me in the direction of any literature using NLP to understand why a neural network makes the choices it does? Like e.g. a chess bot which can explain why it made the move it did. Humans use natural language to communicate our beliefs and reasons: surely there's a way to make our models do the same.,t2_4c4ybmje,False,,0,False,NLP for Interpretability,[],r/LanguageTechnology,False,6,,0,,False,t3_l58u0h,False,dark,1.0,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,True,,1611674538.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can someone point me in the direction of any literature using NLP to understand why a neural network makes the choices it does? Like e.g. a chess bot which can explain why it made the move it did. Humans use natural language to communicate our beliefs and reasons: surely there&amp;#39;s a way to make our models do the same.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l58u0h,True,,Adolphins,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l58u0h/nlp_for_interpretability/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l58u0h/nlp_for_interpretability/,30199,1611645738.0,0,,False,,,,,,317
862,,LanguageTechnology,"[UBIAI](https://ubiai.tools) was born out of frustration with existing solutions, which either have a low quality/price ratio or are expensive and geared towards large companies. We know that data labeling is the bottleneck for creating custom NLP models (NER, entity relations, classification, etc) and is here to stay. We wanted to create the most accessible, easy-to-use and automated solution at an affordable price.

If you are launching a new NLP project, please explore our [tool](https://ubiai.tools) (we offer free 14 day trial) and give us your feedback at [admin@ubiai.tools](mailto:admin@ubiai.tools)

Here are few blog links:

* [Introducing UBIAI](https://chatbotslife.com/introducing-ubiai-easy-to-use-text-annotation-for-nlp-applications-74a2401fa725?sk=cace5030af4ca42ad4fda0eff4f2a229)
* [Building Job Entity Recognizer using Amazon Comprehend](https://medium.com/swlh/building-a-job-entity-recognizer-using-amazon-comprehend-5dd2c33faa82?sk=e34060de1a9a5b9da6a5bfefc0bff7d4)",t2_32tnavmg,False,,0,False,Announcing UBIAI: Easy-to-Use Text Annotation Tool,[],r/LanguageTechnology,False,6,,0,,False,t3_l5jr10,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1611713451.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://ubiai.tools""&gt;UBIAI&lt;/a&gt; was born out of frustration with existing solutions, which either have a low quality/price ratio or are expensive and geared towards large companies. We know that data labeling is the bottleneck for creating custom NLP models (NER, entity relations, classification, etc) and is here to stay. We wanted to create the most accessible, easy-to-use and automated solution at an affordable price.&lt;/p&gt;

&lt;p&gt;If you are launching a new NLP project, please explore our &lt;a href=""https://ubiai.tools""&gt;tool&lt;/a&gt; (we offer free 14 day trial) and give us your feedback at [&lt;a href=""mailto:admin@ubiai.tools""&gt;admin@ubiai.tools&lt;/a&gt;](mailto:&lt;a href=""mailto:admin@ubiai.tools""&gt;admin@ubiai.tools&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Here are few blog links:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://chatbotslife.com/introducing-ubiai-easy-to-use-text-annotation-for-nlp-applications-74a2401fa725?sk=cace5030af4ca42ad4fda0eff4f2a229""&gt;Introducing UBIAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://medium.com/swlh/building-a-job-entity-recognizer-using-amazon-comprehend-5dd2c33faa82?sk=e34060de1a9a5b9da6a5bfefc0bff7d4""&gt;Building Job Entity Recognizer using Amazon Comprehend&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l5jr10,True,,UBIAI,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l5jr10/announcing_ubiai_easytouse_text_annotation_tool/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l5jr10/announcing_ubiai_easytouse_text_annotation_tool/,30199,1611684651.0,0,,False,,,,,,993
863,,LanguageTechnology,"In this post, we will look at different techniques you can use to better understand how well a language model captures the contextual relationship between words. We will do this by: 

1. Looking at the dataset we need to train these models to see if we can come up with a simple one that helps us visualize how these models “learn” the relationship between different words.
1. Looking at the tools and techniques you can use to track the progress of these models and monitor the results while they process our simplified dataset.
1. After that you should hopefully be able to re-use that template for more complex models with some real life datasets.

[understanding word embeddings](https://neptune.ai/blog/word-embeddings-deep-dive-into-custom-datasets?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=blog-word-embeddings-deep-dive-into-custom-datasets&amp;utm_content=languagetechnology)",t2_5hfacnnv,False,,0,False,"[Tutorial] Training, Visualizing, and Understanding Word Embeddings",[],r/LanguageTechnology,False,6,,0,,False,t3_l5i5ph,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1611709144.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In this post, we will look at different techniques you can use to better understand how well a language model captures the contextual relationship between words. We will do this by: &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Looking at the dataset we need to train these models to see if we can come up with a simple one that helps us visualize how these models “learn” the relationship between different words.&lt;/li&gt;
&lt;li&gt;Looking at the tools and techniques you can use to track the progress of these models and monitor the results while they process our simplified dataset.&lt;/li&gt;
&lt;li&gt;After that you should hopefully be able to re-use that template for more complex models with some real life datasets.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=""https://neptune.ai/blog/word-embeddings-deep-dive-into-custom-datasets?utm_source=reddit&amp;amp;utm_medium=post&amp;amp;utm_campaign=blog-word-embeddings-deep-dive-into-custom-datasets&amp;amp;utm_content=languagetechnology""&gt;understanding word embeddings&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l5i5ph,True,,kk_ai,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l5i5ph/tutorial_training_visualizing_and_understanding/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l5i5ph/tutorial_training_visualizing_and_understanding/,30199,1611680344.0,0,,False,,,,,,897
864,,LanguageTechnology,,t2_mdyw3,False,,0,False,What is the most accurate sentiment recognition open source code?,[],r/LanguageTechnology,False,6,,0,,False,t3_l5fack,False,dark,0.6,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1611700810.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l5fack,True,,F1jk,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l5fack/what_is_the_most_accurate_sentiment_recognition/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l5fack/what_is_the_most_accurate_sentiment_recognition/,30199,1611672010.0,0,,False,,,,,,0
865,,LanguageTechnology,,t2_mdyw3,False,,0,False,What is the best summarisation open source code?,[],r/LanguageTechnology,False,6,,0,,False,t3_l5f16m,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1611700013.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l5f16m,True,,F1jk,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l5f16m/what_is_the_best_summarisation_open_source_code/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l5f16m/what_is_the_best_summarisation_open_source_code/,30199,1611671213.0,0,,False,,,,,,0
866,,LanguageTechnology,"Has anybody ever done work on this? Or does anybody know of papers/projects that have done this?

With the new personal data availability created by European Union and California laws, I am currently working on a personal project to unify all of my digital conversation history and data into a personal corpus of everything I've said.

I've come across a number of projects that work with Facebook Messenger, whatsapp, WeChat, etc., but all the projects that I have found only work at an individual app level, and almost all of them are limited to shallow linguistic analysis. I want to perform various types of analysis on my data (mostly for novelty, with some practical application in language learning)

While this is not something everyone would be interested in, and in fact, many privacy minded people would be very opposed to doing this, I think this would be very useful to computational linguists and enthusiasts for analyzing data at a level of intimacy that far overextends what you would get from an academic level.

I'll do this project regardless eventually, but I was just wondering if there is previous work or an open source project to contribute to so I don't have to reinvent the wheel.",t2_14e9i6,False,,0,False,Building a personal corpus based on digital conversation history,[],r/LanguageTechnology,False,6,,0,,False,t3_l535jb,False,dark,0.88,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1611654045.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Has anybody ever done work on this? Or does anybody know of papers/projects that have done this?&lt;/p&gt;

&lt;p&gt;With the new personal data availability created by European Union and California laws, I am currently working on a personal project to unify all of my digital conversation history and data into a personal corpus of everything I&amp;#39;ve said.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve come across a number of projects that work with Facebook Messenger, whatsapp, WeChat, etc., but all the projects that I have found only work at an individual app level, and almost all of them are limited to shallow linguistic analysis. I want to perform various types of analysis on my data (mostly for novelty, with some practical application in language learning)&lt;/p&gt;

&lt;p&gt;While this is not something everyone would be interested in, and in fact, many privacy minded people would be very opposed to doing this, I think this would be very useful to computational linguists and enthusiasts for analyzing data at a level of intimacy that far overextends what you would get from an academic level.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ll do this project regardless eventually, but I was just wondering if there is previous work or an open source project to contribute to so I don&amp;#39;t have to reinvent the wheel.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l535jb,True,,Fuehnix,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l535jb/building_a_personal_corpus_based_on_digital/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l535jb/building_a_personal_corpus_based_on_digital/,30199,1611625245.0,0,,False,,,,,,1206
867,,LanguageTechnology,"I decided to create a fun playground Colab, where you can dabble with CLIP!
https://colab.research.google.com/drive/1ePZiVXnINfJNTHB6T_qsNlbo8fFWDsk_?usp=sharing 

Upload your own pictures, choose silly captions, and find out which ones CLIP thinks are most likely. I was already impressed, but do test it yourself :wink:

Please share any funny results you might encounter! :nerd_face:",t2_2x2g38k8,False,,0,False,Demo colab for openAI's CLIP,[],r/LanguageTechnology,False,6,,0,,False,t3_l59lrc,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1611677865.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I decided to create a fun playground Colab, where you can dabble with CLIP!
&lt;a href=""https://colab.research.google.com/drive/1ePZiVXnINfJNTHB6T_qsNlbo8fFWDsk_?usp=sharing""&gt;https://colab.research.google.com/drive/1ePZiVXnINfJNTHB6T_qsNlbo8fFWDsk_?usp=sharing&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Upload your own pictures, choose silly captions, and find out which ones CLIP thinks are most likely. I was already impressed, but do test it yourself :wink:&lt;/p&gt;

&lt;p&gt;Please share any funny results you might encounter! :nerd_face:&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l59lrc,True,,gurdovonlendogam,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l59lrc/demo_colab_for_openais_clip/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l59lrc/demo_colab_for_openais_clip/,30199,1611649065.0,0,,False,,,,,,386
868,,LanguageTechnology,,t2_5alofkdd,False,,0,False,"Allen Institute launches GENIE, a leaderboard for human-in-the-loop language model benchmarking",[],r/LanguageTechnology,False,6,,0,,False,t3_l4r3zw,False,dark,1.0,,public,14,0,{},,False,[],,False,False,,{},,False,14,,False,False,,False,,[],{},,False,,1611620277.0,text,6,,,text,self.MachineLearning,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l4r3zw,True,,Wiskkey,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l4r3zw/allen_institute_launches_genie_a_leaderboard_for/,all_ads,False,/r/MachineLearning/comments/l44ofg/rallen_institute_launches_genie_a_leaderboard_for/,30199,1611591477.0,0,,False,/r/MachineLearning/comments/l44ofg/rallen_institute_launches_genie_a_leaderboard_for/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': '[GENIE, A new leaderboard for human-in-the-loop evaluation of generated text, open to the community.](https://genie.apps.allenai.org/)', 'author_fullname': 't2_q61j9fl', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[R]Allen Institute launches GENIE, a leaderboard for human-in-the-loop language model benchmarking', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'three', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_l44ofg', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.94, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 14, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Research', 'can_mod_post': False, 'score': 14, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1611541049.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://genie.apps.allenai.org/""&gt;GENIE, A new leaderboard for human-in-the-loop evaluation of generated text, open to the community.&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'l44ofg', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'RichyScrapDad99', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/l44ofg/rallen_institute_launches_genie_a_leaderboard_for/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/l44ofg/rallen_institute_launches_genie_a_leaderboard_for/', 'subreddit_subscribers': 1930725, 'created_utc': 1611512249.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_l44ofg,,,0
869,,LanguageTechnology,"[ERNIE-M](http://arxiv.org/abs/2012.15674) is a new multilingual model that understands 96 languages and tops [XTREME](https://arxiv.org/abs/2003.11080), a substantial multilingual multi-task benchmark proposed by Google, Carnegie Mellon University, and DeepMind. The novel pre-training method can learn semantic alignment across multiple languages on monolingual corpora.

Paper: [https://arxiv.org/abs/2012.15674](https://arxiv.org/abs/2012.15674)

Blog: [http://research.baidu.com/Blog/index-view?id=151](http://research.baidu.com/Blog/index-view?id=151)",t2_920u77v,False,,0,False,"ERNIE-M: Multilingual Model Learns 96 Languages from Monolingual Corpora, Tops Google’s XTREME Benchmark",[],r/LanguageTechnology,False,6,,0,,False,t3_l4vicp,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1611632070.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""http://arxiv.org/abs/2012.15674""&gt;ERNIE-M&lt;/a&gt; is a new multilingual model that understands 96 languages and tops &lt;a href=""https://arxiv.org/abs/2003.11080""&gt;XTREME&lt;/a&gt;, a substantial multilingual multi-task benchmark proposed by Google, Carnegie Mellon University, and DeepMind. The novel pre-training method can learn semantic alignment across multiple languages on monolingual corpora.&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/abs/2012.15674""&gt;https://arxiv.org/abs/2012.15674&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Blog: &lt;a href=""http://research.baidu.com/Blog/index-view?id=151""&gt;http://research.baidu.com/Blog/index-view?id=151&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l4vicp,True,,trcytony,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l4vicp/erniem_multilingual_model_learns_96_languages/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l4vicp/erniem_multilingual_model_learns_96_languages/,30199,1611603270.0,0,,False,,,,,,557
870,,LanguageTechnology,"
# 720+ new  NLP models, 300+ supported languages, translation, summarization, question answering and more with T5 and Marian models!  - John Snow Labs NLU 1.1.0

##  NLU 1.1.0 Release Notes

We are incredibly excited to release NLU 1.1.0!
This release integrates the 720+ new models from the latest [Spark-NLP 2.7.0 + releases](https://github.com/JohnSnowLabs/spark-nlp/releases)
You can now achieve state-of-the-art results with Sequence2Sequence transformers on problems like text summarization, question answering, translation between  192+ languages, and extract Named Entity in various Right to Left written languages like  Arabic, Persian, Urdu, and languages that require segmentation like Koreas, Japanese, Chinese, and many more in 1 line of code!     
These new features are possible because of the integration of the [Google's T5 models](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) and [Microsoft's Marian models](https://marian-nmt.github.io/publications/)  transformers.

NLU 1.1.0 has over 720+ new pretrained models and pipelines while extending the support of multi-lingual models to 192+ languages such as Chinese, Japanese, Korean, Arabic, Persian, Urdu, and Hebrew.


In addition to this, NLU 1.1.0 comes with 9 new notebooks showcasing training classifiers for various review and sentiment datasets and 7 notebooks for the new features and models.


### NLU 1.1.0  New Features
* **720+** new models you can find an overview of all NLU models [here](https://nlu.johnsnowlabs.com/docs/en/namespace) and further documentation in the [models hub](https://nlp.johnsnowlabs.com/models)
* **NEW:** Introducing MarianTransformer annotator for machine translation based on MarianNMT models. Marian is an efficient, free Neural Machine Translation framework mainly being developed by the Microsoft Translator team (646+ pretrained models &amp; pipelines in 192+ languages)
* **NEW:** Introducing T5Transformer annotator for Text-To-Text Transfer Transformer (Google T5) models to achieve state-of-the-art results on multiple NLP tasks such as Translation, Summarization, Question Answering, Sentence Similarity, and so on
* **NEW:** Introducing brand new and refactored language detection and identification models. The new LanguageDetectorDL is faster, more accurate, and supports up to 375 languages
* **NEW:** Introducing WordSegmenter model for word segmentation of languages without any rule-based tokenization such as Chinese, Japanese, or Korean
* **NEW:** Introducing DocumentNormalizer component for cleaning content from HTML or XML documents, applying either data cleansing using an arbitrary number of custom regular expressions either data extraction following the different parameters

## Translation
[Translation example](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/translation_demo.ipynb)       
You can translate between more than 192 Languages pairs with the [Marian Models](https://marian-nmt.github.io/publications/)
You need to specify the language your data is in as `start_language` and the language you want to translate to as `target_language`.    
The language references must be [ISO language codes](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)

`nlu.load('&lt;start_language&gt;.translate.&lt;target_language&gt;')`

**Translate English to French :**     
```
nlu.load('en.translate_to.fr').predict(""Hello from John Snow Labs"")
&gt;&gt;&gt; Output: Bonjour des laboratoires de neige de John!	 

```
**Translate English to Inukitut :**     
```
nlu.load('en.translate_to.lu').predict(""Hello from John Snow Labs"")
&gt;&gt;&gt; Output: kalunganyembo ka mashika makamankate 
```
**Translate English to Hungarian :**
```
nlu.load('en.translate_to.hu').predict(""Hello from John Snow Labs"")
&gt;&gt;&gt; Output: Helló John hó laborjából.
```
**Translate English to German :**
```
nlu.load('en.translate_to.de').predict(""Hello from John Snow Labs!"")
&gt;&gt;&gt; Output: Hallo aus John Schnee Labors 
```


```python
translate_pipe = nlu.load('en.translate_to.de')
df = translate_pipe.predict('Billy likes to go to the mall every sunday')
df
```

|	sentence|	translation|
|-----------|--------------|
|Billy likes to go to the mall every sunday	| Billy geht gerne jeden Sonntag ins Einkaufszentrum|






## T5
[Example of every T5 task](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more)
### Overview of every task available with T5
[The T5 model](https://arxiv.org/pdf/1910.10683.pdf) is trained on various datasets for 17 different tasks which fall into 8 categories.


1. Text summarization
2. Question answering
3. Translation
4. Sentiment analysis
5. Natural Language inference
6. Coreference resolution
7. Sentence Completion
8. Word sense disambiguation

### Every T5 Task with explanation:

|Task Name | Explanation | 
|----------|--------------|
|[1.CoLA](https://nyu-mll.github.io/CoLA/)                   | Classify if a sentence is gramaticaly correct|
|[2.RTE](https://dl.acm.org/doi/10.1007/11736790_9)                    | Classify whether if a statement can be deducted from a sentence|
|[3.MNLI](https://arxiv.org/abs/1704.05426)                   | Classify for a hypothesis and premise whether they contradict or contradict each other or neither of both (3 class).|
|[4.MRPC](https://www.aclweb.org/anthology/I05-5002.pdf)                   | Classify whether a pair of sentences is a re-phrasing of each other (semantically equivalent)|
|[5.QNLI](https://arxiv.org/pdf/1804.07461.pdf)                   | Classify whether the answer to a question can be deducted from an answer candidate.|
|[6.QQP](https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs)                    | Classify whether a pair of questions is a re-phrasing of each other (semantically equivalent)|
|[7.SST2](https://www.aclweb.org/anthology/D13-1170.pdf)                   | Classify the sentiment of a sentence as positive or negative|
|[8.STSB](https://www.aclweb.org/anthology/S17-2001/)                   | Classify the sentiment of a sentence on a scale from 1 to 5 (21 Sentiment classes)|
|[9.CB](https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/601)                     | Classify for a premise and a hypothesis whether they contradict each other or not (binary).|
|[10.COPA](https://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2418/0)                   | Classify for a question, premise, and 2 choices which choice the correct choice is (binary).|
|[11.MultiRc](https://www.aclweb.org/anthology/N18-1023.pdf)                | Classify for a question, a paragraph of text, and an answer candidate, if the answer is correct (binary),|
|[12.WiC](https://arxiv.org/abs/1808.09121)                    | Classify for a pair of sentences and a disambigous word if the word has the same meaning in both sentences.|
|[13.WSC/DPR](https://www.aaai.org/ocs/index.php/KR/KR12/paper/view/4492/0)       | Predict for an ambiguous pronoun in a sentence what it is referring to.  |
|[14.Summarization](https://arxiv.org/abs/1506.03340)          | Summarize text into a shorter representation.|
|[15.SQuAD](https://arxiv.org/abs/1606.05250)                  | Answer a question for a given context.|
|[16.WMT1.](https://arxiv.org/abs/1706.03762)                  | Translate English to German|
|[17.WMT2.](https://arxiv.org/abs/1706.03762)                   | Translate English to French|
|[18.WMT3.](https://arxiv.org/abs/1706.03762)                   | Translate English to Romanian|

- [Every T5 Task example notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more) to see how to use every T5 Task.
- [T5 Open and Closed Book question answering  notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_question_answering.ipynb)

# `Open book` and `Closed book` question answering with Google's T5
[T5 Open and Closed Book question answering tutorial](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_question_answering.ipynb)

With the latest NLU release and Google's T5 you can answer **general knowledge based questions given no context** and in addition answer **questions on text databases**.      
These questions can be asked in natural human language and answerd in just 1 line with NLU!.




## What is a `open book question`?
You can imagine an `open book` question similar to an examen where you are allowed to bring in text documents or cheat sheets that help you answer questions in an examen. Kinda like bringing a history book to an history examen.

In `T5's` terms, this means the model is given a `question` and an **additional piece of textual information** or so called `context`.

This enables the `T5` model to answer questions on textual datasets like `medical records`,`newsarticles` , `wiki-databases` , `stories` and `movie scripts` , `product descriptions`, 'legal documents' and many more.

You can answer `open book question` in 1 line of code, leveraging the latest NLU release and Google's T5.     
All it takes is :



```python
nlu.load('answer_question').predict(""""""
Where did Jebe die?
context: Ghenkis Khan recalled Subtai back to Mongolia soon afterwards,
 and Jebe died on the road back to Samarkand"""""")
&gt;&gt;&gt; Output: Samarkand
```

Example for answering medical questions based on medical context
``` python
question ='''
What does increased oxygen concentrations in the patient’s lungs displace? 
context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. 
Carbon monoxide poisoning, gas gangrene, and decompression sickness (the ’bends’) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin.
 Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment.
'''


#Predict on text data with T5
nlu.load('answer_question').predict(question)
&gt;&gt;&gt; Output: carbon monoxide	
```

Take a look at this example on a recent news article snippet :
```python
question1 = 'Who is Jack ma?'
question2 = 'Who is founder of Alibaba Group?'
question3 = 'When did Jack Ma re-appear?'
question4 = 'How did Alibaba stocks react?'
question5 = 'Whom did Jack Ma meet?'
question6 = 'Who did Jack Ma hide from?'

# from https://www.bbc.com/news/business-55728338 
news_article_snippet = """""" context:
Alibaba Group founder Jack Ma has made his first appearance since Chinese regulators cracked down on his business empire.
His absence had fuelled speculation over his whereabouts amid increasing official scrutiny of his businesses.
The billionaire met 100 rural teachers in China via a video meeting on Wednesday, according to local government media.
Alibaba shares surged 5% on Hong Kong's stock exchange on the news.
""""""
# join question with context, works with Pandas DF aswell!
questions = [
             question1+ news_article_snippet,
             question2+ news_article_snippet,
             question3+ news_article_snippet,
             question4+ news_article_snippet,
             question5+ news_article_snippet,
             question6+ news_article_snippet,]
nlu.load('answer_question').predict(questions)
```
This will output a Pandas Dataframe similar to this :

|Answer|Question|
|-----|---------|
Alibaba Group founder| 	Who is Jack ma? |        
|Jack Ma	|Who is founder of Alibaba Group? |  
Wednesday	| When did Jack Ma re-appear? | 
surged 5%	| How did Alibaba stocks react? | 
100 rural teachers	| Whom did Jack Ma meet? | 
Chinese regulators	|Who did Jack Ma hide from?|



## What is a `closed book question`?
A `closed book question` is the exact opposite of a `open book question`. In an examen scenario, you are only allowed to use what you have memorized in your brain and nothing else.      
In `T5's` terms this means that T5 can only use it's stored weights to answer a `question` and is given **no aditional context**.        
`T5` was pre-trained on the [C4 dataset](https://commoncrawl.org/) which contains **petabytes  of web crawling data**  collected over the last 8 years, including Wikipedia in every language.


This gives `T5` the broad knowledge of the internet stored in it's weights to answer various `closed book questions`

You can answer `closed book question` in 1 line of code, leveraging the latest NLU release and Google's T5.     
You need to pass one string to NLU, which starts which a `question` and is followed by  a `context:` tag and then the actual context contents.
All it takes is :


```python
nlu.load('en.t5').predict('Who is president of Nigeria?')
&gt;&gt;&gt; Muhammadu Buhari 
```


```python
nlu.load('en.t5').predict('What is the most spoken language in India?')
&gt;&gt;&gt; Hindi
```


```python
nlu.load('en.t5').predict('What is the capital of Germany?')
&gt;&gt;&gt; Berlin
```




## Text Summarization with T5
[Summarization example](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more)

`Summarizes` a paragraph into a shorter version with the same semantic meaning, based on [this paper](https://arxiv.org/abs/1506.03340)

```python
# Set the task on T5
pipe = nlu.load('summarize')

# define Data, add additional tags between sentences
data = [
'''
The belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal’s side currently sit two points clear of liverpool in fourth .
''',
'''  Calculus, originally called infinitesimal calculus or ""the calculus of infinitesimals"", is the mathematical study of continuous change, in the same way that geometry is the study of shape and algebra is the study of generalizations of arithmetic operations. It has two major branches, differential calculus and integral calculus; the former concerns instantaneous rates of change, and the slopes of curves, while integral calculus concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus, and they make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit.[1] Infinitesimal calculus was developed independently in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz.[2][3] Today, calculus has widespread uses in science, engineering, and economics.[4] In mathematics education, calculus denotes courses of elementary mathematical analysis, which are mainly devoted to the study of functions and limits. The word calculus (plural calculi) is a Latin word, meaning originally ""small pebble"" (this meaning is kept in medicine – see Calculus (medicine)). Because such pebbles were used for calculation, the meaning of the word has evolved and today usually means a method of computation. It is therefore used for naming specific methods of calculation and related theories, such as propositional calculus, Ricci calculus, calculus of variations, lambda calculus, and process calculus.'''
]


#Predict on text data with T5
pipe.predict(data)
```

| Predicted summary| Text | 
|------------------|-------|
| manchester united face newcastle in the premier league on wednesday . louis van gaal's side currently sit two points clear of liverpool in fourth . the belgian duo took to the dance floor on monday night with some friends .            | the belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal’s side currently sit two points clear of liverpool in fourth . | 


## Binary Sentence similarity/ Paraphrasing
[Binary sentence similarity example](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more)
Classify whether one sentence is a re-phrasing or similar to another sentence      
This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf) and based on [MRPC - Binary Paraphrasing/ sentence similarity classification ](https://www.aclweb.org/anthology/I05-5002.pdf)

```
t5 = nlu.load('en.t5.base')
# Set the task on T5
t5['t5'].setTask('mrpc ')

# define Data, add additional tags between sentences
data = [
''' sentence1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , "" Rumsfeld said .
sentence2: Rather , the US acted because the administration saw "" existing evidence in a new light , through the prism of our experience on September 11 ""
'''
,
'''  
sentence1: I like to eat peanutbutter for breakfast
sentence2: 	I like to play football.
'''
]

#Predict on text data with T5
t5.predict(data)
```
| Sentence1 | Sentence2 | prediction|
|------------|------------|----------|
|We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , "" Rumsfeld said .| Rather , the US acted because the administration saw "" existing evidence in a new light , through the prism of our experience on September 11 "" . | equivalent | 
| I like to eat peanutbutter for breakfast| I like to play football | not_equivalent | 


### How to configure T5 task for MRPC and pre-process text
`.setTask('mrpc sentence1:)` and prefix second sentence with `sentence2:`

### Example pre-processed input for T5 MRPC - Binary Paraphrasing/ sentence similarity

```
mrpc 
sentence1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , "" Rumsfeld said . 
sentence2: Rather , the US acted because the administration saw "" existing evidence in a new light , through the prism of our experience on September 11"",
```



## Regressive Sentence similarity/ Paraphrasing

Measures how similar two sentences are on a scale from 0 to 5 with 21 classes representing a regressive label.     
This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf) and based on[STSB - Regressive semantic sentence similarity](https://www.aclweb.org/anthology/S17-2001/) .

```python
t5 = nlu.load('en.t5.base')
# Set the task on T5
t5['t5'].setTask('stsb ') 

# define Data, add additional tags between sentences
data = [
             
              ''' sentence1:  What attributes would have made you highly desirable in ancient Rome?  
                  sentence2:  How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?'
              '''
             ,
             '''  
              sentence1: What was it like in Ancient rome?
              sentence2: 	What was Ancient rome like?
              ''',
              '''  
              sentence1: What was live like as a King in Ancient Rome??
              sentence2: 	What was Ancient rome like?
              '''

             ]



#Predict on text data with T5
t5.predict(data)

```

| sentence1 | sentence2 | prediction|
|------------|------------|----------|
|What attributes would have made you highly desirable in ancient Rome?        | How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER? | 0 | 
|What was it like in Ancient rome?  | What was Ancient rome like?| 5.0 | 
|What was live like as a King in Ancient Rome??       | What is it like to live in Rome? | 3.2 | 


### How to configure T5 task for stsb and pre-process text
`.setTask('stsb sentence1:)` and prefix second sentence with `sentence2:`




### Example pre-processed input for T5 STSB - Regressive semantic sentence similarity

```
stsb
sentence1: What attributes would have made you highly desirable in ancient Rome?        
sentence2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?',
```





## Grammar Checking
[Grammar checking with T5 example](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more)
Judges if a sentence is grammatically acceptable.    
Based on [CoLA - Binary Grammatical Sentence acceptability classification](https://nyu-mll.github.io/CoLA/)

```python
pipe = nlu.load('grammar_correctness')
# Set the task on T5
pipe['t5'].setTask('cola sentence: ')
# define Data
data = ['Anna and Mike is going skiing and they is liked is','Anna and Mike like to dance']
#Predict on text data with T5
pipe.predict(data)
```
|sentence  | prediction|
|------------|------------|
| Anna and Mike is going skiing and they is liked is | unacceptable |      
| Anna and Mike like to dance | acceptable | 


## Document Normalization
[Document Normalizer example](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/text_pre_processing_and_cleaning/document_normalizer_demo.ipynb)     
The DocumentNormalizer extracts content from HTML or XML documents, applying either data cleansing using an arbitrary number of custom regular expressions either data extraction following the different parameters

```python
pipe = nlu.load('norm_document')
data = '&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Example&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;This is an example of a simple HTML page with one paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt;'
df = pipe.predict(data,output_level='document')
df
```
|text|normalized_text|
|------|-------------|
| `&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Example&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;This is an example of a simple HTML page with one paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt;`       |Example This is an example of a simple HTML page with one paragraph.|

## Word Segmenter
[Word Segmenter Example](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/multilingual/japanese_ner_pos_and_tokenization.ipynb)     
The WordSegmenter segments languages without any rule-based tokenization such as Chinese, Japanese, or Korean
```python
pipe = nlu.load('ja.segment_words')
# japanese for 'Donald Trump and Angela Merkel dont share many opinions'
ja_data = ['ドナルド・トランプとアンゲラ・メルケルは多くの意見を共有していません']
df = pipe.predict(ja_data, output_level='token')
df

```

|	token|
|--------|
|	ドナルド|
|	・|
|	トランプ|
|	と|
|	アンゲラ|
|	・|
|	メルケル|
|	は|
|	多く|
|	の|
|	意見|
|	を|
|	共有|
|	し|
|	て|
|	い|
|	ませ|
|	ん|


# Named Entity Extraction (NER) in Various Languages 
NLU now support NER for over 60 languages, including Korean, Japanese, Chinese and many more!   
```python

# Extract named chinese entities
pipe = nlu.load('zh.ner')
# Chinese for 'Donald Trump and Angela Merkel dont share many opinions'
zh_data = ['唐纳德特朗普和安吉拉·默克尔没有太多意见']
df = pipe.predict(zh_data, output_level='document')
df
&gt;&gt;&gt; Output : [唐纳德, 安吉拉]

# Now translate [唐纳德, 安吉拉] back to english with NLU!
translate_pipe = nlu.load('zh.translate_to.en')
en_entities = translate_pipe.predict(['唐纳德', '安吉拉'])
&gt;&gt;&gt; Output :
```
|Translation|	Chinese| 
|------|------|
|Donald | 唐纳德 |
|Angela | 	安吉拉|

# New NLU Notebooks


### NLU 1.1.0  New Notebooks for new features
- [Translate between 192+ languages with marian](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/translation_demo.ipynb)
- [Try out the 18 Tasks like Summarization Question Answering and more on T5](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more)
- [T5 Open and Closed Book question answering tutorial](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_question_answering.ipynb)
- [Tokenize, extract POS and NER in Chinese](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/multilingual/chinese_ner_pos_and_tokenization.ipynb)
- [Tokenize, extract POS and NER in Korean](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/multilingual/korean_ner_pos_and_tokenization.ipynb)
- [Tokenize, extract POS and NER in Japanese](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/multilingual/japanese_ner_pos_and_tokenization.ipynb)
- [Normalize documents](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/text_pre_processing_and_cleaning/document_normalizer_demo.ipynb)
- [Aspect based sentiment NER sentiment for restaurants](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/named_entity_recognition_(NER)/aspect_based_ner_sentiment_restaurants.ipynb)

### NLU 1.1.0 New Classifier Training Tutorials
#### Binary Classifier training Jupyter tutorials
- [2 class Finance News sentiment classifier training](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary_text_classification/NLU_training_sentiment_classifier_demo_apple_twitter.ipynb)
- [2 class Reddit comment sentiment classifier training](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary_text_classification/NLU_training_sentiment_classifier_demo_reddit.ipynb)
- [2 class Apple Tweets sentiment classifier training](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary_text_classification/NLU_training_sentiment_classifier_demo_IMDB.ipynb)
- [2 class IMDB Movie sentiment classifier training](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary_text_classification/NLU_training_sentiment_classifier_demo_IMDB.ipynb)
- [2 class twitter classifier training](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary_text_classification/NLU_training_sentiment_classifier_demo_twitter.ipynb)

#### Multi Class text Classifier training Jupyter tutorials
- [5 class WineEnthusiast Wine review classifier training](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/multi_class_text_classification/NLU_training_multi_class_text_classifier_demo_wine.ipynb)
- [3 class Amazon Phone review classifier training](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/multi_class_text_classification/NLU_training_multi_class_text_classifier_demo_amazon.ipynb)
- [5 class Amazon Musical Instruments review classifier training](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/multi_class_text_classification/NLU_training_multi_class_text_classifier_demo_musical_instruments.ipynb)
- [5 class Tripadvisor Hotel review classifier training](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/multi_class_text_classification/NLU_training_multi_class_text_classifier_demo_hotel_reviews.ipynb)



### NLU 1.1.0 New Medium Tutorials

- [1 line to Glove Word Embeddings with NLU     with t-SNE plots](https://medium.com/spark-nlp/1-line-to-glove-word-embeddings-with-nlu-in-python-baed152fff4d)
- [1 line to Xlnet Word Embeddings with NLU     with t-SNE plots](https://medium.com/spark-nlp/1-line-to-xlnet-word-embeddings-with-nlu-in-python-5efc57d7ac79)
- [1 line to AlBERT Word Embeddings with NLU    with t-SNE plots](https://medium.com/spark-nlp/1-line-to-albert-word-embeddings-with-nlu-in-python-1691bc048ed1)
- [1 line to CovidBERT Word Embeddings with NLU with t-SNE plots](https://medium.com/spark-nlp/1-line-to-covidbert-word-embeddings-with-nlu-in-python-e67396da2f78)
- [1 line to Electra Word Embeddings with NLU   with t-SNE plots](https://medium.com/spark-nlp/1-line-to-electra-word-embeddings-with-nlu-in-python-25f749bf3e92)
- [1 line to BioBERT Word Embeddings with NLU   with t-SNE plots](https://medium.com/spark-nlp/1-line-to-biobert-word-embeddings-with-nlu-in-python-7224ab52e131)



## Installation

```bash
# PyPi
!pip install nlu pyspark==2.4.7
#Conda
# Install NLU from Anaconda/Conda
conda install -c johnsnowlabs nlu
```


# Additional NLU ressources
- [NLU Website](https://nlu.johnsnowlabs.com/)
- [All NLU Tutorial Notebooks](https://nlu.johnsnowlabs.com/docs/en/notebooks)
- [NLU Videos and Blogposts on NLU](https://nlp.johnsnowlabs.com/learn#pythons-nlu-library)
- [NLU on Github](https://github.com/JohnSnowLabs/nlu)",t2_53n73cus,False,,0,False,"720+ new NLP models, 300+ supported languages, translation, summarization, question answering, and more with T5 and Marian models! - John Snow Labs NLU 1.1.0",[],r/LanguageTechnology,False,6,,0,,False,t3_l4c8t9,False,dark,1.0,,public,54,0,{},,False,[],,False,False,,{},,False,54,,False,False,,False,,[],{},,True,,1611565292.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;h1&gt;720+ new  NLP models, 300+ supported languages, translation, summarization, question answering and more with T5 and Marian models!  - John Snow Labs NLU 1.1.0&lt;/h1&gt;

&lt;h2&gt;NLU 1.1.0 Release Notes&lt;/h2&gt;

&lt;p&gt;We are incredibly excited to release NLU 1.1.0!
This release integrates the 720+ new models from the latest &lt;a href=""https://github.com/JohnSnowLabs/spark-nlp/releases""&gt;Spark-NLP 2.7.0 + releases&lt;/a&gt;
You can now achieve state-of-the-art results with Sequence2Sequence transformers on problems like text summarization, question answering, translation between  192+ languages, and extract Named Entity in various Right to Left written languages like  Arabic, Persian, Urdu, and languages that require segmentation like Koreas, Japanese, Chinese, and many more in 1 line of code!&lt;br/&gt;
These new features are possible because of the integration of the &lt;a href=""https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html""&gt;Google&amp;#39;s T5 models&lt;/a&gt; and &lt;a href=""https://marian-nmt.github.io/publications/""&gt;Microsoft&amp;#39;s Marian models&lt;/a&gt;  transformers.&lt;/p&gt;

&lt;p&gt;NLU 1.1.0 has over 720+ new pretrained models and pipelines while extending the support of multi-lingual models to 192+ languages such as Chinese, Japanese, Korean, Arabic, Persian, Urdu, and Hebrew.&lt;/p&gt;

&lt;p&gt;In addition to this, NLU 1.1.0 comes with 9 new notebooks showcasing training classifiers for various review and sentiment datasets and 7 notebooks for the new features and models.&lt;/p&gt;

&lt;h3&gt;NLU 1.1.0  New Features&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;720+&lt;/strong&gt; new models you can find an overview of all NLU models &lt;a href=""https://nlu.johnsnowlabs.com/docs/en/namespace""&gt;here&lt;/a&gt; and further documentation in the &lt;a href=""https://nlp.johnsnowlabs.com/models""&gt;models hub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NEW:&lt;/strong&gt; Introducing MarianTransformer annotator for machine translation based on MarianNMT models. Marian is an efficient, free Neural Machine Translation framework mainly being developed by the Microsoft Translator team (646+ pretrained models &amp;amp; pipelines in 192+ languages)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NEW:&lt;/strong&gt; Introducing T5Transformer annotator for Text-To-Text Transfer Transformer (Google T5) models to achieve state-of-the-art results on multiple NLP tasks such as Translation, Summarization, Question Answering, Sentence Similarity, and so on&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NEW:&lt;/strong&gt; Introducing brand new and refactored language detection and identification models. The new LanguageDetectorDL is faster, more accurate, and supports up to 375 languages&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NEW:&lt;/strong&gt; Introducing WordSegmenter model for word segmentation of languages without any rule-based tokenization such as Chinese, Japanese, or Korean&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NEW:&lt;/strong&gt; Introducing DocumentNormalizer component for cleaning content from HTML or XML documents, applying either data cleansing using an arbitrary number of custom regular expressions either data extraction following the different parameters&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Translation&lt;/h2&gt;

&lt;p&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/translation_demo.ipynb""&gt;Translation example&lt;/a&gt;&lt;br/&gt;
You can translate between more than 192 Languages pairs with the &lt;a href=""https://marian-nmt.github.io/publications/""&gt;Marian Models&lt;/a&gt;
You need to specify the language your data is in as &lt;code&gt;start_language&lt;/code&gt; and the language you want to translate to as &lt;code&gt;target_language&lt;/code&gt;.&lt;br/&gt;
The language references must be &lt;a href=""https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes""&gt;ISO language codes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nlu.load(&amp;#39;&amp;lt;start_language&amp;gt;.translate.&amp;lt;target_language&amp;gt;&amp;#39;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Translate English to French :&lt;/strong&gt;&lt;br/&gt;
```
nlu.load(&amp;#39;en.translate_to.fr&amp;#39;).predict(&amp;quot;Hello from John Snow Labs&amp;quot;)&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Output: Bonjour des laboratoires de neige de John!   &lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;
**Translate English to Inukitut :**     
&lt;/code&gt;
nlu.load(&amp;#39;en.translate_to.lu&amp;#39;).predict(&amp;quot;Hello from John Snow Labs&amp;quot;)&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Output: kalunganyembo ka mashika makamankate 
&lt;code&gt;
**Translate English to Hungarian :**
&lt;/code&gt;
nlu.load(&amp;#39;en.translate_to.hu&amp;#39;).predict(&amp;quot;Hello from John Snow Labs&amp;quot;)
Output: Helló John hó laborjából.
&lt;code&gt;
**Translate English to German :**
&lt;/code&gt;
nlu.load(&amp;#39;en.translate_to.de&amp;#39;).predict(&amp;quot;Hello from John Snow Labs!&amp;quot;)
Output: Hallo aus John Schnee Labors 
```&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;python
translate_pipe = nlu.load(&amp;#39;en.translate_to.de&amp;#39;)
df = translate_pipe.predict(&amp;#39;Billy likes to go to the mall every sunday&amp;#39;)
df
&lt;/code&gt;&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;sentence&lt;/th&gt;
&lt;th&gt;translation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Billy likes to go to the mall every sunday&lt;/td&gt;
&lt;td&gt;Billy geht gerne jeden Sonntag ins Einkaufszentrum&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;T5&lt;/h2&gt;

&lt;p&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more""&gt;Example of every T5 task&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;Overview of every task available with T5&lt;/h3&gt;

&lt;p&gt;&lt;a href=""https://arxiv.org/pdf/1910.10683.pdf""&gt;The T5 model&lt;/a&gt; is trained on various datasets for 17 different tasks which fall into 8 categories.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Text summarization&lt;/li&gt;
&lt;li&gt;Question answering&lt;/li&gt;
&lt;li&gt;Translation&lt;/li&gt;
&lt;li&gt;Sentiment analysis&lt;/li&gt;
&lt;li&gt;Natural Language inference&lt;/li&gt;
&lt;li&gt;Coreference resolution&lt;/li&gt;
&lt;li&gt;Sentence Completion&lt;/li&gt;
&lt;li&gt;Word sense disambiguation&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Every T5 Task with explanation:&lt;/h3&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task Name&lt;/th&gt;
&lt;th&gt;Explanation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nyu-mll.github.io/CoLA/""&gt;1.CoLA&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Classify if a sentence is gramaticaly correct&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://dl.acm.org/doi/10.1007/11736790_9""&gt;2.RTE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Classify whether if a statement can be deducted from a sentence&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://arxiv.org/abs/1704.05426""&gt;3.MNLI&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Classify for a hypothesis and premise whether they contradict or contradict each other or neither of both (3 class).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://www.aclweb.org/anthology/I05-5002.pdf""&gt;4.MRPC&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Classify whether a pair of sentences is a re-phrasing of each other (semantically equivalent)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://arxiv.org/pdf/1804.07461.pdf""&gt;5.QNLI&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Classify whether the answer to a question can be deducted from an answer candidate.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs""&gt;6.QQP&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Classify whether a pair of questions is a re-phrasing of each other (semantically equivalent)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://www.aclweb.org/anthology/D13-1170.pdf""&gt;7.SST2&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Classify the sentiment of a sentence as positive or negative&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://www.aclweb.org/anthology/S17-2001/""&gt;8.STSB&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Classify the sentiment of a sentence on a scale from 1 to 5 (21 Sentiment classes)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/601""&gt;9.CB&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Classify for a premise and a hypothesis whether they contradict each other or not (binary).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2418/0""&gt;10.COPA&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Classify for a question, premise, and 2 choices which choice the correct choice is (binary).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://www.aclweb.org/anthology/N18-1023.pdf""&gt;11.MultiRc&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Classify for a question, a paragraph of text, and an answer candidate, if the answer is correct (binary),&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://arxiv.org/abs/1808.09121""&gt;12.WiC&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Classify for a pair of sentences and a disambigous word if the word has the same meaning in both sentences.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://www.aaai.org/ocs/index.php/KR/KR12/paper/view/4492/0""&gt;13.WSC/DPR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Predict for an ambiguous pronoun in a sentence what it is referring to.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://arxiv.org/abs/1506.03340""&gt;14.Summarization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Summarize text into a shorter representation.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://arxiv.org/abs/1606.05250""&gt;15.SQuAD&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Answer a question for a given context.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://arxiv.org/abs/1706.03762""&gt;16.WMT1.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Translate English to German&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://arxiv.org/abs/1706.03762""&gt;17.WMT2.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Translate English to French&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://arxiv.org/abs/1706.03762""&gt;18.WMT3.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Translate English to Romanian&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more""&gt;Every T5 Task example notebook&lt;/a&gt; to see how to use every T5 Task.&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_question_answering.ipynb""&gt;T5 Open and Closed Book question answering  notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;&lt;code&gt;Open book&lt;/code&gt; and &lt;code&gt;Closed book&lt;/code&gt; question answering with Google&amp;#39;s T5&lt;/h1&gt;

&lt;p&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_question_answering.ipynb""&gt;T5 Open and Closed Book question answering tutorial&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;With the latest NLU release and Google&amp;#39;s T5 you can answer &lt;strong&gt;general knowledge based questions given no context&lt;/strong&gt; and in addition answer &lt;strong&gt;questions on text databases&lt;/strong&gt;.&lt;br/&gt;
These questions can be asked in natural human language and answerd in just 1 line with NLU!.&lt;/p&gt;

&lt;h2&gt;What is a &lt;code&gt;open book question&lt;/code&gt;?&lt;/h2&gt;

&lt;p&gt;You can imagine an &lt;code&gt;open book&lt;/code&gt; question similar to an examen where you are allowed to bring in text documents or cheat sheets that help you answer questions in an examen. Kinda like bringing a history book to an history examen.&lt;/p&gt;

&lt;p&gt;In &lt;code&gt;T5&amp;#39;s&lt;/code&gt; terms, this means the model is given a &lt;code&gt;question&lt;/code&gt; and an &lt;strong&gt;additional piece of textual information&lt;/strong&gt; or so called &lt;code&gt;context&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This enables the &lt;code&gt;T5&lt;/code&gt; model to answer questions on textual datasets like &lt;code&gt;medical records&lt;/code&gt;,&lt;code&gt;newsarticles&lt;/code&gt; , &lt;code&gt;wiki-databases&lt;/code&gt; , &lt;code&gt;stories&lt;/code&gt; and &lt;code&gt;movie scripts&lt;/code&gt; , &lt;code&gt;product descriptions&lt;/code&gt;, &amp;#39;legal documents&amp;#39; and many more.&lt;/p&gt;

&lt;p&gt;You can answer &lt;code&gt;open book question&lt;/code&gt; in 1 line of code, leveraging the latest NLU release and Google&amp;#39;s T5.&lt;br/&gt;
All it takes is :&lt;/p&gt;

&lt;p&gt;```python
nlu.load(&amp;#39;answer_question&amp;#39;).predict(&amp;quot;&amp;quot;&amp;quot;
Where did Jebe die?
context: Ghenkis Khan recalled Subtai back to Mongolia soon afterwards,
 and Jebe died on the road back to Samarkand&amp;quot;&amp;quot;&amp;quot;)&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Output: Samarkand
```&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;Example for answering medical questions based on medical context
``` python
question =&amp;#39;&amp;#39;&amp;#39;
What does increased oxygen concentrations in the patient’s lungs displace? 
context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. 
Carbon monoxide poisoning, gas gangrene, and decompression sickness (the ’bends’) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin.
 Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment.
&amp;#39;&amp;#39;&amp;#39;&lt;/p&gt;

&lt;h1&gt;Predict on text data with T5&lt;/h1&gt;

&lt;p&gt;nlu.load(&amp;#39;answer_question&amp;#39;).predict(question)&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Output: carbon monoxide 
```&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;Take a look at this example on a recent news article snippet :
```python
question1 = &amp;#39;Who is Jack ma?&amp;#39;
question2 = &amp;#39;Who is founder of Alibaba Group?&amp;#39;
question3 = &amp;#39;When did Jack Ma re-appear?&amp;#39;
question4 = &amp;#39;How did Alibaba stocks react?&amp;#39;
question5 = &amp;#39;Whom did Jack Ma meet?&amp;#39;
question6 = &amp;#39;Who did Jack Ma hide from?&amp;#39;&lt;/p&gt;

&lt;h1&gt;from &lt;a href=""https://www.bbc.com/news/business-55728338""&gt;https://www.bbc.com/news/business-55728338&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;news_article_snippet = &amp;quot;&amp;quot;&amp;quot; context:
Alibaba Group founder Jack Ma has made his first appearance since Chinese regulators cracked down on his business empire.
His absence had fuelled speculation over his whereabouts amid increasing official scrutiny of his businesses.
The billionaire met 100 rural teachers in China via a video meeting on Wednesday, according to local government media.
Alibaba shares surged 5% on Hong Kong&amp;#39;s stock exchange on the news.
&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;

&lt;h1&gt;join question with context, works with Pandas DF aswell!&lt;/h1&gt;

&lt;p&gt;questions = [
             question1+ news_article_snippet,
             question2+ news_article_snippet,
             question3+ news_article_snippet,
             question4+ news_article_snippet,
             question5+ news_article_snippet,
             question6+ news_article_snippet,]
nlu.load(&amp;#39;answer_question&amp;#39;).predict(questions)
```
This will output a Pandas Dataframe similar to this :&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Answer&lt;/th&gt;
&lt;th&gt;Question&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Alibaba Group founder&lt;/td&gt;
&lt;td&gt;Who is Jack ma?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Jack Ma&lt;/td&gt;
&lt;td&gt;Who is founder of Alibaba Group?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Wednesday&lt;/td&gt;
&lt;td&gt;When did Jack Ma re-appear?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;surged 5%&lt;/td&gt;
&lt;td&gt;How did Alibaba stocks react?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100 rural teachers&lt;/td&gt;
&lt;td&gt;Whom did Jack Ma meet?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chinese regulators&lt;/td&gt;
&lt;td&gt;Who did Jack Ma hide from?&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;What is a &lt;code&gt;closed book question&lt;/code&gt;?&lt;/h2&gt;

&lt;p&gt;A &lt;code&gt;closed book question&lt;/code&gt; is the exact opposite of a &lt;code&gt;open book question&lt;/code&gt;. In an examen scenario, you are only allowed to use what you have memorized in your brain and nothing else.&lt;br/&gt;
In &lt;code&gt;T5&amp;#39;s&lt;/code&gt; terms this means that T5 can only use it&amp;#39;s stored weights to answer a &lt;code&gt;question&lt;/code&gt; and is given &lt;strong&gt;no aditional context&lt;/strong&gt;.&lt;br/&gt;
&lt;code&gt;T5&lt;/code&gt; was pre-trained on the &lt;a href=""https://commoncrawl.org/""&gt;C4 dataset&lt;/a&gt; which contains &lt;strong&gt;petabytes  of web crawling data&lt;/strong&gt;  collected over the last 8 years, including Wikipedia in every language.&lt;/p&gt;

&lt;p&gt;This gives &lt;code&gt;T5&lt;/code&gt; the broad knowledge of the internet stored in it&amp;#39;s weights to answer various &lt;code&gt;closed book questions&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You can answer &lt;code&gt;closed book question&lt;/code&gt; in 1 line of code, leveraging the latest NLU release and Google&amp;#39;s T5.&lt;br/&gt;
You need to pass one string to NLU, which starts which a &lt;code&gt;question&lt;/code&gt; and is followed by  a &lt;code&gt;context:&lt;/code&gt; tag and then the actual context contents.
All it takes is :&lt;/p&gt;

&lt;p&gt;```python
nlu.load(&amp;#39;en.t5&amp;#39;).predict(&amp;#39;Who is president of Nigeria?&amp;#39;)&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Muhammadu Buhari 
```&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;```python
nlu.load(&amp;#39;en.t5&amp;#39;).predict(&amp;#39;What is the most spoken language in India?&amp;#39;)&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Hindi
```&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;```python
nlu.load(&amp;#39;en.t5&amp;#39;).predict(&amp;#39;What is the capital of Germany?&amp;#39;)&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Berlin
```&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;h2&gt;Text Summarization with T5&lt;/h2&gt;

&lt;p&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more""&gt;Summarization example&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Summarizes&lt;/code&gt; a paragraph into a shorter version with the same semantic meaning, based on &lt;a href=""https://arxiv.org/abs/1506.03340""&gt;this paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;Set the task on T5&lt;/h1&gt;

&lt;p&gt;pipe = nlu.load(&amp;#39;summarize&amp;#39;)&lt;/p&gt;

&lt;h1&gt;define Data, add additional tags between sentences&lt;/h1&gt;

&lt;p&gt;data = [
&amp;#39;&amp;#39;&amp;#39;
The belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal’s side currently sit two points clear of liverpool in fourth .
&amp;#39;&amp;#39;&amp;#39;,
&amp;#39;&amp;#39;&amp;#39;  Calculus, originally called infinitesimal calculus or &amp;quot;the calculus of infinitesimals&amp;quot;, is the mathematical study of continuous change, in the same way that geometry is the study of shape and algebra is the study of generalizations of arithmetic operations. It has two major branches, differential calculus and integral calculus; the former concerns instantaneous rates of change, and the slopes of curves, while integral calculus concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus, and they make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit.[1] Infinitesimal calculus was developed independently in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz.[2][3] Today, calculus has widespread uses in science, engineering, and economics.[4] In mathematics education, calculus denotes courses of elementary mathematical analysis, which are mainly devoted to the study of functions and limits. The word calculus (plural calculi) is a Latin word, meaning originally &amp;quot;small pebble&amp;quot; (this meaning is kept in medicine – see Calculus (medicine)). Because such pebbles were used for calculation, the meaning of the word has evolved and today usually means a method of computation. It is therefore used for naming specific methods of calculation and related theories, such as propositional calculus, Ricci calculus, calculus of variations, lambda calculus, and process calculus.&amp;#39;&amp;#39;&amp;#39;
]&lt;/p&gt;

&lt;h1&gt;Predict on text data with T5&lt;/h1&gt;

&lt;p&gt;pipe.predict(data)
```&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Predicted summary&lt;/th&gt;
&lt;th&gt;Text&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;manchester united face newcastle in the premier league on wednesday . louis van gaal&amp;#39;s side currently sit two points clear of liverpool in fourth . the belgian duo took to the dance floor on monday night with some friends .&lt;/td&gt;
&lt;td&gt;the belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal’s side currently sit two points clear of liverpool in fourth .&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;Binary Sentence similarity/ Paraphrasing&lt;/h2&gt;

&lt;p&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more""&gt;Binary sentence similarity example&lt;/a&gt;
Classify whether one sentence is a re-phrasing or similar to another sentence&lt;br/&gt;
This is a sub-task of &lt;a href=""https://arxiv.org/pdf/1804.07461.pdf""&gt;GLUE&lt;/a&gt; and based on &lt;a href=""https://www.aclweb.org/anthology/I05-5002.pdf""&gt;MRPC - Binary Paraphrasing/ sentence similarity classification &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;```
t5 = nlu.load(&amp;#39;en.t5.base&amp;#39;)&lt;/p&gt;

&lt;h1&gt;Set the task on T5&lt;/h1&gt;

&lt;p&gt;t5[&amp;#39;t5&amp;#39;].setTask(&amp;#39;mrpc &amp;#39;)&lt;/p&gt;

&lt;h1&gt;define Data, add additional tags between sentences&lt;/h1&gt;

&lt;p&gt;data = [
&amp;#39;&amp;#39;&amp;#39; sentence1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , &amp;quot; Rumsfeld said .
sentence2: Rather , the US acted because the administration saw &amp;quot; existing evidence in a new light , through the prism of our experience on September 11 &amp;quot;
&amp;#39;&amp;#39;&amp;#39;
,
&amp;#39;&amp;#39;&amp;#39;&lt;br/&gt;
sentence1: I like to eat peanutbutter for breakfast
sentence2:  I like to play football.
&amp;#39;&amp;#39;&amp;#39;
]&lt;/p&gt;

&lt;h1&gt;Predict on text data with T5&lt;/h1&gt;

&lt;p&gt;t5.predict(data)
```
| Sentence1 | Sentence2 | prediction|
|------------|------------|----------|
|We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , &amp;quot; Rumsfeld said .| Rather , the US acted because the administration saw &amp;quot; existing evidence in a new light , through the prism of our experience on September 11 &amp;quot; . | equivalent | 
| I like to eat peanutbutter for breakfast| I like to play football | not_equivalent | &lt;/p&gt;

&lt;h3&gt;How to configure T5 task for MRPC and pre-process text&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;.setTask(&amp;#39;mrpc sentence1:)&lt;/code&gt; and prefix second sentence with &lt;code&gt;sentence2:&lt;/code&gt;&lt;/p&gt;

&lt;h3&gt;Example pre-processed input for T5 MRPC - Binary Paraphrasing/ sentence similarity&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;
mrpc 
sentence1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , &amp;quot; Rumsfeld said . 
sentence2: Rather , the US acted because the administration saw &amp;quot; existing evidence in a new light , through the prism of our experience on September 11&amp;quot;,
&lt;/code&gt;&lt;/p&gt;

&lt;h2&gt;Regressive Sentence similarity/ Paraphrasing&lt;/h2&gt;

&lt;p&gt;Measures how similar two sentences are on a scale from 0 to 5 with 21 classes representing a regressive label.&lt;br/&gt;
This is a sub-task of &lt;a href=""https://arxiv.org/pdf/1804.07461.pdf""&gt;GLUE&lt;/a&gt; and based on&lt;a href=""https://www.aclweb.org/anthology/S17-2001/""&gt;STSB - Regressive semantic sentence similarity&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;```python
t5 = nlu.load(&amp;#39;en.t5.base&amp;#39;)&lt;/p&gt;

&lt;h1&gt;Set the task on T5&lt;/h1&gt;

&lt;p&gt;t5[&amp;#39;t5&amp;#39;].setTask(&amp;#39;stsb &amp;#39;) &lt;/p&gt;

&lt;h1&gt;define Data, add additional tags between sentences&lt;/h1&gt;

&lt;p&gt;data = [&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;          &amp;#39;&amp;#39;&amp;#39; sentence1:  What attributes would have made you highly desirable in ancient Rome?  
              sentence2:  How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?&amp;#39;
          &amp;#39;&amp;#39;&amp;#39;
         ,
         &amp;#39;&amp;#39;&amp;#39;  
          sentence1: What was it like in Ancient rome?
          sentence2:    What was Ancient rome like?
          &amp;#39;&amp;#39;&amp;#39;,
          &amp;#39;&amp;#39;&amp;#39;  
          sentence1: What was live like as a King in Ancient Rome??
          sentence2:    What was Ancient rome like?
          &amp;#39;&amp;#39;&amp;#39;

         ]
&lt;/code&gt;&lt;/pre&gt;

&lt;h1&gt;Predict on text data with T5&lt;/h1&gt;

&lt;p&gt;t5.predict(data)&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;sentence1&lt;/th&gt;
&lt;th&gt;sentence2&lt;/th&gt;
&lt;th&gt;prediction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;What attributes would have made you highly desirable in ancient Rome?&lt;/td&gt;
&lt;td&gt;How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;What was it like in Ancient rome?&lt;/td&gt;
&lt;td&gt;What was Ancient rome like?&lt;/td&gt;
&lt;td&gt;5.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;What was live like as a King in Ancient Rome??&lt;/td&gt;
&lt;td&gt;What is it like to live in Rome?&lt;/td&gt;
&lt;td&gt;3.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h3&gt;How to configure T5 task for stsb and pre-process text&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;.setTask(&amp;#39;stsb sentence1:)&lt;/code&gt; and prefix second sentence with &lt;code&gt;sentence2:&lt;/code&gt;&lt;/p&gt;

&lt;h3&gt;Example pre-processed input for T5 STSB - Regressive semantic sentence similarity&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;
stsb
sentence1: What attributes would have made you highly desirable in ancient Rome?        
sentence2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?&amp;#39;,
&lt;/code&gt;&lt;/p&gt;

&lt;h2&gt;Grammar Checking&lt;/h2&gt;

&lt;p&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more""&gt;Grammar checking with T5 example&lt;/a&gt;
Judges if a sentence is grammatically acceptable.&lt;br/&gt;
Based on &lt;a href=""https://nyu-mll.github.io/CoLA/""&gt;CoLA - Binary Grammatical Sentence acceptability classification&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;```python
pipe = nlu.load(&amp;#39;grammar_correctness&amp;#39;)&lt;/p&gt;

&lt;h1&gt;Set the task on T5&lt;/h1&gt;

&lt;p&gt;pipe[&amp;#39;t5&amp;#39;].setTask(&amp;#39;cola sentence: &amp;#39;)&lt;/p&gt;

&lt;h1&gt;define Data&lt;/h1&gt;

&lt;p&gt;data = [&amp;#39;Anna and Mike is going skiing and they is liked is&amp;#39;,&amp;#39;Anna and Mike like to dance&amp;#39;]&lt;/p&gt;

&lt;h1&gt;Predict on text data with T5&lt;/h1&gt;

&lt;p&gt;pipe.predict(data)
```
|sentence  | prediction|
|------------|------------|
| Anna and Mike is going skiing and they is liked is | unacceptable |&lt;br/&gt;
| Anna and Mike like to dance | acceptable | &lt;/p&gt;

&lt;h2&gt;Document Normalization&lt;/h2&gt;

&lt;p&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/text_pre_processing_and_cleaning/document_normalizer_demo.ipynb""&gt;Document Normalizer example&lt;/a&gt;&lt;br/&gt;
The DocumentNormalizer extracts content from HTML or XML documents, applying either data cleansing using an arbitrary number of custom regular expressions either data extraction following the different parameters&lt;/p&gt;

&lt;p&gt;&lt;code&gt;python
pipe = nlu.load(&amp;#39;norm_document&amp;#39;)
data = &amp;#39;&amp;lt;!DOCTYPE html&amp;gt; &amp;lt;html&amp;gt; &amp;lt;head&amp;gt; &amp;lt;title&amp;gt;Example&amp;lt;/title&amp;gt; &amp;lt;/head&amp;gt; &amp;lt;body&amp;gt; &amp;lt;p&amp;gt;This is an example of a simple HTML page with one paragraph.&amp;lt;/p&amp;gt; &amp;lt;/body&amp;gt; &amp;lt;/html&amp;gt;&amp;#39;
df = pipe.predict(data,output_level=&amp;#39;document&amp;#39;)
df
&lt;/code&gt;
|text|normalized_text|
|------|-------------|
| &lt;code&gt;&amp;lt;!DOCTYPE html&amp;gt; &amp;lt;html&amp;gt; &amp;lt;head&amp;gt; &amp;lt;title&amp;gt;Example&amp;lt;/title&amp;gt; &amp;lt;/head&amp;gt; &amp;lt;body&amp;gt; &amp;lt;p&amp;gt;This is an example of a simple HTML page with one paragraph.&amp;lt;/p&amp;gt; &amp;lt;/body&amp;gt; &amp;lt;/html&amp;gt;&lt;/code&gt;       |Example This is an example of a simple HTML page with one paragraph.|&lt;/p&gt;

&lt;h2&gt;Word Segmenter&lt;/h2&gt;

&lt;p&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/multilingual/japanese_ner_pos_and_tokenization.ipynb""&gt;Word Segmenter Example&lt;/a&gt;&lt;br/&gt;
The WordSegmenter segments languages without any rule-based tokenization such as Chinese, Japanese, or Korean
```python
pipe = nlu.load(&amp;#39;ja.segment_words&amp;#39;)&lt;/p&gt;

&lt;h1&gt;japanese for &amp;#39;Donald Trump and Angela Merkel dont share many opinions&amp;#39;&lt;/h1&gt;

&lt;p&gt;ja_data = [&amp;#39;ドナルド・トランプとアンゲラ・メルケルは多くの意見を共有していません&amp;#39;]
df = pipe.predict(ja_data, output_level=&amp;#39;token&amp;#39;)
df&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;token&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ドナルド&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;・&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;トランプ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;と&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;アンゲラ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;・&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;メルケル&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;は&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;多く&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;の&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;意見&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;を&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;共有&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;し&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;て&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;い&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ませ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ん&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;Named Entity Extraction (NER) in Various Languages&lt;/h1&gt;

&lt;p&gt;NLU now support NER for over 60 languages, including Korean, Japanese, Chinese and many more!&lt;br/&gt;
```python&lt;/p&gt;

&lt;h1&gt;Extract named chinese entities&lt;/h1&gt;

&lt;p&gt;pipe = nlu.load(&amp;#39;zh.ner&amp;#39;)&lt;/p&gt;

&lt;h1&gt;Chinese for &amp;#39;Donald Trump and Angela Merkel dont share many opinions&amp;#39;&lt;/h1&gt;

&lt;p&gt;zh_data = [&amp;#39;唐纳德特朗普和安吉拉·默克尔没有太多意见&amp;#39;]
df = pipe.predict(zh_data, output_level=&amp;#39;document&amp;#39;)
df&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Output : [唐纳德, 安吉拉]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;h1&gt;Now translate [唐纳德, 安吉拉] back to english with NLU!&lt;/h1&gt;

&lt;p&gt;translate_pipe = nlu.load(&amp;#39;zh.translate_to.en&amp;#39;)
en_entities = translate_pipe.predict([&amp;#39;唐纳德&amp;#39;, &amp;#39;安吉拉&amp;#39;])&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Output :
```
|Translation|   Chinese| 
|------|------|
|Donald | 唐纳德 |
|Angela |   安吉拉|&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;h1&gt;New NLU Notebooks&lt;/h1&gt;

&lt;h3&gt;NLU 1.1.0  New Notebooks for new features&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/translation_demo.ipynb""&gt;Translate between 192+ languages with marian&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more""&gt;Try out the 18 Tasks like Summarization Question Answering and more on T5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_question_answering.ipynb""&gt;T5 Open and Closed Book question answering tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/multilingual/chinese_ner_pos_and_tokenization.ipynb""&gt;Tokenize, extract POS and NER in Chinese&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/multilingual/korean_ner_pos_and_tokenization.ipynb""&gt;Tokenize, extract POS and NER in Korean&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/multilingual/japanese_ner_pos_and_tokenization.ipynb""&gt;Tokenize, extract POS and NER in Japanese&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/text_pre_processing_and_cleaning/document_normalizer_demo.ipynb""&gt;Normalize documents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/named_entity_recognition_(NER""&gt;Aspect based sentiment NER sentiment for restaurants&lt;/a&gt;/aspect_based_ner_sentiment_restaurants.ipynb)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;NLU 1.1.0 New Classifier Training Tutorials&lt;/h3&gt;

&lt;h4&gt;Binary Classifier training Jupyter tutorials&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary_text_classification/NLU_training_sentiment_classifier_demo_apple_twitter.ipynb""&gt;2 class Finance News sentiment classifier training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary_text_classification/NLU_training_sentiment_classifier_demo_reddit.ipynb""&gt;2 class Reddit comment sentiment classifier training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary_text_classification/NLU_training_sentiment_classifier_demo_IMDB.ipynb""&gt;2 class Apple Tweets sentiment classifier training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary_text_classification/NLU_training_sentiment_classifier_demo_IMDB.ipynb""&gt;2 class IMDB Movie sentiment classifier training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary_text_classification/NLU_training_sentiment_classifier_demo_twitter.ipynb""&gt;2 class twitter classifier training&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Multi Class text Classifier training Jupyter tutorials&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/multi_class_text_classification/NLU_training_multi_class_text_classifier_demo_wine.ipynb""&gt;5 class WineEnthusiast Wine review classifier training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/multi_class_text_classification/NLU_training_multi_class_text_classifier_demo_amazon.ipynb""&gt;3 class Amazon Phone review classifier training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/multi_class_text_classification/NLU_training_multi_class_text_classifier_demo_musical_instruments.ipynb""&gt;5 class Amazon Musical Instruments review classifier training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/multi_class_text_classification/NLU_training_multi_class_text_classifier_demo_hotel_reviews.ipynb""&gt;5 class Tripadvisor Hotel review classifier training&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;NLU 1.1.0 New Medium Tutorials&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://medium.com/spark-nlp/1-line-to-glove-word-embeddings-with-nlu-in-python-baed152fff4d""&gt;1 line to Glove Word Embeddings with NLU     with t-SNE plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://medium.com/spark-nlp/1-line-to-xlnet-word-embeddings-with-nlu-in-python-5efc57d7ac79""&gt;1 line to Xlnet Word Embeddings with NLU     with t-SNE plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://medium.com/spark-nlp/1-line-to-albert-word-embeddings-with-nlu-in-python-1691bc048ed1""&gt;1 line to AlBERT Word Embeddings with NLU    with t-SNE plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://medium.com/spark-nlp/1-line-to-covidbert-word-embeddings-with-nlu-in-python-e67396da2f78""&gt;1 line to CovidBERT Word Embeddings with NLU with t-SNE plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://medium.com/spark-nlp/1-line-to-electra-word-embeddings-with-nlu-in-python-25f749bf3e92""&gt;1 line to Electra Word Embeddings with NLU   with t-SNE plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://medium.com/spark-nlp/1-line-to-biobert-word-embeddings-with-nlu-in-python-7224ab52e131""&gt;1 line to BioBERT Word Embeddings with NLU   with t-SNE plots&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Installation&lt;/h2&gt;

&lt;p&gt;```bash&lt;/p&gt;

&lt;h1&gt;PyPi&lt;/h1&gt;

&lt;p&gt;!pip install nlu pyspark==2.4.7&lt;/p&gt;

&lt;h1&gt;Conda&lt;/h1&gt;

&lt;h1&gt;Install NLU from Anaconda/Conda&lt;/h1&gt;

&lt;p&gt;conda install -c johnsnowlabs nlu
```&lt;/p&gt;

&lt;h1&gt;Additional NLU ressources&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/""&gt;NLU Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/docs/en/notebooks""&gt;All NLU Tutorial Notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlp.johnsnowlabs.com/learn#pythons-nlu-library""&gt;NLU Videos and Blogposts on NLU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu""&gt;NLU on Github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l4c8t9,True,,CKL-IT,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l4c8t9/720_new_nlp_models_300_supported_languages/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l4c8t9/720_new_nlp_models_300_supported_languages/,30199,1611536492.0,0,,False,,,,,,28839
871,,LanguageTechnology,"Hello, I am conducting research to identify ""improvisations"" within the healthcare system as a response to COVID (e.g scuba masks in place of normal face masks). I am doing this by analyzing text data of thousands articles taken from various news apis.

What I have done so far: 

1. familiarized myself with concepts (bag of words, word2vec, tfidf)
2. created a testing and training set (testing set I have created a binary field indicating whether or not the article is the type of article I am looking for)
3. familiarized myself with preprocessing (cleaning the data)

This is my first real exposure to NLP and I really fell in love with what I have seen so far. But as with every new thing, there is of course a lot of unknowns. 

I am looking to see if anyone can give me pointers on how to proceed. I thought about summarizing the articles (reduces dimensions of vector for BoW) then using Bag of words to train a model. Not sure if there is a different and better approach. All comments/critiques/criticisms are welcome and help would be most appreciated :)",t2_45qk0ns9,False,,0,False,New to NLP; Seeking guidance on current project,[],r/LanguageTechnology,False,6,,0,,False,t3_l4g58w,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1611578517.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I am conducting research to identify &amp;quot;improvisations&amp;quot; within the healthcare system as a response to COVID (e.g scuba masks in place of normal face masks). I am doing this by analyzing text data of thousands articles taken from various news apis.&lt;/p&gt;

&lt;p&gt;What I have done so far: &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;familiarized myself with concepts (bag of words, word2vec, tfidf)&lt;/li&gt;
&lt;li&gt;created a testing and training set (testing set I have created a binary field indicating whether or not the article is the type of article I am looking for)&lt;/li&gt;
&lt;li&gt;familiarized myself with preprocessing (cleaning the data)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is my first real exposure to NLP and I really fell in love with what I have seen so far. But as with every new thing, there is of course a lot of unknowns. &lt;/p&gt;

&lt;p&gt;I am looking to see if anyone can give me pointers on how to proceed. I thought about summarizing the articles (reduces dimensions of vector for BoW) then using Bag of words to train a model. Not sure if there is a different and better approach. All comments/critiques/criticisms are welcome and help would be most appreciated :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l4g58w,True,,Reasonable-Medicine2,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l4g58w/new_to_nlp_seeking_guidance_on_current_project/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l4g58w/new_to_nlp_seeking_guidance_on_current_project/,30199,1611549717.0,0,,False,,,,,,1065
872,,LanguageTechnology,"Hi NLP community,

I am a second-year PhD student in Computer Science at a prestigious university in Europe (&lt;100 ranking). 
I would want to start planning for a PhD internship at a research institute/industry in 2022/3. However, I am not too sure how to proceed with the process and what are typical requirements. Specifically, I work on the intersection of Structured Prediction and Bayesian Deep Learning, think deriving uncertainty for Named Entity Recognition. In my first year, I published a workshop paper at ICML and a large journal paper at JMLR.

For example, imagine I target one of the FAANG companies, what would they at least expect me to have done, e.g., publish at top ML conferences (ACL, NeurIPS, ICML, ICLR,...)? How can I increase my chances? Can someone maybe share their experiences on this? 

I appreciate your time in replying, cheers!",t2_2x2g38k8,False,,0,False,PhD internship in NLP at industrial research venue,[],r/LanguageTechnology,False,6,,0,,False,t3_l46o5j,False,dark,0.95,,public,19,0,{},,False,[],,False,False,,{},,False,19,,False,False,,False,,[],{},,True,,1611548309.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi NLP community,&lt;/p&gt;

&lt;p&gt;I am a second-year PhD student in Computer Science at a prestigious university in Europe (&amp;lt;100 ranking). 
I would want to start planning for a PhD internship at a research institute/industry in 2022/3. However, I am not too sure how to proceed with the process and what are typical requirements. Specifically, I work on the intersection of Structured Prediction and Bayesian Deep Learning, think deriving uncertainty for Named Entity Recognition. In my first year, I published a workshop paper at ICML and a large journal paper at JMLR.&lt;/p&gt;

&lt;p&gt;For example, imagine I target one of the FAANG companies, what would they at least expect me to have done, e.g., publish at top ML conferences (ACL, NeurIPS, ICML, ICLR,...)? How can I increase my chances? Can someone maybe share their experiences on this? &lt;/p&gt;

&lt;p&gt;I appreciate your time in replying, cheers!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l46o5j,True,,gurdovonlendogam,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l46o5j/phd_internship_in_nlp_at_industrial_research_venue/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l46o5j/phd_internship_in_nlp_at_industrial_research_venue/,30199,1611519509.0,0,,False,,,,,,862
873,,LanguageTechnology,"I have a couple of GPT-2-backed reddit chatbots (on r/SubSimGPT2Interactive ) that reply to other humans and bots based on random probability. GPT-2 has a pretty broad corpus so it works quite well, generally, except for GPT-2 losing the context as it's well known to do. 

Perhaps choosing which comments to reply to more carefully would help maintain context and in turn keep the bots replying to replies that are closely related to their training material, or 'on-brand' if you want to call it that!

When there is a potential reply, I would like to compare the incoming comment (which is only a few hundred to a thousand characters at most) to the bot's fine tuning material (a 10-30Mb text file) and be able to get some kind of relevance value and if it is over a particular threshold, generate a reply.

Is there some way that I can use a second ML model, or the GPT-2 model itself, to calculate such a relevance value?

Any different ideas appreciated, too! 

Cheers",t2_52c72tch,False,,0,False,Is there a model for comparing the relevance a piece of text to the GPT-2 model's fine tuning data?,[],r/LanguageTechnology,False,6,,0,,False,t3_l4fq8y,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,1611550531.0,,[],{},,True,,1611576985.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a couple of GPT-2-backed reddit chatbots (on &lt;a href=""/r/SubSimGPT2Interactive""&gt;r/SubSimGPT2Interactive&lt;/a&gt; ) that reply to other humans and bots based on random probability. GPT-2 has a pretty broad corpus so it works quite well, generally, except for GPT-2 losing the context as it&amp;#39;s well known to do. &lt;/p&gt;

&lt;p&gt;Perhaps choosing which comments to reply to more carefully would help maintain context and in turn keep the bots replying to replies that are closely related to their training material, or &amp;#39;on-brand&amp;#39; if you want to call it that!&lt;/p&gt;

&lt;p&gt;When there is a potential reply, I would like to compare the incoming comment (which is only a few hundred to a thousand characters at most) to the bot&amp;#39;s fine tuning material (a 10-30Mb text file) and be able to get some kind of relevance value and if it is over a particular threshold, generate a reply.&lt;/p&gt;

&lt;p&gt;Is there some way that I can use a second ML model, or the GPT-2 model itself, to calculate such a relevance value?&lt;/p&gt;

&lt;p&gt;Any different ideas appreciated, too! &lt;/p&gt;

&lt;p&gt;Cheers&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l4fq8y,True,,tateisukannanirase,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l4fq8y/is_there_a_model_for_comparing_the_relevance_a/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l4fq8y/is_there_a_model_for_comparing_the_relevance_a/,30199,1611548185.0,0,,False,,,,,,973
874,,LanguageTechnology,,t2_5fsp2x6v,False,,0,False,Language Understanding with Knowledge-based Embeddings (LUKE) | Research Papers Summary 005,[],r/LanguageTechnology,False,6,,0,,False,t3_l455uw,False,dark,1.0,,public,5,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/oho-i5Ws07g?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Language Understanding with Knowledge-based Embeddings (LUKE) | Research Papers Summary 005', 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/oho-i5Ws07g?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'version': '1.0', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/oho-i5Ws07g/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/oho-i5Ws07g?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/l455uw', 'height': 200}",,False,5,,False,False,,False,,[],{},,False,,1611542434.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l455uw,True,,RyanAI100,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l455uw/language_understanding_with_knowledgebased/,all_ads,False,https://youtu.be/oho-i5Ws07g,30199,1611513634.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Language Understanding with Knowledge-based Embeddings (LUKE) | Research Papers Summary 005', 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/oho-i5Ws07g?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'version': '1.0', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/oho-i5Ws07g/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}, 'type': 'youtube.com'}",False,https://youtu.be/oho-i5Ws07g,,,,,0
875,,LanguageTechnology,"Hi, everyone!

I'm applying to the LCT MSc for Fall 2021, and I would really like your opinions on the best combination of universities to take. 

Background: I'm a Computer Science student with experience in NLP at a foundational level. I have done projects using ML and Deep Learning as well. My linguistics background is not very strong, but I would like to learn more about it.

Requirements: I have already settled on Saarland for one of the universities. I would probably choose it for Year 2. For Year 1, I am focusing on Lorraine, Groningen or Trento. I have some rudimentary knowledge of French. I would like to study at a uni that has good research opportunities, with connections to applying NLP to the humanities, if possible. I realize the LCT programme is a bit disconnected, but which of these three unis would have a better transitory experience? I would also prefer that the classes be taught by profs with a good level of competency in English. Since I aim to work in industry research or at a research institute post this masters, ideally a university with a good CS programme would help, hence the reason for Saarland. As for the other, I am open to a more linguistics focused background, but having relevant options that can tie in with my focus area.

&amp;#x200B;

I would also like a ranking based on living environment - things to do, how expensive it is, how complicated the document processes are, how likely it is to find English speakers in the area, conveniences, food, relative ease of finding accommodation. 

&amp;#x200B;

Your help would be greatly appreciated, Reddit community. I will be cross-posting this on r/CompLing too.",t2_9vs8qz4o,False,,0,False,Erasmus Mundus MSc LCT Questions,[],r/LanguageTechnology,False,6,,0,,False,t3_l3yfcv,False,dark,1.0,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1611519484.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, everyone!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m applying to the LCT MSc for Fall 2021, and I would really like your opinions on the best combination of universities to take. &lt;/p&gt;

&lt;p&gt;Background: I&amp;#39;m a Computer Science student with experience in NLP at a foundational level. I have done projects using ML and Deep Learning as well. My linguistics background is not very strong, but I would like to learn more about it.&lt;/p&gt;

&lt;p&gt;Requirements: I have already settled on Saarland for one of the universities. I would probably choose it for Year 2. For Year 1, I am focusing on Lorraine, Groningen or Trento. I have some rudimentary knowledge of French. I would like to study at a uni that has good research opportunities, with connections to applying NLP to the humanities, if possible. I realize the LCT programme is a bit disconnected, but which of these three unis would have a better transitory experience? I would also prefer that the classes be taught by profs with a good level of competency in English. Since I aim to work in industry research or at a research institute post this masters, ideally a university with a good CS programme would help, hence the reason for Saarland. As for the other, I am open to a more linguistics focused background, but having relevant options that can tie in with my focus area.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I would also like a ranking based on living environment - things to do, how expensive it is, how complicated the document processes are, how likely it is to find English speakers in the area, conveniences, food, relative ease of finding accommodation. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Your help would be greatly appreciated, Reddit community. I will be cross-posting this on &lt;a href=""/r/CompLing""&gt;r/CompLing&lt;/a&gt; too.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l3yfcv,True,,exwordsmythe,,10,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l3yfcv/erasmus_mundus_msc_lct_questions/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l3yfcv/erasmus_mundus_msc_lct_questions/,30199,1611490684.0,0,,False,,,,,,1661
876,,LanguageTechnology,,t2_trjf2,False,,0,False,The Language Interpretability Tool (LIT): Interactive Exploration and Analysis of NLP Models - Google AI,[],r/LanguageTechnology,False,6,,0,,False,t3_l39nfv,False,dark,1.0,,public,28,0,{},,False,[],,False,False,,{},,False,28,,False,False,,False,,[],{},,False,,1611425728.0,text,6,,,text,ai.googleblog.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l39nfv,True,,adammathias,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l39nfv/the_language_interpretability_tool_lit/,all_ads,False,http://ai.googleblog.com/2020/11/the-language-interpretability-tool-lit.html,30199,1611396928.0,0,,False,http://ai.googleblog.com/2020/11/the-language-interpretability-tool-lit.html,,,,,0
877,,LanguageTechnology,"This paper from Microsoft presents a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained transformer) model trained on 147M conversation-like exchanges extracted from Reddit comments. 

Researchers show that the conversational systems that leverage DialoGPT generate more relevant, contentful, and context-consistent responses than strong baseline systems.

Paper Walkthrough: https://youtu.be/Zo679MYoJns

Paper: ⏩ Paper: https://www.aclweb.org/anthology/2020.acl-demos.30.pdf",t2_hkv9s,False,,0,False,DialoGPT Paper Walkthrough,[],r/LanguageTechnology,False,6,,0,,False,t3_l3b856,False,dark,0.93,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,True,,1611433573.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This paper from Microsoft presents a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained transformer) model trained on 147M conversation-like exchanges extracted from Reddit comments. &lt;/p&gt;

&lt;p&gt;Researchers show that the conversational systems that leverage DialoGPT generate more relevant, contentful, and context-consistent responses than strong baseline systems.&lt;/p&gt;

&lt;p&gt;Paper Walkthrough: &lt;a href=""https://youtu.be/Zo679MYoJns""&gt;https://youtu.be/Zo679MYoJns&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Paper: ⏩ Paper: &lt;a href=""https://www.aclweb.org/anthology/2020.acl-demos.30.pdf""&gt;https://www.aclweb.org/anthology/2020.acl-demos.30.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l3b856,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l3b856/dialogpt_paper_walkthrough/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l3b856/dialogpt_paper_walkthrough/,30199,1611404773.0,0,,False,,,,,,531
878,,LanguageTechnology,,t2_12ajj3,False,,0,False,What We Found Analyzing 300 Yelp Reviews of a Michelin Reviewed Restaurant with Natural Language Processing,[],r/LanguageTechnology,False,6,,0,,False,t3_l2y43q,False,dark,0.93,,public,13,0,{},,False,[],,False,False,,{},,False,13,,False,False,,False,,[],{},,False,,1611381509.0,text,6,,,text,blog.diffbot.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l2y43q,True,,misturbusy,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l2y43q/what_we_found_analyzing_300_yelp_reviews_of_a/,all_ads,False,https://blog.diffbot.com/what-we-found-analyzing-300-yelp-reviews-of-a-michelin-reviewed-restaurant-with-natural-language-processing/,30199,1611352709.0,0,,False,https://blog.diffbot.com/what-we-found-analyzing-300-yelp-reviews-of-a-michelin-reviewed-restaurant-with-natural-language-processing/,,,,,0
879,,LanguageTechnology,"In particular, is there any rule-based system that does a deeper analysis than Apertium? I'm interested in making a translator that attempts to preserve the meter and rhyme scheme of verse.",t2_kxps2,False,,0,False,What exists in the way of open source machine translation?,[],r/LanguageTechnology,False,6,,0,,False,t3_l3133x,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1611390593.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In particular, is there any rule-based system that does a deeper analysis than Apertium? I&amp;#39;m interested in making a translator that attempts to preserve the meter and rhyme scheme of verse.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l3133x,True,,Terpomo11,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l3133x/what_exists_in_the_way_of_open_source_machine/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l3133x/what_exists_in_the_way_of_open_source_machine/,30199,1611361793.0,0,,False,,,,,,189
880,,LanguageTechnology,"I built an AI that generates rap music lyrics using TensorFlow and Keras actually posted this project a while ago but since then lots of improvements have done also at that time servers are not capable of running the tf model. you can check the website(live demo you can give it a seed and I generates rap based on the seed) and the GitHub repos all links down below if you star or fork the repo I would be so happy thanks.

Github: [https://github.com/YigitGunduc/Spectrum](https://github.com/YigitGunduc/Spectrum)

Website: [https://spectrumapp.herokuapp.com/](https://spectrumapp.herokuapp.com/)",t2_70tvn3l8,False,,0,False,AI that generates rap lyrics (Open-Source + Live Demo),[],r/LanguageTechnology,False,6,,0,,False,t3_l2qk95,False,dark,0.82,,public,7,1,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{'gid_1': 1},,True,,1611359912.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I built an AI that generates rap music lyrics using TensorFlow and Keras actually posted this project a while ago but since then lots of improvements have done also at that time servers are not capable of running the tf model. you can check the website(live demo you can give it a seed and I generates rap based on the seed) and the GitHub repos all links down below if you star or fork the repo I would be so happy thanks.&lt;/p&gt;

&lt;p&gt;Github: &lt;a href=""https://github.com/YigitGunduc/Spectrum""&gt;https://github.com/YigitGunduc/Spectrum&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Website: &lt;a href=""https://spectrumapp.herokuapp.com/""&gt;https://spectrumapp.herokuapp.com/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l2qk95,True,,_Xeon__,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l2qk95/ai_that_generates_rap_lyrics_opensource_live_demo/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l2qk95/ai_that_generates_rap_lyrics_opensource_live_demo/,30199,1611331112.0,0,,False,,,,,,598
881,,LanguageTechnology,"Hi, I'm a CPO of DeepPavlov, an R&amp;D lab that builds a popular open-source NLP &amp; Conversational AI library also called DeepPavlov. It was born in February 2018, has lots of different state-of-the-art and demo NLP components. 

In addition to our dear users it has been battletested in our Socialbot in Amazon Alexa Prize 3, and it is now used in our Socialbot in Amazon Alexa Prize 4.

This January we've started work on refactoring the DeepPavlov Library on it's path to v1.0.  

I want to welcome you to join our DeepPavlov Community Call #5 to learn more next week on Jan 28 at 7pm MSK/8am PDT.  

Here's the link: [https://bit.ly/DPCommunityCall5](https://bit.ly/DPCommunityCall5) 

We are also interested in your feedback. Let us know what you want from DeepPavlov: 

[https://bit.ly/DPLibrary2021Survey](https://bit.ly/DPLibrary2021Survey)

See you next week!",t2_ueya7,False,,0,False,"Announcing DeepPavlov Community Call #5 - January 28, 8am PDT",[],r/LanguageTechnology,False,6,,0,,False,t3_l2nnaa,False,dark,1.0,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1611350643.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m a CPO of DeepPavlov, an R&amp;amp;D lab that builds a popular open-source NLP &amp;amp; Conversational AI library also called DeepPavlov. It was born in February 2018, has lots of different state-of-the-art and demo NLP components. &lt;/p&gt;

&lt;p&gt;In addition to our dear users it has been battletested in our Socialbot in Amazon Alexa Prize 3, and it is now used in our Socialbot in Amazon Alexa Prize 4.&lt;/p&gt;

&lt;p&gt;This January we&amp;#39;ve started work on refactoring the DeepPavlov Library on it&amp;#39;s path to v1.0.  &lt;/p&gt;

&lt;p&gt;I want to welcome you to join our DeepPavlov Community Call #5 to learn more next week on Jan 28 at 7pm MSK/8am PDT.  &lt;/p&gt;

&lt;p&gt;Here&amp;#39;s the link: &lt;a href=""https://bit.ly/DPCommunityCall5""&gt;https://bit.ly/DPCommunityCall5&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;We are also interested in your feedback. Let us know what you want from DeepPavlov: &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://bit.ly/DPLibrary2021Survey""&gt;https://bit.ly/DPLibrary2021Survey&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;See you next week!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l2nnaa,True,,daniel-kornev,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l2nnaa/announcing_deeppavlov_community_call_5_january_28/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l2nnaa/announcing_deeppavlov_community_call_5_january_28/,30199,1611321843.0,0,,False,,,,,,872
882,,LanguageTechnology,"Hi all,

What reading in linguistics would you recommend to someone entering NLP without much of a linguistics background? 

My background: graduated with double BA in International Studies and Japanese a few years ago. Became interested in linguistics at the end of my BA and took a couple lower level courses. I’m currently finishing prerequisites in stats, math, and CS and am applying to data science MS programs with the hope of concentrating on NLP—I originally wanted to apply to computational linguistics programs but am opting for data science for a few reasons. 

I see people argue back and forth for days about whether linguistic knowledge is helpful at all for for NLP. For those of you who think it is worthwhile, what reading would you recommend?",t2_2pdu7aay,False,,0,False,Linguistics Reading List for NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_l28pwj,False,dark,1.0,,public,23,0,{},,False,[],,False,False,,{},,False,23,,False,False,,False,,[],{},,True,,1611294661.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;What reading in linguistics would you recommend to someone entering NLP without much of a linguistics background? &lt;/p&gt;

&lt;p&gt;My background: graduated with double BA in International Studies and Japanese a few years ago. Became interested in linguistics at the end of my BA and took a couple lower level courses. I’m currently finishing prerequisites in stats, math, and CS and am applying to data science MS programs with the hope of concentrating on NLP—I originally wanted to apply to computational linguistics programs but am opting for data science for a few reasons. &lt;/p&gt;

&lt;p&gt;I see people argue back and forth for days about whether linguistic knowledge is helpful at all for for NLP. For those of you who think it is worthwhile, what reading would you recommend?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l28pwj,True,,avocaiden,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l28pwj/linguistics_reading_list_for_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l28pwj/linguistics_reading_list_for_nlp/,30199,1611265861.0,0,,False,,,,,,761
883,,LanguageTechnology,,t2_yylgy,False,,0,False,How to cluster documents using Word2vec and K-means,[],r/LanguageTechnology,False,6,,0,,False,t3_l2oeyk,False,dark,0.33,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,False,,1611353213.0,text,6,,,text,dylancastillo.co,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l2oeyk,True,,dcastm,,0,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/l2oeyk/how_to_cluster_documents_using_word2vec_and_kmeans/,all_ads,False,https://dylancastillo.co/nlp-snippets-cluster-documents-using-word2vec/,30199,1611324413.0,0,,False,https://dylancastillo.co/nlp-snippets-cluster-documents-using-word2vec/,,,,,0
884,,LanguageTechnology,I want to learn from a live instructor instead of a coursera class. I am ok with it being online. I just want something where I can ask a instructor questions right away.,t2_a8bujs3,False,,0,False,Does anyone know of a certificate program affiliated with a University that teaches NLP?,[],r/LanguageTechnology,False,6,,0,,False,t3_l2ggir,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1611320181.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to learn from a live instructor instead of a coursera class. I am ok with it being online. I just want something where I can ask a instructor questions right away.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l2ggir,True,,immunobio,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l2ggir/does_anyone_know_of_a_certificate_program/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l2ggir/does_anyone_know_of_a_certificate_program/,30199,1611291381.0,0,,False,,,,,,170
885,,LanguageTechnology,"Hello. I would like to attempt extending BERT with my own small corpus of technical documents. They include lots of tables, figures, bulleted lists, mixed in footnotes, and other relics that aren’t always part of my domains natural language discourse. Some may even be riddled with errors from being generated from OCRing pdfs (let’s say about 2% of them)

What’s the optimal way to clean up a big pile of garbage like this to improve my attempt to train a language model?",t2_3ardsqrn,False,,0,False,From document library to corpus for training BERT,[],r/LanguageTechnology,False,6,,0,,False,t3_l23e2v,False,dark,0.95,,public,15,0,{},,False,[],,False,False,,{},,False,15,,False,False,,False,,[],{},,True,,1611279653.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello. I would like to attempt extending BERT with my own small corpus of technical documents. They include lots of tables, figures, bulleted lists, mixed in footnotes, and other relics that aren’t always part of my domains natural language discourse. Some may even be riddled with errors from being generated from OCRing pdfs (let’s say about 2% of them)&lt;/p&gt;

&lt;p&gt;What’s the optimal way to clean up a big pile of garbage like this to improve my attempt to train a language model?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l23e2v,True,,GreenOnGray,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l23e2v/from_document_library_to_corpus_for_training_bert/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l23e2v/from_document_library_to_corpus_for_training_bert/,30199,1611250853.0,0,,False,,,,,,472
886,,LanguageTechnology,"Is it okay to withdraw from naacl submit to acl, looks like my reviewers are being unprofessional, and my paper also has some changes. But I am submiting to the same venue. Would that be ok?",t2_774js6rd,False,,0,False,withdraw from naacl submit to acl,[],r/LanguageTechnology,False,6,,0,,False,t3_l20ggf,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1611271278.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is it okay to withdraw from naacl submit to acl, looks like my reviewers are being unprofessional, and my paper also has some changes. But I am submiting to the same venue. Would that be ok?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l20ggf,True,,anCEOai,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l20ggf/withdraw_from_naacl_submit_to_acl/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l20ggf/withdraw_from_naacl_submit_to_acl/,30199,1611242478.0,0,,False,,,,,,190
887,,LanguageTechnology,"I have a project of multiple web scrapers running, all fetching text data from online. Once I've collected enough data to draw conclusions from it, I would like to analyse the text data (NLP, ML, DL). The scrapers are currently running on a VPS and storing the scraped text data in a local database.

Since the VPS is not strong enough for high performance NLP, I'm thinking about outsourcing the storing- and analysis-part to another provider. But I'm completely overwhelmed by the endless amounts of providers and their rather abstract descriptions of what they provide.

Are there good and cheap (free?) solutions that allow for uploading and storing data (approx. 1MB per upload) in regular intervals (1 upload per minute) and analyzing that data (preferably python; nltk, tensorflow, scikit etc.)? They can be different providers, but I prefer everything in one household.",t2_6bgl1kot,False,,0,False,Which cloud storage provider for uploading scraped data and analyzing it later on using DL and ML.,[],r/LanguageTechnology,False,6,,0,,False,t3_l29131,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1611295596.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a project of multiple web scrapers running, all fetching text data from online. Once I&amp;#39;ve collected enough data to draw conclusions from it, I would like to analyse the text data (NLP, ML, DL). The scrapers are currently running on a VPS and storing the scraped text data in a local database.&lt;/p&gt;

&lt;p&gt;Since the VPS is not strong enough for high performance NLP, I&amp;#39;m thinking about outsourcing the storing- and analysis-part to another provider. But I&amp;#39;m completely overwhelmed by the endless amounts of providers and their rather abstract descriptions of what they provide.&lt;/p&gt;

&lt;p&gt;Are there good and cheap (free?) solutions that allow for uploading and storing data (approx. 1MB per upload) in regular intervals (1 upload per minute) and analyzing that data (preferably python; nltk, tensorflow, scikit etc.)? They can be different providers, but I prefer everything in one household.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l29131,True,,The-Excel-Guy,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l29131/which_cloud_storage_provider_for_uploading/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l29131/which_cloud_storage_provider_for_uploading/,30199,1611266796.0,0,,False,,,,,,877
888,,LanguageTechnology,"Hi, I found a document which uses the formula VerbRoot[^.]*+  , I think it's for finding all the forms in which the verb can appear. But I don't really understand how the formula works, or if It's OK

Hope someone can help me! Thanks in advance!",t2_6nt1fpua,False,,0,False,Meaning of Regex [^.]*+,[],r/LanguageTechnology,False,6,,0,,False,t3_l1we2w,False,dark,0.6,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1611255916.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I found a document which uses the formula VerbRoot[&lt;sup&gt;.]*+&lt;/sup&gt;  , I think it&amp;#39;s for finding all the forms in which the verb can appear. But I don&amp;#39;t really understand how the formula works, or if It&amp;#39;s OK&lt;/p&gt;

&lt;p&gt;Hope someone can help me! Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l1we2w,True,,juliaLPA,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l1we2w/meaning_of_regex/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l1we2w/meaning_of_regex/,30199,1611227116.0,0,,False,,,,,,245
889,,LanguageTechnology,,t2_1la5pk00,False,,0,False,Composable Named Entities in Apache NLPCraft,[],r/LanguageTechnology,False,6,,0,,False,t3_l1to78,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,False,,1611243313.0,text,6,,,text,nlpcraft.apache.org,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l1to78,True,,aradzinski,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l1to78/composable_named_entities_in_apache_nlpcraft/,all_ads,False,http://nlpcraft.apache.org/blogs/composable_named_entities.html,30199,1611214513.0,0,,False,http://nlpcraft.apache.org/blogs/composable_named_entities.html,,,,,0
890,,LanguageTechnology,"Loads of works have been published on automatic summarization in general, and also for summarizing academic papers - it's quite easy to get lost in the literature and start implementing things that do not end up being the right methods. 

I am trying to implement an academic paper summarizer - so going from the full paper to abstract-like sized summary. I would be looking to use abstractive methods, or potentially hybrid solutions (extractive + abstractive). 

Does anyone have experience with a specific paper or method that they recommend for this task? ++ &lt;3 if the paper comes with code 

PS: Honestly, I hope that in the near future, all researchers publishing this kind of work will \*always\* publish code with it. I find it unacceptable that such works are most often without code linked to it.",t2_8qjshhcf,False,,0,False,Looking for an automatic text summarization method for academic papers,[],r/LanguageTechnology,False,6,,0,,False,t3_l1g24c,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1611198601.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Loads of works have been published on automatic summarization in general, and also for summarizing academic papers - it&amp;#39;s quite easy to get lost in the literature and start implementing things that do not end up being the right methods. &lt;/p&gt;

&lt;p&gt;I am trying to implement an academic paper summarizer - so going from the full paper to abstract-like sized summary. I would be looking to use abstractive methods, or potentially hybrid solutions (extractive + abstractive). &lt;/p&gt;

&lt;p&gt;Does anyone have experience with a specific paper or method that they recommend for this task? ++ &amp;lt;3 if the paper comes with code &lt;/p&gt;

&lt;p&gt;PS: Honestly, I hope that in the near future, all researchers publishing this kind of work will *always* publish code with it. I find it unacceptable that such works are most often without code linked to it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l1g24c,True,,taratataw,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l1g24c/looking_for_an_automatic_text_summarization/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l1g24c/looking_for_an_automatic_text_summarization/,30199,1611169801.0,0,,False,,,,,,809
891,,LanguageTechnology,"Hello there.

If I get some embeddings (e.g. Bert based ones) for some article headlines corpus and run some clustering algorithm, the articles about e.g. crime get (understandably) clustered together, but I’d like to cluster by news story/event instead, so I think named entities are the key for this.

When using a more traditional TF-IDF approach I’ve seen people boosting the weight of the words representing named entities, but with sentence embeddings I don’t think that’s possible.

I found some approach in a paper that works reasonably well but I wonder what people are doing about this?",t2_ki8gz,False,,0,False,How to introduce named entities into clustering?,[],r/LanguageTechnology,False,6,,0,,False,t3_l14xga,False,dark,0.9,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1611159362.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello there.&lt;/p&gt;

&lt;p&gt;If I get some embeddings (e.g. Bert based ones) for some article headlines corpus and run some clustering algorithm, the articles about e.g. crime get (understandably) clustered together, but I’d like to cluster by news story/event instead, so I think named entities are the key for this.&lt;/p&gt;

&lt;p&gt;When using a more traditional TF-IDF approach I’ve seen people boosting the weight of the words representing named entities, but with sentence embeddings I don’t think that’s possible.&lt;/p&gt;

&lt;p&gt;I found some approach in a paper that works reasonably well but I wonder what people are doing about this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l14xga,True,,ElCerebroDeLaBestia,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l14xga/how_to_introduce_named_entities_into_clustering/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l14xga/how_to_introduce_named_entities_into_clustering/,30199,1611130562.0,0,,False,,,,,,596
892,,LanguageTechnology,,t2_6zmofkz0,False,,0,False,The field of natural language processing is chasing the wrong goal,[],r/LanguageTechnology,False,6,,0,,False,t3_l0wbu2,False,dark,0.89,,public,22,0,{},,False,[],,False,False,,{},,False,22,,False,False,,False,,[],{},,False,,1611128086.0,text,6,,,text,technologyreview.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l0wbu2,True,,manifoldnd,,19,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l0wbu2/the_field_of_natural_language_processing_is/,all_ads,False,https://www.technologyreview.com/2020/07/31/1005876/natural-language-processing-evaluation-ai-opinion/,30199,1611099286.0,0,,False,https://www.technologyreview.com/2020/07/31/1005876/natural-language-processing-evaluation-ai-opinion/,,,,,0
893,,LanguageTechnology,,t2_6ib7gcgr,False,,0,False,[R] Finding The Words To Say: Hidden State Visualizations For Language Models,[],r/LanguageTechnology,False,6,,0,,False,t3_l0jjuj,False,dark,0.87,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,False,,1611090684.0,text,6,,,text,self.MachineLearning,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l0jjuj,True,,jayalammar,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l0jjuj/r_finding_the_words_to_say_hidden_state/,all_ads,False,/r/MachineLearning/comments/l0jiy0/r_finding_the_words_to_say_hidden_state/,30199,1611061884.0,0,,False,/r/MachineLearning/comments/l0jiy0/r_finding_the_words_to_say_hidden_state/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': "" \n\nHi [r/MachineLearning](https://www.reddit.com/r/MachineLearning/),\n\nThe hidden states of Transformers are an interesting place to seek clues for how the various layers process an input token and collectively select an output token. In this post, I demonstrate three ways of visualizing the hidden state. I build on awesome previous work in the space.\n\nWhat I find most exciting is that this post is only a demo for the open-source package creating these visualizations ([Ecco](https://www.eccox.io/)). I'm sure other people will find more interesting ways to probe these models using this tool. I hope you find it useful!\n\nTL;DR: The output tokens tend to be selected by the latter half of model layers. But there are cases where even layer 0 is certain of the output token (look for solid dark pink columns in the visuals).\n\n[https://jalammar.github.io/hidden-states/](https://jalammar.github.io/hidden-states/)"", 'author_fullname': 't2_6ib7gcgr', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[R] Finding The Words To Say: Hidden State Visualizations For Language Models', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'three', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_l0jiy0', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 11, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Research', 'can_mod_post': False, 'score': 11, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1611090590.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi &lt;a href=""https://www.reddit.com/r/MachineLearning/""&gt;r/MachineLearning&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;The hidden states of Transformers are an interesting place to seek clues for how the various layers process an input token and collectively select an output token. In this post, I demonstrate three ways of visualizing the hidden state. I build on awesome previous work in the space.&lt;/p&gt;\n\n&lt;p&gt;What I find most exciting is that this post is only a demo for the open-source package creating these visualizations (&lt;a href=""https://www.eccox.io/""&gt;Ecco&lt;/a&gt;). I&amp;#39;m sure other people will find more interesting ways to probe these models using this tool. I hope you find it useful!&lt;/p&gt;\n\n&lt;p&gt;TL;DR: The output tokens tend to be selected by the latter half of model layers. But there are cases where even layer 0 is certain of the output token (look for solid dark pink columns in the visuals).&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://jalammar.github.io/hidden-states/""&gt;https://jalammar.github.io/hidden-states/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'l0jiy0', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'jayalammar', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/l0jiy0/r_finding_the_words_to_say_hidden_state/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/l0jiy0/r_finding_the_words_to_say_hidden_state/', 'subreddit_subscribers': 1930725, 'created_utc': 1611061790.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_l0jiy0,,,0
894,,LanguageTechnology,How can I use GPT-2 to teach a model how to summarize a piece of random text?,t2_5gq0mtn1,False,,0,False,GPT-2,[],r/LanguageTechnology,False,6,,0,,False,t3_l0q8gk,False,dark,0.67,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1611110702.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How can I use GPT-2 to teach a model how to summarize a piece of random text?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l0q8gk,True,,jimjam_13,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l0q8gk/gpt2/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l0q8gk/gpt2/,30199,1611081902.0,0,,False,,,,,,77
895,,LanguageTechnology,"**The rising field of natural-language generation**

Research in natural language generation (NLG), a subset of artificial intelligence, is rising. [NLG](https://en.wikipedia.org/wiki/Natural-language_generation) is a software process that changes structured data into natural language. Not to be confused with natural language processing (NLP), NLG synthesizes and writes new content, whereas NLP reads and derives analytic insights from content ([Gartner](https://www.gartner.com/en/documents/3388326)).  

* Natural language generation (NLG) creates (or generates) text. It is when computers write language, turning structured data into text.
* Natural language processing (NLP) reads (or processes) text. It is when computers read language and derive insights.

Read Full Summary: [https://www.marktechpost.com/2021/01/18/google-ai-introduces-totto-a-controlled-table-to-text-generation-dataset-using-novel-annotation-process/](https://www.marktechpost.com/2021/01/18/google-ai-introduces-totto-a-controlled-table-to-text-generation-dataset-using-novel-annotation-process/)

Paper: [https://arxiv.org/abs/2004.14373](https://arxiv.org/abs/2004.14373)

GitHub: [https://github.com/google-research-datasets/totto](https://github.com/google-research-datasets/totto)",t2_2wsvqwhg,False,,0,False,Google AI Introduces ToTTo: A Controlled Table-to-Text Generation Dataset Using Novel Annotation Process,[],r/LanguageTechnology,False,6,,0,,False,t3_l0djzb,False,dark,1.0,,public,24,0,{},,False,[],,False,False,,{},,False,24,,False,False,,1611041099.0,,[],{},,True,,1611064497.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;strong&gt;The rising field of natural-language generation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Research in natural language generation (NLG), a subset of artificial intelligence, is rising. &lt;a href=""https://en.wikipedia.org/wiki/Natural-language_generation""&gt;NLG&lt;/a&gt; is a software process that changes structured data into natural language. Not to be confused with natural language processing (NLP), NLG synthesizes and writes new content, whereas NLP reads and derives analytic insights from content (&lt;a href=""https://www.gartner.com/en/documents/3388326""&gt;Gartner&lt;/a&gt;).  &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Natural language generation (NLG) creates (or generates) text. It is when computers write language, turning structured data into text.&lt;/li&gt;
&lt;li&gt;Natural language processing (NLP) reads (or processes) text. It is when computers read language and derive insights.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Read Full Summary: &lt;a href=""https://www.marktechpost.com/2021/01/18/google-ai-introduces-totto-a-controlled-table-to-text-generation-dataset-using-novel-annotation-process/""&gt;https://www.marktechpost.com/2021/01/18/google-ai-introduces-totto-a-controlled-table-to-text-generation-dataset-using-novel-annotation-process/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/abs/2004.14373""&gt;https://arxiv.org/abs/2004.14373&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;GitHub: &lt;a href=""https://github.com/google-research-datasets/totto""&gt;https://github.com/google-research-datasets/totto&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l0djzb,True,,ai-lover,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l0djzb/google_ai_introduces_totto_a_controlled/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l0djzb/google_ai_introduces_totto_a_controlled/,30199,1611035697.0,0,,False,,,,,,1266
896,,LanguageTechnology,"I imagine this is an incredibly common problem to other NLP practitioners, but I find that anytime I need to work on some sort of sequence-sequence model, there is always a shortage of cleaned, tagged data. (For classification tasks, this problem isn't nearly as much of an issue.) And so, my go to is typically using some combination of SpaCy POS tagging, HuggingFace  NER, regular expressions, and rule based logic. In particular, I like that SpaCy will not just tag the parts of speech but also use dynamic programming to find noun phrases that you can drill down on. This, in combination with rules and regex, usually allows me to find the tokens of interest, label them accordingly, save to disk/db, then model using Keras, PyTorch, etc. (which is much more streamlined for a single task.) 

One example: I was trying to extract ""excitors"" and ""aggravators"" for a marketing-related NLP project. The idea was to take""long wait time"" or ""friendly service"" from Yelp reviews. I found using the above pseudo method, I was able to piece together raw documents and an array of tokens for NER modeling. 

My employers have never seemed inclined to spend on a 3rd party data tagging service (overseas or otherwise), though I do understand such a service exists.

Curious, how do others approach the all-familiar, ""I don't have nearly enough data"" conundrum?",,False,,0,False,[Q] Tips/tricks for dataset synthesis?,[],r/LanguageTechnology,False,6,,0,,False,t3_l0n285,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,,,False,,,{},,True,,1611101925.0,text,6,,,,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I imagine this is an incredibly common problem to other NLP practitioners, but I find that anytime I need to work on some sort of sequence-sequence model, there is always a shortage of cleaned, tagged data. (For classification tasks, this problem isn&amp;#39;t nearly as much of an issue.) And so, my go to is typically using some combination of SpaCy POS tagging, HuggingFace  NER, regular expressions, and rule based logic. In particular, I like that SpaCy will not just tag the parts of speech but also use dynamic programming to find noun phrases that you can drill down on. This, in combination with rules and regex, usually allows me to find the tokens of interest, label them accordingly, save to disk/db, then model using Keras, PyTorch, etc. (which is much more streamlined for a single task.) &lt;/p&gt;

&lt;p&gt;One example: I was trying to extract &amp;quot;excitors&amp;quot; and &amp;quot;aggravators&amp;quot; for a marketing-related NLP project. The idea was to take&amp;quot;long wait time&amp;quot; or &amp;quot;friendly service&amp;quot; from Yelp reviews. I found using the above pseudo method, I was able to piece together raw documents and an array of tokens for NER modeling. &lt;/p&gt;

&lt;p&gt;My employers have never seemed inclined to spend on a 3rd party data tagging service (overseas or otherwise), though I do understand such a service exists.&lt;/p&gt;

&lt;p&gt;Curious, how do others approach the all-familiar, &amp;quot;I don&amp;#39;t have nearly enough data&amp;quot; conundrum?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l0n285,True,,[deleted],,0,True,all_ads,False,[],,dark,/r/LanguageTechnology/comments/l0n285/q_tipstricks_for_dataset_synthesis/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l0n285/q_tipstricks_for_dataset_synthesis/,30199,1611073125.0,0,,False,,,,,,1354
897,,LanguageTechnology,when will the reviews of NAACL be out?,t2_76mm1sy6,False,,0,False,"[R]: NAACL, reviews are out",[],r/LanguageTechnology,False,6,,0,,False,t3_l0px0t,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1611109857.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;when will the reviews of NAACL be out?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l0px0t,True,,randy_wales_qq,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l0px0t/r_naacl_reviews_are_out/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l0px0t/r_naacl_reviews_are_out/,30199,1611081057.0,0,,False,,,,,,38
898,,LanguageTechnology,"This is my problem:

I have a set of essays or articles I know have been ""enjoyed"" by a user, but only have data on one user's preferences. Now, based on this how can I assess whether a new piece of content will suit the user or not?

It's some sort of classification issue but the problem is that most things I have been finding online go about this by looking at what other ""similar"" (as in like the same content) users have liked, and I can't really go that route here.

I'm a beginner, so this might be a stupid question but I'd love to learn.

Thanks!",t2_3w74hpf8,False,,0,False,Best approach for content recommendation,[],r/LanguageTechnology,False,6,,0,,False,t3_l0otz9,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1611106827.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is my problem:&lt;/p&gt;

&lt;p&gt;I have a set of essays or articles I know have been &amp;quot;enjoyed&amp;quot; by a user, but only have data on one user&amp;#39;s preferences. Now, based on this how can I assess whether a new piece of content will suit the user or not?&lt;/p&gt;

&lt;p&gt;It&amp;#39;s some sort of classification issue but the problem is that most things I have been finding online go about this by looking at what other &amp;quot;similar&amp;quot; (as in like the same content) users have liked, and I can&amp;#39;t really go that route here.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m a beginner, so this might be a stupid question but I&amp;#39;d love to learn.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l0otz9,True,,EtherealUnagi,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l0otz9/best_approach_for_content_recommendation/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l0otz9/best_approach_for_content_recommendation/,30199,1611078027.0,0,,False,,,,,,556
899,,LanguageTechnology,My question is inspired by this list [here](https://ethical.net/resources/?resource-category=ethical-tech-orgs). Do you know about any tech company that is putting NLP into ethical applications or at least care about it?,t2_9d6tya8o,False,,0,False,Are there any companies putting NLP into ethical issues?,[],r/LanguageTechnology,False,6,,0,,False,t3_l0kd07,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1611093647.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My question is inspired by this list &lt;a href=""https://ethical.net/resources/?resource-category=ethical-tech-orgs""&gt;here&lt;/a&gt;. Do you know about any tech company that is putting NLP into ethical applications or at least care about it?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,l0kd07,True,,_3a3a_,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/l0kd07/are_there_any_companies_putting_nlp_into_ethical/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/l0kd07/are_there_any_companies_putting_nlp_into_ethical/,30199,1611064847.0,0,,False,,,,,,220
900,,LanguageTechnology,"[*Also posted on /r/ML*](https://www.reddit.com/r/MachineLearning/comments/kzuaie/p_i_made_nlprule_a_library_for_fast_grammatical/) *but this subreddit is more fitting :)*

&amp;#x200B;

Hi /r/LanguageTechnology!

I made NLPRule, a library for fast grammatical error correction in English and German by  checking thousands of rules. It is written in Rust and has bindings for Python.

Repository: [https://github.com/bminixhofer/nlprule](https://github.com/bminixhofer/nlprule)

**Synopsis**

    from nlprule import Tokenizer, Rules, SplitOn
    
    tokenizer = Tokenizer.load(""en"")
    rules = Rules.load(""en"", tokenizer, SplitOn([""."", ""?"", ""!""]))
    
    rules.correct(""He wants that you send him an email."")
    # returns: 'He wants you to send him an email.'
    
    rules.correct(""I can due his homework."")
    # returns: 'I can do his homework.'
    
    rules.correct(""It is enough for all intensive purposes."")
    # returns: 'It is enough for all intents and purposes.'
    
    
    suggestions = rules.suggest(""She was not been here since Monday."")
    for s in suggestions:
      print(s.start, s.end, s.replacements, s.source, s.message)
    # prints:
    # 4 16 ['was not', 'has not been'] WAS_BEEN.1 Did you mean was not or has not been?

**Background**

I've been interested in grammatical error correction for a while and came across [LanguageTool](https://github.com/languagetool-org/languagetool) which is based on [thousands of rules for error correction in an XML file](https://raw.githubusercontent.com/languagetool-org/languagetool/master/languagetool-language-modules/en/src/main/resources/org/languagetool/rules/en/grammar.xml).  You can think of the rule syntax as a restricted form of Regex where  the atoms are words annotated with lemmas, part-of-speech tags, and  chunks.

I'm not a big fan of Java,  wanted to improve my Rust and was interested in how these rules are  parsed so I made a proof of concept reverse-engineering the LanguageTool  logic in Rust. I had this lying around for quite some time and decided  to finish it up and make it into a usable library now during the  holidays.

**Relation to more sophisticated GEC approaches**

There's lots of research in using Neural Networks for Grammatical Error Correction and [there are some exciting recent approaches](https://github.com/grammarly/gector) which capture many more errors than a rule-based approach could. Still, for me there are two reasons to use rules:

1. **Speed.** On my machine with an 8th Gen Intel CPU it takes less than 1ms to correct a sentence.
2. **Dealing with extreme data sparsity of some errors.** The above example ""It is enough for all intensive purposes."" contains a [well known error](https://www.merriam-webster.com/words-at-play/usage-for-all-intensive-purposes-intents).  Yet, I would be surprised if a current ML model corrects this error  unless specifically accounted for since it will almost never have  appeared in its training data. This is even more true for similarly rare  errors in other languages where there is less data available than for  English. So I believe rules are especially useful in conjunction with a  more powerful ML model.

I think of NLPRule as a kind of ""sanity-check"" for text.

**Rule-based postprocessing for NLG**

Two  areas where NLPRule might be interesting are preprocessing for NLP and  postprocessing for NLG. I've tried the latter with texts generated from  GPT2. Applying NLPRule yields a significant amount of suggestions:

    Generated 192300 tokens.
    misspelling:    35 suggestions  (0.18 per 1000 tokens)
    style:          53 suggestions  (0.28 per 1000 tokens)
    typographical:  112 suggestions (0.58 per 1000 tokens)
    grammar:        29 suggestions  (0.15 per 1000 tokens)
    none:           3 suggestions   (0.02 per 1000 tokens)
    inconsistency:  2 suggestions   (0.01 per 1000 tokens)

Not all of these are errors, some are just suggestions for improvement. More information [here](https://github.com/bminixhofer/nlprule/tree/master/examples).

I'm happy to discuss anything in the  comments!",t2_q5v51,False,,0,False,I made NLPRule: A library for fast grammatical error correction,[],r/LanguageTechnology,False,6,,0,,False,t3_kzui94,False,dark,0.98,,public,48,1,{},,False,[],,False,False,,{},,False,48,,False,False,,1611041377.0,,[],{},,True,,1611004754.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.reddit.com/r/MachineLearning/comments/kzuaie/p_i_made_nlprule_a_library_for_fast_grammatical/""&gt;&lt;em&gt;Also posted on /r/ML&lt;/em&gt;&lt;/a&gt; &lt;em&gt;but this subreddit is more fitting :)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Hi &lt;a href=""/r/LanguageTechnology""&gt;/r/LanguageTechnology&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;I made NLPRule, a library for fast grammatical error correction in English and German by  checking thousands of rules. It is written in Rust and has bindings for Python.&lt;/p&gt;

&lt;p&gt;Repository: &lt;a href=""https://github.com/bminixhofer/nlprule""&gt;https://github.com/bminixhofer/nlprule&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Synopsis&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from nlprule import Tokenizer, Rules, SplitOn

tokenizer = Tokenizer.load(&amp;quot;en&amp;quot;)
rules = Rules.load(&amp;quot;en&amp;quot;, tokenizer, SplitOn([&amp;quot;.&amp;quot;, &amp;quot;?&amp;quot;, &amp;quot;!&amp;quot;]))

rules.correct(&amp;quot;He wants that you send him an email.&amp;quot;)
# returns: &amp;#39;He wants you to send him an email.&amp;#39;

rules.correct(&amp;quot;I can due his homework.&amp;quot;)
# returns: &amp;#39;I can do his homework.&amp;#39;

rules.correct(&amp;quot;It is enough for all intensive purposes.&amp;quot;)
# returns: &amp;#39;It is enough for all intents and purposes.&amp;#39;


suggestions = rules.suggest(&amp;quot;She was not been here since Monday.&amp;quot;)
for s in suggestions:
  print(s.start, s.end, s.replacements, s.source, s.message)
# prints:
# 4 16 [&amp;#39;was not&amp;#39;, &amp;#39;has not been&amp;#39;] WAS_BEEN.1 Did you mean was not or has not been?
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been interested in grammatical error correction for a while and came across &lt;a href=""https://github.com/languagetool-org/languagetool""&gt;LanguageTool&lt;/a&gt; which is based on &lt;a href=""https://raw.githubusercontent.com/languagetool-org/languagetool/master/languagetool-language-modules/en/src/main/resources/org/languagetool/rules/en/grammar.xml""&gt;thousands of rules for error correction in an XML file&lt;/a&gt;.  You can think of the rule syntax as a restricted form of Regex where  the atoms are words annotated with lemmas, part-of-speech tags, and  chunks.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m not a big fan of Java,  wanted to improve my Rust and was interested in how these rules are  parsed so I made a proof of concept reverse-engineering the LanguageTool  logic in Rust. I had this lying around for quite some time and decided  to finish it up and make it into a usable library now during the  holidays.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relation to more sophisticated GEC approaches&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There&amp;#39;s lots of research in using Neural Networks for Grammatical Error Correction and &lt;a href=""https://github.com/grammarly/gector""&gt;there are some exciting recent approaches&lt;/a&gt; which capture many more errors than a rule-based approach could. Still, for me there are two reasons to use rules:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Speed.&lt;/strong&gt; On my machine with an 8th Gen Intel CPU it takes less than 1ms to correct a sentence.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dealing with extreme data sparsity of some errors.&lt;/strong&gt; The above example &amp;quot;It is enough for all intensive purposes.&amp;quot; contains a &lt;a href=""https://www.merriam-webster.com/words-at-play/usage-for-all-intensive-purposes-intents""&gt;well known error&lt;/a&gt;.  Yet, I would be surprised if a current ML model corrects this error  unless specifically accounted for since it will almost never have  appeared in its training data. This is even more true for similarly rare  errors in other languages where there is less data available than for  English. So I believe rules are especially useful in conjunction with a  more powerful ML model.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I think of NLPRule as a kind of &amp;quot;sanity-check&amp;quot; for text.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rule-based postprocessing for NLG&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Two  areas where NLPRule might be interesting are preprocessing for NLP and  postprocessing for NLG. I&amp;#39;ve tried the latter with texts generated from  GPT2. Applying NLPRule yields a significant amount of suggestions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Generated 192300 tokens.
misspelling:    35 suggestions  (0.18 per 1000 tokens)
style:          53 suggestions  (0.28 per 1000 tokens)
typographical:  112 suggestions (0.58 per 1000 tokens)
grammar:        29 suggestions  (0.15 per 1000 tokens)
none:           3 suggestions   (0.02 per 1000 tokens)
inconsistency:  2 suggestions   (0.01 per 1000 tokens)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not all of these are errors, some are just suggestions for improvement. More information &lt;a href=""https://github.com/bminixhofer/nlprule/tree/master/examples""&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m happy to discuss anything in the  comments!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 250, 'id': 'award_a67d649d-5aa5-407e-a98b-32fd9e3a9696', 'penny_donate': None, 'award_sub_type': 'APPRECIATION', 'coin_reward': 100, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=16&amp;height=16&amp;auto=webp&amp;s=bbfa251092cce139b37d74237ec28a8c4e8f06b0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=32&amp;height=32&amp;auto=webp&amp;s=e1f9dd28741e2551b1fbbd341b006cc316f48fa1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=48&amp;height=48&amp;auto=webp&amp;s=d93434d26563a534397ff748cce71d4b733c32d9', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=64&amp;height=64&amp;auto=webp&amp;s=cf4a1ddb8474d11682f0d88aa32562f9fcbf30b0', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=128&amp;height=128&amp;auto=webp&amp;s=70b1596cdd0ae75b52db5c2732d8c336d300cc11', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'The more you know... Gives %{coin_symbol}100 Coins to both the author and the community.', 'end_date': None, 'subreddit_coin_reward': 100, 'count': 1, 'static_icon_height': 2048, 'name': 'Today I Learned', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=16&amp;height=16&amp;auto=webp&amp;s=bbfa251092cce139b37d74237ec28a8c4e8f06b0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=32&amp;height=32&amp;auto=webp&amp;s=e1f9dd28741e2551b1fbbd341b006cc316f48fa1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=48&amp;height=48&amp;auto=webp&amp;s=d93434d26563a534397ff748cce71d4b733c32d9', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=64&amp;height=64&amp;auto=webp&amp;s=cf4a1ddb8474d11682f0d88aa32562f9fcbf30b0', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=128&amp;height=128&amp;auto=webp&amp;s=70b1596cdd0ae75b52db5c2732d8c336d300cc11', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kzui94,True,,bminixhofer,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kzui94/i_made_nlprule_a_library_for_fast_grammatical/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kzui94/i_made_nlprule_a_library_for_fast_grammatical/,30199,1610975954.0,0,,False,,,,,,4085
901,,LanguageTechnology,"I am using transformers/simpletransformers.

I have sifted through the transformers source code, but the loose inheritance it uses (where parameters are all defined at the base and not validated when they are inherited) makes it really hard to decipher what is possible. Some Github issues suggest it's possible to add tokens, some not.

Trial-and-error setting has just run me into more errors of arguments not set properly in simpletransformers, so I'm at the stage of trying to call the base tokenizer functions directly like so:

    lmm = LanguageModelingModel(""gpt2"", ""gpt2"", args=args)
    ## special_tokens is a list of strings
    lmm.tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})
    lmm.model.resize_token_embeddings(len(lmm.tokenizer)) 
    lmm.train_tokenizer(train_files=[training_file, eval_file])
    lmm.train_model(train_file=training_file, eval_file=eval_file, args=args, verbose=True)

Which fails with an unclear cuda error, unfortunately.

I am using special tokens to segment parts of conversational text. The model is used by a reddit chatbot.

Am I wasting my time trying to add special tokens?

Update: I seem to have it working by re-setting the vocab\_size argument with the updated value when calling train\_model. I'll find out soon whether there is any kind of improvement.",t2_52c72tch,False,,0,False,Confused about whether I can add special tokens to a pretrained GPT-2 tokenizer,[],r/LanguageTechnology,False,6,,0,,False,t3_kzpwao,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,1610956445.0,,[],{},,True,,1610983849.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am using transformers/simpletransformers.&lt;/p&gt;

&lt;p&gt;I have sifted through the transformers source code, but the loose inheritance it uses (where parameters are all defined at the base and not validated when they are inherited) makes it really hard to decipher what is possible. Some Github issues suggest it&amp;#39;s possible to add tokens, some not.&lt;/p&gt;

&lt;p&gt;Trial-and-error setting has just run me into more errors of arguments not set properly in simpletransformers, so I&amp;#39;m at the stage of trying to call the base tokenizer functions directly like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lmm = LanguageModelingModel(&amp;quot;gpt2&amp;quot;, &amp;quot;gpt2&amp;quot;, args=args)
## special_tokens is a list of strings
lmm.tokenizer.add_special_tokens({&amp;#39;additional_special_tokens&amp;#39;: special_tokens})
lmm.model.resize_token_embeddings(len(lmm.tokenizer)) 
lmm.train_tokenizer(train_files=[training_file, eval_file])
lmm.train_model(train_file=training_file, eval_file=eval_file, args=args, verbose=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which fails with an unclear cuda error, unfortunately.&lt;/p&gt;

&lt;p&gt;I am using special tokens to segment parts of conversational text. The model is used by a reddit chatbot.&lt;/p&gt;

&lt;p&gt;Am I wasting my time trying to add special tokens?&lt;/p&gt;

&lt;p&gt;Update: I seem to have it working by re-setting the vocab_size argument with the updated value when calling train_model. I&amp;#39;ll find out soon whether there is any kind of improvement.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kzpwao,True,,tateisukannanirase,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kzpwao/confused_about_whether_i_can_add_special_tokens/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kzpwao/confused_about_whether_i_can_add_special_tokens/,30199,1610955049.0,0,,False,,,,,,1334
902,,LanguageTechnology,"I asked a team of data creators to generate a dataset for me to be used in implementing a dialect classification in the Arabic language for specific domains.  
Anyone know a way to validate this data.  
I thought of making an EDA listing the most important features like the distribution of the terms, number of unique words and somethings like that.  
Anyone knows any other features to validate the dataset that it's valuable, or know another way to do so !?",t2_8m7o7ys0,False,,0,False,How to validate a dataset?,[],r/LanguageTechnology,False,6,,0,,False,t3_kztqzi,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1611001713.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I asked a team of data creators to generate a dataset for me to be used in implementing a dialect classification in the Arabic language for specific domains.&lt;br/&gt;
Anyone know a way to validate this data.&lt;br/&gt;
I thought of making an EDA listing the most important features like the distribution of the terms, number of unique words and somethings like that.&lt;br/&gt;
Anyone knows any other features to validate the dataset that it&amp;#39;s valuable, or know another way to do so !?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kztqzi,True,,sheriffffffffff,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kztqzi/how_to_validate_a_dataset/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kztqzi/how_to_validate_a_dataset/,30199,1610972913.0,0,,False,,,,,,460
903,,LanguageTechnology,,t2_5fsp2x6v,False,,0,False,Temporally-Informed Analysis of Named Entity Recognition | Research Papers Summary 004,[],r/LanguageTechnology,False,6,,0,,False,t3_kzbkxj,False,dark,1.0,,public,18,1,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/eEkVYgWj2aE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Temporally-Informed Analysis of Named Entity Recognition | Research Papers Summary 004', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/eEkVYgWj2aE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/eEkVYgWj2aE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/eEkVYgWj2aE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/kzbkxj', 'height': 200}",,False,18,,False,False,,False,,[],{},,False,,1610936132.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 250, 'id': 'award_a67d649d-5aa5-407e-a98b-32fd9e3a9696', 'penny_donate': None, 'award_sub_type': 'APPRECIATION', 'coin_reward': 100, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=16&amp;height=16&amp;auto=webp&amp;s=bbfa251092cce139b37d74237ec28a8c4e8f06b0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=32&amp;height=32&amp;auto=webp&amp;s=e1f9dd28741e2551b1fbbd341b006cc316f48fa1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=48&amp;height=48&amp;auto=webp&amp;s=d93434d26563a534397ff748cce71d4b733c32d9', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=64&amp;height=64&amp;auto=webp&amp;s=cf4a1ddb8474d11682f0d88aa32562f9fcbf30b0', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=128&amp;height=128&amp;auto=webp&amp;s=70b1596cdd0ae75b52db5c2732d8c336d300cc11', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'The more you know... Gives %{coin_symbol}100 Coins to both the author and the community.', 'end_date': None, 'subreddit_coin_reward': 100, 'count': 1, 'static_icon_height': 2048, 'name': 'Today I Learned', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=16&amp;height=16&amp;auto=webp&amp;s=bbfa251092cce139b37d74237ec28a8c4e8f06b0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=32&amp;height=32&amp;auto=webp&amp;s=e1f9dd28741e2551b1fbbd341b006cc316f48fa1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=48&amp;height=48&amp;auto=webp&amp;s=d93434d26563a534397ff748cce71d4b733c32d9', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=64&amp;height=64&amp;auto=webp&amp;s=cf4a1ddb8474d11682f0d88aa32562f9fcbf30b0', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=128&amp;height=128&amp;auto=webp&amp;s=70b1596cdd0ae75b52db5c2732d8c336d300cc11', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kzbkxj,True,,RyanAI100,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kzbkxj/temporallyinformed_analysis_of_named_entity/,all_ads,False,https://youtu.be/eEkVYgWj2aE,30199,1610907332.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Temporally-Informed Analysis of Named Entity Recognition | Research Papers Summary 004', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/eEkVYgWj2aE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/eEkVYgWj2aE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,https://youtu.be/eEkVYgWj2aE,,,,,0
904,,LanguageTechnology,"Dear Reddit,

I'm trying to extract/scrape the following data at scale and was looking for recommendations from the community on the best approach. 

1. News articles from the web (*I've found 'news-please' works well. Are there others?* )
2. e-mail Newsletters
3. Arxiv papers   
4. Financial earnings call transcripts. 
5. Twitter posts (*I think the Twitter API might be the best approach*)
6. Instagram posts and comments

Ideally I'd prefer python libraries but I'm open to any tool that does the job. 

Thank You for your help",t2_f0q1w,False,,0,False,Recommendations for extracting data,[],r/LanguageTechnology,False,6,,0,,False,t3_kzla5l,False,dark,0.75,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1610966536.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Dear Reddit,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m trying to extract/scrape the following data at scale and was looking for recommendations from the community on the best approach. &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;News articles from the web (&lt;em&gt;I&amp;#39;ve found &amp;#39;news-please&amp;#39; works well. Are there others?&lt;/em&gt; )&lt;/li&gt;
&lt;li&gt;e-mail Newsletters&lt;/li&gt;
&lt;li&gt;Arxiv papers&lt;br/&gt;&lt;/li&gt;
&lt;li&gt;Financial earnings call transcripts. &lt;/li&gt;
&lt;li&gt;Twitter posts (&lt;em&gt;I think the Twitter API might be the best approach&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Instagram posts and comments&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Ideally I&amp;#39;d prefer python libraries but I&amp;#39;m open to any tool that does the job. &lt;/p&gt;

&lt;p&gt;Thank You for your help&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kzla5l,True,,purplegranite,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kzla5l/recommendations_for_extracting_data/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kzla5l/recommendations_for_extracting_data/,30199,1610937736.0,0,,False,,,,,,532
905,,LanguageTechnology,"Dear Reddit-Fam

I'm looking for resources about GPT-2 (for natural language generation), especially for training, fine-tuning (maybe in another language also). It seems like documentations or courses that go in-depth are very scarce at the moment (maybe/hopefully i'm false).

Greeeets",t2_7cejly7g,False,,0,False,How to learn (more about) GPT-2,[],r/LanguageTechnology,False,6,,0,,False,t3_kz4d03,False,dark,0.8,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1610909399.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Dear Reddit-Fam&lt;/p&gt;

&lt;p&gt;I&amp;#39;m looking for resources about GPT-2 (for natural language generation), especially for training, fine-tuning (maybe in another language also). It seems like documentations or courses that go in-depth are very scarce at the moment (maybe/hopefully i&amp;#39;m false).&lt;/p&gt;

&lt;p&gt;Greeeets&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kz4d03,True,,oxygeneticai,,15,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kz4d03/how_to_learn_more_about_gpt2/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kz4d03/how_to_learn_more_about_gpt2/,30199,1610880599.0,0,,False,,,,,,286
906,,LanguageTechnology,"So you have an NLP system - a chat bot, a search engine, NER, a classifier... - working well for English.

And you want to make it work for other languages, or maybe for all languages.

We see 3 basic approaches:

1. machine-translating at inference (or query) time
2. machine-translating labelled training data (or search indices), and training a multilingual model
3. zero-shot approaches with a multilingual LM like BERT or LASER

When to use which approach?

Machine-translating at inference time [2] is easiest to start with, but it's usually a bad idea.  It's the default at major US tech enterprises, from what I've seen, and even at really smart ML startups like Aylien.  And it's often suggested in this sub.

In Europe, where building a multilingual system is super important, we've even seen researchers *human-labelling* for every language, and ML startups *human-translating* labelled training data, or doing rules-based transliteration with *human post-editing*.

As a guy who thinks around the clock about machine translation risk and automation, all this unscalableness pains me to see.

So we have shared some open guides based on the work of our clients who implemented [multilingual search](https://www.reddit.com/r/LanguageTechnology/comments/k9ui4b/how_to_build_multilingual_search_with_translation/).

Nerses Nersesyan from Polixis and I will give a workshop on this at Applied Machine Learning Days in March.

https://appliedmldays.org/events/amld-epfl-2021/workshops/how-to-make-your-nlp-system-multilingual",t2_trjf2,False,,0,False,How to make your NLP system multilingual,[],r/LanguageTechnology,False,6,,0,,False,t3_kyjgb7,False,dark,0.98,,public,36,0,{},,False,[],,False,False,,{},,False,36,,False,False,,1610808405.0,,[],{},,True,,1610834595.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So you have an NLP system - a chat bot, a search engine, NER, a classifier... - working well for English.&lt;/p&gt;

&lt;p&gt;And you want to make it work for other languages, or maybe for all languages.&lt;/p&gt;

&lt;p&gt;We see 3 basic approaches:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;machine-translating at inference (or query) time&lt;/li&gt;
&lt;li&gt;machine-translating labelled training data (or search indices), and training a multilingual model&lt;/li&gt;
&lt;li&gt;zero-shot approaches with a multilingual LM like BERT or LASER&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When to use which approach?&lt;/p&gt;

&lt;p&gt;Machine-translating at inference time [2] is easiest to start with, but it&amp;#39;s usually a bad idea.  It&amp;#39;s the default at major US tech enterprises, from what I&amp;#39;ve seen, and even at really smart ML startups like Aylien.  And it&amp;#39;s often suggested in this sub.&lt;/p&gt;

&lt;p&gt;In Europe, where building a multilingual system is super important, we&amp;#39;ve even seen researchers &lt;em&gt;human-labelling&lt;/em&gt; for every language, and ML startups &lt;em&gt;human-translating&lt;/em&gt; labelled training data, or doing rules-based transliteration with &lt;em&gt;human post-editing&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;As a guy who thinks around the clock about machine translation risk and automation, all this unscalableness pains me to see.&lt;/p&gt;

&lt;p&gt;So we have shared some open guides based on the work of our clients who implemented &lt;a href=""https://www.reddit.com/r/LanguageTechnology/comments/k9ui4b/how_to_build_multilingual_search_with_translation/""&gt;multilingual search&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Nerses Nersesyan from Polixis and I will give a workshop on this at Applied Machine Learning Days in March.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://appliedmldays.org/events/amld-epfl-2021/workshops/how-to-make-your-nlp-system-multilingual""&gt;https://appliedmldays.org/events/amld-epfl-2021/workshops/how-to-make-your-nlp-system-multilingual&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kyjgb7,True,,adammathias,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kyjgb7/how_to_make_your_nlp_system_multilingual/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kyjgb7/how_to_make_your_nlp_system_multilingual/,30199,1610805795.0,3,,False,,,,,,1531
907,,LanguageTechnology,"So, when you get a conversation history, it'll be in the form of messages obviously. But messages aren't interpreted by people in the same way that they're stored.  People will consciously or unconsciously send multiple messages in a row for an overarching message.

If you were to try to analyze conversation chains though, this would make it harder to do LDA topic modelling, because the occurrences of words in a message are split up.  Another problem too, time gaps mean that sometimes a conversation will die, and the messaging following it will be part of a completely separate conversation. Othertimes, it might just be a while before someone gets back to you, or the conversation may be asynchronous due to conflicting schedules or something.

Another issue is revisiions.  


\*revisions.

How do you interpret revisions or clarifications in text?

Personally, I think these problems are too heavily in the area of computational pragmatics to be solvable, but surely there is something more that can be done than simply pretending the problem doesn't exist.

&amp;#x200B;

What would your solution/suggestion be?

Due to the lack of solutions in this area, I think nearly every suggestion that attempts to address any of these problems has merit.

&amp;#x200B;

Another thing too, do you think that this topic is publishable?  This is mostly a personal project for me, but if it comes out as something worthwhile, maybe I would put forth more effort into it for publication.",t2_14e9i6,False,,0,False,"How would you analyze text message history from Facebook, WhatsApp, Wechat, etc.? How would grouping work?",[],r/LanguageTechnology,False,6,,0,,False,t3_kyoxhs,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,1620812731.0,,[],{},,True,,1610852383.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So, when you get a conversation history, it&amp;#39;ll be in the form of messages obviously. But messages aren&amp;#39;t interpreted by people in the same way that they&amp;#39;re stored.  People will consciously or unconsciously send multiple messages in a row for an overarching message.&lt;/p&gt;

&lt;p&gt;If you were to try to analyze conversation chains though, this would make it harder to do LDA topic modelling, because the occurrences of words in a message are split up.  Another problem too, time gaps mean that sometimes a conversation will die, and the messaging following it will be part of a completely separate conversation. Othertimes, it might just be a while before someone gets back to you, or the conversation may be asynchronous due to conflicting schedules or something.&lt;/p&gt;

&lt;p&gt;Another issue is revisiions.  &lt;/p&gt;

&lt;p&gt;*revisions.&lt;/p&gt;

&lt;p&gt;How do you interpret revisions or clarifications in text?&lt;/p&gt;

&lt;p&gt;Personally, I think these problems are too heavily in the area of computational pragmatics to be solvable, but surely there is something more that can be done than simply pretending the problem doesn&amp;#39;t exist.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;What would your solution/suggestion be?&lt;/p&gt;

&lt;p&gt;Due to the lack of solutions in this area, I think nearly every suggestion that attempts to address any of these problems has merit.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Another thing too, do you think that this topic is publishable?  This is mostly a personal project for me, but if it comes out as something worthwhile, maybe I would put forth more effort into it for publication.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kyoxhs,True,,Fuehnix,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kyoxhs/how_would_you_analyze_text_message_history_from/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kyoxhs/how_would_you_analyze_text_message_history_from/,30199,1610823583.0,0,,False,,,,,,1483
908,,LanguageTechnology,,t2_trjf2,False,,0,False,[p] Ecco – See what your NLP language model is “thinking”,[],r/LanguageTechnology,False,6,,0,,False,t3_kyfkb7,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,False,,1610816189.0,text,6,,,text,self.MachineLearning,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kyfkb7,True,,adammathias,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kyfkb7/p_ecco_see_what_your_nlp_language_model_is/,all_ads,False,/r/MachineLearning/comments/kt33dp/p_ecco_see_what_your_nlp_language_model_is/,30199,1610787389.0,0,,False,/r/MachineLearning/comments/kt33dp/p_ecco_see_what_your_nlp_language_model_is/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': ""Hi r/MachineLearning,\n\nIn case you want to mess around with GPT2 neuron activations, I have released this package to allow you to do that:\n\n[https://www.eccox.io/](https://www.eccox.io/)\n\n&amp;#x200B;\n\nI used this package to create the visualizations in my recent article Interfaces for [Explaining Transformer Language Models](https://jalammar.github.io/explaining-transformers/) ([discussion](https://www.reddit.com/r/MachineLearning/comments/khc3nb/r_interfaces_for_explaining_transformer_language/)).  I had been fascinated with Andrej Karpathy's article ([https://karpathy.github.io/2015/05/21/rnn-effectiveness/](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)) -- especially where it shows neurons being activated in response to brackets and indentation. So I built this to capture them and then visualize underlying patterns (using NMF as suggested in [https://distill.pub/2018/building-blocks/](https://distill.pub/2018/building-blocks/)).\n\n&amp;#x200B;\n\nI hope it can be helpful in your research. Let me know how I can improve it."", 'author_fullname': 't2_6ib7gcgr', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[p] Ecco – See what your NLP language model is “thinking”', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'four', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_kt33dp', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 65, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Project', 'can_mod_post': False, 'score': 65, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1610143635.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi &lt;a href=""/r/MachineLearning""&gt;r/MachineLearning&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;In case you want to mess around with GPT2 neuron activations, I have released this package to allow you to do that:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.eccox.io/""&gt;https://www.eccox.io/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I used this package to create the visualizations in my recent article Interfaces for &lt;a href=""https://jalammar.github.io/explaining-transformers/""&gt;Explaining Transformer Language Models&lt;/a&gt; (&lt;a href=""https://www.reddit.com/r/MachineLearning/comments/khc3nb/r_interfaces_for_explaining_transformer_language/""&gt;discussion&lt;/a&gt;).  I had been fascinated with Andrej Karpathy&amp;#39;s article (&lt;a href=""https://karpathy.github.io/2015/05/21/rnn-effectiveness/""&gt;https://karpathy.github.io/2015/05/21/rnn-effectiveness/&lt;/a&gt;) -- especially where it shows neurons being activated in response to brackets and indentation. So I built this to capture them and then visualize underlying patterns (using NMF as suggested in &lt;a href=""https://distill.pub/2018/building-blocks/""&gt;https://distill.pub/2018/building-blocks/&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I hope it can be helpful in your research. Let me know how I can improve it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'kt33dp', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'jayalammar', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/kt33dp/p_ecco_see_what_your_nlp_language_model_is/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/kt33dp/p_ecco_see_what_your_nlp_language_model_is/', 'subreddit_subscribers': 1930725, 'created_utc': 1610114835.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_kt33dp,,,0
909,,LanguageTechnology,"Hi all,
I am currently working on some NLP tasks on Italian corpuses. I am noticing that, in general, Italian language models perform worse than their english siblings on similar tasks. I Indeed found several articles online raising the problem of low performance in non english NLP models.

So my question is: is this only a problem of low quality /small sized training datasets? Or is it something else? How would you address this problem if you had infinite ?money?

Thanks!",t2_f9lhz7g,False,,0,False,What are non-english language missing in order to reach english-like NLP performances?,[],r/LanguageTechnology,False,6,,0,,False,t3_ky4hci,False,dark,0.89,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,False,,False,,[],{},,True,,1610775339.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,
I am currently working on some NLP tasks on Italian corpuses. I am noticing that, in general, Italian language models perform worse than their english siblings on similar tasks. I Indeed found several articles online raising the problem of low performance in non english NLP models.&lt;/p&gt;

&lt;p&gt;So my question is: is this only a problem of low quality /small sized training datasets? Or is it something else? How would you address this problem if you had infinite ?money?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ky4hci,True,,fr4luc,,11,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ky4hci/what_are_nonenglish_language_missing_in_order_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ky4hci/what_are_nonenglish_language_missing_in_order_to/,30199,1610746539.0,0,,False,,,,,,477
910,,LanguageTechnology,"I have two documents or text data. Document 1 contains information like keywords with its own numeric number (which is the importance of the word):

    *gre (300)    india(290)    art(278)   galleries(257) ...*  

And another document that i have is the tf\*idf matrix. its the extracted keywords from single document with its tf\*idf score. (again, can be interpreted as the importance of the word).

    function   0.6781 art        0.2463 galleries  0.15655 . . ...  

so How do i compute similarities between these two document considering that the similarity between ""**art""** from document 1 and 2 should have higher score (higher similarity) because they are more important keywords as compared to the word ""**galleries""** from document 1 and 2 since they are less important keywords comparatively. How do i do this?",t2_1gai0rv3,False,,0,False,How to perform cosine similarity when taking into account the importance of the words as well.,[],r/LanguageTechnology,False,6,,0,,False,t3_kycu60,False,dark,0.81,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,1610778081.0,,[],{},,True,,1610803642.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have two documents or text data. Document 1 contains information like keywords with its own numeric number (which is the importance of the word):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;*gre (300)    india(290)    art(278)   galleries(257) ...*  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And another document that i have is the tf*idf matrix. its the extracted keywords from single document with its tf*idf score. (again, can be interpreted as the importance of the word).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;function   0.6781 art        0.2463 galleries  0.15655 . . ...  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;so How do i compute similarities between these two document considering that the similarity between &amp;quot;&lt;strong&gt;art&amp;quot;&lt;/strong&gt; from document 1 and 2 should have higher score (higher similarity) because they are more important keywords as compared to the word &amp;quot;&lt;strong&gt;galleries&amp;quot;&lt;/strong&gt; from document 1 and 2 since they are less important keywords comparatively. How do i do this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kycu60,True,,red-hooded9,,12,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kycu60/how_to_perform_cosine_similarity_when_taking_into/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kycu60/how_to_perform_cosine_similarity_when_taking_into/,30199,1610774842.0,0,,False,,,,,,824
911,,LanguageTechnology,"The headline pretty much says it all:

I'm taking my first NLP course and need some inspiration coming up with ideas for my term project. Broad strokes of the project requirements are to find a research paper involving a particular method or somesuch, and then to implement said somesuch and report various metrics on efficiency/f1/etc.

It'll be a lot more fun if the method and or data are new/unique.

Show me what you got! :D",t2_16vv7i,False,,0,False,First NLP course. Seeking ideas for fun/interesting term project. (Interesting data? Hottest research?),[],r/LanguageTechnology,False,6,,0,,False,t3_ky8bbk,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1610787222.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The headline pretty much says it all:&lt;/p&gt;

&lt;p&gt;I&amp;#39;m taking my first NLP course and need some inspiration coming up with ideas for my term project. Broad strokes of the project requirements are to find a research paper involving a particular method or somesuch, and then to implement said somesuch and report various metrics on efficiency/f1/etc.&lt;/p&gt;

&lt;p&gt;It&amp;#39;ll be a lot more fun if the method and or data are new/unique.&lt;/p&gt;

&lt;p&gt;Show me what you got! :D&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ky8bbk,True,,_Cistern,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ky8bbk/first_nlp_course_seeking_ideas_for_funinteresting/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ky8bbk/first_nlp_course_seeking_ideas_for_funinteresting/,30199,1610758422.0,0,,False,,,,,,429
912,,LanguageTechnology,"For those of you who have started out from a traditional ML/DL background, have you ever felt like knowledge of linguistics is necessary to excel in this field?

While I love NLP as a field of study, I've also started to increasingly wonder how non-linguists can break into NLP, when, after all, NLP is a study of the human language. This also makes me think that recent advances in NLP are being driven by hard-core engineering and resource-intensive computing (huge, billion-parameter models trained on terabytes of datasets) rather than refined linguistic approaches that could potentially yield more meaningful insight into how humans learn and understand language. 

But of course, this is coming from someone who has just started to study NLP, so I'm sure there are finer details that I'm missing from the bigger picture.

Thanks for sharing your opinions and experience!",t2_3pk9l2h7,False,,0,False,Knowledge of linguistics in NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_kxnecz,False,dark,0.98,,public,30,0,{},,False,[],,False,False,,{},,False,30,,False,False,,False,,[],{},,True,,1610715289.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For those of you who have started out from a traditional ML/DL background, have you ever felt like knowledge of linguistics is necessary to excel in this field?&lt;/p&gt;

&lt;p&gt;While I love NLP as a field of study, I&amp;#39;ve also started to increasingly wonder how non-linguists can break into NLP, when, after all, NLP is a study of the human language. This also makes me think that recent advances in NLP are being driven by hard-core engineering and resource-intensive computing (huge, billion-parameter models trained on terabytes of datasets) rather than refined linguistic approaches that could potentially yield more meaningful insight into how humans learn and understand language. &lt;/p&gt;

&lt;p&gt;But of course, this is coming from someone who has just started to study NLP, so I&amp;#39;m sure there are finer details that I&amp;#39;m missing from the bigger picture.&lt;/p&gt;

&lt;p&gt;Thanks for sharing your opinions and experience!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kxnecz,True,,JST99,,23,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kxnecz/knowledge_of_linguistics_in_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kxnecz/knowledge_of_linguistics_in_nlp/,30199,1610686489.0,0,,False,,,,,,877
913,,LanguageTechnology,"I am doing a search NLP engine which takes short query of 6-7 tokens  of  type question or a statement which wants to find a answer from database. Can you guys give me a good starting point to have a good pretrained model for short queries and fine tune different tasks like NER, entity relationship, pos tagging. And also data must be created manually. How Fine-tuning can be done wtih  minimal training data.?",t2_6d53o2nl,False,,0,False,Any good model which can be used for fine-tuning,[],r/LanguageTechnology,False,6,,0,,False,t3_ky0u3p,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1610764885.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am doing a search NLP engine which takes short query of 6-7 tokens  of  type question or a statement which wants to find a answer from database. Can you guys give me a good starting point to have a good pretrained model for short queries and fine tune different tasks like NER, entity relationship, pos tagging. And also data must be created manually. How Fine-tuning can be done wtih  minimal training data.?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ky0u3p,True,,EarthlySapien,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ky0u3p/any_good_model_which_can_be_used_for_finetuning/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ky0u3p/any_good_model_which_can_be_used_for_finetuning/,30199,1610736085.0,0,,False,,,,,,411
914,,LanguageTechnology,Can anyone point me to some literature that would be talking about models where you input a text and it returns bullet point summary? I found a very little number of such. Is this even a thing?,t2_6jod89c8,False,,0,False,Bullet point summarizers?,[],r/LanguageTechnology,False,6,,0,,False,t3_kxx9f0,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1610754860.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can anyone point me to some literature that would be talking about models where you input a text and it returns bullet point summary? I found a very little number of such. Is this even a thing?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kxx9f0,True,,samiec_Xi,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kxx9f0/bullet_point_summarizers/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kxx9f0/bullet_point_summarizers/,30199,1610726060.0,0,,False,,,,,,193
915,,LanguageTechnology,"Can anyone point to NLP models/approaches to do fine-grained POS tagging of Nouns. I'm especially interested to distinguish if a Noun is Concrete or Abstract.

[https://www.grammarly.com/blog/concrete-vs-abstract-nouns/](https://www.grammarly.com/blog/concrete-vs-abstract-nouns/)

A concrete noun is a noun that can be identified through one of the five senses (taste, touch, sight, hearing, or smell).  

 An abstract noun is a noun that cannot be perceived using one of the five senses (i.e., taste, touch, sight, hearing, smelling).",t2_a913f4q,False,,0,False,Are there any models to do fine-grained POS tagging for Nouns (Concrete noun and Abstract noun )?,[],r/LanguageTechnology,False,6,,0,,False,t3_ky0sxt,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1610764792.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can anyone point to NLP models/approaches to do fine-grained POS tagging of Nouns. I&amp;#39;m especially interested to distinguish if a Noun is Concrete or Abstract.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.grammarly.com/blog/concrete-vs-abstract-nouns/""&gt;https://www.grammarly.com/blog/concrete-vs-abstract-nouns/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A concrete noun is a noun that can be identified through one of the five senses (taste, touch, sight, hearing, or smell).  &lt;/p&gt;

&lt;p&gt;An abstract noun is a noun that cannot be perceived using one of the five senses (i.e., taste, touch, sight, hearing, smelling).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ky0sxt,True,,ukpredd,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ky0sxt/are_there_any_models_to_do_finegrained_pos/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ky0sxt/are_there_any_models_to_do_finegrained_pos/,30199,1610735992.0,0,,False,,,,,,536
916,,LanguageTechnology,"Any shared tasks in the year 2021 for NLP happening soon. 

Anyone needs collaborator for shared task. ?",t2_xf374,False,,0,False,Upcoming shared tasks in NLP 2021,[],r/LanguageTechnology,False,6,,0,,False,t3_kxqwdn,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1610730481.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Any shared tasks in the year 2021 for NLP happening soon. &lt;/p&gt;

&lt;p&gt;Anyone needs collaborator for shared task. ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kxqwdn,True,,thak123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kxqwdn/upcoming_shared_tasks_in_nlp_2021/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kxqwdn/upcoming_shared_tasks_in_nlp_2021/,30199,1610701681.0,0,,False,,,,,,104
917,,LanguageTechnology,"Hi, here's one very simple question.

Can you guys think of any case where ELMo can be a clearly better design choice than BERT/Transformer family? I tried, but I couldn't think of any case.",t2_8w4c3x9p,False,,0,False,Is there any case ELMo can be a better choice than BERT/Transformer?,[],r/LanguageTechnology,False,6,,0,,False,t3_kxnytw,False,dark,0.9,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1610717461.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, here&amp;#39;s one very simple question.&lt;/p&gt;

&lt;p&gt;Can you guys think of any case where ELMo can be a clearly better design choice than BERT/Transformer family? I tried, but I couldn&amp;#39;t think of any case.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kxnytw,True,,No_Birthday9962,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kxnytw/is_there_any_case_elmo_can_be_a_better_choice/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kxnytw/is_there_any_case_elmo_can_be_a_better_choice/,30199,1610688661.0,0,,False,,,,,,190
918,,LanguageTechnology,"Hey,
I was looking for some good papers on unsupervised multi-document summarisation ?
Any recommendations?

Thanks",t2_hkv9s,False,,0,False,Resource on Multi document summarisation,[],r/LanguageTechnology,False,6,,0,,False,t3_kxsrcq,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1610738977.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey,
I was looking for some good papers on unsupervised multi-document summarisation ?
Any recommendations?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kxsrcq,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kxsrcq/resource_on_multi_document_summarisation/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kxsrcq/resource_on_multi_document_summarisation/,30199,1610710177.0,0,,False,,,,,,115
919,,LanguageTechnology,"Hello everyone! I've posted here a couple of times and you guys helped me a lot. The last time, I tried to extract NOUN and ADJECTIVE together, but the output had a lot of errors. Now I want to try a different approach, to first identify the topic of a sentence and then extract the adjective, like this: 

&amp;#x200B;

The teacher is calm and intelligent.

The material is bad.

I want an output that is like:

{teacher calm}, {teacher intelligent} and {material bad}

Can anyone help me? 

Also, if anyone has a different idea about how to solve this problem in a better way, I'll really appreciate your reply too!  


Thanks in advance!   
 

#",t2_tnl4h5c,False,,0,False,How can I extract the topic of a sentence and then the adjective related to it in spacy?,[],r/LanguageTechnology,False,6,,0,,False,t3_kxo2ga,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1610717860.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone! I&amp;#39;ve posted here a couple of times and you guys helped me a lot. The last time, I tried to extract NOUN and ADJECTIVE together, but the output had a lot of errors. Now I want to try a different approach, to first identify the topic of a sentence and then extract the adjective, like this: &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The teacher is calm and intelligent.&lt;/p&gt;

&lt;p&gt;The material is bad.&lt;/p&gt;

&lt;p&gt;I want an output that is like:&lt;/p&gt;

&lt;p&gt;{teacher calm}, {teacher intelligent} and {material bad}&lt;/p&gt;

&lt;p&gt;Can anyone help me? &lt;/p&gt;

&lt;p&gt;Also, if anyone has a different idea about how to solve this problem in a better way, I&amp;#39;ll really appreciate your reply too!  &lt;/p&gt;

&lt;p&gt;Thanks in advance!   &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kxo2ga,True,,IWantAGoodBattery,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kxo2ga/how_can_i_extract_the_topic_of_a_sentence_and/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kxo2ga/how_can_i_extract_the_topic_of_a_sentence_and/,30199,1610689060.0,0,,False,,,,,,648
920,,LanguageTechnology,"Maybe this is stemming from my not-so-great grasp of supervised vs. unsupervised learning, but my understanding is that if we have access to ground-truth labels then it's supervised learning and if not then it's unsupervised.

I'll take the masked language modeling (MLM) that BERT ([Devlin et al., 2019](https://www.aclweb.org/anthology/N19-1423/)) and many other subsequent language models use.

According to the original paper:

&gt; ...we simply mask some percentage of the input tokens at random, and then predict those masked tokens... In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM.

If we just replace a certain percentage of tokens with `[MASK]` randomly, don't we technically have access to the ground-truth labels (i.e., the original unmasked tokens)? Shouldn't this be considered supervised learning?

My argument is analogous for the next sentence prediction (NSP) task.",t2_m8kccne,False,,0,False,Why are language modeling pre-training objectives considered unsupervised?,[],r/LanguageTechnology,False,6,,0,,False,t3_kxpmnn,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1610724497.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Maybe this is stemming from my not-so-great grasp of supervised vs. unsupervised learning, but my understanding is that if we have access to ground-truth labels then it&amp;#39;s supervised learning and if not then it&amp;#39;s unsupervised.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ll take the masked language modeling (MLM) that BERT (&lt;a href=""https://www.aclweb.org/anthology/N19-1423/""&gt;Devlin et al., 2019&lt;/a&gt;) and many other subsequent language models use.&lt;/p&gt;

&lt;p&gt;According to the original paper:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;...we simply mask some percentage of the input tokens at random, and then predict those masked tokens... In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If we just replace a certain percentage of tokens with &lt;code&gt;[MASK]&lt;/code&gt; randomly, don&amp;#39;t we technically have access to the ground-truth labels (i.e., the original unmasked tokens)? Shouldn&amp;#39;t this be considered supervised learning?&lt;/p&gt;

&lt;p&gt;My argument is analogous for the next sentence prediction (NSP) task.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kxpmnn,True,,Seankala,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kxpmnn/why_are_language_modeling_pretraining_objectives/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kxpmnn/why_are_language_modeling_pretraining_objectives/,30199,1610695697.0,0,,False,,,,,,980
921,,LanguageTechnology,,t2_cu8vpox,False,,0,False,"Introduction to chatbots: what, why and how?[chatbot creation using dialogflow]",[],r/LanguageTechnology,False,6,,0,,False,t3_kxphom,False,dark,0.33,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,False,,1610723848.0,text,6,,,text,shyambhu20.blogspot.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kxphom,True,,shyamcody,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kxphom/introduction_to_chatbots_what_why_and_howchatbot/,all_ads,False,https://shyambhu20.blogspot.com/2020/12/introduction-to-chatbots-what-why-and.html,30199,1610695048.0,0,,False,https://shyambhu20.blogspot.com/2020/12/introduction-to-chatbots-what-why-and.html,,,,,0
922,,LanguageTechnology,"As I currently want to self-study NLP (and information retrieval) I have a few questions and I hope that someone can provide me some insight on them :) .

1) If I want to train a learning-to-rank model with imperfect labels, which loss would be better to choose: point-wise or pair-wise? If I choose a pair-wise loss, will it be computationally more expensive, but less prone to overfitting (and learning the exact non-machine learning) ranking function?

2) Does the term-at-a-time algorithm need to parse the entire posting list of at least one query term?

3) Can the Cranfield paradigm (about the testing datasets) be used for the evaluation of an e-mail search system?

4) Does stemming help or hurt the ranking algorithm for the retrieval system of the entity knowledge graph?

This is the things I could not understand from reading the book about it (and searching it later online), so thank you in advance :)",t2_84nwwiam,False,,0,False,Does anyone maybe know (or has an insight) into the things I could not understand from reading the NLP book?,[],r/LanguageTechnology,False,6,,0,,False,t3_kxjizd,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1610701739.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;As I currently want to self-study NLP (and information retrieval) I have a few questions and I hope that someone can provide me some insight on them :) .&lt;/p&gt;

&lt;p&gt;1) If I want to train a learning-to-rank model with imperfect labels, which loss would be better to choose: point-wise or pair-wise? If I choose a pair-wise loss, will it be computationally more expensive, but less prone to overfitting (and learning the exact non-machine learning) ranking function?&lt;/p&gt;

&lt;p&gt;2) Does the term-at-a-time algorithm need to parse the entire posting list of at least one query term?&lt;/p&gt;

&lt;p&gt;3) Can the Cranfield paradigm (about the testing datasets) be used for the evaluation of an e-mail search system?&lt;/p&gt;

&lt;p&gt;4) Does stemming help or hurt the ranking algorithm for the retrieval system of the entity knowledge graph?&lt;/p&gt;

&lt;p&gt;This is the things I could not understand from reading the book about it (and searching it later online), so thank you in advance :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kxjizd,True,,Signal-University-64,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kxjizd/does_anyone_maybe_know_or_has_an_insight_into_the/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kxjizd/does_anyone_maybe_know_or_has_an_insight_into_the/,30199,1610672939.0,0,,False,,,,,,916
923,,LanguageTechnology,,t2_5alofkdd,False,,0,False,"Article: ""Google’s new trillion-parameter AI language model is almost 6 times bigger than GPT-3""",[],r/LanguageTechnology,False,6,,0,,False,t3_kww3ne,False,dark,1.0,,public,44,0,{},,False,[],,False,False,,{},,False,44,,False,False,,False,,[],{},,False,,1610619722.0,text,6,,,text,self.GPT3,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kww3ne,True,,Wiskkey,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kww3ne/article_googles_new_trillionparameter_ai_language/,all_ads,False,/r/GPT3/comments/kwud6k/article_googles_new_trillionparameter_ai_language/,30199,1610590922.0,0,,False,/r/GPT3/comments/kwud6k/article_googles_new_trillionparameter_ai_language/,"[{'approved_at_utc': None, 'subreddit': 'GPT3', 'selftext': '(I realize that this subreddit is devoted to GPT-3, but I thought that some folks might be interested in this because it\'s the first language model that I know about with more parameters than GPT-3. The two language models aren\'t ""apples to apples"" in the sense that Google\'s new language model uses a [mixture of experts](https://en.wikipedia.org/wiki/Mixture_of_experts) while GPT-3 does not.)\n\n[https://thenextweb.com/neural/2021/01/13/googles-new-trillion-parameter-ai-language-model-is-almost-6-times-bigger-than-gpt-3/](https://thenextweb.com/neural/2021/01/13/googles-new-trillion-parameter-ai-language-model-is-almost-6-times-bigger-than-gpt-3/)\n\n&gt;A trio of researchers from the Google Brain team recently unveiled the next big thing in AI language models: a massive one trillion-parameter transformer system.  \n&gt;  \n&gt;The next biggest model out there, as far as we’re aware, is OpenAI’s GPT-3, which uses a measly 175 billion parameters.\n\nThe largest model in the paper is actually 1.6 trillion parameters, which is about 9.1 times more parameters than GPT-3\'s largest model containing 175 billion parameters.\n\nAnother article: [Google trained a trillion-parameter AI language model](https://venturebeat.com/2021/01/12/google-trained-a-trillion-parameter-ai-language-model/).\n\nSome other Reddit posts covering the same topic:\n\n[https://www.reddit.com/r/MachineLearning/comments/kvlk1j/r\\_switch\\_transformers\\_scaling\\_to\\_trillion/](https://www.reddit.com/r/MachineLearning/comments/kvlk1j/r_switch_transformers_scaling_to_trillion/)\n\n[https://www.reddit.com/r/singularity/comments/kvyc5c/google\\_trained\\_a\\_trillionparameter\\_ai\\_language/](https://www.reddit.com/r/singularity/comments/kvyc5c/google_trained_a_trillionparameter_ai_language/)', 'author_fullname': 't2_5alofkdd', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Article: ""Google’s new trillion-parameter AI language model is almost 6 times bigger than GPT-3""', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/GPT3', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_kwud6k', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.65, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 28, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 28, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': 1610586270.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1610613871.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.GPT3', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;(I realize that this subreddit is devoted to GPT-3, but I thought that some folks might be interested in this because it&amp;#39;s the first language model that I know about with more parameters than GPT-3. The two language models aren&amp;#39;t &amp;quot;apples to apples&amp;quot; in the sense that Google&amp;#39;s new language model uses a &lt;a href=""https://en.wikipedia.org/wiki/Mixture_of_experts""&gt;mixture of experts&lt;/a&gt; while GPT-3 does not.)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://thenextweb.com/neural/2021/01/13/googles-new-trillion-parameter-ai-language-model-is-almost-6-times-bigger-than-gpt-3/""&gt;https://thenextweb.com/neural/2021/01/13/googles-new-trillion-parameter-ai-language-model-is-almost-6-times-bigger-than-gpt-3/&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;A trio of researchers from the Google Brain team recently unveiled the next big thing in AI language models: a massive one trillion-parameter transformer system.  &lt;/p&gt;\n\n&lt;p&gt;The next biggest model out there, as far as we’re aware, is OpenAI’s GPT-3, which uses a measly 175 billion parameters.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;The largest model in the paper is actually 1.6 trillion parameters, which is about 9.1 times more parameters than GPT-3&amp;#39;s largest model containing 175 billion parameters.&lt;/p&gt;\n\n&lt;p&gt;Another article: &lt;a href=""https://venturebeat.com/2021/01/12/google-trained-a-trillion-parameter-ai-language-model/""&gt;Google trained a trillion-parameter AI language model&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Some other Reddit posts covering the same topic:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.reddit.com/r/MachineLearning/comments/kvlk1j/r_switch_transformers_scaling_to_trillion/""&gt;https://www.reddit.com/r/MachineLearning/comments/kvlk1j/r_switch_transformers_scaling_to_trillion/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.reddit.com/r/singularity/comments/kvyc5c/google_trained_a_trillionparameter_ai_language/""&gt;https://www.reddit.com/r/singularity/comments/kvyc5c/google_trained_a_trillionparameter_ai_language/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2vkqv0', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'kwud6k', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Wiskkey', 'discussion_type': None, 'num_comments': 29, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/GPT3/comments/kwud6k/article_googles_new_trillionparameter_ai_language/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/GPT3/comments/kwud6k/article_googles_new_trillionparameter_ai_language/', 'subreddit_subscribers': 7314, 'created_utc': 1610585071.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_kwud6k,,,0
924,,LanguageTechnology,,t2_6zjod2ey,False,,0,False,Folks’Talks video game. Graphic Research Interface. Explanatory note for current temporary interface.,[],r/LanguageTechnology,False,6,,0,,False,t3_kx19kw,False,dark,0.33,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,False,,1610639658.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kx19kw,True,,FolksTalksGame,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kx19kw/folkstalks_video_game_graphic_research_interface/,all_ads,False,/r/learnmachinelearning/comments/kx0nu1/folkstalks_video_game_graphic_research_interface/,30199,1610610858.0,0,,False,/r/learnmachinelearning/comments/kx0nu1/folkstalks_video_game_graphic_research_interface/,"[{'approved_at_utc': None, 'subreddit': 'learnmachinelearning', 'selftext': '[https://www.linkedin.com/posts/chaim-ash-b874504\\_language-machinelearning-ai-activity-6755363720454557696-sZpl](https://www.linkedin.com/posts/chaim-ash-b874504_language-machinelearning-ai-activity-6755363720454557696-sZpl)', 'author_fullname': 't2_6zjod2ey', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Folks’Talks video game. Graphic Research Interface. Explanatory note for current temporary interface.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/learnmachinelearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_kx0nu1', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1610636823.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.learnmachinelearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.linkedin.com/posts/chaim-ash-b874504_language-machinelearning-ai-activity-6755363720454557696-sZpl""&gt;https://www.linkedin.com/posts/chaim-ash-b874504_language-machinelearning-ai-activity-6755363720454557696-sZpl&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_3cqa1', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'kx0nu1', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'FolksTalksGame', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/learnmachinelearning/comments/kx0nu1/folkstalks_video_game_graphic_research_interface/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/learnmachinelearning/comments/kx0nu1/folkstalks_video_game_graphic_research_interface/', 'subreddit_subscribers': 232286, 'created_utc': 1610608023.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_kx0nu1,,,0
925,,LanguageTechnology,"Hello, I am dabbing in NLP transformers, specifically the [Marian MT model released by HuggingFace](https://huggingface.co/transformers/model_doc/marian.html).

I adapted the tutorial example and it does work, but it is super slow. Could you please advise if I am using it correctly? Please note that I wish to use it to translate individual words from a list, not a whole text.


Here is the code:

    from transformers import MarianTokenizer, MarianMTModel
    from typing import List
    mt_model = MarianMTModel.from_pretrained('mt_model/opus-mt-en-es')
    mt_tok = MarianTokenizer.from_pretrained('mt_model/opus-mt-en-es')
    
    def translate(word, model, tok):
        batch = tok.prepare_seq2seq_batch(src_texts=[word], 
                                          return_tensors=""pt"")  
        gen = model.generate(**batch)
        translated_word: List[str] = tok.batch_decode(gen, skip_special_tokens=True)
        return ' '.join(translated_word)",t2_307ui9gq,False,,0,False,Could you please advice if I am using this Marian MT transformer correctly? It runs way too slow.,[],r/LanguageTechnology,False,6,,0,,False,t3_kwuznq,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1610615962.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I am dabbing in NLP transformers, specifically the &lt;a href=""https://huggingface.co/transformers/model_doc/marian.html""&gt;Marian MT model released by HuggingFace&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I adapted the tutorial example and it does work, but it is super slow. Could you please advise if I am using it correctly? Please note that I wish to use it to translate individual words from a list, not a whole text.&lt;/p&gt;

&lt;p&gt;Here is the code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from transformers import MarianTokenizer, MarianMTModel
from typing import List
mt_model = MarianMTModel.from_pretrained(&amp;#39;mt_model/opus-mt-en-es&amp;#39;)
mt_tok = MarianTokenizer.from_pretrained(&amp;#39;mt_model/opus-mt-en-es&amp;#39;)

def translate(word, model, tok):
    batch = tok.prepare_seq2seq_batch(src_texts=[word], 
                                      return_tensors=&amp;quot;pt&amp;quot;)  
    gen = model.generate(**batch)
    translated_word: List[str] = tok.batch_decode(gen, skip_special_tokens=True)
    return &amp;#39; &amp;#39;.join(translated_word)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kwuznq,True,,serioreditor,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kwuznq/could_you_please_advice_if_i_am_using_this_marian/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kwuznq/could_you_please_advice_if_i_am_using_this_marian/,30199,1610587162.0,1,,False,,,,,,961
926,,LanguageTechnology,,t2_trjf2,False,,0,False,Commonsense Reasoning for Natural Language Processing - Vered Shwartz,[],r/LanguageTechnology,False,6,,0,,False,t3_kwnd6u,False,dark,1.0,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,False,,1610593544.0,text,6,,,text,veredshwartz.blogspot.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kwnd6u,True,,adammathias,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kwnd6u/commonsense_reasoning_for_natural_language/,all_ads,False,http://veredshwartz.blogspot.com/2021/01/commonsense-reasoning-for-natural.html,30199,1610564744.0,0,,False,http://veredshwartz.blogspot.com/2021/01/commonsense-reasoning-for-natural.html,,,,,0
927,,LanguageTechnology,"Would love some help with a bit of an odd problem I’m trying to solve… How does one measure the “fullness” of a sentence?

Ex:
Thank you for reaching out about the issue at school
- will score higher than 
Thank you for reaching out

As the first sentence provides more detail, it should score higher. How can I measure the fullness/detail of a sentence?",t2_8xraz5pp,False,,0,False,Measuring Fullness of a Sentence,[],r/LanguageTechnology,False,6,,0,,False,t3_kwj9np,False,dark,1.0,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1610582234.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Would love some help with a bit of an odd problem I’m trying to solve… How does one measure the “fullness” of a sentence?&lt;/p&gt;

&lt;p&gt;Ex:
Thank you for reaching out about the issue at school
- will score higher than 
Thank you for reaching out&lt;/p&gt;

&lt;p&gt;As the first sentence provides more detail, it should score higher. How can I measure the fullness/detail of a sentence?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kwj9np,True,,Effective_Sea_9367,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kwj9np/measuring_fullness_of_a_sentence/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kwj9np/measuring_fullness_of_a_sentence/,30199,1610553434.0,0,,False,,,,,,354
928,,LanguageTechnology,"For anyone into language tech, my colleague is hosting a free event, thought I would share here [https://us02web.zoom.us/webinar/register/1916105631007/WN\_ReYW3SRKRXeSQtuxiya1ng](https://us02web.zoom.us/webinar/register/1916105631007/WN_ReYW3SRKRXeSQtuxiya1ng)",t2_5w21fvbc,False,,0,False,Translation management systems,[],r/LanguageTechnology,False,6,,0,,False,t3_kwmt5r,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1610591991.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For anyone into language tech, my colleague is hosting a free event, thought I would share here &lt;a href=""https://us02web.zoom.us/webinar/register/1916105631007/WN_ReYW3SRKRXeSQtuxiya1ng""&gt;https://us02web.zoom.us/webinar/register/1916105631007/WN_ReYW3SRKRXeSQtuxiya1ng&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kwmt5r,True,,LanguageNurd,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kwmt5r/translation_management_systems/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kwmt5r/translation_management_systems/,30199,1610563191.0,0,,False,,,,,,261
929,,LanguageTechnology,I have been searching for papers on distant supervision for named-entity extraction on Google scholar. Does anyone have some suggestions for high impact papers that worked on distant supervision in named-entity extraction?,t2_9ptho1m,False,,0,False,Great papers for distantly supervised learning?,[],r/LanguageTechnology,False,6,,0,,False,t3_kwgx1f,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1610575140.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have been searching for papers on distant supervision for named-entity extraction on Google scholar. Does anyone have some suggestions for high impact papers that worked on distant supervision in named-entity extraction?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kwgx1f,True,,freaky_eater,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kwgx1f/great_papers_for_distantly_supervised_learning/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kwgx1f/great_papers_for_distantly_supervised_learning/,30199,1610546340.0,0,,False,,,,,,222
930,,LanguageTechnology,"I'm looking for recommendations for a semantic search system that can score thousands of text snippets based on their relevance to a user's question.

`INPUT:`  
`- A question, in natural language English.`  
`- A corpus of thousands of text snippets.`

`OUTPUT:`  
`A score for each of the text snippets based on how relevant it is to the question.`

It should not just be keyword based, but based on meaning / closeness of concept. For example...

`QUESTION: ""Why is LIDAR unneccesary for self driving cars?""`  
`SNIPPET: ""SpaceX uses LIDAR for docking to the ISS"".`

\-&gt; The snippet should get a lower relevance score, because the snippet is about the same subject (LIDAR), but in a different context (space vs. automotive).

It should also be smart enough to rank snippets that use different words for the same concept.

`QUESTION: ""Why is LIDAR unneccesary for self driving cars?""`  
`SNIPPET: ""Tesla's FSD uses cameras to create a 3D vector space representation of the surroundings, which makes costly sensors such as LIDAR unnecessary.""`

\-&gt; The snippet should get a higher relevance score, because the words ""Tesla"" and ""FSD"" in the snippet are semantically linked to the words ""self driving cars"" in the question.

What would you say is the best tool to accomplish this today?",t2_31gcpb79,False,,0,False,Recommendations for Semantic Search,[],r/LanguageTechnology,False,6,,0,,False,t3_kwcfip,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1610555597.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m looking for recommendations for a semantic search system that can score thousands of text snippets based on their relevance to a user&amp;#39;s question.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;INPUT:&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;- A question, in natural language English.&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;- A corpus of thousands of text snippets.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;OUTPUT:&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;A score for each of the text snippets based on how relevant it is to the question.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;It should not just be keyword based, but based on meaning / closeness of concept. For example...&lt;/p&gt;

&lt;p&gt;&lt;code&gt;QUESTION: &amp;quot;Why is LIDAR unneccesary for self driving cars?&amp;quot;&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;SNIPPET: &amp;quot;SpaceX uses LIDAR for docking to the ISS&amp;quot;.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;-&amp;gt; The snippet should get a lower relevance score, because the snippet is about the same subject (LIDAR), but in a different context (space vs. automotive).&lt;/p&gt;

&lt;p&gt;It should also be smart enough to rank snippets that use different words for the same concept.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;QUESTION: &amp;quot;Why is LIDAR unneccesary for self driving cars?&amp;quot;&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;SNIPPET: &amp;quot;Tesla&amp;#39;s FSD uses cameras to create a 3D vector space representation of the surroundings, which makes costly sensors such as LIDAR unnecessary.&amp;quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;-&amp;gt; The snippet should get a higher relevance score, because the words &amp;quot;Tesla&amp;quot; and &amp;quot;FSD&amp;quot; in the snippet are semantically linked to the words &amp;quot;self driving cars&amp;quot; in the question.&lt;/p&gt;

&lt;p&gt;What would you say is the best tool to accomplish this today?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kwcfip,True,,leoplusx,,21,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kwcfip/recommendations_for_semantic_search/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kwcfip/recommendations_for_semantic_search/,30199,1610526797.0,0,,False,,,,,,1292
931,,LanguageTechnology,"Hi everyone,

We just released our lightweight transformer-based NLP toolkit named **Trankit.**

**Our toolkit outperforms the current state-of-the-art Stanford NLP (Stanza)** in many tasks such as sentence segmentation, part-of-speech tagging, and dependency parsing **over** **56 different languages**. Detailed comparison can be found [here](https://trankit.readthedocs.io/en/latest/performance.html#universal-dependencies-v2-5).

For example, for **English**, **Trankit is significantly better than Stanford NLP (Stanza)** on sentence segmentation (**+7.22%**) and dependency parsing (**+3.92%** for UAS and **+4.37%** for LAS). For **Arabic**, our toolkit substantially improves sentence segmentation performance by **16.16%** while **Chinese** observes **12.31%** and **12.72%** improvement of UAS and LAS for dependency parsing.

**Trankit is written in Python and can be easily installed via pip**. Our code and pretrained models are publicly available at: [https://github.com/nlp-uoregon/trankit](https://github.com/nlp-uoregon/trankit)

We also created a documentation page and a demo website for Trankit.Documentation page: [https://trankit.readthedocs.io/en/latest/index.html](https://trankit.readthedocs.io/en/latest/index.html)

Demo website: [http://nlp.uoregon.edu/trankit](http://nlp.uoregon.edu/trankit)

Technical details about Trankit can be found in our paper: [https://arxiv.org/pdf/2101.03289.pdf](https://arxiv.org/pdf/2101.03289.pdf)

Thank you!",t2_7suf1otj,False,,0,False,Our new state-of-the-art multilingual NLP Toolkit - Trankit has been released,[],r/LanguageTechnology,False,6,,0,,False,t3_kvyvfu,False,dark,0.97,,public,47,0,{},,False,[],,False,False,,{},,False,47,,False,False,,1617229506.0,,[],{},,True,,1610508997.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;We just released our lightweight transformer-based NLP toolkit named &lt;strong&gt;Trankit.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Our toolkit outperforms the current state-of-the-art Stanford NLP (Stanza)&lt;/strong&gt; in many tasks such as sentence segmentation, part-of-speech tagging, and dependency parsing &lt;strong&gt;over&lt;/strong&gt; &lt;strong&gt;56 different languages&lt;/strong&gt;. Detailed comparison can be found &lt;a href=""https://trankit.readthedocs.io/en/latest/performance.html#universal-dependencies-v2-5""&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For example, for &lt;strong&gt;English&lt;/strong&gt;, &lt;strong&gt;Trankit is significantly better than Stanford NLP (Stanza)&lt;/strong&gt; on sentence segmentation (&lt;strong&gt;+7.22%&lt;/strong&gt;) and dependency parsing (&lt;strong&gt;+3.92%&lt;/strong&gt; for UAS and &lt;strong&gt;+4.37%&lt;/strong&gt; for LAS). For &lt;strong&gt;Arabic&lt;/strong&gt;, our toolkit substantially improves sentence segmentation performance by &lt;strong&gt;16.16%&lt;/strong&gt; while &lt;strong&gt;Chinese&lt;/strong&gt; observes &lt;strong&gt;12.31%&lt;/strong&gt; and &lt;strong&gt;12.72%&lt;/strong&gt; improvement of UAS and LAS for dependency parsing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Trankit is written in Python and can be easily installed via pip&lt;/strong&gt;. Our code and pretrained models are publicly available at: &lt;a href=""https://github.com/nlp-uoregon/trankit""&gt;https://github.com/nlp-uoregon/trankit&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We also created a documentation page and a demo website for Trankit.Documentation page: &lt;a href=""https://trankit.readthedocs.io/en/latest/index.html""&gt;https://trankit.readthedocs.io/en/latest/index.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Demo website: &lt;a href=""http://nlp.uoregon.edu/trankit""&gt;http://nlp.uoregon.edu/trankit&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Technical details about Trankit can be found in our paper: &lt;a href=""https://arxiv.org/pdf/2101.03289.pdf""&gt;https://arxiv.org/pdf/2101.03289.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kvyvfu,True,,mgl96,,18,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kvyvfu/our_new_stateoftheart_multilingual_nlp_toolkit/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kvyvfu/our_new_stateoftheart_multilingual_nlp_toolkit/,30199,1610480197.0,0,,False,,,,,,1470
932,,LanguageTechnology,"Hi guys. I'm wondering if there is any work done in the NLP field regarding neural summarization where the model is given the constraint to contain a certain subject and object.

For example, if we're given a document about Elon Musk and Tesla, I'm wondering if the model can be constrained to output a summarization in the form of, for example, ""Elon Musk is the founder of Tesla.""

Any tips are appreciated. Thanks.",t2_m8kccne,False,,0,False,"Is there any work in NLP where a neural summarization model is ""forced"" to contain a certain subject and object?",[],r/LanguageTechnology,False,6,,0,,False,t3_kwafut,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1610546529.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys. I&amp;#39;m wondering if there is any work done in the NLP field regarding neural summarization where the model is given the constraint to contain a certain subject and object.&lt;/p&gt;

&lt;p&gt;For example, if we&amp;#39;re given a document about Elon Musk and Tesla, I&amp;#39;m wondering if the model can be constrained to output a summarization in the form of, for example, &amp;quot;Elon Musk is the founder of Tesla.&amp;quot;&lt;/p&gt;

&lt;p&gt;Any tips are appreciated. Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kwafut,True,,Seankala,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kwafut/is_there_any_work_in_nlp_where_a_neural/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kwafut/is_there_any_work_in_nlp_where_a_neural/,30199,1610517729.0,0,,False,,,,,,417
933,,LanguageTechnology,"Hi all

I am looking to fine tune a deep learning model like BERT to classify news sentiment as either negative, neutral or positive. I can find many binary classification datasets used as performance measures by state of the art models, and even [papers where their scores are compared](https://arxiv.org/pdf/2004.03705.pdf),  but i can't seem to find a dataset that is used by  state of the art models to compare 3 class sentiment analysis of positive negative and neutral.

How should i approach this? Should i look at performance metrics for binary classification tasks and make an assumption that the ones that perform best at binary classification will also perform best at 3-class classification? Or should i look at model performances with multi-class labels (which often have 15-30 labels) and use that as a guideline?

Are there any leaderboards that contain rankings of the best models for 3-label negative, neutral and positive sentiment? i can't seem to find any.

My aim is to find good at this task in general, and then fine tune it classifying sentiment of financial news headlines",t2_aao51,False,,0,False,Multiclass dataset for sentiment analysis?,[],r/LanguageTechnology,False,6,,0,,False,t3_kwcay2,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,1610527245.0,,[],{},,True,,1610554969.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all&lt;/p&gt;

&lt;p&gt;I am looking to fine tune a deep learning model like BERT to classify news sentiment as either negative, neutral or positive. I can find many binary classification datasets used as performance measures by state of the art models, and even &lt;a href=""https://arxiv.org/pdf/2004.03705.pdf""&gt;papers where their scores are compared&lt;/a&gt;,  but i can&amp;#39;t seem to find a dataset that is used by  state of the art models to compare 3 class sentiment analysis of positive negative and neutral.&lt;/p&gt;

&lt;p&gt;How should i approach this? Should i look at performance metrics for binary classification tasks and make an assumption that the ones that perform best at binary classification will also perform best at 3-class classification? Or should i look at model performances with multi-class labels (which often have 15-30 labels) and use that as a guideline?&lt;/p&gt;

&lt;p&gt;Are there any leaderboards that contain rankings of the best models for 3-label negative, neutral and positive sentiment? i can&amp;#39;t seem to find any.&lt;/p&gt;

&lt;p&gt;My aim is to find good at this task in general, and then fine tune it classifying sentiment of financial news headlines&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kwcay2,True,,rpatel9,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kwcay2/multiclass_dataset_for_sentiment_analysis/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kwcay2/multiclass_dataset_for_sentiment_analysis/,30199,1610526169.0,0,,False,,,,,,1097
934,,LanguageTechnology,How to extract information from sentences which is present in one excel file and compare it with 700 standard sentences  with unique codes  which is present in another Excel sheet and assign the sentence with that particular unique code of the sentence to which it is almost similar ?,t2_3i23opwh,False,,0,False,Information extraction and comparison,[],r/LanguageTechnology,False,6,,0,,False,t3_kwefqq,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1610565420.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How to extract information from sentences which is present in one excel file and compare it with 700 standard sentences  with unique codes  which is present in another Excel sheet and assign the sentence with that particular unique code of the sentence to which it is almost similar ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kwefqq,True,,rushilk24,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kwefqq/information_extraction_and_comparison/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kwefqq/information_extraction_and_comparison/,30199,1610536620.0,0,,False,,,,,,284
935,,LanguageTechnology,"Hello,

I am wondering if there are algorithms out there for attributing syllables of a romaji/furigana/pronunciation to the different Kanji within a word?

eg  東京 (トウキョウ) -&gt; 東 (トウ) 京 (キョウ)

Maybe a lattice-based approach? I implemented an algorithm using a form of A\* search but I was wondering if there were better algorithms that I could look at.

Thanks!",t2_6ux4p,False,,0,False,Pronunciation Attribution for Japanese Kanji,[],r/LanguageTechnology,False,6,,0,,False,t3_kwa7p2,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,1610560846.0,,[],{},,True,,1610545564.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am wondering if there are algorithms out there for attributing syllables of a romaji/furigana/pronunciation to the different Kanji within a word?&lt;/p&gt;

&lt;p&gt;eg  東京 (トウキョウ) -&amp;gt; 東 (トウ) 京 (キョウ)&lt;/p&gt;

&lt;p&gt;Maybe a lattice-based approach? I implemented an algorithm using a form of A* search but I was wondering if there were better algorithms that I could look at.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kwa7p2,True,,bubushkinator,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kwa7p2/pronunciation_attribution_for_japanese_kanji/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kwa7p2/pronunciation_attribution_for_japanese_kanji/,30199,1610516764.0,0,,False,,,,,,362
936,,LanguageTechnology,"Hi, I am currently working on a problem to order/rank help documents in the most useful way. The ranking criteria is a bit complex.   
The criteria should be based on :  
1. how useful it was in the past 

2. how popular (or trending) it is amongst the users 

3. also how much information(relative to other documents in the list) does the document have. 

 As of now I was thinking I could use some weighted window method to compute the popularity and trending aspect based on click through rate or likes/dislikes the documents get over the window of time.   
I wanted to understand how I could address the information aspect of the criteria i.e what methods would I use to compare the documents based on the amount of information they would have to rank accordingly.   
I would also appreciate ideas that would help me combine scores from both these aspects to get to a final score to rank the documents.",t2_69smcoa0,False,,0,False,[D] Help Document Ordering,[],r/LanguageTechnology,False,6,,0,,False,t3_kw5wpy,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1610530409.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I am currently working on a problem to order/rank help documents in the most useful way. The ranking criteria is a bit complex.&lt;br/&gt;
The criteria should be based on :&lt;br/&gt;
1. how useful it was in the past &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;how popular (or trending) it is amongst the users &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;also how much information(relative to other documents in the list) does the document have. &lt;/p&gt;

&lt;p&gt;As of now I was thinking I could use some weighted window method to compute the popularity and trending aspect based on click through rate or likes/dislikes the documents get over the window of time.&lt;br/&gt;
I wanted to understand how I could address the information aspect of the criteria i.e what methods would I use to compare the documents based on the amount of information they would have to rank accordingly.&lt;br/&gt;
I would also appreciate ideas that would help me combine scores from both these aspects to get to a final score to rank the documents.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kw5wpy,True,,Yipyip34,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kw5wpy/d_help_document_ordering/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kw5wpy/d_help_document_ordering/,30199,1610501609.0,0,,False,,,,,,906
937,,LanguageTechnology,"I know that property paths in SPARQL can aid you with that. However, you need to be really specific with your nodes and relationships in between two entity nodes that mark the start and the end of the path, if you want to extract them. I am searching for a arbitrary way to do this.

What i want is basically:

Input: start entity, amount of max hops for the path (f.e. 4), RDF knowledge graph

Extract all paths with hops &lt;= 4 beginning from the start entity. A hop is basically a jump between entities via SPO representation.

Output:

\[start entity - relationship 1.1 - entity 1.1 - relationship 2.1 - entity 2.1 - relationship 3.1 - entity 3.1 - relationship 4.1 - entity 4.1\],

\[start entity - relationship 1.2 - entity 1.2 - relationship 2.2 - entity 2.2 - relationship 3.2 - entity 3.2 - relationship 4.2 - entity 4.2\],

etc.

Does not need to be in array representation. All i want is to be able to analyze a path programmatically. Im am currently considering doing this manually via loops and multiple queries on a local RDF TTL-graph.",t2_knz7y,False,,0,False,Is there any tool (preferably for python) that helps with extracting all entities and relationships in a path of a RDF graph?,[],r/LanguageTechnology,False,6,,0,,False,t3_kvtng6,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1610493692.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know that property paths in SPARQL can aid you with that. However, you need to be really specific with your nodes and relationships in between two entity nodes that mark the start and the end of the path, if you want to extract them. I am searching for a arbitrary way to do this.&lt;/p&gt;

&lt;p&gt;What i want is basically:&lt;/p&gt;

&lt;p&gt;Input: start entity, amount of max hops for the path (f.e. 4), RDF knowledge graph&lt;/p&gt;

&lt;p&gt;Extract all paths with hops &amp;lt;= 4 beginning from the start entity. A hop is basically a jump between entities via SPO representation.&lt;/p&gt;

&lt;p&gt;Output:&lt;/p&gt;

&lt;p&gt;[start entity - relationship 1.1 - entity 1.1 - relationship 2.1 - entity 2.1 - relationship 3.1 - entity 3.1 - relationship 4.1 - entity 4.1],&lt;/p&gt;

&lt;p&gt;[start entity - relationship 1.2 - entity 1.2 - relationship 2.2 - entity 2.2 - relationship 3.2 - entity 3.2 - relationship 4.2 - entity 4.2],&lt;/p&gt;

&lt;p&gt;etc.&lt;/p&gt;

&lt;p&gt;Does not need to be in array representation. All i want is to be able to analyze a path programmatically. Im am currently considering doing this manually via loops and multiple queries on a local RDF TTL-graph.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kvtng6,True,,ibnhajj,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kvtng6/is_there_any_tool_preferably_for_python_that/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kvtng6/is_there_any_tool_preferably_for_python_that/,30199,1610464892.0,0,,False,,,,,,1051
938,,LanguageTechnology," Like the title says, I would like to create an NLP project which consists of a search bar and when someone types something domain specific, it can start generating suggestions. The problem is, I want these suggestions to only be domain specific. Can someone guide me to tutorials or frameworks that can help me. Thank you.

Just as an example - I would create a bank and clients would type up specific processes in that bank. So if a client were to type ""I would like"" it would generate some suggestions like ""I would like to request a loan"" and ""I would like to speak to customer service"" and ""I would like to open a bank account""

Thanks again.",t2_42eiufnp,False,,0,False,Create search bar NLP recommendation in Python,[],r/LanguageTechnology,False,6,,0,,False,t3_kw2oc4,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1610520077.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Like the title says, I would like to create an NLP project which consists of a search bar and when someone types something domain specific, it can start generating suggestions. The problem is, I want these suggestions to only be domain specific. Can someone guide me to tutorials or frameworks that can help me. Thank you.&lt;/p&gt;

&lt;p&gt;Just as an example - I would create a bank and clients would type up specific processes in that bank. So if a client were to type &amp;quot;I would like&amp;quot; it would generate some suggestions like &amp;quot;I would like to request a loan&amp;quot; and &amp;quot;I would like to speak to customer service&amp;quot; and &amp;quot;I would like to open a bank account&amp;quot;&lt;/p&gt;

&lt;p&gt;Thanks again.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kw2oc4,True,,DataPlug,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kw2oc4/create_search_bar_nlp_recommendation_in_python/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kw2oc4/create_search_bar_nlp_recommendation_in_python/,30199,1610491277.0,0,,False,,,,,,647
939,,LanguageTechnology,,t2_hkv9s,False,,0,False,T5: Exploring Limits of Transfer Learning with Text-to-Text Transformer | Research Paper Walkthrough,[],r/LanguageTechnology,False,6,,0,,False,t3_kvlrxo,False,dark,1.0,,public,13,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/91iLu6OOrwk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'T5: Exploring Limits of Transfer Learning with Text-to-Text Transformer | Research Paper Walkthrough', 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/91iLu6OOrwk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'version': '1.0', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/91iLu6OOrwk/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/91iLu6OOrwk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/kvlrxo', 'height': 200}",,False,13,,False,False,,False,,[],{},,False,,1610459911.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kvlrxo,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kvlrxo/t5_exploring_limits_of_transfer_learning_with/,all_ads,False,https://youtu.be/91iLu6OOrwk,30199,1610431111.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'T5: Exploring Limits of Transfer Learning with Text-to-Text Transformer | Research Paper Walkthrough', 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/91iLu6OOrwk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'version': '1.0', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/91iLu6OOrwk/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}, 'type': 'youtube.com'}",False,https://youtu.be/91iLu6OOrwk,,,,,0
940,,LanguageTechnology,"I’m working on a problem where I need to take two sentences:

1. Input sentence (this needs to be restyled)
2. Reference sentence (we match the style of this sentence)

I need to rephrase the input sentence to be more similar to the reference sentence.

Very simple example — if the reference sentence is needlessly verbose, has misspellings and lots of commas, we rephrase the input sentence to have commas and misspellings, and add words to make it more verbose.

How should I approach this problem?",t2_8xraz5pp,False,,0,False,Is there a method for restyling a sentence given another sentence?,[],r/LanguageTechnology,False,6,,0,,False,t3_kvas7j,False,dark,0.91,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1610424309.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m working on a problem where I need to take two sentences:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Input sentence (this needs to be restyled)&lt;/li&gt;
&lt;li&gt;Reference sentence (we match the style of this sentence)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I need to rephrase the input sentence to be more similar to the reference sentence.&lt;/p&gt;

&lt;p&gt;Very simple example — if the reference sentence is needlessly verbose, has misspellings and lots of commas, we rephrase the input sentence to have commas and misspellings, and add words to make it more verbose.&lt;/p&gt;

&lt;p&gt;How should I approach this problem?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kvas7j,True,,Effective_Sea_9367,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kvas7j/is_there_a_method_for_restyling_a_sentence_given/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kvas7j/is_there_a_method_for_restyling_a_sentence_given/,30199,1610395509.0,0,,False,,,,,,501
941,,LanguageTechnology,"Is there a way to know if there was a phrase in an LM’s training data w/o looking at the training data itself? It is not as trivial as looking at the vocab: for example if the vocab contains Donald and Trump, doesn’t mean that it the phrase “Donald Trump” was in the training data.",t2_3figh64x,False,,0,False,How to know if my language model saw a phrase during training,[],r/LanguageTechnology,False,6,,0,,False,t3_kuzi88,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1610386787.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there a way to know if there was a phrase in an LM’s training data w/o looking at the training data itself? It is not as trivial as looking at the vocab: for example if the vocab contains Donald and Trump, doesn’t mean that it the phrase “Donald Trump” was in the training data.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kuzi88,True,,sagnikpsu,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kuzi88/how_to_know_if_my_language_model_saw_a_phrase/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kuzi88/how_to_know_if_my_language_model_saw_a_phrase/,30199,1610357987.0,0,,False,,,,,,281
942,,LanguageTechnology,"Hello All

I am looking to crawl data for academic research (most likely need to release/open-source the dataset).  Do you guys know the license? (I have already read their webpage, terms and condition), however, I don't find too many open source twitter data set, wondering if there is any hidden terms that I am not awared off?",t2_76mm1sy6,False,,0,False,[R]: Twitter Data crawling for research,[],r/LanguageTechnology,False,6,,0,,False,t3_kusrxx,False,dark,0.75,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1610359659.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello All&lt;/p&gt;

&lt;p&gt;I am looking to crawl data for academic research (most likely need to release/open-source the dataset).  Do you guys know the license? (I have already read their webpage, terms and condition), however, I don&amp;#39;t find too many open source twitter data set, wondering if there is any hidden terms that I am not awared off?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kusrxx,True,,randy_wales_qq,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kusrxx/r_twitter_data_crawling_for_research/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kusrxx/r_twitter_data_crawling_for_research/,30199,1610330859.0,0,,False,,,,,,329
943,,LanguageTechnology,,t2_ddgvd,False,,0,False,Simple LSTM to Detect Passive and Active Voice in Your Writing,[],r/LanguageTechnology,False,6,,0,,False,t3_kui5yj,False,dark,0.73,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,False,,1610326921.0,text,6,,,text,bobbywlindsey.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kui5yj,True,,funnythingaboutmybak,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kui5yj/simple_lstm_to_detect_passive_and_active_voice_in/,all_ads,False,https://www.bobbywlindsey.com/2021/01/09/how-to-detect-passive-and-active-voice-in-your-writing-using-an-lstm/,30199,1610298121.0,0,,False,https://www.bobbywlindsey.com/2021/01/09/how-to-detect-passive-and-active-voice-in-your-writing-using-an-lstm/,"[{'approved_at_utc': None, 'subreddit': 'datascience', 'selftext': '', 'author_fullname': 't2_ddgvd', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Simple LSTM to Detect Passive and Active Voice in Your Writing', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/datascience', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'projects', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_kuhn73', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.67, 'author_flair_background_color': '', 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Projects', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': 'seniorflair', 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1610325315.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': 'moderator', 'banned_by': None, 'author_flair_type': 'text', 'domain': 'bobbywlindsey.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': 'confidence', 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.bobbywlindsey.com/2021/01/09/how-to-detect-passive-and-active-voice-in-your-writing-using-an-lstm/', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '937a6f50-d780-11e7-826d-0ed1beddcc82', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'MS | Data Scientist | Energy', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2sptq', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'kuhn73', 'is_robot_indexable': False, 'report_reasons': None, 'author': 'funnythingaboutmybak', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/datascience/comments/kuhn73/simple_lstm_to_detect_passive_and_active_voice_in/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.bobbywlindsey.com/2021/01/09/how-to-detect-passive-and-active-voice-in-your-writing-using-an-lstm/', 'subreddit_subscribers': 512458, 'created_utc': 1610296515.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_kuhn73,,,0
944,,LanguageTechnology,"Is there an NLP technique for rephrasing question-answer pairs as a full and grammatically correct sentence? For example:

Question: Where does Joe live?

Answer: Joe lives in Los Angeles.

I’ve looked into Answer Ellipsis, which is essentially the opposite of this issue and what most Machine Reading Comprehension or Question Answering systems already generate. For example, the answer to the above with answer ellipsis would be “Los Angeles”.

For the sake of reader comprehension, I want to turn question-answer pairs into something understandable. What techniques exist to do this?",t2_7ki4n,False,,0,False,Is there an NLP technique for rephrasing question-answer pairs as a full sentence?,[],r/LanguageTechnology,False,6,,0,,False,t3_kujpgj,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1610331644.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there an NLP technique for rephrasing question-answer pairs as a full and grammatically correct sentence? For example:&lt;/p&gt;

&lt;p&gt;Question: Where does Joe live?&lt;/p&gt;

&lt;p&gt;Answer: Joe lives in Los Angeles.&lt;/p&gt;

&lt;p&gt;I’ve looked into Answer Ellipsis, which is essentially the opposite of this issue and what most Machine Reading Comprehension or Question Answering systems already generate. For example, the answer to the above with answer ellipsis would be “Los Angeles”.&lt;/p&gt;

&lt;p&gt;For the sake of reader comprehension, I want to turn question-answer pairs into something understandable. What techniques exist to do this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kujpgj,True,,giantsxx21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kujpgj/is_there_an_nlp_technique_for_rephrasing/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kujpgj/is_there_an_nlp_technique_for_rephrasing/,30199,1610302844.0,0,,False,,,,,,586
945,,LanguageTechnology,,t2_lvxu2,False,,0,False,GAP: Text2sql with Generation-Augmented Pre-Training,[],r/LanguageTechnology,False,6,,0,,False,t3_ku5u1y,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,False,,1610276081.0,text,6,,,text,arxiv.org,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ku5u1y,True,,skepnannab,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ku5u1y/gap_text2sql_with_generationaugmented_pretraining/,all_ads,False,https://arxiv.org/abs/2012.10309,30199,1610247281.0,0,,False,https://arxiv.org/abs/2012.10309,,,,,0
946,,LanguageTechnology,,t2_lvxu2,False,,0,False,Spider: Yale Semantic Parsing and Text-to-SQL Challenge,[],r/LanguageTechnology,False,6,,0,,False,t3_kttt6h,False,dark,1.0,,public,19,0,{},,False,[],,False,False,,{},,False,19,,False,False,,False,,[],{},,False,,1610238096.0,text,6,,,text,yale-lily.github.io,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kttt6h,True,,skepnannab,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kttt6h/spider_yale_semantic_parsing_and_texttosql/,all_ads,False,https://yale-lily.github.io/spider,30199,1610209296.0,0,,False,https://yale-lily.github.io/spider,,,,,0
947,,LanguageTechnology,"Got sent here from r/learnprogramming! 

&amp;#x200B;

Hi, for a long time I've ""collected"" and made up sentences that \*almost\* make sense, but don't. It's similar to the kinds of things you might see on [r/ihadastroke](https://www.reddit.com/r/ihadastroke/). Such as:

***Appreciate what you what, be are the make you appreciate what you dad.***

or

***Why do they call it oven when you of in the cold food of out hot eat the food?***

or

***Don't think that carrot big because carrot big leaf because small leaf carrot not big leaf sizes.***

As a fun quarantine side project, I wanted to train an AI to generate these almost-sensical sentences for my own amusement. Since I typically only program games, I wanted something simple and I'm currently using Max Woolfe's [GPT-2 simple](https://minimaxir.com/2019/09/howto-gpt2/) since its extremely easy to input data sets and quickly train a model right from a google collab project. I've considered that perhaps using a ""worse"" platform to create a model might be better for my goals though.

Anyway, I'm considering from where I should pull input sets to train the model. Some ideas I have right now are English as second language forums, mass-translating sentences through a bunch of different languages then back to english, bad sentences generated by other bots like on [r/SubredditSimulator](https://www.reddit.com/r/SubredditSimulator/), or mixing proper english sentences with a smattering of ones that are nonsensical. The nuance to this is that I'd want sentences that ***almost*** make sense, but don't. Oftentimes they'll have a proper grammatic opening or ending, but then will start to deviate or repeat verbs when the clause should end. It might also be possible to not use ML but just take fully formed sentences and start swapping around and subbing out words algorithmically. Any and all suggestions are welcome! This is my first time trying any type of model training so I appreciate any tips, but would probably need to keep it simple.",t2_12sbs9,False,,0,False,Making a natural language bot.. that sounds like its having a stroke. What data sets to use?,[],r/LanguageTechnology,False,6,,0,,False,t3_ku0d7b,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1610258136.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Got sent here from &lt;a href=""/r/learnprogramming""&gt;r/learnprogramming&lt;/a&gt;! &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Hi, for a long time I&amp;#39;ve &amp;quot;collected&amp;quot; and made up sentences that *almost* make sense, but don&amp;#39;t. It&amp;#39;s similar to the kinds of things you might see on &lt;a href=""https://www.reddit.com/r/ihadastroke/""&gt;r/ihadastroke&lt;/a&gt;. Such as:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Appreciate what you what, be are the make you appreciate what you dad.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Why do they call it oven when you of in the cold food of out hot eat the food?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Don&amp;#39;t think that carrot big because carrot big leaf because small leaf carrot not big leaf sizes.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As a fun quarantine side project, I wanted to train an AI to generate these almost-sensical sentences for my own amusement. Since I typically only program games, I wanted something simple and I&amp;#39;m currently using Max Woolfe&amp;#39;s &lt;a href=""https://minimaxir.com/2019/09/howto-gpt2/""&gt;GPT-2 simple&lt;/a&gt; since its extremely easy to input data sets and quickly train a model right from a google collab project. I&amp;#39;ve considered that perhaps using a &amp;quot;worse&amp;quot; platform to create a model might be better for my goals though.&lt;/p&gt;

&lt;p&gt;Anyway, I&amp;#39;m considering from where I should pull input sets to train the model. Some ideas I have right now are English as second language forums, mass-translating sentences through a bunch of different languages then back to english, bad sentences generated by other bots like on &lt;a href=""https://www.reddit.com/r/SubredditSimulator/""&gt;r/SubredditSimulator&lt;/a&gt;, or mixing proper english sentences with a smattering of ones that are nonsensical. The nuance to this is that I&amp;#39;d want sentences that &lt;strong&gt;&lt;em&gt;almost&lt;/em&gt;&lt;/strong&gt; make sense, but don&amp;#39;t. Oftentimes they&amp;#39;ll have a proper grammatic opening or ending, but then will start to deviate or repeat verbs when the clause should end. It might also be possible to not use ML but just take fully formed sentences and start swapping around and subbing out words algorithmically. Any and all suggestions are welcome! This is my first time trying any type of model training so I appreciate any tips, but would probably need to keep it simple.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ku0d7b,True,,NeedUnusedName,,10,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ku0d7b/making_a_natural_language_bot_that_sounds_like/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ku0d7b/making_a_natural_language_bot_that_sounds_like/,30199,1610229336.0,0,,False,,,,,,2009
948,,LanguageTechnology,"Hello everyone !

I'm a beginner in NLP, and as part of a school project, I created a model allowing to automatically generate emails subjects based on the content of the email. I need to assess the quality of the generated objects, so I need the help of many people to fill a questionnaire :  
[https://forms.gle/8cnnNqtyd8Qv1wbU6](https://forms.gle/8cnnNqtyd8Qv1wbU6?fbclid=IwAR0rqHJOHBQ-jHFX8E522UiB3CUORtBqgpmGJ2hXw2HfUDdKR25NkjP1SU0)

In this questionnaire there are ten sentences, and for each sentence you can say, in your opinion, if this could or not be an email subject.

Thank you so much !",t2_2c0tigsr,False,,0,False,NLP School project,[],r/LanguageTechnology,False,6,,0,,False,t3_ku0c4w,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1610258037.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone !&lt;/p&gt;

&lt;p&gt;I&amp;#39;m a beginner in NLP, and as part of a school project, I created a model allowing to automatically generate emails subjects based on the content of the email. I need to assess the quality of the generated objects, so I need the help of many people to fill a questionnaire :&lt;br/&gt;
&lt;a href=""https://forms.gle/8cnnNqtyd8Qv1wbU6?fbclid=IwAR0rqHJOHBQ-jHFX8E522UiB3CUORtBqgpmGJ2hXw2HfUDdKR25NkjP1SU0""&gt;https://forms.gle/8cnnNqtyd8Qv1wbU6&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this questionnaire there are ten sentences, and for each sentence you can say, in your opinion, if this could or not be an email subject.&lt;/p&gt;

&lt;p&gt;Thank you so much !&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ku0c4w,True,,ToxicLandfill,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ku0c4w/nlp_school_project/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ku0c4w/nlp_school_project/,30199,1610229237.0,0,,False,,,,,,601
949,,LanguageTechnology,"I have created a clean(ish) image with a running implementation of GPT-2 and Tensorflow 1.13.1 that runs on the Pi 4B 8 gig (might also run on the 4G, but possibly slower.)  GitHub limits file uploads to 25MB.  The image is 64 GB......  does anybody have a suggestion of how best to make this available?  Maybe I should sell them for $ on Etsy??? :)

By clean-ish, I mean that I have cleaned up *files I have created* that pertain to other functions the robots have, but there are a LOT of things still *installed* that do not pertain specifically to GPT-2 or Tensorflow 1.13.1 (things like speech recognition, synthesis, zmq &amp; imagezmq, face and object detection &amp; recognition, etc., etc.......)  This image is highly customized, and folks seeking to experiment with GPT-2 on the Pi should be able to do quite a bit with this as a starting point.  Apparently others have had a very hard time (as did I) getting GPT-2 to run on the Pi.  All this needs you to do is burn the image, boot the Pi, open a terminal window, cd ~/Desktop/HOSTCORE/gpt-2 and then python src/speakGPT2.py

You'll get a boatload of tensorflow deprication warnings and memory allocation warnings, but it will eventually offer you the ""Model prompt &gt;&gt;&gt;"" and you can enter your prompt and you're off to the races.

Here's a sample I just ran to test that I didn't break anything when I cleaned up the ~100 files....  Everything after ""SAMPLE 1"" is generated by GPT-2 lol...  Anyway - please let me know if there is interest.

Dave


Model prompt &gt;&gt;&gt; I've worked hard to create this disk image for folks who want to experiment so they can get a working copy of GPT-2.  Now I hope to see some inspiring results, and I hope you'll all share 

============SAMPLE 1 =============

 with your own experience and how it goes for you! Enjoy!
I was fortunate enough to meet a guy at an airport this week who gave me a great introduction to his GPT-2 system. I had never been able to get a working copy of a GPT-2, but his system was working, and he had me work with it. I did a lot of research and spent hours trying to figure out how to get it to work. It was really nice to see someone with experience and knowledge of the system, and how to get it to work. When I went to see him I knew what to expect.

-----------------------------

Model prompt &gt;&gt;&gt;",t2_16rqfm,False,,0,False,Any interest in a disk image for running GPT-2 on a Raspberry Pi4? How to make it available - it's 64GB,[],r/LanguageTechnology,False,6,,0,,False,t3_ktx75b,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1610248222.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have created a clean(ish) image with a running implementation of GPT-2 and Tensorflow 1.13.1 that runs on the Pi 4B 8 gig (might also run on the 4G, but possibly slower.)  GitHub limits file uploads to 25MB.  The image is 64 GB......  does anybody have a suggestion of how best to make this available?  Maybe I should sell them for $ on Etsy??? :)&lt;/p&gt;

&lt;p&gt;By clean-ish, I mean that I have cleaned up &lt;em&gt;files I have created&lt;/em&gt; that pertain to other functions the robots have, but there are a LOT of things still &lt;em&gt;installed&lt;/em&gt; that do not pertain specifically to GPT-2 or Tensorflow 1.13.1 (things like speech recognition, synthesis, zmq &amp;amp; imagezmq, face and object detection &amp;amp; recognition, etc., etc.......)  This image is highly customized, and folks seeking to experiment with GPT-2 on the Pi should be able to do quite a bit with this as a starting point.  Apparently others have had a very hard time (as did I) getting GPT-2 to run on the Pi.  All this needs you to do is burn the image, boot the Pi, open a terminal window, cd ~/Desktop/HOSTCORE/gpt-2 and then python src/speakGPT2.py&lt;/p&gt;

&lt;p&gt;You&amp;#39;ll get a boatload of tensorflow deprication warnings and memory allocation warnings, but it will eventually offer you the &amp;quot;Model prompt &amp;gt;&amp;gt;&amp;gt;&amp;quot; and you can enter your prompt and you&amp;#39;re off to the races.&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s a sample I just ran to test that I didn&amp;#39;t break anything when I cleaned up the ~100 files....  Everything after &amp;quot;SAMPLE 1&amp;quot; is generated by GPT-2 lol...  Anyway - please let me know if there is interest.&lt;/p&gt;

&lt;p&gt;Dave&lt;/p&gt;

&lt;p&gt;Model prompt &amp;gt;&amp;gt;&amp;gt; I&amp;#39;ve worked hard to create this disk image for folks who want to experiment so they can get a working copy of GPT-2.  Now I hope to see some inspiring results, and I hope you&amp;#39;ll all share &lt;/p&gt;

&lt;p&gt;============SAMPLE 1 =============&lt;/p&gt;

&lt;p&gt;with your own experience and how it goes for you! Enjoy!
I was fortunate enough to meet a guy at an airport this week who gave me a great introduction to his GPT-2 system. I had never been able to get a working copy of a GPT-2, but his system was working, and he had me work with it. I did a lot of research and spent hours trying to figure out how to get it to work. It was really nice to see someone with experience and knowledge of the system, and how to get it to work. When I went to see him I knew what to expect.&lt;/p&gt;

&lt;hr/&gt;

&lt;p&gt;Model prompt &amp;gt;&amp;gt;&amp;gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ktx75b,True,,DelosBoard2052,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ktx75b/any_interest_in_a_disk_image_for_running_gpt2_on/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ktx75b/any_interest_in_a_disk_image_for_running_gpt2_on/,30199,1610219422.0,0,,False,,,,,,2350
950,,LanguageTechnology,"This is more so on the theoretical side of things but I'm curious to know if anybody is familiar with any research that deals with calculating the likelihood that a verb will take a particular set of arguments as it's complement, in particular the likelihood that a verb will take the overt complementiser *that* as in \[think + \[C\[that\]\]? 

There's a tonne of theoretical linguistic research into this but I'm curious to learn about any different approaches.",t2_48eyl33r,False,,0,False,Probabilistic verbal subcategorisation frames research,[],r/LanguageTechnology,False,6,,0,,False,t3_ktv513,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1610242106.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is more so on the theoretical side of things but I&amp;#39;m curious to know if anybody is familiar with any research that deals with calculating the likelihood that a verb will take a particular set of arguments as it&amp;#39;s complement, in particular the likelihood that a verb will take the overt complementiser &lt;em&gt;that&lt;/em&gt; as in [think + [C[that]]? &lt;/p&gt;

&lt;p&gt;There&amp;#39;s a tonne of theoretical linguistic research into this but I&amp;#39;m curious to learn about any different approaches.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ktv513,True,,crowpup783,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ktv513/probabilistic_verbal_subcategorisation_frames/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ktv513/probabilistic_verbal_subcategorisation_frames/,30199,1610213306.0,0,,False,,,,,,463
951,,LanguageTechnology,,t2_5fsp2x6v,False,,0,False,ICLR 2020 Workshop Paper on Distant Supervision for Low-Resource Named Entity Recognition | Research Papers Summary 003,[],r/LanguageTechnology,False,6,,0,,False,t3_ktqmbw,False,dark,1.0,,public,3,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/qcrkSZ4l1dk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Distant Supervision for Low-Resource Named Entity Recognition | Research Papers Summary 003', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/qcrkSZ4l1dk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/qcrkSZ4l1dk/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/qcrkSZ4l1dk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ktqmbw', 'height': 200}",,False,3,,False,False,,False,,[],{},,False,,1610226413.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ktqmbw,True,,RyanAI100,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ktqmbw/iclr_2020_workshop_paper_on_distant_supervision/,all_ads,False,https://youtu.be/qcrkSZ4l1dk,30199,1610197613.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Distant Supervision for Low-Resource Named Entity Recognition | Research Papers Summary 003', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/qcrkSZ4l1dk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/qcrkSZ4l1dk/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,https://youtu.be/qcrkSZ4l1dk,,,,,0
952,,LanguageTechnology,,t2_21qaqh1p,False,,0,False,ecco - Visualize &amp; Explain NLP Language Models in Python,[],r/LanguageTechnology,False,6,,0,,False,t3_ktbyfj,False,dark,0.95,,public,33,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/gJPMXgvnX4Y?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'ecco - Visualize &amp; Explain NLP Language Models in Python', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/gJPMXgvnX4Y?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/gJPMXgvnX4Y/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/gJPMXgvnX4Y?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ktbyfj', 'height': 200}",,False,33,,False,False,,False,,[],{},,False,,1610169264.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ktbyfj,True,,dulldata,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ktbyfj/ecco_visualize_explain_nlp_language_models_in/,all_ads,False,https://www.youtube.com/watch?v=gJPMXgvnX4Y,30199,1610140464.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'ecco - Visualize &amp; Explain NLP Language Models in Python', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/gJPMXgvnX4Y?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/gJPMXgvnX4Y/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,https://www.youtube.com/watch?v=gJPMXgvnX4Y,,,,,0
953,,LanguageTechnology,"Most of my pdfs are fine. However, some the text is not formatted correctly.",t2_a8bujs3,False,,0,False,"If I am processing pdfs and are formatted incorrectly, what algorithm should I use?",[],r/LanguageTechnology,False,6,,0,,False,t3_ktmhnu,False,dark,0.67,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1610206295.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Most of my pdfs are fine. However, some the text is not formatted correctly.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ktmhnu,True,,immunobio,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ktmhnu/if_i_am_processing_pdfs_and_are_formatted/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ktmhnu/if_i_am_processing_pdfs_and_are_formatted/,30199,1610177495.0,0,,False,,,,,,76
954,,LanguageTechnology,"The large universal sentence encoder was updated today and it seems to no longer be working, I'm trying to load it in colab and it doesn't ever stop loading.

&amp;#x200B;

Does anyone know anything about this?",t2_79zbgb3u,False,,0,False,Universal Sentence Encoder large not working?,[],r/LanguageTechnology,False,6,,0,,False,t3_ktal3s,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1610165332.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The large universal sentence encoder was updated today and it seems to no longer be working, I&amp;#39;m trying to load it in colab and it doesn&amp;#39;t ever stop loading.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Does anyone know anything about this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ktal3s,True,,dfish357,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ktal3s/universal_sentence_encoder_large_not_working/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ktal3s/universal_sentence_encoder_large_not_working/,30199,1610136532.0,0,,False,,,,,,210
955,,LanguageTechnology,"With the knowledge that we now have of language embedding like BERT I was wondering if there exist a better way to select for example the top 5000 must useful english word.  


This have traditionally been done by simply sorting by word frequency.  
But this have many problem:  
1- word like ""stick"" have multiple meaning which increase the frequency by grouping them under the same count. ex: a) Popsicle sticks b) ""just stick that sandwich on my desk"" c) up the stick (pregnant) ...  
2- word have synonyms that are not grouped together   
3- word at different tense are not grouped together (is vs has), (go vs going) (be vs been)  


I am curious if anyone have explored tagging word usage in a corpus with its meaning before taking a count the the usage frequency?  
Have anyone used some alternative approach to select a subset of word, for example if you wanted to produce a pocket english dictionary.",t2_38k7g,False,,0,False,How to go about building a word list (subset of english word).,[],r/LanguageTechnology,False,6,,0,,False,t3_ktcz4r,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,1610144090.0,,[],{},,True,,1610172277.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;With the knowledge that we now have of language embedding like BERT I was wondering if there exist a better way to select for example the top 5000 must useful english word.  &lt;/p&gt;

&lt;p&gt;This have traditionally been done by simply sorting by word frequency.&lt;br/&gt;
But this have many problem:&lt;br/&gt;
1- word like &amp;quot;stick&amp;quot; have multiple meaning which increase the frequency by grouping them under the same count. ex: a) Popsicle sticks b) &amp;quot;just stick that sandwich on my desk&amp;quot; c) up the stick (pregnant) ...&lt;br/&gt;
2- word have synonyms that are not grouped together&lt;br/&gt;
3- word at different tense are not grouped together (is vs has), (go vs going) (be vs been)  &lt;/p&gt;

&lt;p&gt;I am curious if anyone have explored tagging word usage in a corpus with its meaning before taking a count the the usage frequency?&lt;br/&gt;
Have anyone used some alternative approach to select a subset of word, for example if you wanted to produce a pocket english dictionary.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ktcz4r,True,,skyde,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ktcz4r/how_to_go_about_building_a_word_list_subset_of/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ktcz4r/how_to_go_about_building_a_word_list_subset_of/,30199,1610143477.0,0,,False,,,,,,909
956,,LanguageTechnology,"[https://www.sciencedirect.com/science/article/pii/S2589004220311196](https://www.sciencedirect.com/science/article/pii/S2589004220311196)

This paper talks about how context-free word embeddings can capture the domain knowledge in a corpus of papers and can be used to extrapolate the same by finding novel polymers for existing applications.

Let me know what you think!",t2_9mdkmzqj,False,,0,False,Extrapolating Materials Science domain knowledge through context-free embeddings.,[],r/LanguageTechnology,False,6,,0,,False,t3_kt83z6,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1610158341.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.sciencedirect.com/science/article/pii/S2589004220311196""&gt;https://www.sciencedirect.com/science/article/pii/S2589004220311196&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This paper talks about how context-free word embeddings can capture the domain knowledge in a corpus of papers and can be used to extrapolate the same by finding novel polymers for existing applications.&lt;/p&gt;

&lt;p&gt;Let me know what you think!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kt83z6,True,,blisferatu,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kt83z6/extrapolating_materials_science_domain_knowledge/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kt83z6/extrapolating_materials_science_domain_knowledge/,30199,1610129541.0,0,,False,,,,,,372
957,,LanguageTechnology,"Hi,

Is there any research or empirical rule of thumb for minimum number of text pairs needed to train an encoder-decoder style model (seq2seq LSTM, BART, T5, etc.)?",t2_16diqth,False,,0,False,Minimum number of samples for encoder-decoder models?,[],r/LanguageTechnology,False,6,,0,,False,t3_kt4xix,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1610149294.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;Is there any research or empirical rule of thumb for minimum number of text pairs needed to train an encoder-decoder style model (seq2seq LSTM, BART, T5, etc.)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kt4xix,True,,amitness,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kt4xix/minimum_number_of_samples_for_encoderdecoder/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kt4xix/minimum_number_of_samples_for_encoderdecoder/,30199,1610120494.0,0,,False,,,,,,165
958,,LanguageTechnology,"Hello! 

I know this is a little late, but I was looking for some opinions/suggestions on this dilemma I'm facing: 

I'm applying for a masters in language technology and I currently have 2 Swedish programs that I'm confused about: the masters programs at Uppsala and Gothenburg. While the Gothenburg syllabus looks more interesting with fun profs and labs to be around, Uppsala offers more (2-3 as opposed to 1) options in terms of funding. Both require that they be my first choice if I am to be considered for funding, and I need some form of funding if I'm not to drown in student debt. 

My background: I'm from South Asia and currently finishing up a master's in English with an 8-pointer GPA. I've done half a dozen courses in linguistics and I know some C++ (only the basics - functions, data structures, loops, sorting, recursion that sort of thing). I also know some calculus, probability, and basic linear algebra. I do not have related research or internship experience.

I understand that this background is as such kinda poor and that the programs I'm applying to are very competitive, but any and all help is appreciated. Especially because Gothenburg says that only one applicant out of all fee paying applicants will be awarded the Axel Adler scholarship, I wanted to know if by opting for Uppsala as my first choice I might be losing out on much in terms of what Gothenburg has to offer. What I'm hoping to do if I get accepted is to pick up enough math and programming for an industrial career as opposed to an academic one, and I'm looking for a program that has good teachers and facilities as opposed to say, all repute but no teaching. 

Thanks!",t2_9ps9nw86,False,,0,False,Masters in LT - Sweden - Any and all advice welcome,[],r/LanguageTechnology,False,6,,0,,False,t3_kswlx8,False,dark,1.0,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,1618324583.0,,[],{},,True,,1610114891.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! &lt;/p&gt;

&lt;p&gt;I know this is a little late, but I was looking for some opinions/suggestions on this dilemma I&amp;#39;m facing: &lt;/p&gt;

&lt;p&gt;I&amp;#39;m applying for a masters in language technology and I currently have 2 Swedish programs that I&amp;#39;m confused about: the masters programs at Uppsala and Gothenburg. While the Gothenburg syllabus looks more interesting with fun profs and labs to be around, Uppsala offers more (2-3 as opposed to 1) options in terms of funding. Both require that they be my first choice if I am to be considered for funding, and I need some form of funding if I&amp;#39;m not to drown in student debt. &lt;/p&gt;

&lt;p&gt;My background: I&amp;#39;m from South Asia and currently finishing up a master&amp;#39;s in English with an 8-pointer GPA. I&amp;#39;ve done half a dozen courses in linguistics and I know some C++ (only the basics - functions, data structures, loops, sorting, recursion that sort of thing). I also know some calculus, probability, and basic linear algebra. I do not have related research or internship experience.&lt;/p&gt;

&lt;p&gt;I understand that this background is as such kinda poor and that the programs I&amp;#39;m applying to are very competitive, but any and all help is appreciated. Especially because Gothenburg says that only one applicant out of all fee paying applicants will be awarded the Axel Adler scholarship, I wanted to know if by opting for Uppsala as my first choice I might be losing out on much in terms of what Gothenburg has to offer. What I&amp;#39;m hoping to do if I get accepted is to pick up enough math and programming for an industrial career as opposed to an academic one, and I&amp;#39;m looking for a program that has good teachers and facilities as opposed to say, all repute but no teaching. &lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kswlx8,True,,plane_dosa,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kswlx8/masters_in_lt_sweden_any_and_all_advice_welcome/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/kswlx8/masters_in_lt_sweden_any_and_all_advice_welcome/,30199,1610086091.0,0,,False,,,,,,1668
959,,LanguageTechnology,I am trying to build a taxonomic representation of football related stuffs from football news articles.,t2_60c2pw8h,False,,0,False,How to construct a taxonomy from news articles?,[],r/LanguageTechnology,False,6,,0,,False,t3_ksx59h,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1610117220.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to build a taxonomic representation of football related stuffs from football news articles.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ksx59h,True,,sanjeevr5,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ksx59h/how_to_construct_a_taxonomy_from_news_articles/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ksx59h/how_to_construct_a_taxonomy_from_news_articles/,30199,1610088420.0,0,,False,,,,,,103
960,,LanguageTechnology,,t2_cu8vpox,False,,0,False,Introduction to Rasa: the NLU chatbot framework,[],r/LanguageTechnology,False,6,,0,,False,t3_ksporj,False,dark,1.0,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,False,,1610090738.0,text,6,,,text,shyambhu20.blogspot.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ksporj,True,,shyamcody,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ksporj/introduction_to_rasa_the_nlu_chatbot_framework/,all_ads,False,https://shyambhu20.blogspot.com/2021/01/introduction-to-rasa-nlu-chatbot.html,30199,1610061938.0,0,,False,https://shyambhu20.blogspot.com/2021/01/introduction-to-rasa-nlu-chatbot.html,,,,,0
961,,LanguageTechnology,,t2_kj0nv,False,,0,False,"500 AI, Machine learning, Deep learning, Computer vision, and NLP Projects with code",[],r/LanguageTechnology,False,6,,0,,False,t3_ksgusq,False,dark,0.93,,public,23,0,{},,False,[],,False,False,,{},,False,23,,False,False,,False,,[],{},,False,,1610066025.0,text,6,,,text,github.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ksgusq,True,,binaryfor,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ksgusq/500_ai_machine_learning_deep_learning_computer/,all_ads,False,https://github.com/ashishpatel26/500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code,30199,1610037225.0,0,,False,https://github.com/ashishpatel26/500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code,,,,,0
962,,LanguageTechnology,"A VSCode Language Extension for NLP++ has been released in the VisualStudio Marketplace.  NLP++ is a open source computer language specifically dedicated to creating text analyzers that mimic human readers and includes the NLP++ language and knowledge based system called the ""conceptual grammar"". NLP++ is used for any type of text processing from simple tagging or extraction, to full language parsing. There is a full English parser that is free an available for use. More information can be found at [http://visualtext.org](http://visualtext.org).",t2_pxkg5,False,,0,False,VSCode NLP++ Language Extension Released,[],r/LanguageTechnology,False,6,,0,,False,t3_ksm3vc,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1610080529.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;A VSCode Language Extension for NLP++ has been released in the VisualStudio Marketplace.  NLP++ is a open source computer language specifically dedicated to creating text analyzers that mimic human readers and includes the NLP++ language and knowledge based system called the &amp;quot;conceptual grammar&amp;quot;. NLP++ is used for any type of text processing from simple tagging or extraction, to full language parsing. There is a full English parser that is free an available for use. More information can be found at &lt;a href=""http://visualtext.org""&gt;http://visualtext.org&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ksm3vc,True,,dehilster,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ksm3vc/vscode_nlp_language_extension_released/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ksm3vc/vscode_nlp_language_extension_released/,30199,1610051729.0,0,,False,,,,,,551
963,,LanguageTechnology,"I'm new to NLP, and I'm trying to find an open-source alternative to GPT-3, which I've been using for text generation. I've been using prompt programming to give GPT-3 some examples of emails and have it generate one for me based on parameters I indicate. I'm looking for a way to do the same in the easiest and cheapest manner. I know that T5, Bert, and GPT-2 are all potential alternatives that require fine-tuning on my use case, but I don't exactly know what that entails (other than the fact I probably need a GPU). Can anyone shed some light?",t2_1r0bnwsm,False,,0,False,How to Fine-tune / Use GPT-3 Alternatives for Text Generation,[],r/LanguageTechnology,False,6,,0,,False,t3_ksjuo5,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1610074404.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m new to NLP, and I&amp;#39;m trying to find an open-source alternative to GPT-3, which I&amp;#39;ve been using for text generation. I&amp;#39;ve been using prompt programming to give GPT-3 some examples of emails and have it generate one for me based on parameters I indicate. I&amp;#39;m looking for a way to do the same in the easiest and cheapest manner. I know that T5, Bert, and GPT-2 are all potential alternatives that require fine-tuning on my use case, but I don&amp;#39;t exactly know what that entails (other than the fact I probably need a GPU). Can anyone shed some light?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ksjuo5,True,,scratchinKiller445,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ksjuo5/how_to_finetune_use_gpt3_alternatives_for_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ksjuo5/how_to_finetune_use_gpt3_alternatives_for_text/,30199,1610045604.0,0,,False,,,,,,548
964,,LanguageTechnology,"I’m trying to solve a problem where I have 10 sentences that all mean the same thing (semantically very similar). 

Given a different, somewhat semantically similar sentence, how can I find the most structurally similar sentence out of the 10?

First thoughts are searching for occurrences of punctuation, words like “the”, etc., but is there a better way (or better yet, an API) to do this?",t2_8xraz5pp,False,,0,False,Rank text by structural similarity?,[],r/LanguageTechnology,False,6,,0,,False,t3_ksg1ey,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1610063745.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m trying to solve a problem where I have 10 sentences that all mean the same thing (semantically very similar). &lt;/p&gt;

&lt;p&gt;Given a different, somewhat semantically similar sentence, how can I find the most structurally similar sentence out of the 10?&lt;/p&gt;

&lt;p&gt;First thoughts are searching for occurrences of punctuation, words like “the”, etc., but is there a better way (or better yet, an API) to do this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ksg1ey,True,,Effective_Sea_9367,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ksg1ey/rank_text_by_structural_similarity/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ksg1ey/rank_text_by_structural_similarity/,30199,1610034945.0,0,,False,,,,,,391
965,,LanguageTechnology,"This ICLR 2020 paper introduces a novel text decoding strategy called Nucleus Sampling (Top-p sampling). This strategy overcomes the limitation of generating bland, repetitive and incoherent long text from other decoding strategies like Beam Search, Top-k sampling, etc. 🔥 

Watch Paper Walkthrough: https://youtu.be/dCORspO2yVY


⏩ Paper Title: The Curious Case of Neural Text Degeneration
⏩ Paper: https://arxiv.org/abs/1904.09751
⏩ Author: Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi
⏩ Organisation: Allen School of Computer Science &amp; Engineering, University of Washington, Allen Institute for Artificial Intelligence, University of Cape Town",t2_hkv9s,False,,0,False,The Curious Case of Neural Text DeGeneration | Research Paper Walkthrough,[],r/LanguageTechnology,False,6,,0,,False,t3_ksflp5,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1610062440.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This ICLR 2020 paper introduces a novel text decoding strategy called Nucleus Sampling (Top-p sampling). This strategy overcomes the limitation of generating bland, repetitive and incoherent long text from other decoding strategies like Beam Search, Top-k sampling, etc. 🔥 &lt;/p&gt;

&lt;p&gt;Watch Paper Walkthrough: &lt;a href=""https://youtu.be/dCORspO2yVY""&gt;https://youtu.be/dCORspO2yVY&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;⏩ Paper Title: The Curious Case of Neural Text Degeneration
⏩ Paper: &lt;a href=""https://arxiv.org/abs/1904.09751""&gt;https://arxiv.org/abs/1904.09751&lt;/a&gt;
⏩ Author: Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi
⏩ Organisation: Allen School of Computer Science &amp;amp; Engineering, University of Washington, Allen Institute for Artificial Intelligence, University of Cape Town&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ksflp5,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ksflp5/the_curious_case_of_neural_text_degeneration/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ksflp5/the_curious_case_of_neural_text_degeneration/,30199,1610033640.0,0,,False,,,,,,663
966,,LanguageTechnology,"So I have a custom pre-trained RoBERTa model that I want to fine tune with NER, multi class classification, and outlier / new class detection.  Currently using Huggingface Transformers for pre-training and fine-tuning.

I’ve recently realized that naively fine tuning on each task separately would require loading in 3 instances of the pre trained model.  My understanding is that fine tuning makes small adjustments to the embedding layers.  Fine tuning separately but using the initial embedding may take a hit to performance.  

Any thoughts or experience on fine tuning across all 3 tasks simultaneously? Or maybe some hacky approach?  Also, if anyone has anyone insights on parameter tuning (specifically BPE vocab size), scheduling, or noising.  Any help would be much appreciated!",t2_1r1zp54n,False,,0,False,"Has anyone deployed a BERT like model across multiple tasks (Multi-class, NER, outlier detection)? Seeking advice.",[],r/LanguageTechnology,False,6,,0,,False,t3_krumbu,False,dark,1.0,,public,18,0,{},,False,[],,False,False,,{},,False,18,,False,False,,False,,[],{},,True,,1609988516.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I have a custom pre-trained RoBERTa model that I want to fine tune with NER, multi class classification, and outlier / new class detection.  Currently using Huggingface Transformers for pre-training and fine-tuning.&lt;/p&gt;

&lt;p&gt;I’ve recently realized that naively fine tuning on each task separately would require loading in 3 instances of the pre trained model.  My understanding is that fine tuning makes small adjustments to the embedding layers.  Fine tuning separately but using the initial embedding may take a hit to performance.  &lt;/p&gt;

&lt;p&gt;Any thoughts or experience on fine tuning across all 3 tasks simultaneously? Or maybe some hacky approach?  Also, if anyone has anyone insights on parameter tuning (specifically BPE vocab size), scheduling, or noising.  Any help would be much appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,krumbu,True,,Achrus,,11,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/krumbu/has_anyone_deployed_a_bert_like_model_across/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/krumbu/has_anyone_deployed_a_bert_like_model_across/,30199,1609959716.0,0,,False,,,,,,787
967,,LanguageTechnology,,t2_ldped,False,,0,False,NLP in 2020: The Year In Review,[],r/LanguageTechnology,False,6,,0,,False,t3_krkmif,False,dark,1.0,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,False,,1609954307.0,text,6,,,text,linkedin.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,krkmif,True,,DemiourgosD,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/krkmif/nlp_in_2020_the_year_in_review/,all_ads,False,https://www.linkedin.com/pulse/natural-language-processing-2020-year-review-ivan-bilan,30199,1609925507.0,0,,False,https://www.linkedin.com/pulse/natural-language-processing-2020-year-review-ivan-bilan,,,,,0
968,,LanguageTechnology,"NLP educators-- I made some free mastery-based assignments to reinforce learning in your NLP classes this semester. These assignments leverage cognitive neuroscience principles proven to optimize knowledge retention &amp; adapt to unique student needs:

[https://docs.google.com/document/d/1PJPc8mTdkm-QSwN4FCjjTxEup7npqE-uUNwz4uhdtzM/edit?usp=sharing](https://docs.google.com/document/d/1PJPc8mTdkm-QSwN4FCjjTxEup7npqE-uUNwz4uhdtzM/edit?usp=sharing)",t2_2uzfmau,False,,0,False,Free NLP assignments,[],r/LanguageTechnology,False,6,,0,,False,t3_krb5vq,False,dark,0.95,,public,42,0,{},,False,[],,False,False,,{},,False,42,,False,False,,False,,[],{},,True,,1609919448.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;NLP educators-- I made some free mastery-based assignments to reinforce learning in your NLP classes this semester. These assignments leverage cognitive neuroscience principles proven to optimize knowledge retention &amp;amp; adapt to unique student needs:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://docs.google.com/document/d/1PJPc8mTdkm-QSwN4FCjjTxEup7npqE-uUNwz4uhdtzM/edit?usp=sharing""&gt;https://docs.google.com/document/d/1PJPc8mTdkm-QSwN4FCjjTxEup7npqE-uUNwz4uhdtzM/edit?usp=sharing&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,krb5vq,True,,galalalal,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/krb5vq/free_nlp_assignments/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/krb5vq/free_nlp_assignments/,30199,1609890648.0,0,,False,,,,,,450
969,,LanguageTechnology,,t2_5s89k712,False,,0,False,OpenAI's DALL·E: Creating Images from Text - Explained,[],r/LanguageTechnology,False,6,,0,,False,t3_kri32o,False,dark,0.7,,public,4,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/UfAE-1vdj_E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': ""OpenAI's DALL·E: Creating Images from Text - Explained"", 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/UfAE-1vdj_E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Deep Learning Explainer', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/UfAE-1vdj_E/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC9aifsrhLEt4cL4mbPiehMg'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/UfAE-1vdj_E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/kri32o', 'height': 200}",,False,4,,False,False,,False,,[],{},,False,,1609942930.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,kri32o,True,,deeplearningperson,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/kri32o/openais_dalle_creating_images_from_text_explained/,all_ads,False,https://youtu.be/UfAE-1vdj_E,30199,1609914130.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': ""OpenAI's DALL·E: Creating Images from Text - Explained"", 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/UfAE-1vdj_E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Deep Learning Explainer', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/UfAE-1vdj_E/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC9aifsrhLEt4cL4mbPiehMg'}}",False,https://youtu.be/UfAE-1vdj_E,,,,,0
970,,LanguageTechnology,"Hi all, I hope I’ve posted this in the right place! I’ve just graduated with an undergraduate degree in linguistics with hopes to do my masters in computational linguistics, but was ultimately unsuccessful with getting into a graduate program. I have basic experience with python, R, foma, and took computational ling courses during my degree so I’m a bit familiar with CYK, FSTs, etc. I’m just wondering if there’s any entry level job I could do with little experience/no masters that would keep me in the area of computational lingusitics/NLP, or if anyone else has had any success or advice about this sort of situation? Thanks so much :)",t2_bfgklqek,False,,0,False,Best way to work into an NLP/Computational Linguist career?,[],r/LanguageTechnology,False,6,,0,,False,t3_o0ut84,False,dark,0.94,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,False,,False,,[],{},,True,,1623840003.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, I hope I’ve posted this in the right place! I’ve just graduated with an undergraduate degree in linguistics with hopes to do my masters in computational linguistics, but was ultimately unsuccessful with getting into a graduate program. I have basic experience with python, R, foma, and took computational ling courses during my degree so I’m a bit familiar with CYK, FSTs, etc. I’m just wondering if there’s any entry level job I could do with little experience/no masters that would keep me in the area of computational lingusitics/NLP, or if anyone else has had any success or advice about this sort of situation? Thanks so much :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,o0ut84,True,,omnomnami82,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/o0ut84/best_way_to_work_into_an_nlpcomputational/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/o0ut84/best_way_to_work_into_an_nlpcomputational/,30199,1623811203.0,0,,False,,,,,,641
971,,LanguageTechnology,,t2_307ui9gq,False,,0,False,Big tech fails to recognize African languages | DW News,[],r/LanguageTechnology,False,6,,0,,False,t3_o0dsly,False,dark,0.78,,public,17,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/iU0Lj-mR9DQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Big tech fails to recognize African languages | DW News', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/iU0Lj-mR9DQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'DW News', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/iU0Lj-mR9DQ/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/dwnews'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/iU0Lj-mR9DQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/o0dsly', 'height': 200}",,False,17,,False,False,,False,,[],{},,False,,1623790816.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,o0dsly,True,,serioreditor,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/o0dsly/big_tech_fails_to_recognize_african_languages_dw/,all_ads,False,https://www.youtube.com/watch?v=iU0Lj-mR9DQ,30199,1623762016.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Big tech fails to recognize African languages | DW News', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/iU0Lj-mR9DQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'DW News', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/iU0Lj-mR9DQ/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/dwnews'}}",False,https://www.youtube.com/watch?v=iU0Lj-mR9DQ,"[{'approved_at_utc': None, 'subreddit': 'linguistics', 'selftext': '', 'author_fullname': 't2_1vz39gi', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Big tech fails to recognize African languages | DW News', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/linguistics', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'video', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_o0b8ky', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 305, 'total_awards_received': 0, 'media_embed': {'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/iU0Lj-mR9DQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': {'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Big tech fails to recognize African languages | DW News', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/iU0Lj-mR9DQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'DW News', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/iU0Lj-mR9DQ/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/dwnews'}}, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/iU0Lj-mR9DQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/o0b8ky', 'height': 200}, 'link_flair_text': 'Video', 'can_mod_post': False, 'score': 305, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1623782121.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'youtube.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.youtube.com/watch?v=iU0Lj-mR9DQ', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2qhos', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'o0b8ky', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'andrewvanzyl', 'discussion_type': None, 'num_comments': 98, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/linguistics/comments/o0b8ky/big_tech_fails_to_recognize_african_languages_dw/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.youtube.com/watch?v=iU0Lj-mR9DQ', 'subreddit_subscribers': 249863, 'created_utc': 1623753321.0, 'num_crossposts': 1, 'media': {'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Big tech fails to recognize African languages | DW News', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/iU0Lj-mR9DQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'DW News', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/iU0Lj-mR9DQ/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/dwnews'}}, 'is_video': False}]",t3_o0b8ky,,,0
972,,LanguageTechnology,,t2_4rn3unhb,False,,0,False,"As a part of Pincone, a bookmark manager, we added auto-labeling of websites and wrote a bit about how we did it.",[],r/LanguageTechnology,False,6,,0,,False,t3_o0ehx9,False,dark,1.0,,public,10,1,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,False,,1623792894.0,text,6,,,text,pincone.com,False,,,,,,False,True,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,o0ehx9,True,,arsfutura,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/o0ehx9/as_a_part_of_pincone_a_bookmark_manager_we_added/,all_ads,False,https://pincone.com/blog/unsupervised-auto-labeling-of-websites,30199,1623764094.0,0,,False,https://pincone.com/blog/unsupervised-auto-labeling-of-websites,,,,,0
973,,LanguageTechnology,,t2_37ged,False,,0,False,[D] Hugging Face has released an official course,[],r/LanguageTechnology,False,6,,0,,False,t3_o0f54a,False,dark,1.0,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,False,,1623794658.0,text,6,,,text,self.MachineLearning,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,o0f54a,True,,driscoll42,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/o0f54a/d_hugging_face_has_released_an_official_course/,all_ads,False,/r/MachineLearning/comments/o04ort/d_hugging_face_has_released_an_official_course/,30199,1623765858.0,0,,False,/r/MachineLearning/comments/o04ort/d_hugging_face_has_released_an_official_course/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': ""Link: [https://huggingface.co/course/](https://huggingface.co/course/chapter1)\n\nThe incredible team over at hugging face has put out a course covering almost the entirety of their ecosystem:\n\n\\- Transformers  \n\\- Datasets  \n\\- Tokenizers  \n\\- Accelerate  \n\\- Model Hub\n\nThey also plan on hosting live office hours and facilitating study groups via their forums. \n\n&amp;#x200B;\n\nPS: If there's enough interest from APAC regions, I would love to help organise a study group. (I do not work at HF, but I'm excited to dive into this course)"", 'author_fullname': 't2_2s77fkel', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] Hugging Face has released an official course', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_o04ort', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.98, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 585, 'total_awards_received': 5, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 585, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {'gid_1': 1}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1623756772.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': True, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Link: &lt;a href=""https://huggingface.co/course/chapter1""&gt;https://huggingface.co/course/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The incredible team over at hugging face has put out a course covering almost the entirety of their ecosystem:&lt;/p&gt;\n\n&lt;p&gt;- Transformers&lt;br/&gt;\n- Datasets&lt;br/&gt;\n- Tokenizers&lt;br/&gt;\n- Accelerate&lt;br/&gt;\n- Model Hub&lt;/p&gt;\n\n&lt;p&gt;They also plan on hosting live office hours and facilitating study groups via their forums. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;PS: If there&amp;#39;s enough interest from APAC regions, I would love to help organise a study group. (I do not work at HF, but I&amp;#39;m excited to dive into this course)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 2, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}, {'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'o04ort', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'init__27', 'discussion_type': None, 'num_comments': 54, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/o04ort/d_hugging_face_has_released_an_official_course/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/o04ort/d_hugging_face_has_released_an_official_course/', 'subreddit_subscribers': 1930726, 'created_utc': 1623727972.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_o04ort,,,0
974,,LanguageTechnology,"Hey fellow NLPlers,

I'd love to ask you for some feedback on our NLG project, as we have just released our documentation to our AI copywriter API!

Now we are inviting developers to test out the current platform. Feedback from you is gold for us as we are trying to better understand where we can generate value in your daily needs! And please do share it with people which would enjoy our work!

**How we got started?**

We are two NLG Enthusiasts in Berlin who wanted to take away the complexity till somebody could leverage some of the newest GPT models. Hence, we built an infrastructure to get your AI copywriter ready in a matter of minutes!

What started as a GUI to validate that individuals and businesses show interest in natural language generation is now also just one API request away.

Please ask me any question in the chat below or on twitter via dom\_does.

Links for accessing HemingwAI API.

Documentation: [https://textcortex.com/documentation/api](https://textcortex.com/documentation/api)

Github: [https://github.com/textcortex/hemingwai](https://github.com/textcortex/hemingwai)",t2_p8q7k,False,,0,False,"HemingwAI, the API writing text with you is looking for feedback",[],r/LanguageTechnology,False,6,,0,,False,t3_o0bpf6,False,dark,0.92,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1623783815.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey fellow NLPlers,&lt;/p&gt;

&lt;p&gt;I&amp;#39;d love to ask you for some feedback on our NLG project, as we have just released our documentation to our AI copywriter API!&lt;/p&gt;

&lt;p&gt;Now we are inviting developers to test out the current platform. Feedback from you is gold for us as we are trying to better understand where we can generate value in your daily needs! And please do share it with people which would enjoy our work!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How we got started?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We are two NLG Enthusiasts in Berlin who wanted to take away the complexity till somebody could leverage some of the newest GPT models. Hence, we built an infrastructure to get your AI copywriter ready in a matter of minutes!&lt;/p&gt;

&lt;p&gt;What started as a GUI to validate that individuals and businesses show interest in natural language generation is now also just one API request away.&lt;/p&gt;

&lt;p&gt;Please ask me any question in the chat below or on twitter via dom_does.&lt;/p&gt;

&lt;p&gt;Links for accessing HemingwAI API.&lt;/p&gt;

&lt;p&gt;Documentation: &lt;a href=""https://textcortex.com/documentation/api""&gt;https://textcortex.com/documentation/api&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Github: &lt;a href=""https://github.com/textcortex/hemingwai""&gt;https://github.com/textcortex/hemingwai&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,o0bpf6,True,,kotanasu,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/o0bpf6/hemingwai_the_api_writing_text_with_you_is/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/o0bpf6/hemingwai_the_api_writing_text_with_you_is/,30199,1623755015.0,0,,False,,,,,,1103
975,,LanguageTechnology,,t2_oupz3m9,False,,0,False,How to fine-tune with BertForPretraining,[],r/LanguageTechnology,False,6,,0,,False,t3_o0hr4a,False,dark,0.84,,public,4,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/IC9FaVPKlYc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Tuning BERT #5 - Fine-tuning With BERT For Pretraining', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/IC9FaVPKlYc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'James Briggs', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/IC9FaVPKlYc/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/JamesBriggs'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/IC9FaVPKlYc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/o0hr4a', 'height': 200}",,False,4,,False,True,,False,,[],{},,False,,1623801660.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,o0hr4a,True,,jamescalam,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/o0hr4a/how_to_finetune_with_bertforpretraining/,all_ads,False,https://youtube.com/watch?v=IC9FaVPKlYc&amp;feature=share,30199,1623772860.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Tuning BERT #5 - Fine-tuning With BERT For Pretraining', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/IC9FaVPKlYc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'James Briggs', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/IC9FaVPKlYc/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/JamesBriggs'}}",False,https://youtube.com/watch?v=IC9FaVPKlYc&amp;feature=share,,,,,0
976,,LanguageTechnology,"Hi all,

I'm new to machine learning so please bear with me and forgive me for my ignorance.

I am working with medical data, specifically radiology reports. One of my potential projects hopes to take free text and be able to capture specific anatomy and their measurements, and output it into an excel in an organized fashion. For instance, would it be possible to identify the thoracic and abdominal aorta within free text, see their maximum measurements, and to output it into an excel report? 

I have already annotated all the free text reports with the anatomy and measurements of interest. How would I go about finding the correct machine learning algorithm and start training it to properly identify what I am looking for? If anyone could help point me in the right direction to get started, I would be so very grateful. Thank you",t2_2644kxj8,False,,0,False,Research project: Free text to excel output,[],r/LanguageTechnology,False,6,,0,,False,t3_o0eski,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1623793728.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m new to machine learning so please bear with me and forgive me for my ignorance.&lt;/p&gt;

&lt;p&gt;I am working with medical data, specifically radiology reports. One of my potential projects hopes to take free text and be able to capture specific anatomy and their measurements, and output it into an excel in an organized fashion. For instance, would it be possible to identify the thoracic and abdominal aorta within free text, see their maximum measurements, and to output it into an excel report? &lt;/p&gt;

&lt;p&gt;I have already annotated all the free text reports with the anatomy and measurements of interest. How would I go about finding the correct machine learning algorithm and start training it to properly identify what I am looking for? If anyone could help point me in the right direction to get started, I would be so very grateful. Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,o0eski,True,,MikeHawksGinny,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/o0eski/research_project_free_text_to_excel_output/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/o0eski/research_project_free_text_to_excel_output/,30199,1623764928.0,0,,False,,,,,,838
977,,LanguageTechnology,"I have downloaded gpt 124M on my local machine and i was able to run the [interactivesample.py](https://interactivesample.py) that was provided by them

&amp;#x200B;

But when i try to load 124M using transformers, i get following error:

*\_OSError: Can't load config for 'models\\124M'. Make sure that:*



*- 'models\\124M' is a correct model identifier listed on '*[*https://huggingface.co/models*](https://huggingface.co/models)*'*



*- or 'models\\124M' is the correct path to a directory containing a config.json file\_*

&amp;#x200B;

**\*\*My code:\*\***

tokenizer = AutoTokenizer.from\_pretrained(""models\\\\124M"")

&amp;#x200B;

124M contains following json file : encoder",t2_7wron7g8,False,,0,False,Can't run 124M using transformers,[],r/LanguageTechnology,False,6,,0,,False,t3_o0iccg,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1623803237.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have downloaded gpt 124M on my local machine and i was able to run the &lt;a href=""https://interactivesample.py""&gt;interactivesample.py&lt;/a&gt; that was provided by them&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;But when i try to load 124M using transformers, i get following error:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;_OSError: Can&amp;#39;t load config for &amp;#39;models\124M&amp;#39;. Make sure that:&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;- &amp;#39;models\124M&amp;#39; is a correct model identifier listed on &amp;#39;&lt;/em&gt;&lt;a href=""https://huggingface.co/models""&gt;&lt;em&gt;https://huggingface.co/models&lt;/em&gt;&lt;/a&gt;&lt;em&gt;&amp;#39;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;- or &amp;#39;models\124M&amp;#39; is the correct path to a directory containing a config.json file_&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;**My code:*\&lt;/strong&gt;*&lt;/p&gt;

&lt;p&gt;tokenizer = AutoTokenizer.from_pretrained(&amp;quot;models\\124M&amp;quot;)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;124M contains following json file : encoder&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,o0iccg,True,,arkhamrising,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/o0iccg/cant_run_124m_using_transformers/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/o0iccg/cant_run_124m_using_transformers/,30199,1623774437.0,0,,False,,,,,,685
978,,LanguageTechnology,"Hi all, I am currently working through a “Build ChatBots with Python” course on Codecademy which obviously spends some time on NLP. However, as a Python beginner, I feel like their explanations on a lot of topics are glazed over leaving me confused. Thus I am looking for some beginner-friendly resources I can use to supplement my course. Ultimately I want to be able to program my own chatbots with Python, and am fascinated by NLP, linguistics, and machine learning. I am open to online courses, projects to work through, YouTube videos, books, etc. Thanks in advance!",t2_4ofh5nra,False,,0,False,Best learning resources,[],r/LanguageTechnology,False,6,,0,,False,t3_o0hco1,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1623800599.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, I am currently working through a “Build ChatBots with Python” course on Codecademy which obviously spends some time on NLP. However, as a Python beginner, I feel like their explanations on a lot of topics are glazed over leaving me confused. Thus I am looking for some beginner-friendly resources I can use to supplement my course. Ultimately I want to be able to program my own chatbots with Python, and am fascinated by NLP, linguistics, and machine learning. I am open to online courses, projects to work through, YouTube videos, books, etc. Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,o0hco1,True,,fromseatosi,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/o0hco1/best_learning_resources/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/o0hco1/best_learning_resources/,30199,1623771799.0,0,,False,,,,,,571
979,,LanguageTechnology,,t2_apv7o2y8,False,,0,False,"Open Source Grammar Correction Service with Gramformer- Google Collab, fastapi,pyngrok -Python demo",[],r/LanguageTechnology,False,6,,0,,False,t3_o0ggfs,False,dark,1.0,,public,1,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3rKmeAf2p0A?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Open Source Grammar Correction Service with Gramformer- Google Collab, fastapi,pyngrok -Python demo', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3rKmeAf2p0A?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3rKmeAf2p0A/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3rKmeAf2p0A?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/o0ggfs', 'height': 200}",,False,1,,False,False,,False,,[],{},,False,,1623798236.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,o0ggfs,True,,rsree123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/o0ggfs/open_source_grammar_correction_service_with/,all_ads,False,https://youtu.be/3rKmeAf2p0A,30199,1623769436.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Open Source Grammar Correction Service with Gramformer- Google Collab, fastapi,pyngrok -Python demo', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3rKmeAf2p0A?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3rKmeAf2p0A/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,https://youtu.be/3rKmeAf2p0A,,,,,0
980,,LanguageTechnology,"I have finetune wav2vec2 large xlsr53 on WOLOF audio data set, for more info visit the [here](https://huggingface.co/kingabzpro/wav2vec2-large-xlsr-53-wolof). You can also check my [Github](https://github.com/kingabzpro/WOLOF-ASR-Wav2Vec2) repo. You can also look at my [Kaggle](https://www.kaggle.com/kingabzpro/fine-tuning-xlsr-wav2vec2-for-wolof-asr-with) notebook.",t2_yeda6sl,False,,0,False,My first contribution into hugging face,[],r/LanguageTechnology,False,6,,0,,False,t3_nzwin4,False,dark,0.94,,public,31,0,{},,False,[],,False,False,,{},,False,31,,False,False,,False,,[],{},,True,,1623732238.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have finetune wav2vec2 large xlsr53 on WOLOF audio data set, for more info visit the &lt;a href=""https://huggingface.co/kingabzpro/wav2vec2-large-xlsr-53-wolof""&gt;here&lt;/a&gt;. You can also check my &lt;a href=""https://github.com/kingabzpro/WOLOF-ASR-Wav2Vec2""&gt;Github&lt;/a&gt; repo. You can also look at my &lt;a href=""https://www.kaggle.com/kingabzpro/fine-tuning-xlsr-wav2vec2-for-wolof-asr-with""&gt;Kaggle&lt;/a&gt; notebook.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nzwin4,True,,kingabzpro,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nzwin4/my_first_contribution_into_hugging_face/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nzwin4/my_first_contribution_into_hugging_face/,30199,1623703438.0,0,,False,,,,,,368
981,,LanguageTechnology,"Hi, I'm into both U of Washington and U of Edinburgh for NLP Master's and Master's of Speech and Language Processing, respectively. Washington is remote and tuition is 40k (and 1 year in duration if full-time) and Edinburgh seems to be 1 year + dissertation at 60k for tuition. I was hoping to get some insight on which path would be more lucrative considering the costs, as well as U wash having an internship option. I am certain I want to continue onwards to industry as opposed to PHD if that helps. My concerns are getting a degree from an overseas uni, as well as slight concern about the tuition. I am a new grad (May 2021) and realized I need further education for most NLP positions in the US. 

Thank you everyone in this community for your help!",t2_br19o,False,,0,False,Edinburgh or U Washington NLP Masters? accepted and need help deciding.,[],r/LanguageTechnology,False,6,,0,,False,t3_nzurca,False,dark,0.9,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1623727609.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m into both U of Washington and U of Edinburgh for NLP Master&amp;#39;s and Master&amp;#39;s of Speech and Language Processing, respectively. Washington is remote and tuition is 40k (and 1 year in duration if full-time) and Edinburgh seems to be 1 year + dissertation at 60k for tuition. I was hoping to get some insight on which path would be more lucrative considering the costs, as well as U wash having an internship option. I am certain I want to continue onwards to industry as opposed to PHD if that helps. My concerns are getting a degree from an overseas uni, as well as slight concern about the tuition. I am a new grad (May 2021) and realized I need further education for most NLP positions in the US. &lt;/p&gt;

&lt;p&gt;Thank you everyone in this community for your help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nzurca,True,,hypoxify,,11,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nzurca/edinburgh_or_u_washington_nlp_masters_accepted/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nzurca/edinburgh_or_u_washington_nlp_masters_accepted/,30199,1623698809.0,0,,False,,,,,,756
982,,LanguageTechnology,"Normally, only fine-tuning the 125M GPT Neo model is possible due to the fact that even that model uses over 10GB of VRAM, and the 1.3B and 2.7B taking much more. By following this video and the provided Jupyter notebook, it is possible to fine-tune the larger GPT Neo models on high-end consumer hardware or on cheap cloud options

[https://www.youtube.com/watch?v=Igr1tP8WaRc](https://www.youtube.com/watch?v=Igr1tP8WaRc)",t2_3tpgoyxl,False,,0,False,Fine-tuning the larger GPT Neo models(1.3B and 2.7B) with a Jupyter notebook,[],r/LanguageTechnology,False,6,,0,,False,t3_nzzfjl,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1623740259.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Normally, only fine-tuning the 125M GPT Neo model is possible due to the fact that even that model uses over 10GB of VRAM, and the 1.3B and 2.7B taking much more. By following this video and the provided Jupyter notebook, it is possible to fine-tune the larger GPT Neo models on high-end consumer hardware or on cheap cloud options&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.youtube.com/watch?v=Igr1tP8WaRc""&gt;https://www.youtube.com/watch?v=Igr1tP8WaRc&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nzzfjl,True,,l33thaxman,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nzzfjl/finetuning_the_larger_gpt_neo_models13b_and_27b/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nzzfjl/finetuning_the_larger_gpt_neo_models13b_and_27b/,30199,1623711459.0,0,,False,,,,,,423
983,,LanguageTechnology,"I was looking at the T5 transformer model and was thinking about using it for a german generation task. This model seems really powerful but it is only trained for English. Then I found the multilingual T5 model which supports many languages. Is this way better for the non-english tasks (for example paraphrase generation in my case)? I am confused because the T5 model is able to translate from English to German, it can be trained on other languages etc.",t2_6fr49wdr,False,,0,False,Should you use MT5 instead of T5 for every non-english task?,[],r/LanguageTechnology,False,6,,0,,False,t3_o06vpi,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1623764511.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was looking at the T5 transformer model and was thinking about using it for a german generation task. This model seems really powerful but it is only trained for English. Then I found the multilingual T5 model which supports many languages. Is this way better for the non-english tasks (for example paraphrase generation in my case)? I am confused because the T5 model is able to translate from English to German, it can be trained on other languages etc.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,o06vpi,True,,LargeBrick7,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/o06vpi/should_you_use_mt5_instead_of_t5_for_every/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/o06vpi/should_you_use_mt5_instead_of_t5_for_every/,30199,1623735711.0,0,,False,,,,,,457
984,,LanguageTechnology,"A team of Researchers at BUET (Bangladesh University of Engineering and Technology) and UCLA (University of California- Los Angeles) has created a [framework that can be used to develop Android applications from text descriptions.](https://arxiv.org/pdf/2104.08301.pdf)

According to Masum Hasan, a researcher who carried out the study, their team wondered whether a full-fledged software could be built from natural language specification. Almost all the existing models for creating software based on text descriptions are based on end-to-end neural machine translation (NMT) models, which are similar to the one behind Google Translate. Usually, these models use NMT frameworks to translate human language into a source code.

Summary: https://www.marktechpost.com/2021/06/14/researchers-from-bangladesh-university-and-ucla-use-ai-to-develop-a-framework-text2app-to-create-android-apps-from-text-descriptions/

Paper: https://arxiv.org/pdf/2104.08301.pdf

Github: https://text2app.github.io/",t2_2wsvqwhg,False,,0,False,Researchers From Bangladesh University And UCLA Use AI To Develop A Framework (Text2App) To Create Android Apps From Text Descriptions,[],r/LanguageTechnology,False,6,,0,,False,t3_o06asl,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1623762363.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;A team of Researchers at BUET (Bangladesh University of Engineering and Technology) and UCLA (University of California- Los Angeles) has created a &lt;a href=""https://arxiv.org/pdf/2104.08301.pdf""&gt;framework that can be used to develop Android applications from text descriptions.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;According to Masum Hasan, a researcher who carried out the study, their team wondered whether a full-fledged software could be built from natural language specification. Almost all the existing models for creating software based on text descriptions are based on end-to-end neural machine translation (NMT) models, which are similar to the one behind Google Translate. Usually, these models use NMT frameworks to translate human language into a source code.&lt;/p&gt;

&lt;p&gt;Summary: &lt;a href=""https://www.marktechpost.com/2021/06/14/researchers-from-bangladesh-university-and-ucla-use-ai-to-develop-a-framework-text2app-to-create-android-apps-from-text-descriptions/""&gt;https://www.marktechpost.com/2021/06/14/researchers-from-bangladesh-university-and-ucla-use-ai-to-develop-a-framework-text2app-to-create-android-apps-from-text-descriptions/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/pdf/2104.08301.pdf""&gt;https://arxiv.org/pdf/2104.08301.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Github: &lt;a href=""https://text2app.github.io/""&gt;https://text2app.github.io/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,o06asl,True,,ai-lover,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/o06asl/researchers_from_bangladesh_university_and_ucla/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/o06asl/researchers_from_bangladesh_university_and_ucla/,30199,1623733563.0,0,,False,,,,,,994
985,,LanguageTechnology,"I hate ending up going in google spirals , forgetting terms, confusing terms. I am only talking about books for English and maybe any Arabic letter languages although only Eng is necessary.

Research papers and articles are welcome as well.",t2_iu54y,False,,0,False,"I want to learn about Language, Linguistics, Etymology, and everything in between in one book. Any recommendations. Bonus points if it's computationally relevant.",[],r/LanguageTechnology,False,6,,0,,False,t3_nzinae,False,dark,0.87,,public,17,0,{},,False,[],,False,False,,{},,False,17,,False,False,,False,,[],{},,True,,1623690673.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I hate ending up going in google spirals , forgetting terms, confusing terms. I am only talking about books for English and maybe any Arabic letter languages although only Eng is necessary.&lt;/p&gt;

&lt;p&gt;Research papers and articles are welcome as well.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nzinae,True,,redditssexiestguy,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nzinae/i_want_to_learn_about_language_linguistics/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nzinae/i_want_to_learn_about_language_linguistics/,30199,1623661873.0,0,,False,,,,,,240
986,,LanguageTechnology,"Hello,

I have seen his sentiment analysis tutorial here: [https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment?text=I+like+you.+I+love+you](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment?text=I+like+you.+I+love+you)

Now, I was wondering, how do I do such a task using my own dataset? I have list of different sentences and I want to find out the sentiment of each sentence, which can be either ""happy"", ""sad"" or ""angry"". 

Do I need to fine tune BERT with my own data? If  so, how?

Thanks.",t2_13ussz,False,,0,False,BERT: How to do sentiment analysis on a custom dataset?,[],r/LanguageTechnology,False,6,,0,,False,t3_nznn4m,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1623708601.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I have seen his sentiment analysis tutorial here: &lt;a href=""https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment?text=I+like+you.+I+love+you""&gt;https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment?text=I+like+you.+I+love+you&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now, I was wondering, how do I do such a task using my own dataset? I have list of different sentences and I want to find out the sentiment of each sentence, which can be either &amp;quot;happy&amp;quot;, &amp;quot;sad&amp;quot; or &amp;quot;angry&amp;quot;. &lt;/p&gt;

&lt;p&gt;Do I need to fine tune BERT with my own data? If  so, how?&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nznn4m,True,,saucyhambon,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nznn4m/bert_how_to_do_sentiment_analysis_on_a_custom/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nznn4m/bert_how_to_do_sentiment_analysis_on_a_custom/,30199,1623679801.0,0,,False,,,,,,536
987,,LanguageTechnology,"I downloaded gpt 124 M and i was able to run the interactive sample file.
How can I run 124M using transformers.
While running:
Auto tokenizer. Frompretrained(""c:\\path\\124M"")

I get an error :configuration not found.

Note:124 M contains encoder. Json, hoarams. Json,... Etc.",t2_7wron7g8,False,,0,False,Gpt 2 124m using transformers,[],r/LanguageTechnology,False,6,,0,,False,t3_nzudh7,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1623726567.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I downloaded gpt 124 M and i was able to run the interactive sample file.
How can I run 124M using transformers.
While running:
Auto tokenizer. Frompretrained(&amp;quot;c:\path\124M&amp;quot;)&lt;/p&gt;

&lt;p&gt;I get an error :configuration not found.&lt;/p&gt;

&lt;p&gt;Note:124 M contains encoder. Json, hoarams. Json,... Etc.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nzudh7,True,,arkhamrising,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nzudh7/gpt_2_124m_using_transformers/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nzudh7/gpt_2_124m_using_transformers/,30199,1623697767.0,0,,False,,,,,,277
988,,LanguageTechnology,,t2_l7idt,False,,0,False,Energy-Based Models for Code Generation under Compilability Constraints,[],r/LanguageTechnology,False,6,,0,,False,t3_nzyir7,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,False,,1623737595.0,text,6,,,text,arxiv.org,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nzyir7,True,,pigdogsheep,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nzyir7/energybased_models_for_code_generation_under/,all_ads,False,https://arxiv.org/pdf/2106.04985.pdf,30199,1623708795.0,0,,False,https://arxiv.org/pdf/2106.04985.pdf,,,,,0
989,,LanguageTechnology,"I’m working on training a STT model with DeepSpeech that is based on a pre-trained model but will specialize in transcribing medical conversation (diseases, medications, etc). The most frustrating part so far has been a lack of medical audio data for training. I’m looking for high quality audio on the scale of thousands of hours with accurate transcriptions. Does anyone know if such a dataset exists?",t2_10wkv7,False,,0,False,Looking for medical audio/transcription dataset for training STT model with DeepSpeech.,[],r/LanguageTechnology,False,6,,0,,False,t3_nzp9s4,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1623713011.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m working on training a STT model with DeepSpeech that is based on a pre-trained model but will specialize in transcribing medical conversation (diseases, medications, etc). The most frustrating part so far has been a lack of medical audio data for training. I’m looking for high quality audio on the scale of thousands of hours with accurate transcriptions. Does anyone know if such a dataset exists?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nzp9s4,True,,SimplyJacoby,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nzp9s4/looking_for_medical_audiotranscription_dataset/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nzp9s4/looking_for_medical_audiotranscription_dataset/,30199,1623684211.0,0,,False,,,,,,403
990,,LanguageTechnology,,t2_3qfhbsn9,False,,0,False,How To Build and Deploy an NLP Model with FastAPI: Part 1,[],r/LanguageTechnology,False,6,,0,,False,t3_nzglu3,False,dark,0.83,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,False,,1623681435.0,text,6,,,text,hackernoon.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nzglu3,True,,LimarcAmbalina,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nzglu3/how_to_build_and_deploy_an_nlp_model_with_fastapi/,all_ads,False,https://hackernoon.com/how-to-build-and-deploy-an-nlp-model-with-fastapi-part-1-n5w35cj,30199,1623652635.0,0,,False,https://hackernoon.com/how-to-build-and-deploy-an-nlp-model-with-fastapi-part-1-n5w35cj,,,,,0
991,,LanguageTechnology,"Deep learning is one area of technology where ambitiousness has no barriers. According to a recent announcement by [The Beijing Academy of Artificial Intelligence (BAAI)](https://www.baai.ac.cn/), in China, yet another milestone has been achieved in the field with its “Wu Dao” AI system. The [GPT 3](https://www.marktechpost.com/2020/08/02/gpt-3-a-new-breakthrough-in-language-generator/) brought in new interest for all the AI researchers, the super scale pre training models. By this approach and making use of 175 billion parameters, it managed to achieve exceptional performance results across the natural language processing tasks (NLP). However, the lacking component is its inability to have any form of cognitive abilities or common sense. Therefore, despite the size, even these models cannot indulge in tasks such as open dialogues, visual reasoning, and so on. With Wu Dao, the researchers plan to address this issue. This is China’s first attempt at a home-grown super-scale intelligent model system. 

Article: [https://www.marktechpost.com/2021/06/13/this-chinese-super-scale-intelligence-model-wu-dao-2-0-claims-to-be-trained-using-1-75-trillion-parameters-surpassing-all-prior-models-to-achieve-a-new-breakthrough-in-deep-learning/](https://www.marktechpost.com/2021/06/13/this-chinese-super-scale-intelligence-model-wu-dao-2-0-claims-to-be-trained-using-1-75-trillion-parameters-surpassing-all-prior-models-to-achieve-a-new-breakthrough-in-deep-learning/?_ga=2.13897584.636390090.1623335762-488125022.1618729090)

Reference: [https://syncedreview.com/2021/03/23/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0/](https://syncedreview.com/2021/03/23/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0/)",t2_2wsvqwhg,False,,0,False,"This Chinese Super Scale Intelligence Model, ‘Wu Dao 2.0’, Claims To Be Trained Using 1.75 Trillion Parameters, Surpassing All Prior Models to Achieve a New Breakthrough in Deep Learning",[],r/LanguageTechnology,False,6,,0,,False,t3_nzgkbn,False,dark,0.67,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1623681251.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Deep learning is one area of technology where ambitiousness has no barriers. According to a recent announcement by &lt;a href=""https://www.baai.ac.cn/""&gt;The Beijing Academy of Artificial Intelligence (BAAI)&lt;/a&gt;, in China, yet another milestone has been achieved in the field with its “Wu Dao” AI system. The &lt;a href=""https://www.marktechpost.com/2020/08/02/gpt-3-a-new-breakthrough-in-language-generator/""&gt;GPT 3&lt;/a&gt; brought in new interest for all the AI researchers, the super scale pre training models. By this approach and making use of 175 billion parameters, it managed to achieve exceptional performance results across the natural language processing tasks (NLP). However, the lacking component is its inability to have any form of cognitive abilities or common sense. Therefore, despite the size, even these models cannot indulge in tasks such as open dialogues, visual reasoning, and so on. With Wu Dao, the researchers plan to address this issue. This is China’s first attempt at a home-grown super-scale intelligent model system. &lt;/p&gt;

&lt;p&gt;Article: &lt;a href=""https://www.marktechpost.com/2021/06/13/this-chinese-super-scale-intelligence-model-wu-dao-2-0-claims-to-be-trained-using-1-75-trillion-parameters-surpassing-all-prior-models-to-achieve-a-new-breakthrough-in-deep-learning/?_ga=2.13897584.636390090.1623335762-488125022.1618729090""&gt;https://www.marktechpost.com/2021/06/13/this-chinese-super-scale-intelligence-model-wu-dao-2-0-claims-to-be-trained-using-1-75-trillion-parameters-surpassing-all-prior-models-to-achieve-a-new-breakthrough-in-deep-learning/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Reference: &lt;a href=""https://syncedreview.com/2021/03/23/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0/""&gt;https://syncedreview.com/2021/03/23/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nzgkbn,True,,ai-lover,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nzgkbn/this_chinese_super_scale_intelligence_model_wu/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nzgkbn/this_chinese_super_scale_intelligence_model_wu/,30199,1623652451.0,0,,False,,,,,,1759
992,,LanguageTechnology,"Anybody participating?   


https://www.aicrowd.com/challenges/ai-blitz-9?utm\_source=reddit&amp;utm\_medium=languagetechnology&amp;utm\_campaign=blitz9",t2_ibqd1,False,,0,False,The NLP challenge: 5 Puzzles in 2 Weeks,[],r/LanguageTechnology,False,6,,0,,False,t3_nzfnll,False,dark,0.81,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1623677729.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Anybody participating?   &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.aicrowd.com/challenges/ai-blitz-9?utm%5C_source=reddit&amp;amp;utm%5C_medium=languagetechnology&amp;amp;utm%5C_campaign=blitz9""&gt;https://www.aicrowd.com/challenges/ai-blitz-9?utm\_source=reddit&amp;amp;utm\_medium=languagetechnology&amp;amp;utm\_campaign=blitz9&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nzfnll,True,,EscapedLaughter,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nzfnll/the_nlp_challenge_5_puzzles_in_2_weeks/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nzfnll/the_nlp_challenge_5_puzzles_in_2_weeks/,30199,1623648929.0,0,,False,,,,,,152
993,,LanguageTechnology,"Hi. I'm currently wondering if the NLP task of question answering (QA) can be formulated as classification as opposed to span extraction. For example, if I feed a model `[CLS] Where does President Roosevelt live? [SEP] CONTEXT` it seems that almost all (if not all) QA models formulate the task as span extraction and return the correct answer that's contained within the document. However, I'm curious whether we could have labels and formulate it as classification instead.

If anybody knows any papers or resources I could look into, that'd be great. Thanks!",t2_m8kccne,False,,0,False,Can the task of question answering ever be formulated as classification?,[],r/LanguageTechnology,False,6,,0,,False,t3_nzb0jq,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1623661974.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi. I&amp;#39;m currently wondering if the NLP task of question answering (QA) can be formulated as classification as opposed to span extraction. For example, if I feed a model &lt;code&gt;[CLS] Where does President Roosevelt live? [SEP] CONTEXT&lt;/code&gt; it seems that almost all (if not all) QA models formulate the task as span extraction and return the correct answer that&amp;#39;s contained within the document. However, I&amp;#39;m curious whether we could have labels and formulate it as classification instead.&lt;/p&gt;

&lt;p&gt;If anybody knows any papers or resources I could look into, that&amp;#39;d be great. Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nzb0jq,True,,Seankala,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nzb0jq/can_the_task_of_question_answering_ever_be/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nzb0jq/can_the_task_of_question_answering_ever_be/,30199,1623633174.0,0,,False,,,,,,561
994,,LanguageTechnology,"Hi , i’m working on a project to build knowledge graph , so that the relationships within the document will help us to query documents with those attributes from the entire corpus.
Would like some help what methods can be used to mine relationships from the document.",t2_5jruncrw,False,,0,False,Knowledge Graph,[],r/LanguageTechnology,False,6,,0,,False,t3_nyzdmz,False,dark,1.0,,public,16,0,{},,False,[],,False,False,,{},,False,16,,False,False,,False,,[],{},,True,,1623628643.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi , i’m working on a project to build knowledge graph , so that the relationships within the document will help us to query documents with those attributes from the entire corpus.
Would like some help what methods can be used to mine relationships from the document.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nyzdmz,True,,dipanjann_,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nyzdmz/knowledge_graph/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nyzdmz/knowledge_graph/,30199,1623599843.0,0,,False,,,,,,267
995,,LanguageTechnology,"[FLORES-101](https://l.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2Ffacebookresearch%2Fflores%3Ffbclid%3DIwAR18XHNF2irnZUwv6ZwTrfX9LTX7NFQW-GHaVPTMffhKWPQcgBjgMD1A8mo&amp;h=AT03MSblPXA3bufCYi2Xy_GR-fxCQFim0iX2w2ZkT5fiiJLjE-Zltua9obJXi4oRaOaqVHEEZ3QNRN8SXrC8lt5lLw3P4Hnlvw6L3ldX5I_z35thn6992Ve6b8H5wYE_JiYd_A), a first-of-its-kind, many-to-many evaluation data set that covers 101 languages from around the world, is now open-sourced. FLORES-101 is a tool that allows researchers to test and refine multilingual translation models such as M2M-100 quickly. To speed work on many-to-many translation systems worldwide, Facebook AI makes [the complete FLORES-101 data set](https://github.com/facebookresearch/flores?fbclid=IwAR1pjSZbSRQhxv9QccaCCApZLS0zltZy0uji24rWt9QxTR0HgWkNCGu_F2M), and associated technical report, and various models freely available for anybody to use.

Full Article: [https://www.marktechpost.com/2021/06/12/facebook-ai-open-source-the-flores-101-data-set-for-better-translation-systems-around-the-world/](https://www.marktechpost.com/2021/06/12/facebook-ai-open-source-the-flores-101-data-set-for-better-translation-systems-around-the-world/) 

Paper: https://arxiv.org/abs/2106.03193",t2_2wsvqwhg,False,,0,False,"Facebook AI Open-Source ‘The FLORES-101 Data Set’, For Better Translation Systems Around The World (Paper included)",[],r/LanguageTechnology,False,6,,0,,False,t3_nypgu0,False,dark,1.0,,public,17,0,{},,False,[],,False,False,,{},,False,17,,False,False,,False,,[],{},,True,,1623590618.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://l.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2Ffacebookresearch%2Fflores%3Ffbclid%3DIwAR18XHNF2irnZUwv6ZwTrfX9LTX7NFQW-GHaVPTMffhKWPQcgBjgMD1A8mo&amp;amp;h=AT03MSblPXA3bufCYi2Xy_GR-fxCQFim0iX2w2ZkT5fiiJLjE-Zltua9obJXi4oRaOaqVHEEZ3QNRN8SXrC8lt5lLw3P4Hnlvw6L3ldX5I_z35thn6992Ve6b8H5wYE_JiYd_A""&gt;FLORES-101&lt;/a&gt;, a first-of-its-kind, many-to-many evaluation data set that covers 101 languages from around the world, is now open-sourced. FLORES-101 is a tool that allows researchers to test and refine multilingual translation models such as M2M-100 quickly. To speed work on many-to-many translation systems worldwide, Facebook AI makes &lt;a href=""https://github.com/facebookresearch/flores?fbclid=IwAR1pjSZbSRQhxv9QccaCCApZLS0zltZy0uji24rWt9QxTR0HgWkNCGu_F2M""&gt;the complete FLORES-101 data set&lt;/a&gt;, and associated technical report, and various models freely available for anybody to use.&lt;/p&gt;

&lt;p&gt;Full Article: &lt;a href=""https://www.marktechpost.com/2021/06/12/facebook-ai-open-source-the-flores-101-data-set-for-better-translation-systems-around-the-world/""&gt;https://www.marktechpost.com/2021/06/12/facebook-ai-open-source-the-flores-101-data-set-for-better-translation-systems-around-the-world/&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/abs/2106.03193""&gt;https://arxiv.org/abs/2106.03193&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nypgu0,True,,ai-lover,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nypgu0/facebook_ai_opensource_the_flores101_data_set_for/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nypgu0/facebook_ai_opensource_the_flores101_data_set_for/,30199,1623561818.0,0,,False,,,,,,1208
996,,LanguageTechnology,,t2_159buvym,False,,0,False,Can anyone please guide me to a video based Automatic speech recognition (ASR) course ?,[],r/LanguageTechnology,False,6,,0,,False,t3_nyw4qu,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1623619057.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nyw4qu,True,,rakshith291,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nyw4qu/can_anyone_please_guide_me_to_a_video_based/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nyw4qu/can_anyone_please_guide_me_to_a_video_based/,30199,1623590257.0,0,,False,,,,,,0
997,,LanguageTechnology,"I am confused about which out of BERT transformer should be fed to dense layer. BERT transformer output is \`batch,sequence length, 768 \`. So I am confused about how to pass it to the dense layer",t2_5owk7j7,False,,0,False,How to pass BERT output to dense layer,[],r/LanguageTechnology,False,6,,0,,False,t3_nyx09y,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1623621795.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am confused about which out of BERT transformer should be fed to dense layer. BERT transformer output is `batch,sequence length, 768 `. So I am confused about how to pass it to the dense layer&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nyx09y,True,,mrtac96,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nyx09y/how_to_pass_bert_output_to_dense_layer/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nyx09y/how_to_pass_bert_output_to_dense_layer/,30199,1623592995.0,0,,False,,,,,,196
998,,LanguageTechnology,,t2_hkv9s,False,,0,False,Detecting Hallucinated Content in Conditional Neural Sequence Generation (NLP Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_nyvm5d,False,dark,0.33,,public,0,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/fD2g9s1Isi4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Detecting Hallucinated Content in Conditional Neural Sequence Generation (NLP Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/fD2g9s1Isi4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/fD2g9s1Isi4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/fD2g9s1Isi4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nyvm5d', 'height': 200}",,False,0,,False,False,,False,,[],{},,False,,1623617362.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nyvm5d,True,,prakhar21,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nyvm5d/detecting_hallucinated_content_in_conditional/,all_ads,False,https://youtu.be/fD2g9s1Isi4,30199,1623588562.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Detecting Hallucinated Content in Conditional Neural Sequence Generation (NLP Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/fD2g9s1Isi4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/fD2g9s1Isi4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/fD2g9s1Isi4,,,,,0
999,,LanguageTechnology,,t2_apv7o2y8,False,,0,False,Text Classification using spaCy v3.0 transformers in Python | Natural Language Processing Tutorial,[],r/LanguageTechnology,False,6,,0,,False,t3_nyv1pp,False,dark,0.5,,public,0,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/NkuqNItEbsc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Text Classification using spaCy v3.0 transformers  in Python | Natural Language Processing Tutorial', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/NkuqNItEbsc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/NkuqNItEbsc/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/NkuqNItEbsc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nyv1pp', 'height': 200}",,False,0,,False,False,,False,,[],{},,False,,1623615362.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nyv1pp,True,,rsree123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nyv1pp/text_classification_using_spacy_v30_transformers/,all_ads,False,https://youtu.be/NkuqNItEbsc,30199,1623586562.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Text Classification using spaCy v3.0 transformers  in Python | Natural Language Processing Tutorial', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/NkuqNItEbsc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/NkuqNItEbsc/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,https://youtu.be/NkuqNItEbsc,,,,,0
1000,,LanguageTechnology,"Hello everyone,

Really excited to share with you that our paper ""Towards Emotional Support Dialog Systems"" got accepted to the main conference of the Association for Computational Linguistics this year (ACL2021). We strongly believe that dialogue systems have the potential to become powerful emotional supporters that can help many individuals with their daily struggles. Towards this goal, we created a high-quality dataset of emotional conversations between trained crowdsourcing workers and will be making this dataset publicly available.

Feel free to read our [paper](https://arxiv.org/abs/2106.01144) for more details.

Thank you and wish you a great day!",t2_aoiqtg2t,False,,0,False,Towards Emotional Support Dialog Systems,[],r/LanguageTechnology,False,6,,0,,False,t3_nyndog,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,1623564425.0,,[],{},,True,,1623582401.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;

&lt;p&gt;Really excited to share with you that our paper &amp;quot;Towards Emotional Support Dialog Systems&amp;quot; got accepted to the main conference of the Association for Computational Linguistics this year (ACL2021). We strongly believe that dialogue systems have the potential to become powerful emotional supporters that can help many individuals with their daily struggles. Towards this goal, we created a high-quality dataset of emotional conversations between trained crowdsourcing workers and will be making this dataset publicly available.&lt;/p&gt;

&lt;p&gt;Feel free to read our &lt;a href=""https://arxiv.org/abs/2106.01144""&gt;paper&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;Thank you and wish you a great day!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nyndog,True,,Sahand_sab,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nyndog/towards_emotional_support_dialog_systems/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nyndog/towards_emotional_support_dialog_systems/,30199,1623553601.0,0,,False,,,,,,663
1001,,LanguageTechnology,"Hi,
I downloaded gpt neo from theeye.eye on my pc.
It downloaded a various checkpoints.
How do i use them? ... Because in order too load and use model I'd need encoder. Json, pytorch. Bin, etc..",t2_7wron7g8,False,,0,False,Using gpt neo checkpoints,[],r/LanguageTechnology,False,6,,0,,False,t3_nys2gg,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1623602422.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,
I downloaded gpt neo from theeye.eye on my pc.
It downloaded a various checkpoints.
How do i use them? ... Because in order too load and use model I&amp;#39;d need encoder. Json, pytorch. Bin, etc..&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nys2gg,True,,arkhamrising,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nys2gg/using_gpt_neo_checkpoints/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nys2gg/using_gpt_neo_checkpoints/,30199,1623573622.0,0,,False,,,,,,194
1002,,LanguageTechnology,"Hey!  
I am involved in a project where I am trying to create a programming language that uses machine learning to compile text as computer code, some info here ([https://github.com/quantleaf/quantleaf-language-documentation](https://github.com/quantleaf/quantleaf-language-documentation)). This is not yet open source as a whole, but I am currently in the process of doing so. The first subproject to be released is a search engine library that enables you to score documents with a value between 0 and 1 (I call it “zero-to-one” score). In short, this is done by evaluating the product of how close we are to a perfect match regarding document length and query length, but also in terms of the amount of tokens, in the document and in the query.

I have created a small recipe search demo for this to showcase it benefits (and potential drawbacks)

[https://quantleaf.github.io/probly-search-demo/](https://quantleaf.github.io/probly-search-demo/)

When searching “Garlic Chicken”, the first two results are:

For the “zero-to-one” scoring
“Garlic Chicken” score 1.
“Garlic-Sherry Chicken” score: 0.7307692307692308

For BM25 (standard parameters)
“Garlic Oven Fried Chicken” score 8.564332563809089
“Garlic Chicken“ score 8.455662889754347

For the BM25 algorithm the perfect match is not the top score. You could circumvent this behaviour by adjusting the parameters of the BM25 algorithm. Or is it in conjunction with a term matching algorithm. But for my programming language project, this was not good enough. I needed a score to be 1, to know when we are 100% matching, 0.5 if we are matching with a 50% relevance, hence I created this.

The search engine is written in Rust, but you could use it in any Node project if you write a little bit of WASM bindgen code (see the demo source code).

Library source code: [https://github.com/quantleaf/probly-search](https://github.com/quantleaf/probly-search)Demo source code: [https://github.com/quantleaf/probly-search-demo](https://github.com/quantleaf/probly-search-demo)

I am curious with what your take is on this scoring function, would you find it useful in comparison to the solution that you currently are using? (Especially for title/label matching)",t2_4db2l3yk,False,,0,False,A Search Engine with a normalized scoring function,[],r/LanguageTechnology,False,6,,0,,False,t3_nybd9c,False,dark,0.82,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,True,,1623604248.0,,[],{},,True,,1623546518.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey!&lt;br/&gt;
I am involved in a project where I am trying to create a programming language that uses machine learning to compile text as computer code, some info here (&lt;a href=""https://github.com/quantleaf/quantleaf-language-documentation""&gt;https://github.com/quantleaf/quantleaf-language-documentation&lt;/a&gt;). This is not yet open source as a whole, but I am currently in the process of doing so. The first subproject to be released is a search engine library that enables you to score documents with a value between 0 and 1 (I call it “zero-to-one” score). In short, this is done by evaluating the product of how close we are to a perfect match regarding document length and query length, but also in terms of the amount of tokens, in the document and in the query.&lt;/p&gt;

&lt;p&gt;I have created a small recipe search demo for this to showcase it benefits (and potential drawbacks)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://quantleaf.github.io/probly-search-demo/""&gt;https://quantleaf.github.io/probly-search-demo/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;When searching “Garlic Chicken”, the first two results are:&lt;/p&gt;

&lt;p&gt;For the “zero-to-one” scoring
“Garlic Chicken” score 1.
“Garlic-Sherry Chicken” score: 0.7307692307692308&lt;/p&gt;

&lt;p&gt;For BM25 (standard parameters)
“Garlic Oven Fried Chicken” score 8.564332563809089
“Garlic Chicken“ score 8.455662889754347&lt;/p&gt;

&lt;p&gt;For the BM25 algorithm the perfect match is not the top score. You could circumvent this behaviour by adjusting the parameters of the BM25 algorithm. Or is it in conjunction with a term matching algorithm. But for my programming language project, this was not good enough. I needed a score to be 1, to know when we are 100% matching, 0.5 if we are matching with a 50% relevance, hence I created this.&lt;/p&gt;

&lt;p&gt;The search engine is written in Rust, but you could use it in any Node project if you write a little bit of WASM bindgen code (see the demo source code).&lt;/p&gt;

&lt;p&gt;Library source code: &lt;a href=""https://github.com/quantleaf/probly-search""&gt;https://github.com/quantleaf/probly-search&lt;/a&gt;Demo source code: &lt;a href=""https://github.com/quantleaf/probly-search-demo""&gt;https://github.com/quantleaf/probly-search-demo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I am curious with what your take is on this scoring function, would you find it useful in comparison to the solution that you currently are using? (Especially for title/label matching)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nybd9c,True,,marcus-pousette,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nybd9c/a_search_engine_with_a_normalized_scoring_function/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nybd9c/a_search_engine_with_a_normalized_scoring_function/,30199,1623517718.0,0,,False,,,,,,2212
1003,,LanguageTechnology,"Context: I'm a hobbyist that got into NLP superficially, so I don't expect a definitive answer. I'd be grateful if you could just point me in the right direction, because I was overwhelmed with all the different approaches after googling it.

Here's what I want to do:

I have laws and legislation split into its smallest pieces. Each piece is a specific string. So one or two lines, maybe a paragraph, with a specific rule saying what you can and cannot do, who is responsible for what, how many days you have to do something, what is the punishment for breaking it etc. I also have a corpus of written tests (about 70k total), with questions that are often about said rules. For each small string, I want to see how many times it was mentioned in tests, so I can rank them in regards to which subjects are more likely to be asked about in tests.

How would you go about doing something like that?",t2_ix5xm,False,,0,False,Best approach to find (similar) matches for a string in a corpus of documents?,[],r/LanguageTechnology,False,6,,0,,False,t3_nyctxd,False,dark,0.72,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1623550725.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Context: I&amp;#39;m a hobbyist that got into NLP superficially, so I don&amp;#39;t expect a definitive answer. I&amp;#39;d be grateful if you could just point me in the right direction, because I was overwhelmed with all the different approaches after googling it.&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s what I want to do:&lt;/p&gt;

&lt;p&gt;I have laws and legislation split into its smallest pieces. Each piece is a specific string. So one or two lines, maybe a paragraph, with a specific rule saying what you can and cannot do, who is responsible for what, how many days you have to do something, what is the punishment for breaking it etc. I also have a corpus of written tests (about 70k total), with questions that are often about said rules. For each small string, I want to see how many times it was mentioned in tests, so I can rank them in regards to which subjects are more likely to be asked about in tests.&lt;/p&gt;

&lt;p&gt;How would you go about doing something like that?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nyctxd,True,,LiarsEverywhere,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nyctxd/best_approach_to_find_similar_matches_for_a/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nyctxd/best_approach_to_find_similar_matches_for_a/,30199,1623521925.0,0,,False,,,,,,898
1004,,LanguageTechnology,"Hi, I want to extract text from image where text is printed columnwise (like research papers) does anybody know any libraries that will help me with it? Or advice how to approach with such use case? 

Thanks in advance!",t2_4qmstn7u,False,,0,False,Extract text from image which is printed columnwise,[],r/LanguageTechnology,False,6,,0,,False,t3_nyfn5w,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1623558422.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I want to extract text from image where text is printed columnwise (like research papers) does anybody know any libraries that will help me with it? Or advice how to approach with such use case? &lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nyfn5w,True,,karishmaD,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nyfn5w/extract_text_from_image_which_is_printed/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nyfn5w/extract_text_from_image_which_is_printed/,30199,1623529622.0,0,,False,,,,,,219
1005,,LanguageTechnology,"Hi All,
I downloaded the model from
https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/

after which i changed model_path in config.json to:
""model_path"" : ""C:\Users\GPT_NEO_2\GPT3_XL""

Whenever i run the following code:
model = GPTNeoForCausalLM.from_pretrained(""C:\Users\GPT_NEO_2\GPT3_XL"")

i get an error:
f""Error no file named {[WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME + '.index', FLAX_WEIGHTS_NAME]} found in ""
OSError: Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index', 'flax_model.msgpack'] found in directory C:\Users\GPT_NEO_2\GPT3_XL or from_tf and from_flax set to False.

and while running :
generator = pipeline('text-generation', model=""C:\Users\GPT_NEO_2\GPT3_XL"")

i get following error:
f""Unrecognized model in {pretrained_model_name_or_path}. ""

I have the latest TF and torch (both cpu).

Thanks",t2_7wron7g8,False,,0,False,Can't run gpt3 xl,[],r/LanguageTechnology,False,6,,0,,False,t3_ny88wd,False,dark,0.4,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1623537663.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All,
I downloaded the model from
&lt;a href=""https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/""&gt;https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;after which i changed model_path in config.json to:
&amp;quot;model_path&amp;quot; : &amp;quot;C:\Users\GPT_NEO_2\GPT3_XL&amp;quot;&lt;/p&gt;

&lt;p&gt;Whenever i run the following code:
model = GPTNeoForCausalLM.from_pretrained(&amp;quot;C:\Users\GPT_NEO_2\GPT3_XL&amp;quot;)&lt;/p&gt;

&lt;p&gt;i get an error:
f&amp;quot;Error no file named {[WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME + &amp;#39;.index&amp;#39;, FLAX_WEIGHTS_NAME]} found in &amp;quot;
OSError: Error no file named [&amp;#39;pytorch_model.bin&amp;#39;, &amp;#39;tf_model.h5&amp;#39;, &amp;#39;model.ckpt.index&amp;#39;, &amp;#39;flax_model.msgpack&amp;#39;] found in directory C:\Users\GPT_NEO_2\GPT3_XL or from_tf and from_flax set to False.&lt;/p&gt;

&lt;p&gt;and while running :
generator = pipeline(&amp;#39;text-generation&amp;#39;, model=&amp;quot;C:\Users\GPT_NEO_2\GPT3_XL&amp;quot;)&lt;/p&gt;

&lt;p&gt;i get following error:
f&amp;quot;Unrecognized model in {pretrained_model_name_or_path}. &amp;quot;&lt;/p&gt;

&lt;p&gt;I have the latest TF and torch (both cpu).&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ny88wd,True,,arkhamrising,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ny88wd/cant_run_gpt3_xl/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ny88wd/cant_run_gpt3_xl/,30199,1623508863.0,0,,False,,,,,,850
1006,,LanguageTechnology,"Hi folks. I wanted to find all words that indicated a certain attribute. For example, consider finding all words that are 'hot'. That could include 'fire', 'flame', 'lava', 'heat' or a 'stove'.

The approach I came up with is to use word embeddings and the resulting word graph. If I start with 'hot', I could add all words within a certain distance, and then perform this process for all subsequent nodes to find paths of a certain max total distance. All vertices in this subgraph would be my answer. But I would still have to account for colloquialism.

Are there any existing frameworks or datasets that already do it like this or with any other methods?

Thanks in advance!",t2_1sw9bp4y,False,,0,False,Detecting all words/entities with a certain attribute,[],r/LanguageTechnology,False,6,,0,,False,t3_nxlmi0,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1623461786.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi folks. I wanted to find all words that indicated a certain attribute. For example, consider finding all words that are &amp;#39;hot&amp;#39;. That could include &amp;#39;fire&amp;#39;, &amp;#39;flame&amp;#39;, &amp;#39;lava&amp;#39;, &amp;#39;heat&amp;#39; or a &amp;#39;stove&amp;#39;.&lt;/p&gt;

&lt;p&gt;The approach I came up with is to use word embeddings and the resulting word graph. If I start with &amp;#39;hot&amp;#39;, I could add all words within a certain distance, and then perform this process for all subsequent nodes to find paths of a certain max total distance. All vertices in this subgraph would be my answer. But I would still have to account for colloquialism.&lt;/p&gt;

&lt;p&gt;Are there any existing frameworks or datasets that already do it like this or with any other methods?&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nxlmi0,True,,aklagoo,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nxlmi0/detecting_all_wordsentities_with_a_certain/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nxlmi0/detecting_all_wordsentities_with_a_certain/,30199,1623432986.0,0,,False,,,,,,678
1007,,LanguageTechnology,,t2_3lgg4,False,,0,False,At OpenDialog we make NLU more useful by making it work less. Combining context and pro-active conversation management to reduce reliance on NLU understanding state.,[],r/LanguageTechnology,False,6,,0,,False,t3_nxb3ar,False,dark,0.76,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,False,,1623429460.0,text,6,,,text,opendialog.ai,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nxb3ar,True,,istos,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nxb3ar/at_opendialog_we_make_nlu_more_useful_by_making/,all_ads,False,https://opendialog.ai/2021/06/08/how-opendialog-approaches-natural-language-understanding/,30199,1623400660.0,0,,False,https://opendialog.ai/2021/06/08/how-opendialog-approaches-natural-language-understanding/,,,,,0
1008,,LanguageTechnology,,t2_apv7o2y8,False,,0,False,Custom Named Entity (Disease) Recognition in clinical text with spaCy v3 Transformers in Python,[],r/LanguageTechnology,False,6,,0,,False,t3_nxie9m,False,dark,0.67,,public,2,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Nv3TqzT2RLI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Custom  Named Entity (Disease) Recognition in clinical text with spaCy v3 Transformers in Python', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Nv3TqzT2RLI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Nv3TqzT2RLI/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Nv3TqzT2RLI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nxie9m', 'height': 200}",,False,2,,False,False,,False,,[],{},,False,,1623453424.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nxie9m,True,,rsree123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nxie9m/custom_named_entity_disease_recognition_in/,all_ads,False,https://youtu.be/Nv3TqzT2RLI,30199,1623424624.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Custom  Named Entity (Disease) Recognition in clinical text with spaCy v3 Transformers in Python', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Nv3TqzT2RLI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Nv3TqzT2RLI/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,https://youtu.be/Nv3TqzT2RLI,,,,,0
1009,,LanguageTechnology,"Im able to predict document topics (such as document A is 60% Topic 1, and 40% topic 2). But cant find a way to classify what sentences are Topic 1 and which ones are Topic 2. One way of doing this could be to use sentence boundaries to extract sentences and then use gensim to predict each sentence but im looking for a formal way to do it.",t2_5r4mo7fh,False,,0,False,How to use gensim topic modeling to predict sentences in a document?,[],r/LanguageTechnology,False,6,,0,,False,t3_nxj9dj,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,1623427603.0,,[],{},,True,,1623455695.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Im able to predict document topics (such as document A is 60% Topic 1, and 40% topic 2). But cant find a way to classify what sentences are Topic 1 and which ones are Topic 2. One way of doing this could be to use sentence boundaries to extract sentences and then use gensim to predict each sentence but im looking for a formal way to do it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nxj9dj,True,,Epiphany925,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nxj9dj/how_to_use_gensim_topic_modeling_to_predict/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nxj9dj/how_to_use_gensim_topic_modeling_to_predict/,30199,1623426895.0,0,,False,,,,,,341
1010,,LanguageTechnology,"I have read the paper "" Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency "", which proposed a classical model DMV( Dependency Model with Valence ) for unsupervised dependency parsing.

As far as I know, the conception ""valence"" is used to describe the number of  “action element” which is dependenct by a verb.

But I can't find more details about how the DMV used ""valence"" in aforementioned paper excepted the model's name.

Can anyone solve my doubts? Thank you very much!",t2_437sde26,False,,0,False,"How to understand the ""valence"" of DMV in unsupervised dependency parsing?",[],r/LanguageTechnology,False,6,,0,,False,t3_nxh2t8,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1623449962.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have read the paper &amp;quot; Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency &amp;quot;, which proposed a classical model DMV( Dependency Model with Valence ) for unsupervised dependency parsing.&lt;/p&gt;

&lt;p&gt;As far as I know, the conception &amp;quot;valence&amp;quot; is used to describe the number of  “action element” which is dependenct by a verb.&lt;/p&gt;

&lt;p&gt;But I can&amp;#39;t find more details about how the DMV used &amp;quot;valence&amp;quot; in aforementioned paper excepted the model&amp;#39;s name.&lt;/p&gt;

&lt;p&gt;Can anyone solve my doubts? Thank you very much!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nxh2t8,True,,Boda_Lin,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nxh2t8/how_to_understand_the_valence_of_dmv_in/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nxh2t8/how_to_understand_the_valence_of_dmv_in/,30199,1623421162.0,0,,False,,,,,,510
1011,,LanguageTechnology,"I've been trying for weeks to do daily topic modeling in this [methodology](https://github.com/Stveshawn/contextual_topic_identification). What I did was divide my dataset into groups by day and loop through the architecture by mini dataset. The first ""problem"" is that I'm getting a warning that I can't loop in an autoencoder and the second is that my metric values don't change much despite the dataset I use (I'm using average coherence at the end, but if I run the code with another dataset with other texts, the average coherence remains similar). Anyone could tell me what I'm doing wrong and how do I model the daily topics correctly, as I'm doing this without any examples (because I couldn't find any).

Code:

    for i, group in enumerate(data_groups.groups):
        #LDA
        
        #BERT
        
        #Concatenation 
        
        #Autoencoder
        AE = Autoencoder()
        AE.fit(ldabert)
        vec = AE.encoder.predict(ldabert)
        
        #Kmeans
    
        #Metrics

Warning:

    WARNING:tensorflow:5 out of the last 5 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x00000161989E3AF0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.

Metric values I'm having: +- 0.55 in coherence, doesn't change much beyond that",t2_6fzlklsz,False,,0,False,Problem while doing daily topic modeling,[],r/LanguageTechnology,False,6,,0,,False,t3_nxgveu,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1623449395.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been trying for weeks to do daily topic modeling in this &lt;a href=""https://github.com/Stveshawn/contextual_topic_identification""&gt;methodology&lt;/a&gt;. What I did was divide my dataset into groups by day and loop through the architecture by mini dataset. The first &amp;quot;problem&amp;quot; is that I&amp;#39;m getting a warning that I can&amp;#39;t loop in an autoencoder and the second is that my metric values don&amp;#39;t change much despite the dataset I use (I&amp;#39;m using average coherence at the end, but if I run the code with another dataset with other texts, the average coherence remains similar). Anyone could tell me what I&amp;#39;m doing wrong and how do I model the daily topics correctly, as I&amp;#39;m doing this without any examples (because I couldn&amp;#39;t find any).&lt;/p&gt;

&lt;p&gt;Code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i, group in enumerate(data_groups.groups):
    #LDA

    #BERT

    #Concatenation 

    #Autoencoder
    AE = Autoencoder()
    AE.fit(ldabert)
    vec = AE.encoder.predict(ldabert)

    #Kmeans

    #Metrics
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Warning:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WARNING:tensorflow:5 out of the last 5 calls to &amp;lt;function Model.make_predict_function.&amp;lt;locals&amp;gt;.predict_function at 0x00000161989E3AF0&amp;gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Metric values I&amp;#39;m having: +- 0.55 in coherence, doesn&amp;#39;t change much beyond that&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nxgveu,True,,OrranaLhaynher,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nxgveu/problem_while_doing_daily_topic_modeling/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nxgveu/problem_while_doing_daily_topic_modeling/,30199,1623420595.0,0,,False,,,,,,1879
1012,,LanguageTechnology,"I recently started, kaggle notebooks don't suggest it's that important.


 But I recently got access to Prodigy, and it took me an embarassngly long time to install it in Pycharm. But the concept and ease of use is  convenient but i am not sure how often business/ orgs require this. 

If you're working in a real world project/Enterprise, what is your exp. with NER. 

Also if you happen to know a repo or list of public projects lemmino.",t2_iu54y,False,,0,False,How often is NER (named entity recognition) a process in your model building process?,[],r/LanguageTechnology,False,6,,0,,False,t3_nwwsuc,False,dark,0.9,,public,14,0,{},,False,[],,False,False,,{},,False,14,,False,False,,False,,[],{},,True,,1623383670.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I recently started, kaggle notebooks don&amp;#39;t suggest it&amp;#39;s that important.&lt;/p&gt;

&lt;p&gt;But I recently got access to Prodigy, and it took me an embarassngly long time to install it in Pycharm. But the concept and ease of use is  convenient but i am not sure how often business/ orgs require this. &lt;/p&gt;

&lt;p&gt;If you&amp;#39;re working in a real world project/Enterprise, what is your exp. with NER. &lt;/p&gt;

&lt;p&gt;Also if you happen to know a repo or list of public projects lemmino.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nwwsuc,True,,redditssexiestguy,,16,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nwwsuc/how_often_is_ner_named_entity_recognition_a/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nwwsuc/how_often_is_ner_named_entity_recognition_a/,30199,1623354870.0,0,,False,,,,,,439
1013,,LanguageTechnology,"I am writing a text classifier and want to use MI for feature selection but not sure how to compute when the features are words. If I compute scores for features, say using TFIDF, and then apply MI on it, then I get the same top features as with just TFIDF and there is no improvement in results.

Is there any resource I can refer for the same?",t2_a40fenez,False,,0,False,How to apply Mutual Information for feature selection in Text Classification where features are words?,[],r/LanguageTechnology,False,6,,0,,False,t3_nxa329,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1623425357.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am writing a text classifier and want to use MI for feature selection but not sure how to compute when the features are words. If I compute scores for features, say using TFIDF, and then apply MI on it, then I get the same top features as with just TFIDF and there is no improvement in results.&lt;/p&gt;

&lt;p&gt;Is there any resource I can refer for the same?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nxa329,True,,puzzled-cognition,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nxa329/how_to_apply_mutual_information_for_feature/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nxa329/how_to_apply_mutual_information_for_feature/,30199,1623396557.0,0,,False,,,,,,345
1014,,LanguageTechnology,"Hey, I've created a tutorial on how to replace missing values in a data frame by the column mean (i.e. mean imputation) using the R programming language: [https://statisticsglobe.com/replace-missing-values-by-column-mean-in-r](https://statisticsglobe.com/replace-missing-values-by-column-mean-in-r)",t2_77cigax1,False,,0,False,Tutorial on how to replace missing values in a data frame by the column mean (i.e. mean imputation) using R,[],r/LanguageTechnology,False,6,,0,,False,t3_nxb4wm,False,dark,0.29,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1623429643.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, I&amp;#39;ve created a tutorial on how to replace missing values in a data frame by the column mean (i.e. mean imputation) using the R programming language: &lt;a href=""https://statisticsglobe.com/replace-missing-values-by-column-mean-in-r""&gt;https://statisticsglobe.com/replace-missing-values-by-column-mean-in-r&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nxb4wm,True,,JoachimSchork,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nxb4wm/tutorial_on_how_to_replace_missing_values_in_a/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nxb4wm/tutorial_on_how_to_replace_missing_values_in_a/,30199,1623400843.0,0,,False,,,,,,298
1015,,LanguageTechnology,"Hi everyone,

For my project I have a set of word vectors which I have to classify in an unsupervised manner to identify topics, similar concepts, etc. The requirement is to allow each word to belong to multiple topics (overlapping), and allow the topics to include other subtopics (hierarchical).

The problem is that because I don't have documents it's not straightforward to apply topic modeling ideas here.

I know one can look at the problem from pure clustering perspective and use kmeans/GMMs/HDBSCAN/deep learning based clustering, but the problem is that most of such methods assume non-overlapping or non-hierarchical clusters, and there's little research on hierarchical overlapping clustering.

I've been also thinking to leverage community detection methods on graphs, as it's possible to treat each word as a node, however, such methods could be computationally expensive, and I just want to make sure there's no a more natural choice before pursuing this.

Would appreciate any ideas, thank you!",t2_jzgetov,False,,0,False,Clustering word vectors for topic discovery without the actual documents (overlapping &amp; hierarchical),[],r/LanguageTechnology,False,6,,0,,False,t3_nwk09n,False,dark,0.96,,public,18,0,{},,False,[],,False,False,,{},,False,18,,False,False,,False,,[],{},,True,,1623348719.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;For my project I have a set of word vectors which I have to classify in an unsupervised manner to identify topics, similar concepts, etc. The requirement is to allow each word to belong to multiple topics (overlapping), and allow the topics to include other subtopics (hierarchical).&lt;/p&gt;

&lt;p&gt;The problem is that because I don&amp;#39;t have documents it&amp;#39;s not straightforward to apply topic modeling ideas here.&lt;/p&gt;

&lt;p&gt;I know one can look at the problem from pure clustering perspective and use kmeans/GMMs/HDBSCAN/deep learning based clustering, but the problem is that most of such methods assume non-overlapping or non-hierarchical clusters, and there&amp;#39;s little research on hierarchical overlapping clustering.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been also thinking to leverage community detection methods on graphs, as it&amp;#39;s possible to treat each word as a node, however, such methods could be computationally expensive, and I just want to make sure there&amp;#39;s no a more natural choice before pursuing this.&lt;/p&gt;

&lt;p&gt;Would appreciate any ideas, thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nwk09n,True,,vlfom,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nwk09n/clustering_word_vectors_for_topic_discovery/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nwk09n/clustering_word_vectors_for_topic_discovery/,30199,1623319919.0,0,,False,,,,,,1010
1016,,LanguageTechnology,"I'm working on a question answering system based on the Wikipedia API. A problem is that ""wikipedia.search"" yields a couple of articles, the first one not always being the best one. Any ideas how to chose the most appropriate article from the results?",t2_1a5ug84,False,,0,False,Question answering with Wikipedia API,[],r/LanguageTechnology,False,6,,0,,False,t3_nwk5sp,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1623349324.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working on a question answering system based on the Wikipedia API. A problem is that &amp;quot;wikipedia.search&amp;quot; yields a couple of articles, the first one not always being the best one. Any ideas how to chose the most appropriate article from the results?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nwk5sp,True,,johannadambergk,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nwk5sp/question_answering_with_wikipedia_api/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nwk5sp/question_answering_with_wikipedia_api/,30199,1623320524.0,0,,False,,,,,,251
1017,,LanguageTechnology,"Hi everyone,

I'm currently learning about text summarization and I'd really like to fine-tune a transformer model for summarizing speeches (i.e. texts of speeches by political leaders or parliament debates, NOT voice recordings).

Does anyone know about an English dataset containing both the speeches and the respective summaries? It would also be great if someone had an idea as to where one could source such a dataset (e.g. websites from parliaments). I already checked the websites of the US senate, UK government etc., but no luck so far.

Thank you!",t2_b2c5gp0a,False,,0,False,Speech summarization datasets,[],r/LanguageTechnology,False,6,,0,,False,t3_nwmezs,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1623356976.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m currently learning about text summarization and I&amp;#39;d really like to fine-tune a transformer model for summarizing speeches (i.e. texts of speeches by political leaders or parliament debates, NOT voice recordings).&lt;/p&gt;

&lt;p&gt;Does anyone know about an English dataset containing both the speeches and the respective summaries? It would also be great if someone had an idea as to where one could source such a dataset (e.g. websites from parliaments). I already checked the websites of the US senate, UK government etc., but no luck so far.&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nwmezs,True,,looking_for_speeches,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nwmezs/speech_summarization_datasets/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nwmezs/speech_summarization_datasets/,30199,1623328176.0,0,,False,,,,,,557
1018,,LanguageTechnology,,t2_8l6cewm2,False,,0,False,The creators of GPT-Neo just released a 6B parameter open-source version of GPT-3 called GPT-J-6B,[],r/LanguageTechnology,False,6,,0,,False,t3_nw6yx3,False,dark,0.89,,public,7,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/6w5sgWo68E0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Introducing GPT-J -- An Open Source Version Of GPT-3 (NLP News)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/6w5sgWo68E0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Vennify AI', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/6w5sgWo68E0/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC7-EWrr8YdcQgPPk76OiUVw'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/6w5sgWo68E0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nw6yx3', 'height': 200}",,False,7,,False,False,,False,,[],{},,False,,1623303219.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nw6yx3,True,,VennifyAI,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nw6yx3/the_creators_of_gptneo_just_released_a_6b/,all_ads,False,https://www.youtube.com/watch?v=6w5sgWo68E0,30199,1623274419.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Introducing GPT-J -- An Open Source Version Of GPT-3 (NLP News)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/6w5sgWo68E0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Vennify AI', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/6w5sgWo68E0/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC7-EWrr8YdcQgPPk76OiUVw'}}",False,https://www.youtube.com/watch?v=6w5sgWo68E0,,,,,0
1019,,LanguageTechnology,"Google researcher’s new study suggests modifying the conventional transformer architecture to process byte sequences in natural language processing (NLP). The new competitive byte-level models can effectively balance computational cost trade-offs of contemporary large language models.

Tokenization splits the sentences into a sequence of tokens. Most NLP tasks follow a tokenization procedure to preprocess the data. However, tokenization can struggle with typos, irregularities in spelling and capitalization, morphological changes, and out-of-vocabulary tokenization problems.

Summary: [https://www.marktechpost.com/2021/06/08/google-ai-introduces-byt5-pre-trained-byte-to-byte-models-for-nlp-tasks/](https://www.marktechpost.com/2021/06/08/google-ai-introduces-byt5-pre-trained-byte-to-byte-models-for-nlp-tasks/) 

GitHub: https://github.com/google-research/byt5

Paper: https://arxiv.org/abs/2105.13626",t2_2wsvqwhg,False,,0,False,Google AI Introduces ByT5: Pre-Trained Byte-to-Byte Models for NLP Tasks,[],r/LanguageTechnology,False,6,,0,,False,t3_nvot6o,False,dark,1.0,,public,46,0,{},,False,[],,False,False,,{},,False,46,,False,False,,False,,[],{},,True,,1623247927.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Google researcher’s new study suggests modifying the conventional transformer architecture to process byte sequences in natural language processing (NLP). The new competitive byte-level models can effectively balance computational cost trade-offs of contemporary large language models.&lt;/p&gt;

&lt;p&gt;Tokenization splits the sentences into a sequence of tokens. Most NLP tasks follow a tokenization procedure to preprocess the data. However, tokenization can struggle with typos, irregularities in spelling and capitalization, morphological changes, and out-of-vocabulary tokenization problems.&lt;/p&gt;

&lt;p&gt;Summary: &lt;a href=""https://www.marktechpost.com/2021/06/08/google-ai-introduces-byt5-pre-trained-byte-to-byte-models-for-nlp-tasks/""&gt;https://www.marktechpost.com/2021/06/08/google-ai-introduces-byt5-pre-trained-byte-to-byte-models-for-nlp-tasks/&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;GitHub: &lt;a href=""https://github.com/google-research/byt5""&gt;https://github.com/google-research/byt5&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/abs/2105.13626""&gt;https://arxiv.org/abs/2105.13626&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nvot6o,True,,ai-lover,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nvot6o/google_ai_introduces_byt5_pretrained_bytetobyte/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nvot6o/google_ai_introduces_byt5_pretrained_bytetobyte/,30199,1623219127.0,0,,False,,,,,,910
1020,,LanguageTechnology,"Hi all,

What is the best word embedding to use for automatic text summarization? I want to make extractive summaries of documents.",t2_1i3ta8mq,False,,0,False,Language Model for Summarization,[],r/LanguageTechnology,False,6,,0,,False,t3_nvxizn,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1623278427.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;What is the best word embedding to use for automatic text summarization? I want to make extractive summaries of documents.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nvxizn,True,,panteahk,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nvxizn/language_model_for_summarization/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nvxizn/language_model_for_summarization/,30199,1623249627.0,0,,False,,,,,,131
1021,,LanguageTechnology,"Hey, 

Given that I have a Named Entity extractor trained over a BERT pre-trained model, is it possible to utilize the already computed attention scores for extracting Relations between these entities?  

Obviously, categorizing the active relations is still a challenge, but is it possible to detect if a relation is active only by using the attention scores? Specifically if the BERT model is only trained for NER.",t2_1uz63xco,False,,0,False,Unsupervised Relation Extraction using BERT attention scores,[],r/LanguageTechnology,False,6,,0,,False,t3_nvvvzs,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1623273906.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, &lt;/p&gt;

&lt;p&gt;Given that I have a Named Entity extractor trained over a BERT pre-trained model, is it possible to utilize the already computed attention scores for extracting Relations between these entities?  &lt;/p&gt;

&lt;p&gt;Obviously, categorizing the active relations is still a challenge, but is it possible to detect if a relation is active only by using the attention scores? Specifically if the BERT model is only trained for NER.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nvvvzs,True,,pauloamed,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nvvvzs/unsupervised_relation_extraction_using_bert/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nvvvzs/unsupervised_relation_extraction_using_bert/,30199,1623245106.0,0,,False,,,,,,416
1022,,LanguageTechnology,"I would like to extract relationships in plain text between two named entities.  

I first wanted to try with Machine Learning but it's complicated to find an annotated corpus for training ... Then I wanted to create patterns (for example: PERSON live LOCATION) but it is not not very precise because I'm trying to find relationships between each pair of named entities (and honestly it takes a long time to write a good dictionary).

Do you have any suggestions for doing this more efficiently please? Maybe a corpus that exists, an algorithm next to which I pass ? 

Thaaaanks :)",t2_66evjh50,False,,0,False,Relationship extraction,[],r/LanguageTechnology,False,6,,0,,False,t3_nvuwiw,False,dark,0.75,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1623271033.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I would like to extract relationships in plain text between two named entities.  &lt;/p&gt;

&lt;p&gt;I first wanted to try with Machine Learning but it&amp;#39;s complicated to find an annotated corpus for training ... Then I wanted to create patterns (for example: PERSON live LOCATION) but it is not not very precise because I&amp;#39;m trying to find relationships between each pair of named entities (and honestly it takes a long time to write a good dictionary).&lt;/p&gt;

&lt;p&gt;Do you have any suggestions for doing this more efficiently please? Maybe a corpus that exists, an algorithm next to which I pass ? &lt;/p&gt;

&lt;p&gt;Thaaaanks :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nvuwiw,True,,happisland,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nvuwiw/relationship_extraction/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nvuwiw/relationship_extraction/,30199,1623242233.0,0,,False,,,,,,581
1023,,LanguageTechnology,"I need to generate german paraphrases and I was already looking at some huggingface models which work really well for english sentences. For example tuner007/pegasus\_paraphrase or Vamsi/T5\_Paraphrase\_Paws.

    tokenizer = PegasusTokenizer.from_pretrained(model_name)
    model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device) 

I can't find any german models for paraphrasing on Huggingface. How do I build a model on my own? Is this the best way to generate german paraphrases with transformers or should I use other methods? Thanks!",t2_6fr49wdr,False,,0,False,How to build a model for german paraphrase generation?,[],r/LanguageTechnology,False,6,,0,,False,t3_nvr3zl,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1623257600.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I need to generate german paraphrases and I was already looking at some huggingface models which work really well for english sentences. For example tuner007/pegasus_paraphrase or Vamsi/T5_Paraphrase_Paws.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I can&amp;#39;t find any german models for paraphrasing on Huggingface. How do I build a model on my own? Is this the best way to generate german paraphrases with transformers or should I use other methods? Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nvr3zl,True,,LargeBrick7,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nvr3zl/how_to_build_a_model_for_german_paraphrase/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nvr3zl/how_to_build_a_model_for_german_paraphrase/,30199,1623228800.0,0,,False,,,,,,568
1024,,LanguageTechnology,,t2_6ib7gcgr,False,,0,False,[d] Inspecting Neural Networks with Canonical Correlation Analysis (CKA/SVCCA Video),[],r/LanguageTechnology,False,6,,0,,False,t3_nvr1cf,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,False,,1623257288.0,text,6,,,text,self.MachineLearning,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nvr1cf,True,,jayalammar,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nvr1cf/d_inspecting_neural_networks_with_canonical/,all_ads,False,/r/MachineLearning/comments/nvr0v0/d_inspecting_neural_networks_with_canonical/,30199,1623228488.0,0,,False,/r/MachineLearning/comments/nvr0v0/d_inspecting_neural_networks_with_canonical/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'Canonical Correlation Analysis is one of the methods used to explore deep neural networks. Methods like CKA and SVCCA reveal to us insights into how a neural network processes its inputs. This is often done by using CKA and SVCCA as a similarity measure for different activation matrices. In this video, we look at a number of papers that compare different neural networks together. We also look at papers that compare the representations of the various layers of a neural network. \n\n[https://www.youtube.com/watch?v=u7Dvb\\_a1D-0](https://www.youtube.com/watch?v=u7Dvb_a1D-0)\n\n&amp;#x200B;\n\nPapers covered:\n\n   \nSVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability  \n[https://arxiv.org/pdf/1706.05806.pdf](https://arxiv.org/pdf/1706.05806.pdf)\n\nUnderstanding Learning Dynamics Of Language Models with SVCCA   \n[https://arxiv.org/pdf/1811.00225.pdf](https://arxiv.org/pdf/1811.00225.pdf)  \n\nInsights on representational similarity in neural networks with canonical correlation   \n[https://arxiv.org/pdf/1806.05759.pdf](https://arxiv.org/pdf/1806.05759.pdf)  \n\nBERT is Not an Interlingua and the Bias of Tokenization   \n[https://www.aclweb.org/anthology/D19-6106.pdf](https://www.aclweb.org/anthology/D19-6106.pdf)  \n\nSimilarity of Neural Network Representations Revisited   \n[http://proceedings.mlr.press/v97/kornblith19a/kornblith19a.pdf](http://proceedings.mlr.press/v97/kornblith19a/kornblith19a.pdf)  \n\nSimilarity Analysis of Contextual Word Representation Models   \n[https://arxiv.org/pdf/2005.01172.pdf](https://arxiv.org/pdf/2005.01172.pdf)  \n\nDo Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth   \n[https://arxiv.org/pdf/2010.15327.pdf](https://arxiv.org/pdf/2010.15327.pdf)', 'author_fullname': 't2_6ib7gcgr', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[d] Inspecting Neural Networks with Canonical Correlation Analysis (CKA/SVCCA Video)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_nvr0v0', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1623257236.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Canonical Correlation Analysis is one of the methods used to explore deep neural networks. Methods like CKA and SVCCA reveal to us insights into how a neural network processes its inputs. This is often done by using CKA and SVCCA as a similarity measure for different activation matrices. In this video, we look at a number of papers that compare different neural networks together. We also look at papers that compare the representations of the various layers of a neural network. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.youtube.com/watch?v=u7Dvb_a1D-0""&gt;https://www.youtube.com/watch?v=u7Dvb_a1D-0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Papers covered:&lt;/p&gt;\n\n&lt;p&gt;SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability&lt;br/&gt;\n&lt;a href=""https://arxiv.org/pdf/1706.05806.pdf""&gt;https://arxiv.org/pdf/1706.05806.pdf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Understanding Learning Dynamics Of Language Models with SVCCA&lt;br/&gt;\n&lt;a href=""https://arxiv.org/pdf/1811.00225.pdf""&gt;https://arxiv.org/pdf/1811.00225.pdf&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Insights on representational similarity in neural networks with canonical correlation&lt;br/&gt;\n&lt;a href=""https://arxiv.org/pdf/1806.05759.pdf""&gt;https://arxiv.org/pdf/1806.05759.pdf&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;BERT is Not an Interlingua and the Bias of Tokenization&lt;br/&gt;\n&lt;a href=""https://www.aclweb.org/anthology/D19-6106.pdf""&gt;https://www.aclweb.org/anthology/D19-6106.pdf&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Similarity of Neural Network Representations Revisited&lt;br/&gt;\n&lt;a href=""http://proceedings.mlr.press/v97/kornblith19a/kornblith19a.pdf""&gt;http://proceedings.mlr.press/v97/kornblith19a/kornblith19a.pdf&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Similarity Analysis of Contextual Word Representation Models&lt;br/&gt;\n&lt;a href=""https://arxiv.org/pdf/2005.01172.pdf""&gt;https://arxiv.org/pdf/2005.01172.pdf&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth&lt;br/&gt;\n&lt;a href=""https://arxiv.org/pdf/2010.15327.pdf""&gt;https://arxiv.org/pdf/2010.15327.pdf&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'nvr0v0', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'jayalammar', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/nvr0v0/d_inspecting_neural_networks_with_canonical/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/nvr0v0/d_inspecting_neural_networks_with_canonical/', 'subreddit_subscribers': 1930728, 'created_utc': 1623228436.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_nvr0v0,,,0
1025,,LanguageTechnology,,t2_an5dtfk8,False,,0,False,"I am experimenting with using NLP to measure market sentiment with custom classifiers for neutrality, FUD, hype etc. stuff typical for crypto world. I trained my classifiers in natural.js with data I pulled from social media. This is my attempt number one",[],r/LanguageTechnology,False,6,,0,,False,t3_nvycft,False,dark,0.33,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,False,,1623280649.0,text,6,,,text,comint.ai,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nvycft,True,,flexxxatron,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nvycft/i_am_experimenting_with_using_nlp_to_measure/,all_ads,False,https://comint.ai/coins/btc,30199,1623251849.0,0,,False,https://comint.ai/coins/btc,,,,,0
1026,,LanguageTechnology,"For example:

exist exists existed existing 

run ran running 

And so forth?",t2_aw7plin,False,,0,False,Is there a text list of words and their variations?,[],r/LanguageTechnology,False,6,,0,,False,t3_nv89vk,False,dark,0.78,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1623198866.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For example:&lt;/p&gt;

&lt;p&gt;exist exists existed existing &lt;/p&gt;

&lt;p&gt;run ran running &lt;/p&gt;

&lt;p&gt;And so forth?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nv89vk,True,,blessedarethegeek,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nv89vk/is_there_a_text_list_of_words_and_their_variations/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nv89vk/is_there_a_text_list_of_words_and_their_variations/,30199,1623170066.0,0,,False,,,,,,77
1027,,LanguageTechnology,"Very interesting job to teach at a new [MSc. in Voice technology](https://www.rug.nl/masters/voice-technology/)!

Key points:

* English language program
* 80-100% full-time position (depending on how many classes you want to teach)
* Balance between teaching and research is 60/40 (really!)

In addition to supervising theses within your area of expertise, you will support the teaching and/or curriculum development of courses in speech synthesis, speech recognition, Python, and machine learning for voice tech (all courses already have detailed week-by-week descriptions but lack student-ready syllabi, giving you some creative freedom -- more information about the courses, including learning outcomes, is available upon request):

● Speech Synthesis I and II  
● Speech Recognition I and II  
● Python for Voice Technology (and Intro to Python at the undergraduate level)  
● Machine Learning for Voice Technology

[More details](https://www.rug.nl/about-ug/work-with-us/job-opportunities/?details=00347-02S0008E4P)  (qualifications, application procedure, etc.)

Deadline: 13 June 11:59pm (CEST - European time)",t2_8wbdz21a,False,,0,False,Job opportunity: Assistant Professor in Speech Technology at the University of Groningen (the Netherlands),[],r/LanguageTechnology,False,6,,0,,False,t3_nuzw46,False,dark,0.92,,public,24,0,{},,False,[],,False,False,,{},,False,24,,False,False,,False,,[],{},,True,,1623169612.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Very interesting job to teach at a new &lt;a href=""https://www.rug.nl/masters/voice-technology/""&gt;MSc. in Voice technology&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;Key points:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;English language program&lt;/li&gt;
&lt;li&gt;80-100% full-time position (depending on how many classes you want to teach)&lt;/li&gt;
&lt;li&gt;Balance between teaching and research is 60/40 (really!)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition to supervising theses within your area of expertise, you will support the teaching and/or curriculum development of courses in speech synthesis, speech recognition, Python, and machine learning for voice tech (all courses already have detailed week-by-week descriptions but lack student-ready syllabi, giving you some creative freedom -- more information about the courses, including learning outcomes, is available upon request):&lt;/p&gt;

&lt;p&gt;● Speech Synthesis I and II&lt;br/&gt;
● Speech Recognition I and II&lt;br/&gt;
● Python for Voice Technology (and Intro to Python at the undergraduate level)&lt;br/&gt;
● Machine Learning for Voice Technology&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.rug.nl/about-ug/work-with-us/job-opportunities/?details=00347-02S0008E4P""&gt;More details&lt;/a&gt;  (qualifications, application procedure, etc.)&lt;/p&gt;

&lt;p&gt;Deadline: 13 June 11:59pm (CEST - European time)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nuzw46,True,,VoiceTech,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nuzw46/job_opportunity_assistant_professor_in_speech/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nuzw46/job_opportunity_assistant_professor_in_speech/,30199,1623140812.0,0,,False,,,,,,1118
1028,,LanguageTechnology,"I saw this paper and thought it was an interesting application of sentiment analysis. 

[https://arxiv.org/abs/2106.00665](https://arxiv.org/abs/2106.00665)

The paper proposes using GAN-BERT with BioBERT for 3 class sentiment classification in clinical trial abstracts as a way to assess reporting trends in literature. 

They also found that the accuracy of the algorithm was far better than using an expert rater (which is the standard right now in clinical literature for these types of studies).

I'm curious what the r/LanguageTechnology world thinks; This seems like a really cool application but I'm a total novice when it comes to NLP (coming from a signal processing background)",t2_buawjx1q,False,,0,False,GAN-BioBERT: A Methodology For Assessing Reporting Trends In Clinical Trials (Paper Summary),[],r/LanguageTechnology,False,6,,0,,False,t3_nv5yb8,False,dark,0.89,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1623193039.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I saw this paper and thought it was an interesting application of sentiment analysis. &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://arxiv.org/abs/2106.00665""&gt;https://arxiv.org/abs/2106.00665&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The paper proposes using GAN-BERT with BioBERT for 3 class sentiment classification in clinical trial abstracts as a way to assess reporting trends in literature. &lt;/p&gt;

&lt;p&gt;They also found that the accuracy of the algorithm was far better than using an expert rater (which is the standard right now in clinical literature for these types of studies).&lt;/p&gt;

&lt;p&gt;I&amp;#39;m curious what the &lt;a href=""/r/LanguageTechnology""&gt;r/LanguageTechnology&lt;/a&gt; world thinks; This seems like a really cool application but I&amp;#39;m a total novice when it comes to NLP (coming from a signal processing background)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nv5yb8,True,,Jornkatarn,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nv5yb8/ganbiobert_a_methodology_for_assessing_reporting/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nv5yb8/ganbiobert_a_methodology_for_assessing_reporting/,30199,1623164239.0,0,,False,,,,,,688
1029,,LanguageTechnology,I want to do a contextual analysis on two topics and learn the in what  context some common keywords has been used in both of the topics. Thank  you.,t2_4rzsj2ib,False,,0,False,"Need direction related a project, like what topics(study material) should I look into.",[],r/LanguageTechnology,False,6,,0,,False,t3_nvaccr,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1623204330.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to do a contextual analysis on two topics and learn the in what  context some common keywords has been used in both of the topics. Thank  you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nvaccr,True,,mrbrownstone07__,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nvaccr/need_direction_related_a_project_like_what/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nvaccr/need_direction_related_a_project_like_what/,30199,1623175530.0,0,,False,,,,,,149
1030,,LanguageTechnology,,t2_8hz4mq4a,False,,0,False,Benefits of Using PHP for Web Development,[],r/LanguageTechnology,False,6,,0,,False,t3_nvqkl1,False,dark,0.13,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,False,,1623255313.0,text,6,,,text,coresumo.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nvqkl1,True,,mcscs,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nvqkl1/benefits_of_using_php_for_web_development/,all_ads,False,https://coresumo.com/benefits-of-using-php-for-web-development-2021/,30199,1623226513.0,0,,False,https://coresumo.com/benefits-of-using-php-for-web-development-2021/,,,,,0
1031,,LanguageTechnology,"Hello,
I am planning on studying NLP in the fall and have been accepted to two programs that really interest me.
They are the University of Strasbourg's Master in Language Technology and the PluriTAL program in Paris that is organized by Paris 3, Paris Nanterre, and Inalco.
I was wondering if anyone here has studied in one of these programs or knows anyone that has and would have any pros and cons between them.

Thank you for your help!",t2_m9rqx,False,,0,False,Advice - Choosing a Master's in NLP (France),[],r/LanguageTechnology,False,6,,0,,False,t3_nuylqi,False,dark,0.95,,public,19,0,{},,False,[],,False,False,,{},,False,19,,False,False,,False,,[],{},,True,,1623164305.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,
I am planning on studying NLP in the fall and have been accepted to two programs that really interest me.
They are the University of Strasbourg&amp;#39;s Master in Language Technology and the PluriTAL program in Paris that is organized by Paris 3, Paris Nanterre, and Inalco.
I was wondering if anyone here has studied in one of these programs or knows anyone that has and would have any pros and cons between them.&lt;/p&gt;

&lt;p&gt;Thank you for your help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nuylqi,True,,ConnorIsGreat,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nuylqi/advice_choosing_a_masters_in_nlp_france/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nuylqi/advice_choosing_a_masters_in_nlp_france/,30199,1623135505.0,0,,False,,,,,,440
1032,,LanguageTechnology," 

I'm doing topic modeling for the first time in my life and I have a problem. My intention is to model daily topics, but my number of daily samples varies a lot, from 5 samples in one day to 100 in another, for example. The desired number of topics is 7, so I have problems from the first day of the dataset.

The methodology I'm following is [this](https://blog.insightdatascience.com/contextual-topic-identification-4291d256a032).

Then the vector resulting from the LDA+BERT concatenation is passed in an Autoencoder and then used in a clustering model. This is where I have the problem at hand. My number of clusters is 7, but the representation vectors are 5.

With this I have the error:

ValueError: n\_samples=5 should be &gt;= n\_clusters=7.

Does anyone know how I could fix this?",t2_6fzlklsz,False,,0,False,Clustering latent representation vectors with a size less than the number of clusters,[],r/LanguageTechnology,False,6,,0,,False,t3_nv1y5k,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1623180908.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m doing topic modeling for the first time in my life and I have a problem. My intention is to model daily topics, but my number of daily samples varies a lot, from 5 samples in one day to 100 in another, for example. The desired number of topics is 7, so I have problems from the first day of the dataset.&lt;/p&gt;

&lt;p&gt;The methodology I&amp;#39;m following is &lt;a href=""https://blog.insightdatascience.com/contextual-topic-identification-4291d256a032""&gt;this&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Then the vector resulting from the LDA+BERT concatenation is passed in an Autoencoder and then used in a clustering model. This is where I have the problem at hand. My number of clusters is 7, but the representation vectors are 5.&lt;/p&gt;

&lt;p&gt;With this I have the error:&lt;/p&gt;

&lt;p&gt;ValueError: n_samples=5 should be &amp;gt;= n_clusters=7.&lt;/p&gt;

&lt;p&gt;Does anyone know how I could fix this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nv1y5k,True,,OrranaLhaynher,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nv1y5k/clustering_latent_representation_vectors_with_a/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nv1y5k/clustering_latent_representation_vectors_with_a/,30199,1623152108.0,0,,False,,,,,,792
1033,,LanguageTechnology,,t2_l6ugg,False,,0,False,FREE WEBINAR - Automating Data Annotation with MicroModels - Automating processes within the workflow to improve efficiency &amp; guarantee high quality,[],r/LanguageTechnology,False,6,,0,,False,t3_nv0z0r,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1623174055.0,text,6,,,text,re-work.co,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nv0z0r,True,,nikitaljohnson,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nv0z0r/free_webinar_automating_data_annotation_with/,all_ads,False,https://www.re-work.co/events/webinar:automating-data-annotation-with-micromodels?utm_source=Promo&amp;utm_medium=Promo&amp;utm_campaign=LK_Promo_Sama_Webinar,30199,1623145255.0,0,,False,https://www.re-work.co/events/webinar:automating-data-annotation-with-micromodels?utm_source=Promo&amp;utm_medium=Promo&amp;utm_campaign=LK_Promo_Sama_Webinar,,,,,0
1034,,LanguageTechnology,"Hi all. I have a little problem I am facing and I'm not quite sure where I should start looking. I have an application where I want to identify sentences inline as the user types. The goal is to operate on the sentences and provide feedback to the user without them needing to indicate the sentence as completed.

Does anyone have any ideas on how to effectively identify complete sentences as they are being typed? The quick solutions I've thought of seem a little too simple and error prone. 

Please let me know.",t2_3wufr1fg,False,,0,False,How to identify sentences from a stream of characters?,[],r/LanguageTechnology,False,6,,0,,False,t3_nuvgii,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1623152430.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all. I have a little problem I am facing and I&amp;#39;m not quite sure where I should start looking. I have an application where I want to identify sentences inline as the user types. The goal is to operate on the sentences and provide feedback to the user without them needing to indicate the sentence as completed.&lt;/p&gt;

&lt;p&gt;Does anyone have any ideas on how to effectively identify complete sentences as they are being typed? The quick solutions I&amp;#39;ve thought of seem a little too simple and error prone. &lt;/p&gt;

&lt;p&gt;Please let me know.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nuvgii,True,,mr_super_doodle,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nuvgii/how_to_identify_sentences_from_a_stream_of/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nuvgii/how_to_identify_sentences_from_a_stream_of/,30199,1623123630.0,0,,False,,,,,,515
1035,,LanguageTechnology,,t2_v7nu2,False,,0,False,"John Snow Labs Spark-NLP 3.1.0: Over 2600+ new models and pipelines in 200+ languages, new DistilBERT, RoBERTa, and XLM-RoBERTa transformers, support for external Transformers, and lots more!",[],r/LanguageTechnology,False,6,,0,,False,t3_nufyy8,False,dark,0.93,,public,26,0,{},,False,[],,False,False,,{},,False,26,,False,False,,False,,[],{},,False,,1623110201.0,text,6,,,text,github.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nufyy8,True,,dark-night-rises,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nufyy8/john_snow_labs_sparknlp_310_over_2600_new_models/,all_ads,False,https://github.com/JohnSnowLabs/spark-nlp/releases/tag/3.1.0,30199,1623081401.0,0,,False,https://github.com/JohnSnowLabs/spark-nlp/releases/tag/3.1.0,,,,,0
1036,,LanguageTechnology,"I tried to find some well-summarized answers, but there really isn't any.

I can only guess some combination of:

* Query Expansion
* Query Embedding and Finding Semantically Similar Queries
* Query-Answer scores(from the field data) for raking PAA suggestions
* Text Summarization(for the snippet of answers)
* Natural Language Generation(for the ""general"" form of questions)

Anyone can give a bit more detailed version of what is actually going on behind?

Thanks!",t2_cidssi7w,False,,0,False,"What's the algorithm pipeline for Google's ""People Also Ask""?",[],r/LanguageTechnology,False,6,,0,,False,t3_nuo2xl,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1623130011.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I tried to find some well-summarized answers, but there really isn&amp;#39;t any.&lt;/p&gt;

&lt;p&gt;I can only guess some combination of:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Query Expansion&lt;/li&gt;
&lt;li&gt;Query Embedding and Finding Semantically Similar Queries&lt;/li&gt;
&lt;li&gt;Query-Answer scores(from the field data) for raking PAA suggestions&lt;/li&gt;
&lt;li&gt;Text Summarization(for the snippet of answers)&lt;/li&gt;
&lt;li&gt;Natural Language Generation(for the &amp;quot;general&amp;quot; form of questions)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Anyone can give a bit more detailed version of what is actually going on behind?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nuo2xl,True,,nlp_ttt,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nuo2xl/whats_the_algorithm_pipeline_for_googles_people/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nuo2xl/whats_the_algorithm_pipeline_for_googles_people/,30199,1623101211.0,0,,False,,,,,,467
1037,,LanguageTechnology,,t2_qig8p,False,,0,False,"google-research/mozolm - A language model serving library, with middleware functionality including mixing of probabilities from disparate base language model types and tokenizations along with RPC client/server interactions.",[],r/LanguageTechnology,False,6,,0,,False,t3_nunqg8,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1623129117.0,text,6,,,text,github.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nunqg8,True,,acecentre,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nunqg8/googleresearchmozolm_a_language_model_serving/,all_ads,False,https://github.com/google-research/mozolm,30199,1623100317.0,0,,False,https://github.com/google-research/mozolm,,,,,0
1038,,LanguageTechnology,,t2_qig8p,False,,0,False,"Building a corpus of AAC users speech/writing: ""Know of any AAC users who want a faster system? This research study aims to build a large open database for researchers and developers to make far better systems. End users can now help this effort directly""",[],r/LanguageTechnology,False,6,,0,,False,t3_nunmmp,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1623128853.0,text,6,,,text,spark.adobe.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nunmmp,True,,acecentre,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nunmmp/building_a_corpus_of_aac_users_speechwriting_know/,all_ads,False,https://spark.adobe.com/video/F8MNMohwepvrG,30199,1623100053.0,0,,False,https://spark.adobe.com/video/F8MNMohwepvrG,"[{'approved_at_utc': None, 'subreddit': 'slp', 'selftext': '', 'author_fullname': 't2_qig8p', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Know of any AAC users who want a faster system? This research study aims to build a large open database for researchers and developers to make far better systems. End users can now help this effort directly', 'link_flair_richtext': [{'e': 'text', 't': '[AAC]'}], 'subreddit_name_prefixed': 'r/slp', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_ntzce4', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': '[AAC]', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1623053343.0, 'link_flair_type': 'richtext', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'spark.adobe.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://spark.adobe.com/video/F8MNMohwepvrG', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '15944b8a-a920-11e2-995a-12313d1841d1', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2sjju', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'ntzce4', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'acecentre', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/slp/comments/ntzce4/know_of_any_aac_users_who_want_a_faster_system/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://spark.adobe.com/video/F8MNMohwepvrG', 'subreddit_subscribers': 26128, 'created_utc': 1623024543.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_ntzce4,,,0
1039,,LanguageTechnology,,t2_x7cw3eo,False,,0,False,Preventing ‘Hallucination’ In GPT-3 And Other Complex Language Models,[],r/LanguageTechnology,False,6,,0,,False,t3_nu8qyy,False,dark,0.94,,public,15,0,{},,False,[],,False,False,,{},,False,15,,False,False,,False,,[],{},,False,,1623089094.0,text,6,,,text,unite.ai,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nu8qyy,True,,Symbiot10000,,1,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/nu8qyy/preventing_hallucination_in_gpt3_and_other/,all_ads,False,https://www.unite.ai/preventing-hallucination-in-gpt-3-and-other-complex-language-models/,30199,1623060294.0,0,,False,https://www.unite.ai/preventing-hallucination-in-gpt-3-and-other-complex-language-models/,,,,,0
1040,,LanguageTechnology,"Most of the pre-trained language models operate on sequences of tokens corresponding to word or subword units. 

This paper proposes Token-free models that instead operate directly on raw bytes. 

https://link.medium.com/qKwrXYXqTgb

P.S. Most of this blog was written from my mobile device. Please excuse brevity and typos.


Actual Paper: https://arxiv.org/pdf/2105.13626v1.pdf",t2_hkv9s,False,,0,False,ByT5: Towards a token-free future with pre-trained byte-to-byte models (Research Paper Summary),[],r/LanguageTechnology,False,6,,0,,False,t3_nu9ty9,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1623093228.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Most of the pre-trained language models operate on sequences of tokens corresponding to word or subword units. &lt;/p&gt;

&lt;p&gt;This paper proposes Token-free models that instead operate directly on raw bytes. &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://link.medium.com/qKwrXYXqTgb""&gt;https://link.medium.com/qKwrXYXqTgb&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;P.S. Most of this blog was written from my mobile device. Please excuse brevity and typos.&lt;/p&gt;

&lt;p&gt;Actual Paper: &lt;a href=""https://arxiv.org/pdf/2105.13626v1.pdf""&gt;https://arxiv.org/pdf/2105.13626v1.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nu9ty9,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nu9ty9/byt5_towards_a_tokenfree_future_with_pretrained/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nu9ty9/byt5_towards_a_tokenfree_future_with_pretrained/,30199,1623064428.0,0,,False,,,,,,379
1041,,LanguageTechnology,"I am new to Hugging Face and masked language modelling (MLM), and I was wondering how to include emojis when doing such a task.

I have a dataset with tweets, with each tweet containing an emoji at the end - here is a sample of my data:

| ID |        Tweet |
| -------- | -------------- |
| 1    | Looking good today 😎         |
| 2   | Weather is so hot, lol ☀️          |
| 3  | I hate you!!! 🤬        |

At the moment, I have fully trained my masked language model using my dataset, but when I predict something, it does **NOT** output or predict the emojis. It just predicts words.

This is my desired input from using my dataset for MLM:

```
""You look great [MASK]""
```

This is my desired output from using my dataset for MLM:

```
[{'score': 0.26041436195373535,
  'sequence': 'You look great 😎""',
  'token': 72,
  'token_str': '.""'},
 {'score': 0.1813151091337204,
  'sequence': 'you look great 💯""',
  'token': 2901,
  'token_str': '!""'},
 {'score': 0.14516998827457428,
  'sequence': 'you look great 👌',
  'token': 328,
  'token_str': '!'},]
```

However, this is what I am actually getting from my output:

```
[{'score': 0.26041436195373535,
  'sequence': 'You look great?""',
  'token': 72,
  'token_str': '.""'},
 {'score': 0.1813151091337204,
  'sequence': 'You look great.""',
  'token': 2901,
  'token_str': '!""'},
 {'score': 0.14516998827457428,
  'sequence': 'You look great!',
  'token': 328,
  'token_str': '!'},]
```

I know it is possible to do this, but how do I do it? I am close, but not very.

Likewise, I have my model fully trained on my dataset, but it just does not seem to output emojis, even though I have included them in the training. 

Does something need to be included to accept emoji? If so, what?

Thanks - I would really appreciate the help!",t2_13ussz,False,,0,False,Hugging Face: Unable to use emojis for masked language modelling?,[],r/LanguageTechnology,False,6,,0,,False,t3_nu90dm,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1623090148.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am new to Hugging Face and masked language modelling (MLM), and I was wondering how to include emojis when doing such a task.&lt;/p&gt;

&lt;p&gt;I have a dataset with tweets, with each tweet containing an emoji at the end - here is a sample of my data:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;ID&lt;/th&gt;
&lt;th&gt;Tweet&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Looking good today 😎&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;Weather is so hot, lol ☀️&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;I hate you!!! 🤬&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;At the moment, I have fully trained my masked language model using my dataset, but when I predict something, it does &lt;strong&gt;NOT&lt;/strong&gt; output or predict the emojis. It just predicts words.&lt;/p&gt;

&lt;p&gt;This is my desired input from using my dataset for MLM:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
&amp;quot;You look great [MASK]&amp;quot;
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This is my desired output from using my dataset for MLM:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
[{&amp;#39;score&amp;#39;: 0.26041436195373535,
  &amp;#39;sequence&amp;#39;: &amp;#39;You look great 😎&amp;quot;&amp;#39;,
  &amp;#39;token&amp;#39;: 72,
  &amp;#39;token_str&amp;#39;: &amp;#39;.&amp;quot;&amp;#39;},
 {&amp;#39;score&amp;#39;: 0.1813151091337204,
  &amp;#39;sequence&amp;#39;: &amp;#39;you look great 💯&amp;quot;&amp;#39;,
  &amp;#39;token&amp;#39;: 2901,
  &amp;#39;token_str&amp;#39;: &amp;#39;!&amp;quot;&amp;#39;},
 {&amp;#39;score&amp;#39;: 0.14516998827457428,
  &amp;#39;sequence&amp;#39;: &amp;#39;you look great 👌&amp;#39;,
  &amp;#39;token&amp;#39;: 328,
  &amp;#39;token_str&amp;#39;: &amp;#39;!&amp;#39;},]
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;However, this is what I am actually getting from my output:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
[{&amp;#39;score&amp;#39;: 0.26041436195373535,
  &amp;#39;sequence&amp;#39;: &amp;#39;You look great?&amp;quot;&amp;#39;,
  &amp;#39;token&amp;#39;: 72,
  &amp;#39;token_str&amp;#39;: &amp;#39;.&amp;quot;&amp;#39;},
 {&amp;#39;score&amp;#39;: 0.1813151091337204,
  &amp;#39;sequence&amp;#39;: &amp;#39;You look great.&amp;quot;&amp;#39;,
  &amp;#39;token&amp;#39;: 2901,
  &amp;#39;token_str&amp;#39;: &amp;#39;!&amp;quot;&amp;#39;},
 {&amp;#39;score&amp;#39;: 0.14516998827457428,
  &amp;#39;sequence&amp;#39;: &amp;#39;You look great!&amp;#39;,
  &amp;#39;token&amp;#39;: 328,
  &amp;#39;token_str&amp;#39;: &amp;#39;!&amp;#39;},]
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I know it is possible to do this, but how do I do it? I am close, but not very.&lt;/p&gt;

&lt;p&gt;Likewise, I have my model fully trained on my dataset, but it just does not seem to output emojis, even though I have included them in the training. &lt;/p&gt;

&lt;p&gt;Does something need to be included to accept emoji? If so, what?&lt;/p&gt;

&lt;p&gt;Thanks - I would really appreciate the help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nu90dm,True,,saucyhambon,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nu90dm/hugging_face_unable_to_use_emojis_for_masked/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nu90dm/hugging_face_unable_to_use_emojis_for_masked/,30199,1623061348.0,0,,False,,,,,,1780
1042,,LanguageTechnology,"Hello everyone!

I'm an Italian uni student, and I'm going to graduate in foreign languages and literature in October (English and German).

I'm interested in studying CL after my Bachelor (Stuttgart seems to be a good choice, and spending some time in a German-speaking country would enable me to improve my German), but I don't know whether I stand any chance of getting accepted into this programme.

I took a couple of linguistics exams, but since in Italy Bachelors of linguistics don't exist at all, all I could do was enrol in the languages programme. Stuttgart university accepts Bachelors related to linguistics, but I'm still afraid that my degree won't be enough to satisfy their admission requirements.

I know the basics of programming (Python), but I actually don't know what I could do so as to stand a better chance of getting accepted. 

Is there anything else I could do?",t2_5abdti3m,False,,0,False,Master's degree in Computational Linguistics (Stuttgart),[],r/LanguageTechnology,False,6,,0,,False,t3_ntw7yu,False,dark,0.92,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1623043955.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m an Italian uni student, and I&amp;#39;m going to graduate in foreign languages and literature in October (English and German).&lt;/p&gt;

&lt;p&gt;I&amp;#39;m interested in studying CL after my Bachelor (Stuttgart seems to be a good choice, and spending some time in a German-speaking country would enable me to improve my German), but I don&amp;#39;t know whether I stand any chance of getting accepted into this programme.&lt;/p&gt;

&lt;p&gt;I took a couple of linguistics exams, but since in Italy Bachelors of linguistics don&amp;#39;t exist at all, all I could do was enrol in the languages programme. Stuttgart university accepts Bachelors related to linguistics, but I&amp;#39;m still afraid that my degree won&amp;#39;t be enough to satisfy their admission requirements.&lt;/p&gt;

&lt;p&gt;I know the basics of programming (Python), but I actually don&amp;#39;t know what I could do so as to stand a better chance of getting accepted. &lt;/p&gt;

&lt;p&gt;Is there anything else I could do?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ntw7yu,True,,sardina9,,14,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ntw7yu/masters_degree_in_computational_linguistics/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ntw7yu/masters_degree_in_computational_linguistics/,30199,1623015155.0,0,,False,,,,,,889
1043,,LanguageTechnology,"Three days ago I made a video explaining how my NLP transformers course would be entirely free as part of a limited-time promo. I shared that video here and in a couple of other subreddits too, r/learnmachinglearning and r/Python being two.

Three days and 10823 downloads later, here we are! I thought we'd be lucky to hit 1K!

Incredible response, and very happy to be able to have been able to give so many of you an opportunity to access the course where some of you may not have been able to otherwise. I'm looking forward to working with all the students and helping you guys out, just please don't all ask me questions at once! 😬

Thanks all, truly humbled by the response - it's really *really* cool, it has blown my mind.

For any of you that are still interested, I will leave a final discount link [here](https://www.udemy.com/course/nlp-with-transformers/?couponCode=MEDIUM), thanks all!",t2_oupz3m9,False,,0,False,More than 10K of you downloaded the free NLP transformers course... Wow!,[],r/LanguageTechnology,False,6,,0,,False,t3_nti8vm,False,dark,0.94,,public,48,1,{},,False,[],,False,False,,{},,False,48,,False,True,,False,,[],{},,True,,1623001846.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Three days ago I made a video explaining how my NLP transformers course would be entirely free as part of a limited-time promo. I shared that video here and in a couple of other subreddits too, &lt;a href=""/r/learnmachinglearning""&gt;r/learnmachinglearning&lt;/a&gt; and &lt;a href=""/r/Python""&gt;r/Python&lt;/a&gt; being two.&lt;/p&gt;

&lt;p&gt;Three days and 10823 downloads later, here we are! I thought we&amp;#39;d be lucky to hit 1K!&lt;/p&gt;

&lt;p&gt;Incredible response, and very happy to be able to have been able to give so many of you an opportunity to access the course where some of you may not have been able to otherwise. I&amp;#39;m looking forward to working with all the students and helping you guys out, just please don&amp;#39;t all ask me questions at once! 😬&lt;/p&gt;

&lt;p&gt;Thanks all, truly humbled by the response - it&amp;#39;s really &lt;em&gt;really&lt;/em&gt; cool, it has blown my mind.&lt;/p&gt;

&lt;p&gt;For any of you that are still interested, I will leave a final discount link &lt;a href=""https://www.udemy.com/course/nlp-with-transformers/?couponCode=MEDIUM""&gt;here&lt;/a&gt;, thanks all!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nti8vm,True,,jamescalam,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nti8vm/more_than_10k_of_you_downloaded_the_free_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nti8vm/more_than_10k_of_you_downloaded_the_free_nlp/,30199,1622973046.0,0,,False,,,,,,899
1044,,LanguageTechnology,,t2_l7idt,False,,0,False,Debiasing large pretrained language models using distributional control,[],r/LanguageTechnology,False,6,,0,,False,t3_ntqm8h,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,False,,1623028654.0,text,6,,,text,europe.naverlabs.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ntqm8h,True,,pigdogsheep,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ntqm8h/debiasing_large_pretrained_language_models_using/,all_ads,False,https://europe.naverlabs.com/blog/debiasing-large-pretrained-language-models-using-distributional-control/,30199,1622999854.0,0,,False,https://europe.naverlabs.com/blog/debiasing-large-pretrained-language-models-using-distributional-control/,,,,,0
1045,,LanguageTechnology,,t2_m8kccne,False,,0,False,Does anyone know of coreference resolution tools where you can specify the entity?,[],r/LanguageTechnology,False,6,,0,,False,t3_ntb3m6,False,dark,0.91,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,False,,1622973933.0,text,6,,,text,self.MachineLearning,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ntb3m6,True,,Seankala,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ntb3m6/does_anyone_know_of_coreference_resolution_tools/,all_ads,False,/r/MachineLearning/comments/nta7gu/d_does_anyone_know_of_coreference_resolution/,30199,1622945133.0,0,,False,/r/MachineLearning/comments/nta7gu/d_does_anyone_know_of_coreference_resolution/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': ""Hi. Let me elaborate on the title. I'm currently working on paragraph-level data and want to perform coreference resolution. I've tried working with spaCy's [NeuralCoref](https://github.com/huggingface/neuralcoref), and although it works great it receives a string as input and returns all entities and mentions it deems appropriate. Rather than that I'm looking for something where you can specify the entity and the model will return all such instances for that particular entity.\n\nDoes anyone know if something like that exists? Thanks."", 'author_fullname': 't2_m8kccne', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] Does anyone know of coreference resolution tools where you can specify the entity?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_nta7gu', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': 'c5cf3b2a-6abd-11ea-a37b-0ebd427f43f1', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1622970877.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi. Let me elaborate on the title. I&amp;#39;m currently working on paragraph-level data and want to perform coreference resolution. I&amp;#39;ve tried working with spaCy&amp;#39;s &lt;a href=""https://github.com/huggingface/neuralcoref""&gt;NeuralCoref&lt;/a&gt;, and although it works great it receives a string as input and returns all entities and mentions it deems appropriate. Rather than that I&amp;#39;m looking for something where you can specify the entity and the model will return all such instances for that particular entity.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know if something like that exists? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Student', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'nta7gu', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Seankala', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/MachineLearning/comments/nta7gu/d_does_anyone_know_of_coreference_resolution/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/nta7gu/d_does_anyone_know_of_coreference_resolution/', 'subreddit_subscribers': 1930728, 'created_utc': 1622942077.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_nta7gu,,,0
1046,,LanguageTechnology,"Hello! I am a PhD researcher. I am looking urgently for someone having used ConvoKit and knowing how to extract coded variables through it. Please reply here or email me at:

[ax23@kent.ac.uk](mailto:ax23@kent.ac.uk)",t2_4yilft17,False,,0,False,ConvoKit corpus research,[],r/LanguageTechnology,False,6,,0,,False,t3_ntia70,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1623002003.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! I am a PhD researcher. I am looking urgently for someone having used ConvoKit and knowing how to extract coded variables through it. Please reply here or email me at:&lt;/p&gt;

&lt;p&gt;[&lt;a href=""mailto:ax23@kent.ac.uk""&gt;ax23@kent.ac.uk&lt;/a&gt;](mailto:&lt;a href=""mailto:ax23@kent.ac.uk""&gt;ax23@kent.ac.uk&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ntia70,True,,annaksig,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ntia70/convokit_corpus_research/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ntia70/convokit_corpus_research/,30199,1622973203.0,0,,False,,,,,,216
1047,,LanguageTechnology,"I have followed this tutorial for masked language modelling from Hugging Face using BERT, but I am unsure how to actually deploy the model.

Tutorial: [https://github.com/huggingface/notebooks/blob/master/examples/language\_modeling.ipynb](https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb)

I have trained the model using my own dataset, which has worked fine, but I don't know how to actually use the model, as the notebook does not include an example on how to do this, sadly.

&amp;#x200B;

On the Hugging Face website, this is the code used in the example; hence, I want to do this exact thing but with my model:

    &gt;&gt;&gt; from transformers import pipeline
    &gt;&gt;&gt; unmasker = pipeline('fill-mask', model='bert-base-uncased')
    &gt;&gt;&gt; unmasker(""Hello I'm a [MASK] model."")
    
    [{'sequence': ""[CLS] hello i'm a fashion model. [SEP]"",
      'score': 0.1073106899857521,
      'token': 4827,
      'token_str': 'fashion'},
     {'sequence': ""[CLS] hello i'm a role model. [SEP]"",
      'score': 0.08774490654468536,
      'token': 2535,
      'token_str': 'role'},
     {'sequence': ""[CLS] hello i'm a new model. [SEP]"",
      'score': 0.05338378623127937,
      'token': 2047,
      'token_str': 'new'},
     {'sequence': ""[CLS] hello i'm a super model. [SEP]"",
      'score': 0.04667217284440994,
      'token': 3565,
      'token_str': 'super'},
     {'sequence': ""[CLS] hello i'm a fine model. [SEP]"",
      'score': 0.027095865458250046,
      'token': 2986,
      'token_str': 'fine'}

Any help on how to do this would be great.",t2_13ussz,False,,0,False,Hugging Face: How to test masked language model after training it?,[],r/LanguageTechnology,False,6,,0,,False,t3_nt04tj,False,dark,1.0,,public,17,0,{},,False,[],,False,False,,{},,False,17,,False,False,,False,,[],{},,True,,1622941039.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have followed this tutorial for masked language modelling from Hugging Face using BERT, but I am unsure how to actually deploy the model.&lt;/p&gt;

&lt;p&gt;Tutorial: &lt;a href=""https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb""&gt;https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have trained the model using my own dataset, which has worked fine, but I don&amp;#39;t know how to actually use the model, as the notebook does not include an example on how to do this, sadly.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;On the Hugging Face website, this is the code used in the example; hence, I want to do this exact thing but with my model:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from transformers import pipeline
&amp;gt;&amp;gt;&amp;gt; unmasker = pipeline(&amp;#39;fill-mask&amp;#39;, model=&amp;#39;bert-base-uncased&amp;#39;)
&amp;gt;&amp;gt;&amp;gt; unmasker(&amp;quot;Hello I&amp;#39;m a [MASK] model.&amp;quot;)

[{&amp;#39;sequence&amp;#39;: &amp;quot;[CLS] hello i&amp;#39;m a fashion model. [SEP]&amp;quot;,
  &amp;#39;score&amp;#39;: 0.1073106899857521,
  &amp;#39;token&amp;#39;: 4827,
  &amp;#39;token_str&amp;#39;: &amp;#39;fashion&amp;#39;},
 {&amp;#39;sequence&amp;#39;: &amp;quot;[CLS] hello i&amp;#39;m a role model. [SEP]&amp;quot;,
  &amp;#39;score&amp;#39;: 0.08774490654468536,
  &amp;#39;token&amp;#39;: 2535,
  &amp;#39;token_str&amp;#39;: &amp;#39;role&amp;#39;},
 {&amp;#39;sequence&amp;#39;: &amp;quot;[CLS] hello i&amp;#39;m a new model. [SEP]&amp;quot;,
  &amp;#39;score&amp;#39;: 0.05338378623127937,
  &amp;#39;token&amp;#39;: 2047,
  &amp;#39;token_str&amp;#39;: &amp;#39;new&amp;#39;},
 {&amp;#39;sequence&amp;#39;: &amp;quot;[CLS] hello i&amp;#39;m a super model. [SEP]&amp;quot;,
  &amp;#39;score&amp;#39;: 0.04667217284440994,
  &amp;#39;token&amp;#39;: 3565,
  &amp;#39;token_str&amp;#39;: &amp;#39;super&amp;#39;},
 {&amp;#39;sequence&amp;#39;: &amp;quot;[CLS] hello i&amp;#39;m a fine model. [SEP]&amp;quot;,
  &amp;#39;score&amp;#39;: 0.027095865458250046,
  &amp;#39;token&amp;#39;: 2986,
  &amp;#39;token_str&amp;#39;: &amp;#39;fine&amp;#39;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Any help on how to do this would be great.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nt04tj,True,,saucyhambon,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nt04tj/hugging_face_how_to_test_masked_language_model/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nt04tj/hugging_face_how_to_test_masked_language_model/,30199,1622912239.0,0,,False,,,,,,1601
1048,,LanguageTechnology,"I have a task where i want to use multilingual embeddings for 2 different languages(one of them being english). 
I first checked fasttext but its aligned vectors does have one pf my language. So i check a basic vector aligning algo and it was using common words between two languages to align them. But one of my language does not have english characters so cant use that algo.

Then i read about BERT embeddings and found multilingual model on their git repo. But i dont know how to get word embeddings using using that model.
So, does anybody know how to use the embeddings from bert multilingual model.",t2_5pmdgm1l,False,,0,False,How to use BERT multilingual embedding,[],r/LanguageTechnology,False,6,,0,,False,t3_nt6wsx,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1622960253.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a task where i want to use multilingual embeddings for 2 different languages(one of them being english). 
I first checked fasttext but its aligned vectors does have one pf my language. So i check a basic vector aligning algo and it was using common words between two languages to align them. But one of my language does not have english characters so cant use that algo.&lt;/p&gt;

&lt;p&gt;Then i read about BERT embeddings and found multilingual model on their git repo. But i dont know how to get word embeddings using using that model.
So, does anybody know how to use the embeddings from bert multilingual model.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nt6wsx,True,,inopico3,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nt6wsx/how_to_use_bert_multilingual_embedding/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nt6wsx/how_to_use_bert_multilingual_embedding/,30199,1622931453.0,0,,False,,,,,,605
1049,,LanguageTechnology,"Hi, I'd like to write a program that does the following: you put in a word (or words) and as an output you get a similar word (words) to it. So if I give the words 'investment banker' as an input, it should give back the words 'investment analyst' or 'private equity manager'.

The language wouldn't be English, but French, German and Dutch. Which frameworks, libraries and techniques should I look at to implement this and how do I make it work as accurate as possible? Does anyone have any experience with this that you'd like or good guides on it? Is it hard to implement this program? Thanks in advance.",t2_6eu9a1fu,False,,0,False,"How do I find similar words to certain words (NLP, other techniques)?",[],r/LanguageTechnology,False,6,,0,,False,t3_nt3kgx,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1622950683.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;d like to write a program that does the following: you put in a word (or words) and as an output you get a similar word (words) to it. So if I give the words &amp;#39;investment banker&amp;#39; as an input, it should give back the words &amp;#39;investment analyst&amp;#39; or &amp;#39;private equity manager&amp;#39;.&lt;/p&gt;

&lt;p&gt;The language wouldn&amp;#39;t be English, but French, German and Dutch. Which frameworks, libraries and techniques should I look at to implement this and how do I make it work as accurate as possible? Does anyone have any experience with this that you&amp;#39;d like or good guides on it? Is it hard to implement this program? Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nt3kgx,True,,throwaway228526,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nt3kgx/how_do_i_find_similar_words_to_certain_words_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nt3kgx/how_do_i_find_similar_words_to_certain_words_nlp/,30199,1622921883.0,0,,False,,,,,,607
1050,,LanguageTechnology,"I’m interested in parsing manual test cases into classified/formatted data so I can extract the intent of the tester and then generate automated test cases (e.g. Selenium). I’ve been playing around with Python’s spaCy library and noticed that when I process the following string, I get incorrect tags:

“User enters Password and press tab key”

“press” is incorrectly tagged as a noun when it should really be a verb. I realize that the word should have been “pressED” and may be the reason why tags are coming out wrong, but is there anything that can be done to work around typo issues like these?",t2_cqf7g,False,,0,False,Incorrect tags when parsing test case,[],r/LanguageTechnology,False,6,,0,,False,t3_nt6aqr,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1622958469.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m interested in parsing manual test cases into classified/formatted data so I can extract the intent of the tester and then generate automated test cases (e.g. Selenium). I’ve been playing around with Python’s spaCy library and noticed that when I process the following string, I get incorrect tags:&lt;/p&gt;

&lt;p&gt;“User enters Password and press tab key”&lt;/p&gt;

&lt;p&gt;“press” is incorrectly tagged as a noun when it should really be a verb. I realize that the word should have been “pressED” and may be the reason why tags are coming out wrong, but is there anything that can be done to work around typo issues like these?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nt6aqr,True,,coolio777,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nt6aqr/incorrect_tags_when_parsing_test_case/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nt6aqr/incorrect_tags_when_parsing_test_case/,30199,1622929669.0,0,,False,,,,,,599
1051,,LanguageTechnology,Hi everyone I search for good resource to implement ALBERT for sarcasm detection on Reddit dataset...can u help me?,t2_66lguvzc,False,,0,False,How to implement ALBERT for sarcasm detection,[],r/LanguageTechnology,False,6,,0,,False,t3_nsr07m,False,dark,0.85,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1622910099.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone I search for good resource to implement ALBERT for sarcasm detection on Reddit dataset...can u help me?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nsr07m,True,,mahdir74,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nsr07m/how_to_implement_albert_for_sarcasm_detection/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nsr07m/how_to_implement_albert_for_sarcasm_detection/,30199,1622881299.0,0,,False,,,,,,115
1052,,LanguageTechnology,,t2_21qaqh1p,False,,0,False,Building Grammar Correction API in Python with Gramformer and FastApI,[],r/LanguageTechnology,False,6,,0,,False,t3_nsdki0,False,dark,0.89,,public,15,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/yH36NQGp4NQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Build Grammarly API Opensource Grammar Correction Alternative with Gramformer &amp; FastAPI in Python', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/yH36NQGp4NQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/yH36NQGp4NQ/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/yH36NQGp4NQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nsdki0', 'height': 200}",,False,15,,False,False,,False,,[],{},,False,,1622864402.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nsdki0,True,,dulldata,,0,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/nsdki0/building_grammar_correction_api_in_python_with/,all_ads,False,https://youtu.be/yH36NQGp4NQ,30199,1622835602.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Build Grammarly API Opensource Grammar Correction Alternative with Gramformer &amp; FastAPI in Python', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/yH36NQGp4NQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/yH36NQGp4NQ/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,https://youtu.be/yH36NQGp4NQ,,,,,0
1053,,LanguageTechnology,"Hey,

I'm working with rule-based language models and I'm wondering if there is anything that could be called a ""comprehensive set of rules""  for the English grammar. My gut feeling tells me that it is close to impossible to catch all possible grammatical variations of the English language, but I'd love to be proven wrong!

I'm know of Lambeks Pregroup grammar and Montague grammar but I'm not sure if any of those formalism captures the English language without major flaws.

I appreciate any references!",t2_43ky8wby,False,,0,False,Does a truly comprehensive rule-based grammar for the English language exist? Or is there any (recent) study that discusses their limitations?,[],r/LanguageTechnology,False,6,,0,,False,t3_ns27v5,False,dark,1.0,,public,20,0,{},,False,[],,False,False,,{},,False,20,,False,False,,False,,[],{},,True,,1622832539.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working with rule-based language models and I&amp;#39;m wondering if there is anything that could be called a &amp;quot;comprehensive set of rules&amp;quot;  for the English grammar. My gut feeling tells me that it is close to impossible to catch all possible grammatical variations of the English language, but I&amp;#39;d love to be proven wrong!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m know of Lambeks Pregroup grammar and Montague grammar but I&amp;#39;m not sure if any of those formalism captures the English language without major flaws.&lt;/p&gt;

&lt;p&gt;I appreciate any references!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ns27v5,True,,chappy_tha_janitor,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ns27v5/does_a_truly_comprehensive_rulebased_grammar_for/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ns27v5/does_a_truly_comprehensive_rulebased_grammar_for/,30199,1622803739.0,0,,False,,,,,,507
1054,,LanguageTechnology,"I've been using gImageReader for a few years and I've always used it to export the OCR'ed text as an invisible text layer over the existing pdf scan, resulting in a searchable pdf file, which is useful for research, working with the text etc.

I was doing this on Windows 10, and recently installed a dual boot with Linux Mint and reinstalled Windows due to slow-down. Now I reinstalled gImageReader and I can no longer find the option to export to PDF with text layer. In fact I'm having trouble even finding much mention of said function online. I don't know if I originally installed some fork or beta or what, but the way the (probably) official version of the program looks is not what I'd been using so far.

I was always able to set certain post-production parameters, such as size of the invisible text etc.

Any ideas where I could find that version again? I'd prefer a Linux Mint compatible version, but Windows would also be fine, if that's the only one.  
Thanks!",t2_5lrs9ing,False,,0,False,Trouble with gImageReader (Tesseract) - Exporting to PDF (Linux Mint),[],r/LanguageTechnology,False,6,,0,,False,t3_nsd4bj,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1622863252.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been using gImageReader for a few years and I&amp;#39;ve always used it to export the OCR&amp;#39;ed text as an invisible text layer over the existing pdf scan, resulting in a searchable pdf file, which is useful for research, working with the text etc.&lt;/p&gt;

&lt;p&gt;I was doing this on Windows 10, and recently installed a dual boot with Linux Mint and reinstalled Windows due to slow-down. Now I reinstalled gImageReader and I can no longer find the option to export to PDF with text layer. In fact I&amp;#39;m having trouble even finding much mention of said function online. I don&amp;#39;t know if I originally installed some fork or beta or what, but the way the (probably) official version of the program looks is not what I&amp;#39;d been using so far.&lt;/p&gt;

&lt;p&gt;I was always able to set certain post-production parameters, such as size of the invisible text etc.&lt;/p&gt;

&lt;p&gt;Any ideas where I could find that version again? I&amp;#39;d prefer a Linux Mint compatible version, but Windows would also be fine, if that&amp;#39;s the only one.&lt;br/&gt;
Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nsd4bj,True,,Party-Permission,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nsd4bj/trouble_with_gimagereader_tesseract_exporting_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nsd4bj/trouble_with_gimagereader_tesseract_exporting_to/,30199,1622834452.0,0,,False,,,,,,975
1055,,LanguageTechnology,I would like to subscribe to some weekly news about NLP in order to be up-to-date with latest research in NLP. Could somebody please recommend me one?,t2_5pyrrh3o,False,,0,False,Best weekly digest to keep up with NLP research?,[],r/LanguageTechnology,False,6,,0,,False,t3_ns2x5i,False,dark,0.81,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1622835103.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I would like to subscribe to some weekly news about NLP in order to be up-to-date with latest research in NLP. Could somebody please recommend me one?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ns2x5i,True,,luisda2994,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ns2x5i/best_weekly_digest_to_keep_up_with_nlp_research/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ns2x5i/best_weekly_digest_to_keep_up_with_nlp_research/,30199,1622806303.0,0,,False,,,,,,150
1056,,LanguageTechnology,I am playing to write a paper in the NLP field on fake news classifier. Any suggestions about what I could try out and write on? ,t2_85ssy9vj,False,,0,False,Advice for fake news classifier research paper,[],r/LanguageTechnology,False,6,,0,,False,t3_nscnd2,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1622862033.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am playing to write a paper in the NLP field on fake news classifier. Any suggestions about what I could try out and write on? &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nscnd2,True,,The-Bored-Guy10,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nscnd2/advice_for_fake_news_classifier_research_paper/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nscnd2/advice_for_fake_news_classifier_research_paper/,30199,1622833233.0,0,,False,,,,,,129
1057,,LanguageTechnology,I understand that models like GPT 3 are auto regressive models which uses word statistics but what I am struggling to understand is why the discernable semantic information that you can get from a word2vec model is not symbolic representation in at least in some way?,t2_ci2ras9r,False,,0,False,Can you guys help me understand how word embeddings is not a symbolic representation of language?,[],r/LanguageTechnology,False,6,,0,,False,t3_ns23bt,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1622832034.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I understand that models like GPT 3 are auto regressive models which uses word statistics but what I am struggling to understand is why the discernable semantic information that you can get from a word2vec model is not symbolic representation in at least in some way?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ns23bt,True,,FairVector,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ns23bt/can_you_guys_help_me_understand_how_word/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ns23bt/can_you_guys_help_me_understand_how_word/,30199,1622803234.0,0,,False,,,,,,267
1058,,LanguageTechnology,,t2_hkv9s,False,,0,False,Training T5 model in just 3 lines of code with ONNX Inference,[],r/LanguageTechnology,False,6,,0,,False,t3_ns3hip,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,False,,1622837048.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ns3hip,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ns3hip/training_t5_model_in_just_3_lines_of_code_with/,all_ads,False,https://link.medium.com/KYA6WERvOgb,30199,1622808248.0,0,,False,https://link.medium.com/KYA6WERvOgb,,,,,0
1059,,LanguageTechnology,"Hi, I am relatively new to NLP. I am trying to write a code that allows me to grade elementary student's  Science questions. For keyword-based answers, I am able to easily match student's answer to the model answer using lemmatization and such.

**Question: Why did you classify Animal A as a mammal?**

**Answer: fur/ produces milk/ give birth/ warm-blooded**

matching the keywords in the answer is relatively straightforward.

&amp;#x200B;

However, the issue comes when we need to match answers that are more complex.

**Question: what is the relationship between the amount of water given and the height of the seedling?**

**Answer: As the amount of water given increases, the height of the seedling increases.**

&amp;#x200B;

A student may phrase the above answer differently and get a correct answer. (e.g. more water given, taller the plant grows). Take note that students are not graded on language skills ( i.e. grammar and spelling to a reasonable extent.)

I have tried using a pre-trained corpus to calculate the cosine similarity and use that to determine whether an answer matches the model answer or not. However, the resulting cosine values are rather indistinguishable. As a result, I am not able to pick a suitable threshold to ""differentiate"" right from wrong.

I have considered breaking down the answers into clauses and perform a similar analysis on them individually before adding up the scores for each clause. I could possibly use the dependency matcher of spacy to fix the correct subject-verb-object relation in an attempt to extract semantic meaning using algorithms. I guess the next step forward would be to apply machine learning. Thank you for taking the time to read this. I would greatly appreciate any inputs from anyone here. Thanks again!",t2_3rs6f9dy,False,,0,False,Answer matching using sentence similarity,[],r/LanguageTechnology,False,6,,0,,False,t3_ns4nw3,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1622840718.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I am relatively new to NLP. I am trying to write a code that allows me to grade elementary student&amp;#39;s  Science questions. For keyword-based answers, I am able to easily match student&amp;#39;s answer to the model answer using lemmatization and such.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Question: Why did you classify Animal A as a mammal?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Answer: fur/ produces milk/ give birth/ warm-blooded&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;matching the keywords in the answer is relatively straightforward.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;However, the issue comes when we need to match answers that are more complex.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Question: what is the relationship between the amount of water given and the height of the seedling?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Answer: As the amount of water given increases, the height of the seedling increases.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;A student may phrase the above answer differently and get a correct answer. (e.g. more water given, taller the plant grows). Take note that students are not graded on language skills ( i.e. grammar and spelling to a reasonable extent.)&lt;/p&gt;

&lt;p&gt;I have tried using a pre-trained corpus to calculate the cosine similarity and use that to determine whether an answer matches the model answer or not. However, the resulting cosine values are rather indistinguishable. As a result, I am not able to pick a suitable threshold to &amp;quot;differentiate&amp;quot; right from wrong.&lt;/p&gt;

&lt;p&gt;I have considered breaking down the answers into clauses and perform a similar analysis on them individually before adding up the scores for each clause. I could possibly use the dependency matcher of spacy to fix the correct subject-verb-object relation in an attempt to extract semantic meaning using algorithms. I guess the next step forward would be to apply machine learning. Thank you for taking the time to read this. I would greatly appreciate any inputs from anyone here. Thanks again!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ns4nw3,True,,tyx8099,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ns4nw3/answer_matching_using_sentence_similarity/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ns4nw3/answer_matching_using_sentence_similarity/,30199,1622811918.0,0,,False,,,,,,1778
1060,,LanguageTechnology,"Hi there, I am building a text classification model to match the name and description of a customer's item (e.g. name: ""suction press nip"", category: ""paper machine parts"") to a list of 10k basic items (name: ""steel, unalloyed"", category: ""metals""). I have some initial matched data to test and I will get more and more, hopefully.

I've build a sentiment analysis program in the past, this is a good example of what I used: [https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/](https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/) (Spacy, Scikitlearn).

This current problem is more complex though, it's 1 to 10k+ match and not binary (or max 5, 6 values), the string for the item is short and absolutely at the discretion of the source (client item log).

Which reads/tutorials/examples would you suggest to take a look at? (in Python please)",t2_10p8w7,False,,0,False,"Text classification for item matching, best setup?",[],r/LanguageTechnology,False,6,,0,,False,t3_ns1dt0,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1622829326.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there, I am building a text classification model to match the name and description of a customer&amp;#39;s item (e.g. name: &amp;quot;suction press nip&amp;quot;, category: &amp;quot;paper machine parts&amp;quot;) to a list of 10k basic items (name: &amp;quot;steel, unalloyed&amp;quot;, category: &amp;quot;metals&amp;quot;). I have some initial matched data to test and I will get more and more, hopefully.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve build a sentiment analysis program in the past, this is a good example of what I used: &lt;a href=""https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/""&gt;https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/&lt;/a&gt; (Spacy, Scikitlearn).&lt;/p&gt;

&lt;p&gt;This current problem is more complex though, it&amp;#39;s 1 to 10k+ match and not binary (or max 5, 6 values), the string for the item is short and absolutely at the discretion of the source (client item log).&lt;/p&gt;

&lt;p&gt;Which reads/tutorials/examples would you suggest to take a look at? (in Python please)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ns1dt0,True,,lele-canfora,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ns1dt0/text_classification_for_item_matching_best_setup/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ns1dt0/text_classification_for_item_matching_best_setup/,30199,1622800526.0,0,,False,,,,,,904
1061,,LanguageTechnology," Given a table, I am able to convert natural language questions into appropriate SQL query with transformers.

The architecture of a chatbot should be:

1. Natural language question to SQL query translation using transformers (this part is completed)
2. Fed SQL query to SQL engine and collect response (this part is completed)
3. Convert SQL engine response to Natural Language Response

How can I accomplish the last part? What kind of architecture or model should I use?",t2_8tvf8evl,False,,0,False,[Discussion] What should be the data driven chatbot architecture using NLP2SQL?,[],r/LanguageTechnology,False,6,,0,,False,t3_nrxxpr,False,dark,0.99,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1622815079.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Given a table, I am able to convert natural language questions into appropriate SQL query with transformers.&lt;/p&gt;

&lt;p&gt;The architecture of a chatbot should be:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Natural language question to SQL query translation using transformers (this part is completed)&lt;/li&gt;
&lt;li&gt;Fed SQL query to SQL engine and collect response (this part is completed)&lt;/li&gt;
&lt;li&gt;Convert SQL engine response to Natural Language Response&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;How can I accomplish the last part? What kind of architecture or model should I use?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nrxxpr,True,,Current_Dark6603,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nrxxpr/discussion_what_should_be_the_data_driven_chatbot/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nrxxpr/discussion_what_should_be_the_data_driven_chatbot/,30199,1622786279.0,0,,False,,,,,,473
1062,,LanguageTechnology,Any method to negate a Verb or Noun word in the sentence using NLP,t2_cgu1aqt8,False,,0,False,To Negate a word,[],r/LanguageTechnology,False,6,,0,,False,t3_nrwarv,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1622808862.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Any method to negate a Verb or Noun word in the sentence using NLP&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nrwarv,True,,Effective-Piglet-334,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nrwarv/to_negate_a_word/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nrwarv/to_negate_a_word/,30199,1622780062.0,0,,False,,,,,,66
1063,,LanguageTechnology,"Hi,

It might be a novice question, but could be a deeper one as well.

Anyone out there please guide me with what is actually happening when you say,

**Play me Butter**

to your Alexa/Google/Siri?

What I know is that, the ML/NLP engine under the hood will

1. Classify the intent -&gt; Music Play
2. Recognize what ""Butter"" means -&gt; It is a song by BTS
3. Tell ""here's Butter by BTS"" to the user
4. And actually play it.

But this is just the high-level sketch. For example, the word ""Butter"" was meaning the food ingredient before BTS launched the single.

**How will the engine behind update their Knowledge Base according to new releases? In a timely manner?** 

I can't imagine it's being done manually. And,

What will be the pipeline of classifiers for finally recognizing Butter as a BTS song? I doubt it'll be done with a single classifier.

Thanks a ton in advance!",t2_cidssi7w,False,,0,False,"What is happening when you say ""play me butter"" to your Alexa/Google/Siri?",[],r/LanguageTechnology,False,6,,0,,False,t3_nrmv0a,False,dark,0.92,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1622780194.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;It might be a novice question, but could be a deeper one as well.&lt;/p&gt;

&lt;p&gt;Anyone out there please guide me with what is actually happening when you say,&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Play me Butter&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;to your Alexa/Google/Siri?&lt;/p&gt;

&lt;p&gt;What I know is that, the ML/NLP engine under the hood will&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Classify the intent -&amp;gt; Music Play&lt;/li&gt;
&lt;li&gt;Recognize what &amp;quot;Butter&amp;quot; means -&amp;gt; It is a song by BTS&lt;/li&gt;
&lt;li&gt;Tell &amp;quot;here&amp;#39;s Butter by BTS&amp;quot; to the user&lt;/li&gt;
&lt;li&gt;And actually play it.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;But this is just the high-level sketch. For example, the word &amp;quot;Butter&amp;quot; was meaning the food ingredient before BTS launched the single.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How will the engine behind update their Knowledge Base according to new releases? In a timely manner?&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;I can&amp;#39;t imagine it&amp;#39;s being done manually. And,&lt;/p&gt;

&lt;p&gt;What will be the pipeline of classifiers for finally recognizing Butter as a BTS song? I doubt it&amp;#39;ll be done with a single classifier.&lt;/p&gt;

&lt;p&gt;Thanks a ton in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nrmv0a,True,,nlp_ttt,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nrmv0a/what_is_happening_when_you_say_play_me_butter_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nrmv0a/what_is_happening_when_you_say_play_me_butter_to/,30199,1622751394.0,0,,False,,,,,,880
1064,,LanguageTechnology,"I have a deadline tonight and I wont make it because the code takes several days to finish making the corpus and then passing them it through LDA.

&amp;#x200B;

Edit: I use gensim in python",t2_5r4mo7fh,False,,0,False,Is there a cloud service where I can run my LDA code at a good speed?,[],r/LanguageTechnology,False,6,,0,,False,t3_ns09fc,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,1622799902.0,,[],{},,True,,1622824825.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a deadline tonight and I wont make it because the code takes several days to finish making the corpus and then passing them it through LDA.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Edit: I use gensim in python&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ns09fc,True,,Epiphany925,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ns09fc/is_there_a_cloud_service_where_i_can_run_my_lda/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ns09fc/is_there_a_cloud_service_where_i_can_run_my_lda/,30199,1622796025.0,0,,False,,,,,,190
1065,,LanguageTechnology,"&amp;#x200B;

https://reddit.com/link/nrp6bw/video/dzi6hndkh4371/player

This application builds a local txtai index using book data from [openlibrary.org](https://openlibrary.org). It supports natural language queries to find the best matching books.

Applications that support natural language queries open up exciting possibilities. Take conversational AI as an example, you wouldn't expect users to speak in an abrupt way that is typical with traditional token-based search systems.

GitHub: [https://github.com/neuml/txtai](https://github.com/neuml/txtai)  
Application: [https://github.com/neuml/txtai/blob/master/examples/books.py](https://github.com/neuml/txtai/blob/master/examples/books.py)",t2_536lg1nv,False,,0,False,Build and query a book similarity index,[],r/LanguageTechnology,False,6,,0,,False,t3_nrp6bw,False,dark,0.79,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1622786334.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://reddit.com/link/nrp6bw/video/dzi6hndkh4371/player""&gt;https://reddit.com/link/nrp6bw/video/dzi6hndkh4371/player&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This application builds a local txtai index using book data from &lt;a href=""https://openlibrary.org""&gt;openlibrary.org&lt;/a&gt;. It supports natural language queries to find the best matching books.&lt;/p&gt;

&lt;p&gt;Applications that support natural language queries open up exciting possibilities. Take conversational AI as an example, you wouldn&amp;#39;t expect users to speak in an abrupt way that is typical with traditional token-based search systems.&lt;/p&gt;

&lt;p&gt;GitHub: &lt;a href=""https://github.com/neuml/txtai""&gt;https://github.com/neuml/txtai&lt;/a&gt;&lt;br/&gt;
Application: &lt;a href=""https://github.com/neuml/txtai/blob/master/examples/books.py""&gt;https://github.com/neuml/txtai/blob/master/examples/books.py&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nrp6bw,True,,davidmezzetti,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nrp6bw/build_and_query_a_book_similarity_index/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nrp6bw/build_and_query_a_book_similarity_index/,30199,1622757534.0,0,,False,,,,"{'dzi6hndkh4371': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/nrp6bw/asset/dzi6hndkh4371/DASHPlaylist.mpd?a=1626418261%2CNTg1MWFhZWM4YWM5MzliY2JmNzc1NWJjNjRmMDE5OTA1MTFiNmU5ZWJmOTFhNWQzYjk2OWQ2MGFkZjEyNjdkMw%3D%3D&amp;v=1&amp;f=sd', 'x': 655, 'y': 480, 'hlsUrl': 'https://v.redd.it/link/nrp6bw/asset/dzi6hndkh4371/HLSPlaylist.m3u8?a=1626418261%2CMzA3NzZkODJiNjhmNDg4ZGNkNWRhOWM2YTY5MzgzZjljYzI0NTVmOTNlZGI1MTc3N2YwZGZhYjEzNGE0Njk5NA%3D%3D&amp;v=1&amp;f=sd', 'id': 'dzi6hndkh4371', 'isGif': False}}",,700
1066,,LanguageTechnology,"In the recent update on his upcoming book ""Deep Learning with Python. 2nd edition"" F. Chollet refers to research done in 2017: He and his team did a systematic analysis of text classification using different data sets. He claims that they discovered a simple rule of thumb: If the (number of samples / mean sample length)  &gt; 1500 one should use a sequence model, if  it is &lt; 1500, then one should use a bag-of-bigrams. 

It seems they didn't publish this finding in a research paper but only in a [Google guide to text classification](https://developers.google.com/machine-learning/guides/text-classification) (without any names except 'Google'). I am gathering this from the fact that Chollet only refers to the guide. The guide gives a little bit more information: they ran 450k experiments "" across problems of different types (especially sentiment analysis and topic classification problems), using 12 datasets, alternating for each dataset between different data preprocessing techniques and different model architectures"" ([source](https://developers.google.com/machine-learning/guides/text-classification/step-2-5)). 

As this was done 2017, it would be very interesting to see whether this rule is still valid with the context-sensitive language models like Bert. Does anyone know about research checking this claim in recent years?",t2_w801bv,False,,0,False,Text classification: When to use sequence models over bag-of-words model?,[],r/LanguageTechnology,False,6,,0,,False,t3_nrcaqj,False,dark,0.91,,public,22,1,{},,False,[],,False,False,,{},,False,22,,False,False,,False,,[],{},,True,,1622752118.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In the recent update on his upcoming book &amp;quot;Deep Learning with Python. 2nd edition&amp;quot; F. Chollet refers to research done in 2017: He and his team did a systematic analysis of text classification using different data sets. He claims that they discovered a simple rule of thumb: If the (number of samples / mean sample length)  &amp;gt; 1500 one should use a sequence model, if  it is &amp;lt; 1500, then one should use a bag-of-bigrams. &lt;/p&gt;

&lt;p&gt;It seems they didn&amp;#39;t publish this finding in a research paper but only in a &lt;a href=""https://developers.google.com/machine-learning/guides/text-classification""&gt;Google guide to text classification&lt;/a&gt; (without any names except &amp;#39;Google&amp;#39;). I am gathering this from the fact that Chollet only refers to the guide. The guide gives a little bit more information: they ran 450k experiments &amp;quot; across problems of different types (especially sentiment analysis and topic classification problems), using 12 datasets, alternating for each dataset between different data preprocessing techniques and different model architectures&amp;quot; (&lt;a href=""https://developers.google.com/machine-learning/guides/text-classification/step-2-5""&gt;source&lt;/a&gt;). &lt;/p&gt;

&lt;p&gt;As this was done 2017, it would be very interesting to see whether this rule is still valid with the context-sensitive language models like Bert. Does anyone know about research checking this claim in recent years?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nrcaqj,True,,rickschott,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nrcaqj/text_classification_when_to_use_sequence_models/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nrcaqj/text_classification_when_to_use_sequence_models/,30199,1622723318.0,0,,False,,,,,,1346
1067,,LanguageTechnology,"I want to develop software that helps streamline connecting people with roomates/apartments. Right now I'm using a few facebook groups where people post about either looking for someone to rent a room in their apartment or they post about needing a room. I'd like to write something that automatically parses the data out of these that people commonly need. Like: Is this apartment pet friendly? How much is the room? What neighborhood is it in? How many bedrooms? etc.

My background is in computer vision for robotics with CNNs, so this is a totally different domain I'm not familiar with. From the research I've done so far, it sounds like I should look into entity recognition and relationship extraction. But I'm not sure what models are good for that, how much labeled data I need to get started.

I'd be willing to put a few thousand dollars into data annotation I think if that could get me something I could use for my own apartment search.

What models should I look into? What data labeling services/tools? How should I approach this?",t2_56mozcie,False,,0,False,Advice for how to approach classifying apartment posts on facebook?,[],r/LanguageTechnology,False,6,,0,,False,t3_nrv8xt,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1622805212.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to develop software that helps streamline connecting people with roomates/apartments. Right now I&amp;#39;m using a few facebook groups where people post about either looking for someone to rent a room in their apartment or they post about needing a room. I&amp;#39;d like to write something that automatically parses the data out of these that people commonly need. Like: Is this apartment pet friendly? How much is the room? What neighborhood is it in? How many bedrooms? etc.&lt;/p&gt;

&lt;p&gt;My background is in computer vision for robotics with CNNs, so this is a totally different domain I&amp;#39;m not familiar with. From the research I&amp;#39;ve done so far, it sounds like I should look into entity recognition and relationship extraction. But I&amp;#39;m not sure what models are good for that, how much labeled data I need to get started.&lt;/p&gt;

&lt;p&gt;I&amp;#39;d be willing to put a few thousand dollars into data annotation I think if that could get me something I could use for my own apartment search.&lt;/p&gt;

&lt;p&gt;What models should I look into? What data labeling services/tools? How should I approach this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nrv8xt,True,,robotic-rambling,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nrv8xt/advice_for_how_to_approach_classifying_apartment/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nrv8xt/advice_for_how_to_approach_classifying_apartment/,30199,1622776412.0,0,,False,,,,,,1045
1068,,LanguageTechnology,,t2_7wbpamj7,False,,0,False,What are state-of-the-art methods for abstractive text summarization ?,[],r/LanguageTechnology,False,6,,0,,False,t3_nr6xx7,False,dark,1.0,,public,13,0,{},,False,[],,False,False,,{},,False,13,,False,False,,False,,[],{},,True,,1622731569.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nr6xx7,True,,Melodic_Stomach_2704,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nr6xx7/what_are_stateoftheart_methods_for_abstractive/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nr6xx7/what_are_stateoftheart_methods_for_abstractive/,30199,1622702769.0,0,,False,,,,,,0
1069,,LanguageTechnology,"Hi, All. I am new to NLP but have a problem I'd like to solve. I host a podcast of sorts at work and I am always on the lookout for great questions. Since I also enjoy python and data science I decided to see if I could uncover anything using those tools.

Did a bit of research and found a corpus of transcribed npr interviews. That seemed like a good place to start so I wrote the code to tokenize all the sentences and find those ending in '?'. So, that's all the questions.

In 3M+ utterances, there are a bunch of questions. Many are...not good questions.

'Beth, are you out there?'

'What do you say, Bob?'

'Is that right?'

Fewer are actually good or more meaningful questions.

'Clinton and Obama are pretty similar on policy issues, so why would a Democrat switch loyalties like that?'

""The issue in the Hollywood writer's strike is, do writers get paid for work that winds up online?""

In general, these questions tend to be longer and have larger words in them but, I am curious if there are any established methods for determining the meaning or value of a question? I am not really certain even what the right word is to use (meaning, value, etc).

Anyways, I always open to learn something new. If anyone can point me in the right direction, i'd appreciate it. Thanks!

&amp;#x200B;

Code thus far for anyone interested:

`import nltk`

`from nltk.corpus import stopwords`

`from nltk import word_tokenize`

`from nltk import sent_tokenize`

`import numpy as np`

`import pandas as pd`

[`nltk.download`](https://nltk.download)`('nps_chat')`

&amp;#x200B;

`npr = pd.read_csv('C:\Users\...\Desktop\Python Scripts\Data\utterances.csv')`

`npr = npr[npr['utterance'].notna()]`

&amp;#x200B;

`tokens = npr['utterance'].apply(lambda x: sent_tokenize(x))`

`#put tokens into the DF`

`npr['utterance_tokenized'] = tokens`

`#build lists of question sentences`

`is_question = npr['utterance_tokenized'].apply(lambda x:`

`[q for q in x if '?' in q])`

`#put questions into the DF`

`npr['questions'] = is_question`

`#identify non-empty lists of questions`

`has_question = npr['questions'].apply(lambda x: True if (len(x) &gt; 0) else False)`

`#put boolean results into data frame`

`npr['has_questions'] = has_question`",t2_13s54e,False,,0,False,Assessing the “Value” of a question.,[],r/LanguageTechnology,False,6,,0,,False,t3_nqzvav,False,dark,1.0,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,1622685259.0,,[],{},,True,,1622706688.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, All. I am new to NLP but have a problem I&amp;#39;d like to solve. I host a podcast of sorts at work and I am always on the lookout for great questions. Since I also enjoy python and data science I decided to see if I could uncover anything using those tools.&lt;/p&gt;

&lt;p&gt;Did a bit of research and found a corpus of transcribed npr interviews. That seemed like a good place to start so I wrote the code to tokenize all the sentences and find those ending in &amp;#39;?&amp;#39;. So, that&amp;#39;s all the questions.&lt;/p&gt;

&lt;p&gt;In 3M+ utterances, there are a bunch of questions. Many are...not good questions.&lt;/p&gt;

&lt;p&gt;&amp;#39;Beth, are you out there?&amp;#39;&lt;/p&gt;

&lt;p&gt;&amp;#39;What do you say, Bob?&amp;#39;&lt;/p&gt;

&lt;p&gt;&amp;#39;Is that right?&amp;#39;&lt;/p&gt;

&lt;p&gt;Fewer are actually good or more meaningful questions.&lt;/p&gt;

&lt;p&gt;&amp;#39;Clinton and Obama are pretty similar on policy issues, so why would a Democrat switch loyalties like that?&amp;#39;&lt;/p&gt;

&lt;p&gt;&amp;quot;The issue in the Hollywood writer&amp;#39;s strike is, do writers get paid for work that winds up online?&amp;quot;&lt;/p&gt;

&lt;p&gt;In general, these questions tend to be longer and have larger words in them but, I am curious if there are any established methods for determining the meaning or value of a question? I am not really certain even what the right word is to use (meaning, value, etc).&lt;/p&gt;

&lt;p&gt;Anyways, I always open to learn something new. If anyone can point me in the right direction, i&amp;#39;d appreciate it. Thanks!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Code thus far for anyone interested:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;import nltk&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;from nltk.corpus import stopwords&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;from nltk import word_tokenize&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;from nltk import sent_tokenize&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;import numpy as np&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;import pandas as pd&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://nltk.download""&gt;&lt;code&gt;nltk.download&lt;/code&gt;&lt;/a&gt;&lt;code&gt;(&amp;#39;nps_chat&amp;#39;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;npr = pd.read_csv(&amp;#39;C:\Users\...\Desktop\Python Scripts\Data\utterances.csv&amp;#39;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;npr = npr[npr[&amp;#39;utterance&amp;#39;].notna()]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tokens = npr[&amp;#39;utterance&amp;#39;].apply(lambda x: sent_tokenize(x))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#put tokens into the DF&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;npr[&amp;#39;utterance_tokenized&amp;#39;] = tokens&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#build lists of question sentences&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;is_question = npr[&amp;#39;utterance_tokenized&amp;#39;].apply(lambda x:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;[q for q in x if &amp;#39;?&amp;#39; in q])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#put questions into the DF&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;npr[&amp;#39;questions&amp;#39;] = is_question&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#identify non-empty lists of questions&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;has_question = npr[&amp;#39;questions&amp;#39;].apply(lambda x: True if (len(x) &amp;gt; 0) else False)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#put boolean results into data frame&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;npr[&amp;#39;has_questions&amp;#39;] = has_question&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nqzvav,True,,mtnbiker98,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nqzvav/assessing_the_value_of_a_question/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nqzvav/assessing_the_value_of_a_question/,30199,1622677888.0,0,,False,,,,,,2235
1070,,LanguageTechnology,"I work in the field of NLP and realized I wasn't reading enough papers on what's going around in the field. What is the latest paper you read and that leaves you mindblown (or at least excited)?

It can be any fields of NLP, I'm just curious.",t2_npixkyh,False,,0,False,What have you read recently?,[],r/LanguageTechnology,False,6,,0,,False,t3_nqvvka,False,dark,0.9,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1622695503.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I work in the field of NLP and realized I wasn&amp;#39;t reading enough papers on what&amp;#39;s going around in the field. What is the latest paper you read and that leaves you mindblown (or at least excited)?&lt;/p&gt;

&lt;p&gt;It can be any fields of NLP, I&amp;#39;m just curious.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nqvvka,True,,Arthurion9,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nqvvka/what_have_you_read_recently/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nqvvka/what_have_you_read_recently/,30199,1622666703.0,0,,False,,,,,,242
1071,,LanguageTechnology,"Hi, I'm interested to know whether the few-shot learning can be done on BERT for sentence similarity.  


Update: Found the pipeline in hugging face for zero-shot learning.  
[https://huggingface.co/facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)  
But still, I'm not sure about the few-shot though.  
",t2_7yhonl6y,False,,0,False,Few Shot Learning in BERT,[],r/LanguageTechnology,False,6,,0,,False,t3_nqh6jw,False,dark,0.95,,public,16,0,{},,False,[],,False,False,,{},,False,16,,False,False,,1622641154.0,,[],{},,True,,1622652264.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m interested to know whether the few-shot learning can be done on BERT for sentence similarity.  &lt;/p&gt;

&lt;p&gt;Update: Found the pipeline in hugging face for zero-shot learning.&lt;br/&gt;
&lt;a href=""https://huggingface.co/facebook/bart-large-mnli""&gt;https://huggingface.co/facebook/bart-large-mnli&lt;/a&gt;&lt;br/&gt;
But still, I&amp;#39;m not sure about the few-shot though.  &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nqh6jw,True,,pingu_henry,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nqh6jw/few_shot_learning_in_bert/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nqh6jw/few_shot_learning_in_bert/,30199,1622623464.0,0,,False,,,,,,329
1072,,LanguageTechnology,"Hi! I am a digital humanities student and a newbie in NLP and for my dissertation I tried sentiment analysis with flair library on around 517 poems. My problem is: I used a pre-trained model and it labeled my poems as Negative/Positive, but I do not know how to evaluate how accurate this was. My data was unlabeled before applying the pre-trained model.  

If someone could please help me, that would be great. Thank you!",t2_725zn8ev,False,,0,False,Sentiment Analysis with flair on poetry - model evaluation,[],r/LanguageTechnology,False,6,,0,,False,t3_nqhf6g,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1622653296.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi! I am a digital humanities student and a newbie in NLP and for my dissertation I tried sentiment analysis with flair library on around 517 poems. My problem is: I used a pre-trained model and it labeled my poems as Negative/Positive, but I do not know how to evaluate how accurate this was. My data was unlabeled before applying the pre-trained model.  &lt;/p&gt;

&lt;p&gt;If someone could please help me, that would be great. Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nqhf6g,True,,yuraeh,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nqhf6g/sentiment_analysis_with_flair_on_poetry_model/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nqhf6g/sentiment_analysis_with_flair_on_poetry_model/,30199,1622624496.0,0,,False,,,,,,422
1073,,LanguageTechnology,"[https://towardsdatascience.com/how-to-annotate-pdfs-and-scanned-images-for-nlp-applications-f7b7b1db5c4a](https://towardsdatascience.com/how-to-annotate-pdfs-and-scanned-images-for-nlp-applications-f7b7b1db5c4a)

Disclaimer: The OCR feature mentioned in the article is only available for paid subscriptions, if you are interested by a demo send us an email at [admin@ubiai.tools](mailto:admin@ubiai.tools).",t2_32tnavmg,False,,0,False,"If you are looking to automatically extract information from PDFs or scanned images, check out this article on how to leverage OCR to create training data.",[],r/LanguageTechnology,False,6,,0,,False,t3_nq8ahy,False,dark,0.55,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,1622656672.0,,[],{},,True,,1622620738.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://towardsdatascience.com/how-to-annotate-pdfs-and-scanned-images-for-nlp-applications-f7b7b1db5c4a""&gt;https://towardsdatascience.com/how-to-annotate-pdfs-and-scanned-images-for-nlp-applications-f7b7b1db5c4a&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Disclaimer: The OCR feature mentioned in the article is only available for paid subscriptions, if you are interested by a demo send us an email at [&lt;a href=""mailto:admin@ubiai.tools""&gt;admin@ubiai.tools&lt;/a&gt;](mailto:&lt;a href=""mailto:admin@ubiai.tools""&gt;admin@ubiai.tools&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nq8ahy,True,,UBIAI,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nq8ahy/if_you_are_looking_to_automatically_extract/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nq8ahy/if_you_are_looking_to_automatically_extract/,30199,1622591938.0,0,,False,,,,,,407
1074,,LanguageTechnology,"Hey there,
I am waiting currently waiting my master's thesis and I had an idea for an analysis. 
I have a list of domain names and I was wondering whether I could automatically categorize them. I already tried the word embedding methods GloVe methods as presented by Stanford in 2013, googleNews negative 300 and Facebooks fasttext. My best results were with fasttext but they are still somewhat unreliable and when I look at the vectorization of words I find the their neighbours but I don't really get ""super-categories"" of these words. 
I would love to have something like:
f(""cars-for-sale.com"")=  ""car sales website"".
Am I overlooking some super obvious method for this? I am new to the field of NLP so please forgive me my ignorance.",t2_76x7914w,False,,0,False,Assigning a category to a word,[],r/LanguageTechnology,False,6,,0,,False,t3_nq0nl5,False,dark,0.75,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1622600285.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey there,
I am waiting currently waiting my master&amp;#39;s thesis and I had an idea for an analysis. 
I have a list of domain names and I was wondering whether I could automatically categorize them. I already tried the word embedding methods GloVe methods as presented by Stanford in 2013, googleNews negative 300 and Facebooks fasttext. My best results were with fasttext but they are still somewhat unreliable and when I look at the vectorization of words I find the their neighbours but I don&amp;#39;t really get &amp;quot;super-categories&amp;quot; of these words. 
I would love to have something like:
f(&amp;quot;cars-for-sale.com&amp;quot;)=  &amp;quot;car sales website&amp;quot;.
Am I overlooking some super obvious method for this? I am new to the field of NLP so please forgive me my ignorance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nq0nl5,True,,bwdmn,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nq0nl5/assigning_a_category_to_a_word/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nq0nl5/assigning_a_category_to_a_word/,30199,1622571485.0,0,,False,,,,,,739
1075,,LanguageTechnology," Many of the papers and tutorials I'm reading on sentiment analysis seem to treat the problem as classification problem where the goal is to classify some text as ""positive"", ""negative"", or ""neutral"". However, sentiment is discrete concept, so why would you treat is as such? Wouldn't you need to model sentiment as a float value between say -1 and 1, and then preform a regression? Is there are problem with the latter solution that the former method fixes?",t2_6ybo2,False,,0,False,What advantage is there to treating sentiment analysis as a classification problem vs a regression problem?,[],r/LanguageTechnology,False,6,,0,,False,t3_nq5zz8,False,dark,0.75,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1622614034.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Many of the papers and tutorials I&amp;#39;m reading on sentiment analysis seem to treat the problem as classification problem where the goal is to classify some text as &amp;quot;positive&amp;quot;, &amp;quot;negative&amp;quot;, or &amp;quot;neutral&amp;quot;. However, sentiment is discrete concept, so why would you treat is as such? Wouldn&amp;#39;t you need to model sentiment as a float value between say -1 and 1, and then preform a regression? Is there are problem with the latter solution that the former method fixes?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nq5zz8,True,,Revlong57,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nq5zz8/what_advantage_is_there_to_treating_sentiment/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nq5zz8/what_advantage_is_there_to_treating_sentiment/,30199,1622585234.0,0,,False,,,,,,458
1076,,LanguageTechnology,"Hi, 

I am designing a study where I will collect daily diary entries and try to predict emotions/sentiment as well as levels of mental health issues (the ground truth scores will be acquired through daily questionnaires). Since I am new to NLP, what would be a good minimum text length of these diary entries for using machine learning/NLP methods and why? I would be grateful for any relevant sources as well.

Thanks!",t2_dvgke,False,,0,False,Minimum text length in diary entries for detecting emotions/sentiment and classifying levels of mental health issues,[],r/LanguageTechnology,False,6,,0,,False,t3_npw3c1,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1622588692.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, &lt;/p&gt;

&lt;p&gt;I am designing a study where I will collect daily diary entries and try to predict emotions/sentiment as well as levels of mental health issues (the ground truth scores will be acquired through daily questionnaires). Since I am new to NLP, what would be a good minimum text length of these diary entries for using machine learning/NLP methods and why? I would be grateful for any relevant sources as well.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,npw3c1,True,,tiensss,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/npw3c1/minimum_text_length_in_diary_entries_for/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/npw3c1/minimum_text_length_in_diary_entries_for/,30199,1622559892.0,0,,False,,,,,,420
1077,,LanguageTechnology,"Autocompletion has become a handy and widely used tool in contemporary messaging and other writing tasks. It is also an essential feature of an integrated development environment (IDE) for computer programming. Recently, research has shown that autocompletion can be powered by deep learning, thus allowing software language models to achieve significant accuracy improvements by training on real-world datasets collected from programmers’ IDE activity. However, a common issue with less popular programming languages is that the available IDE datasets may be insufficient for training.

In a [paper](https://arxiv.org/pdf/2105.05991.pdf), a research team from Facebook demonstrates how transfer learning can enable pre-training on non-IDE, non-autocompletion, and different-language example code sequences before fine-tuning on the autocompletion prediction task. The proposed method improves model accuracy by more than 50 percent on small fine-tuning datasets and over 10 percent on 50k labeled examples.

Summary: [https://www.marktechpost.com/2021/05/31/facebook-demonstrates-how-the-power-of-transfer-learning-can-boost-code-autocompletion-accuracy-by-over-50/](https://www.marktechpost.com/2021/05/31/facebook-demonstrates-how-the-power-of-transfer-learning-can-boost-code-autocompletion-accuracy-by-over-50/?_ga=2.133350797.2041055336.1622397040-488125022.1618729090)

Paper: https://arxiv.org/pdf/2105.05991.pdf",t2_4wudjgid,False,,0,False,Facebook AI Demonstrates How The Power Of Transfer Learning Can Boost Code Autocompletion Accuracy By Over 50%,[],r/LanguageTechnology,False,6,,0,,False,t3_nplea7,False,dark,0.94,,public,16,0,{},,False,[],,False,False,,{},,False,16,,False,False,,False,,[],{},,True,,1622549507.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Autocompletion has become a handy and widely used tool in contemporary messaging and other writing tasks. It is also an essential feature of an integrated development environment (IDE) for computer programming. Recently, research has shown that autocompletion can be powered by deep learning, thus allowing software language models to achieve significant accuracy improvements by training on real-world datasets collected from programmers’ IDE activity. However, a common issue with less popular programming languages is that the available IDE datasets may be insufficient for training.&lt;/p&gt;

&lt;p&gt;In a &lt;a href=""https://arxiv.org/pdf/2105.05991.pdf""&gt;paper&lt;/a&gt;, a research team from Facebook demonstrates how transfer learning can enable pre-training on non-IDE, non-autocompletion, and different-language example code sequences before fine-tuning on the autocompletion prediction task. The proposed method improves model accuracy by more than 50 percent on small fine-tuning datasets and over 10 percent on 50k labeled examples.&lt;/p&gt;

&lt;p&gt;Summary: &lt;a href=""https://www.marktechpost.com/2021/05/31/facebook-demonstrates-how-the-power-of-transfer-learning-can-boost-code-autocompletion-accuracy-by-over-50/?_ga=2.133350797.2041055336.1622397040-488125022.1618729090""&gt;https://www.marktechpost.com/2021/05/31/facebook-demonstrates-how-the-power-of-transfer-learning-can-boost-code-autocompletion-accuracy-by-over-50/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/pdf/2105.05991.pdf""&gt;https://arxiv.org/pdf/2105.05991.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nplea7,True,,techsucker,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nplea7/facebook_ai_demonstrates_how_the_power_of/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nplea7/facebook_ai_demonstrates_how_the_power_of/,30199,1622520707.0,0,,False,,,,,,1420
1078,,LanguageTechnology,,t2_6ib7gcgr,False,,0,False,[D] Probing Classifiers: A Gentle Intro (Explainable AI for Deep Learning) [Video],[],r/LanguageTechnology,False,6,,0,,False,t3_nprdjx,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,False,,1622574248.0,text,6,,,text,self.MachineLearning,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nprdjx,True,,jayalammar,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nprdjx/d_probing_classifiers_a_gentle_intro_explainable/,all_ads,False,/r/MachineLearning/comments/npraw7/d_probing_classifiers_a_gentle_intro_explainable/,30199,1622545448.0,0,,False,/r/MachineLearning/comments/npraw7/d_probing_classifiers_a_gentle_intro_explainable/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': '[removed]', 'author_fullname': 't2_6ib7gcgr', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] Probing Classifiers: A Gentle Intro (Explainable AI for Deep Learning) [Video]', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_npraw7', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Rule 6 - Beginner tutorial or project', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1622574007.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': 'moderator', 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '53594546-128c-11eb-a1a3-0e29e49f7ff7', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'npraw7', 'is_robot_indexable': False, 'report_reasons': None, 'author': 'jayalammar', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/npraw7/d_probing_classifiers_a_gentle_intro_explainable/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/npraw7/d_probing_classifiers_a_gentle_intro_explainable/', 'subreddit_subscribers': 1930728, 'created_utc': 1622545207.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_npraw7,,,0
1079,,LanguageTechnology,"Hi. I'm working in a company that has recently started accepting projects on conversational and vocal interfaces but I am personaly at a point in my path where I often doubt that these kind of applications with the current level of performance are actually useful outside very specific cases (like for disabled people).

So, in the spirit of the ""are chatbots just a fad?"" I would like to know your opinions on the following subjects:

# Are the current tecnologies for conversational and vocal interfaces really useful? Are we just offering on the market something that is ""cool"" but useless? What are examples of conversational and vocal interfaces the general public is using in their lifes right now? Where might we use these technologies in the very near future that we still don't?",t2_4i3g89wl,False,,0,False,Your opinions: Where are conversational interfaces actually useful?,[],r/LanguageTechnology,False,6,,0,,False,t3_np4mqr,False,dark,1.0,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1622500018.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi. I&amp;#39;m working in a company that has recently started accepting projects on conversational and vocal interfaces but I am personaly at a point in my path where I often doubt that these kind of applications with the current level of performance are actually useful outside very specific cases (like for disabled people).&lt;/p&gt;

&lt;p&gt;So, in the spirit of the &amp;quot;are chatbots just a fad?&amp;quot; I would like to know your opinions on the following subjects:&lt;/p&gt;

&lt;h1&gt;Are the current tecnologies for conversational and vocal interfaces really useful? Are we just offering on the market something that is &amp;quot;cool&amp;quot; but useless? What are examples of conversational and vocal interfaces the general public is using in their lifes right now? Where might we use these technologies in the very near future that we still don&amp;#39;t?&lt;/h1&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,np4mqr,True,,francesco_on_the_job,,17,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/np4mqr/your_opinions_where_are_conversational_interfaces/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/np4mqr/your_opinions_where_are_conversational_interfaces/,30199,1622471218.0,0,,False,,,,,,787
1080,,LanguageTechnology,,t2_5smymm7p,False,,0,False,Into NLP - Part-of-speech tagging,[],r/LanguageTechnology,False,6,,0,,False,t3_np15gv,False,dark,0.85,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,False,,1622489061.0,text,6,,,text,qualicen.de,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,np15gv,True,,QualicenDS,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/np15gv/into_nlp_partofspeech_tagging/,all_ads,False,https://www.qualicen.de/into-nlp-5-numerous-language-parts-pos-tagging/,30199,1622460261.0,0,,False,https://www.qualicen.de/into-nlp-5-numerous-language-parts-pos-tagging/,,,,,0
1081,,LanguageTechnology,,t2_5cvo57qv,False,,0,False,[2105.13626] ByT5: Towards a token-free future with pre-trained byte-to-byte models,[],r/LanguageTechnology,False,6,,0,,False,t3_np39fr,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,False,,1622496023.0,text,6,,,text,arxiv.org,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,np39fr,True,,argosopentech,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/np39fr/210513626_byt5_towards_a_tokenfree_future_with/,all_ads,False,https://arxiv.org/abs/2105.13626,30199,1622467223.0,0,,False,https://arxiv.org/abs/2105.13626,,,,,0
1082,,LanguageTechnology,,t2_hkv9s,False,,0,False,Entity-level Factual Consistency of Abstractive Text Summarization (Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_noyvf4,False,dark,1.0,,public,8,1,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/P9wr8IBfDQs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Entity-level Factual Consistency of Abstractive Text Summarization (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/P9wr8IBfDQs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/P9wr8IBfDQs/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/P9wr8IBfDQs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/noyvf4', 'height': 200}",,False,8,,False,False,,False,,[],{},,False,,1622480294.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,noyvf4,True,,prakhar21,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/noyvf4/entitylevel_factual_consistency_of_abstractive/,all_ads,False,https://youtu.be/P9wr8IBfDQs,30199,1622451494.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Entity-level Factual Consistency of Abstractive Text Summarization (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/P9wr8IBfDQs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/P9wr8IBfDQs/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/P9wr8IBfDQs,,,,,0
1083,,LanguageTechnology,Generate code*,t2_bdd86e85,False,,0,False,Noob question - What are the better tools/programs that general code using instructions in natural language/plain English?,[],r/LanguageTechnology,False,6,,0,,False,t3_noxvs0,False,dark,0.9,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,1622551494.0,,[],{},,True,,1622476154.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Generate code*&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,noxvs0,True,,IHateSelectingNames,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/noxvs0/noob_question_what_are_the_better_toolsprograms/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/noxvs0/noob_question_what_are_the_better_toolsprograms/,30199,1622447354.0,0,,False,,,,,,14
1084,,LanguageTechnology,"Everyone who is interested in NLP or even DL and ML for that matter, has definitely heard about the BERT family of models. BERT, RoBERTa, DistilBERT and many many more. This paper ""BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"" first introduced this and it has now completely changed the way AI practitioners are solving and looking at NLP problems these days.

As a part of my Paper Notes series, I have gone through the paper and created an informative summary of the paper. This time it goes a bit longer than the previous paper summaries, but it had to be done. The paper contained many tiny interesting nuggets that I had to include. Check out the links below and happy reading!

Paper Summary -  [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/)

Annotated Paper -  [https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/BERT.pdf](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/BERT.pdf)",t2_5xzd9om,False,,0,False,BERT - Annotated Paper + Paper Summary,[],r/LanguageTechnology,False,6,,0,,False,t3_np03n3,False,dark,0.75,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1622485150.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Everyone who is interested in NLP or even DL and ML for that matter, has definitely heard about the BERT family of models. BERT, RoBERTa, DistilBERT and many many more. This paper &amp;quot;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&amp;quot; first introduced this and it has now completely changed the way AI practitioners are solving and looking at NLP problems these days.&lt;/p&gt;

&lt;p&gt;As a part of my Paper Notes series, I have gone through the paper and created an informative summary of the paper. This time it goes a bit longer than the previous paper summaries, but it had to be done. The paper contained many tiny interesting nuggets that I had to include. Check out the links below and happy reading!&lt;/p&gt;

&lt;p&gt;Paper Summary -  &lt;a href=""https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/""&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Annotated Paper -  &lt;a href=""https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/BERT.pdf""&gt;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/BERT.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,np03n3,True,,shreyansh26,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/np03n3/bert_annotated_paper_paper_summary/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/np03n3/bert_annotated_paper_paper_summary/,30199,1622456350.0,0,,False,,,,,,1079
1085,,LanguageTechnology,,t2_4wudjgid,False,,0,False,Google’s Multitask Unified Model (MUM) Transforms How Google AI Understands Complex Queries,[],r/LanguageTechnology,False,6,,0,,False,t3_np2pfo,False,dark,0.4,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,False,,1622494329.0,text,6,,,text,marktechpost.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,np2pfo,True,,techsucker,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/np2pfo/googles_multitask_unified_model_mum_transforms/,all_ads,False,https://www.marktechpost.com/2021/05/31/googles-multitask-unified-model-mum-transforms-how-google-ai-understands-complex-queries/,30199,1622465529.0,0,,False,https://www.marktechpost.com/2021/05/31/googles-multitask-unified-model-mum-transforms-how-google-ai-understands-complex-queries/,,,,,0
1086,,LanguageTechnology,"Last year, Facebook AI released [Dynabench](https://dynabench.org/?fbclid=IwAR1ScAjtZoAb_PwA0-rPlCQYxWS-9-iGJpcYKQ5eyFUvLnQVVbSZtV8j3as), a platform that radically rethinks benchmarking in AI, starting with natural language processing (NLP) models. Going forward, they have now announced a new evaluation-as-a-service platform for comprehensive, standardized evaluations of NLP models called [Dynaboard](https://dynabench.org/dynaboard.pdf?fbclid=IwAR3rTJa8jRQtaJp8FFp5wf-eyWZ2QmXHTapxHjmpqw-j1t0Sw9n1xD31ASs). Dynaboard can perform apples-to-apples comparisons dynamically without common issues from bugs in evaluation code, inconsistencies in filtering test data, backward compatibility, accessibility, and several other reproducibility issues.

Dynaboard enables AI researchers to customize a new Dynascore metric based on multiple axes of evaluation, including compute, accuracy, robustness, memory, and fairness.

Full Summary: [https://www.marktechpost.com/2021/05/30/facebook-ai-releases-dynaboard-a-new-evaluation-platform-for-nlp-models/](https://www.marktechpost.com/2021/05/30/facebook-ai-releases-dynaboard-a-new-evaluation-platform-for-nlp-models/?_ga=2.190349480.2041055336.1622397040-488125022.1618729090)

Github: https://github.com/facebookresearch/dynalab

Paper: https://dynabench.org/dynaboard.pdf",t2_4wudjgid,False,,0,False,Facebook AI releases Dynaboard: A New Evaluation platform for NLP Models,[],r/LanguageTechnology,False,6,,0,,False,t3_nogmtg,False,dark,0.9,,public,28,0,{},,False,[],,False,False,,{},,False,28,,False,False,,False,,[],{},,True,,1622426400.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Last year, Facebook AI released &lt;a href=""https://dynabench.org/?fbclid=IwAR1ScAjtZoAb_PwA0-rPlCQYxWS-9-iGJpcYKQ5eyFUvLnQVVbSZtV8j3as""&gt;Dynabench&lt;/a&gt;, a platform that radically rethinks benchmarking in AI, starting with natural language processing (NLP) models. Going forward, they have now announced a new evaluation-as-a-service platform for comprehensive, standardized evaluations of NLP models called &lt;a href=""https://dynabench.org/dynaboard.pdf?fbclid=IwAR3rTJa8jRQtaJp8FFp5wf-eyWZ2QmXHTapxHjmpqw-j1t0Sw9n1xD31ASs""&gt;Dynaboard&lt;/a&gt;. Dynaboard can perform apples-to-apples comparisons dynamically without common issues from bugs in evaluation code, inconsistencies in filtering test data, backward compatibility, accessibility, and several other reproducibility issues.&lt;/p&gt;

&lt;p&gt;Dynaboard enables AI researchers to customize a new Dynascore metric based on multiple axes of evaluation, including compute, accuracy, robustness, memory, and fairness.&lt;/p&gt;

&lt;p&gt;Full Summary: &lt;a href=""https://www.marktechpost.com/2021/05/30/facebook-ai-releases-dynaboard-a-new-evaluation-platform-for-nlp-models/?_ga=2.190349480.2041055336.1622397040-488125022.1618729090""&gt;https://www.marktechpost.com/2021/05/30/facebook-ai-releases-dynaboard-a-new-evaluation-platform-for-nlp-models/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Github: &lt;a href=""https://github.com/facebookresearch/dynalab""&gt;https://github.com/facebookresearch/dynalab&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://dynabench.org/dynaboard.pdf""&gt;https://dynabench.org/dynaboard.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nogmtg,True,,techsucker,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nogmtg/facebook_ai_releases_dynaboard_a_new_evaluation/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nogmtg/facebook_ai_releases_dynaboard_a_new_evaluation/,30199,1622397600.0,0,,False,,,,,,1317
1087,,LanguageTechnology,"Hi reddit, NLP newbie here

I am trying to understand if there is any value in creating a table out of free text, versus predictive analysis on the free text itself. 

For context, I am working with 2 million clinical notes from the MIMIC-III dataset, and I would like to tabluate all this unstructured data. Would this yield much value, considering I could design a bespoke predicitve model directly on to the free text? Would there be much difference in results from the free text compared to its structured counterpart?",t2_1477nx,False,,0,False,"Structuring free text, then performing analysis vs. Performing analysis on unstructured free text",[],r/LanguageTechnology,False,6,,0,,False,t3_not78h,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1622462985.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi reddit, NLP newbie here&lt;/p&gt;

&lt;p&gt;I am trying to understand if there is any value in creating a table out of free text, versus predictive analysis on the free text itself. &lt;/p&gt;

&lt;p&gt;For context, I am working with 2 million clinical notes from the MIMIC-III dataset, and I would like to tabluate all this unstructured data. Would this yield much value, considering I could design a bespoke predicitve model directly on to the free text? Would there be much difference in results from the free text compared to its structured counterpart?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,not78h,True,,dannyhoes,,10,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/not78h/structuring_free_text_then_performing_analysis_vs/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/not78h/structuring_free_text_then_performing_analysis_vs/,30199,1622434185.0,0,,False,,,,,,522
1088,,LanguageTechnology,,t2_5cvo57qv,False,,0,False,Python bindings for LibreTranslate,[],r/LanguageTechnology,False,6,,0,,False,t3_nooqsl,False,dark,0.76,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1622451137.0,text,6,,,text,github.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nooqsl,True,,argosopentech,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nooqsl/python_bindings_for_libretranslate/,all_ads,False,https://github.com/argosopentech/LibreTranslate-py,30199,1622422337.0,0,,False,https://github.com/argosopentech/LibreTranslate-py,,,,,0
1089,,LanguageTechnology,"So basically what i'm attempting is to benchmark Cosine and WMD for a very generic use-case of textual similarity (This is probably quite the noob question).

My dataset is a parallel corpus made of pairs of paraphrased sentences. My issue is mainly in how I can compare the two metrics since they are of different order of magnitude:

* Cosine is \[-1, 1\] (or \[1, 0\]).
* WMD is not bound by an upper limit and goes from 0 to \~3.83 for my small dataset, and I can't think of a way to normalise it without introducing a bias.

My use case does not involve some form of classification, in which case I could just compare normally by how good the classification is. As for setting a threshold, (for example if WMD(S1, S2) &lt; 0.5 then consider the pair to be similar) and I really don't know how I can set a threshold based on this benchmarking alone because all pairs are actually paraphrases, so the expected output is always a distance of 0, while my results are quite sparse.

What would you guys suggest (a metric to compare with or a way to determine a threshold for the two of them, or anything else) to pick which of the 2 is performing better?",t2_4ba7h526,False,,0,False,Comparing between Cosine Similarity and Word Mover's Distance,[],r/LanguageTechnology,False,6,,0,,False,t3_noobre,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1622449605.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So basically what i&amp;#39;m attempting is to benchmark Cosine and WMD for a very generic use-case of textual similarity (This is probably quite the noob question).&lt;/p&gt;

&lt;p&gt;My dataset is a parallel corpus made of pairs of paraphrased sentences. My issue is mainly in how I can compare the two metrics since they are of different order of magnitude:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Cosine is [-1, 1] (or [1, 0]).&lt;/li&gt;
&lt;li&gt;WMD is not bound by an upper limit and goes from 0 to ~3.83 for my small dataset, and I can&amp;#39;t think of a way to normalise it without introducing a bias.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My use case does not involve some form of classification, in which case I could just compare normally by how good the classification is. As for setting a threshold, (for example if WMD(S1, S2) &amp;lt; 0.5 then consider the pair to be similar) and I really don&amp;#39;t know how I can set a threshold based on this benchmarking alone because all pairs are actually paraphrases, so the expected output is always a distance of 0, while my results are quite sparse.&lt;/p&gt;

&lt;p&gt;What would you guys suggest (a metric to compare with or a way to determine a threshold for the two of them, or anything else) to pick which of the 2 is performing better?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,noobre,True,,Meaveready,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/noobre/comparing_between_cosine_similarity_and_word/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/noobre/comparing_between_cosine_similarity_and_word/,30199,1622420805.0,0,,False,,,,,,1154
1090,,LanguageTechnology,"Hi everyone, 

I'm a language learning/teaching researcher who has been trying to develop little tools to help other researchers recently. As you might guess, low-proficiency learner texts are quite a pain in the neck in this context due to frequent grammatical errors. 

So I'm actually looking for suggestions regarding a particular POS tagging problem as seen below:

""I think very good idea."" 

There, the learner obviously means ""I think -it is a- very good idea"" but since the function words are not there, POS taggers cannot accurately identify the parts of speech in the sentence. 

How could one deal with such sentences from a matching/POS tagging perspective? 

I have been considering ""next word generation"" but so far I haven't tested it properly. So any suggestion is welcome. 

Thanks in advance big time.",t2_qrcj0xf,False,,0,False,"Spacy Matcher, POS Tagging and Grammatical Errors",[],r/LanguageTechnology,False,6,,0,,False,t3_no7wrt,False,dark,0.94,,public,13,0,{},,False,[],,False,False,,{},,False,13,,False,False,,False,,[],{},,True,,1622397292.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;

&lt;p&gt;I&amp;#39;m a language learning/teaching researcher who has been trying to develop little tools to help other researchers recently. As you might guess, low-proficiency learner texts are quite a pain in the neck in this context due to frequent grammatical errors. &lt;/p&gt;

&lt;p&gt;So I&amp;#39;m actually looking for suggestions regarding a particular POS tagging problem as seen below:&lt;/p&gt;

&lt;p&gt;&amp;quot;I think very good idea.&amp;quot; &lt;/p&gt;

&lt;p&gt;There, the learner obviously means &amp;quot;I think -it is a- very good idea&amp;quot; but since the function words are not there, POS taggers cannot accurately identify the parts of speech in the sentence. &lt;/p&gt;

&lt;p&gt;How could one deal with such sentences from a matching/POS tagging perspective? &lt;/p&gt;

&lt;p&gt;I have been considering &amp;quot;next word generation&amp;quot; but so far I haven&amp;#39;t tested it properly. So any suggestion is welcome. &lt;/p&gt;

&lt;p&gt;Thanks in advance big time.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,no7wrt,True,,applingu,,10,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/no7wrt/spacy_matcher_pos_tagging_and_grammatical_errors/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/no7wrt/spacy_matcher_pos_tagging_and_grammatical_errors/,30199,1622368492.0,0,,False,,,,,,820
1091,,LanguageTechnology,"Whenever i try to run :
model = MBartForConditionalGeneration.from_pretrained("" [local path]/mbart-large-50-one-to-many-mmt"")
My computer ether freezes or it takes 15-20 minutes to load the model.

I am using it for translation
Code: https://huggingface.co/facebook/mbart-large-50-one-to-many-mmt

Any solution fo this?

-Thanks",t2_7wron7g8,False,,0,False,Loading mbart-large-50-one-to-many-mmt is very slow,[],r/LanguageTechnology,False,6,,0,,False,t3_no6idv,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1622391064.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Whenever i try to run :
model = MBartForConditionalGeneration.from_pretrained(&amp;quot; [local path]/mbart-large-50-one-to-many-mmt&amp;quot;)
My computer ether freezes or it takes 15-20 minutes to load the model.&lt;/p&gt;

&lt;p&gt;I am using it for translation
Code: &lt;a href=""https://huggingface.co/facebook/mbart-large-50-one-to-many-mmt""&gt;https://huggingface.co/facebook/mbart-large-50-one-to-many-mmt&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Any solution fo this?&lt;/p&gt;

&lt;p&gt;-Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,no6idv,True,,arkhamrising,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/no6idv/loading_mbartlarge50onetomanymmt_is_very_slow/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/no6idv/loading_mbartlarge50onetomanymmt_is_very_slow/,30199,1622362264.0,0,,False,,,,,,328
1092,,LanguageTechnology,,t2_9pdfk3qa,False,,0,False,The competition that involves linguistics and logic,[],r/LanguageTechnology,False,6,,0,,False,t3_nniznw,False,dark,0.95,,public,15,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/9tyG71ogKlE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'International Linguistics Olympiad --- Basque Numbers', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/9tyG71ogKlE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'LogicaLing', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/9tyG71ogKlE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCw97O4KNq__XiyyS7IWqARA'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/9tyG71ogKlE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nniznw', 'height': 200}",,False,15,,False,False,,False,,[],{},,False,,1622305872.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nniznw,True,,Proof_Ad2445,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nniznw/the_competition_that_involves_linguistics_and/,all_ads,False,https://youtu.be/9tyG71ogKlE,30199,1622277072.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'International Linguistics Olympiad --- Basque Numbers', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/9tyG71ogKlE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'LogicaLing', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/9tyG71ogKlE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCw97O4KNq__XiyyS7IWqARA'}}",False,https://youtu.be/9tyG71ogKlE,,,,,0
1093,,LanguageTechnology,,t2_1sev,False,,0,False,How to dramatically improve the reasoning ability of GPT-3,[],r/LanguageTechnology,False,6,,0,,False,t3_nn75r9,False,dark,1.0,,public,18,0,{},,False,[],,False,False,,{},,False,18,,False,False,,False,,[],{},,False,,1622263427.0,text,6,,,text,blog.andrewcantino.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nn75r9,True,,tectonic,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nn75r9/how_to_dramatically_improve_the_reasoning_ability/,all_ads,False,https://blog.andrewcantino.com/blog/2021/05/28/how-to-dramatically-improve-the-reasoning-ability-of-GPT-3/,30199,1622234627.0,0,,False,https://blog.andrewcantino.com/blog/2021/05/28/how-to-dramatically-improve-the-reasoning-ability-of-GPT-3/,,,,,0
1094,,LanguageTechnology,"I am looking for a web based tool for an NLP annotation task (similar to docanno for example). 

The catch is that I have to create a project and make it easily accessed with a link without the need to create an account per each annotator.

Any tool that comes close to this criteria?",t2_60btb74b,False,,0,False,Any free open-access NLP annotation tools?,[],r/LanguageTechnology,False,6,,0,,False,t3_nn0lyf,False,dark,1.0,,public,19,0,{},,False,[],,False,False,,{},,False,19,,False,False,,False,,[],{},,True,,1622245310.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am looking for a web based tool for an NLP annotation task (similar to docanno for example). &lt;/p&gt;

&lt;p&gt;The catch is that I have to create a project and make it easily accessed with a link without the need to create an account per each annotator.&lt;/p&gt;

&lt;p&gt;Any tool that comes close to this criteria?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nn0lyf,True,,shoowmewhatyougoot,,11,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nn0lyf/any_free_openaccess_nlp_annotation_tools/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nn0lyf/any_free_openaccess_nlp_annotation_tools/,30199,1622216510.0,0,,False,,,,,,284
1095,,LanguageTechnology,"Aside from self-hosting, where can I try non-fine-tuned T5 11B? 

If that’s not possible, are there any publicly available examples of its output?",t2_8xraz5pp,False,,0,False,Trying T5 11B,[],r/LanguageTechnology,False,6,,0,,False,t3_nnggrj,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1622295130.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Aside from self-hosting, where can I try non-fine-tuned T5 11B? &lt;/p&gt;

&lt;p&gt;If that’s not possible, are there any publicly available examples of its output?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nnggrj,True,,Effective_Sea_9367,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nnggrj/trying_t5_11b/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nnggrj/trying_t5_11b/,30199,1622266330.0,0,,False,,,,,,146
1096,,LanguageTechnology,"Just as the title says, I'm looking for a way to benchmark the sentiment analysis component of two or more NLP tools like retextjs and Stanford Core NLP. Are there any tools or techniques out for doing this?",t2_5xfvbvw9,False,,0,False,Benchmarking An NLP Tool,[],r/LanguageTechnology,False,6,,0,,False,t3_nnas13,False,dark,0.75,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1622274228.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Just as the title says, I&amp;#39;m looking for a way to benchmark the sentiment analysis component of two or more NLP tools like retextjs and Stanford Core NLP. Are there any tools or techniques out for doing this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nnas13,True,,ProperWait1,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nnas13/benchmarking_an_nlp_tool/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nnas13/benchmarking_an_nlp_tool/,30199,1622245428.0,0,,False,,,,,,207
1097,,LanguageTechnology,Which are some coveted NLP Certifications available in the market having good industry recognition?,t2_b3gv09ny,False,,0,False,NLP Certifications,[],r/LanguageTechnology,False,6,,0,,False,t3_nmxhny,False,dark,0.8,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1622236167.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Which are some coveted NLP Certifications available in the market having good industry recognition?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nmxhny,True,,Key-Feature-1228,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nmxhny/nlp_certifications/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nmxhny/nlp_certifications/,30199,1622207367.0,0,,False,,,,,,99
1098,,LanguageTechnology,"I've wondered how good neural nets can get at predicting semantic similarity at a more abstract level between different topics. For example, finding context between math terminology like 'vector' and 'matrix' and programming terminology like 'list/array' and '2d array'. Also, finding context between common data types, functions, concepts between different languages and frameworks/libraries. Ideally, I'd want a binary output that takes two strings as input to compare.

Question is what would be the approach to a problem like this, would it need carefully labelled data ('translating' the terminology between the two topics) or is a self-supervised method at all possible? I've only recently got into data so I could be way off here on what is possible and what is not.",t2_4av6pmbp,False,,0,False,Semantic similarity between programming languages and math terminology,[],r/LanguageTechnology,False,6,,0,,False,t3_nmuy6b,False,dark,0.71,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1622227070.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve wondered how good neural nets can get at predicting semantic similarity at a more abstract level between different topics. For example, finding context between math terminology like &amp;#39;vector&amp;#39; and &amp;#39;matrix&amp;#39; and programming terminology like &amp;#39;list/array&amp;#39; and &amp;#39;2d array&amp;#39;. Also, finding context between common data types, functions, concepts between different languages and frameworks/libraries. Ideally, I&amp;#39;d want a binary output that takes two strings as input to compare.&lt;/p&gt;

&lt;p&gt;Question is what would be the approach to a problem like this, would it need carefully labelled data (&amp;#39;translating&amp;#39; the terminology between the two topics) or is a self-supervised method at all possible? I&amp;#39;ve only recently got into data so I could be way off here on what is possible and what is not.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nmuy6b,True,,01jonathanf,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nmuy6b/semantic_similarity_between_programming_languages/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nmuy6b/semantic_similarity_between_programming_languages/,30199,1622198270.0,0,,False,,,,,,773
1099,,LanguageTechnology,"Hi all, at the moment im working on a TTS system and i want to evaluate it. Unfortunately i can not do the usual Mean opinion score, so i'm looking at more numeric evaluations to compare my generated speech to gt.

While looking for some i found the fastspeech2 paper where they use Dynamic Time Warping (DTW) to evaluate the pitch in the generated speech. Here they explain it as: ""average DTW distances (DTW) of pitch in ground-truth and synthesized audio"".

Now, when i try to calculate the DTW with [this](https://dynamictimewarping.github.io/python/) python package i seem to get strange results. In the fast speech 2 paper their DTW distances are between 24-26. Mine are all between 9000-10000 when i try to do this.

the way i try to do this is as followed(in a short pseudo code):

    for all gt and prediction:
        distance = DTW(gt,pred)
        total += distance
    result = total/number_of_samples

I've tried several distances from the [scipy page](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html) but none of them seem to get close to the range of 24-26 (when i use the seuclidean distance i get a total of 96, but this still seems too high)

the only way i can come close to the 24-26 range is by also averaging the dtw (with normal euclidean distance) by the time series length as followed:

    for all gt and prediction:
        distance = DTW(gt,pred)/length_of_prediction
        total += distance
    result = total/number_of_samples

can someone tell me if this is correct or how i should calculate the average DTW distance instead? Maybe any links (i tried to read the refrence from the paper to Meinard Muller's chapter on this, but i couldnt figure out which distance to use from there).

Any other metrics to evaluate speech is also always welcome :)

Thank you and kind regards!

(i'm tagging u/rayeren since he posted the FS1/FS2 papers on r/MachineLearning and i think he's one of the authors. if so, can you comment on this please? :) )",t2_4k7iew3x,False,,0,False,"FastSpeech2 DTW computing, and other TTS evaluation methods",[],r/LanguageTechnology,False,6,,0,,False,t3_nn272c,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,1622221538.0,,[],{},,True,,1622249691.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, at the moment im working on a TTS system and i want to evaluate it. Unfortunately i can not do the usual Mean opinion score, so i&amp;#39;m looking at more numeric evaluations to compare my generated speech to gt.&lt;/p&gt;

&lt;p&gt;While looking for some i found the fastspeech2 paper where they use Dynamic Time Warping (DTW) to evaluate the pitch in the generated speech. Here they explain it as: &amp;quot;average DTW distances (DTW) of pitch in ground-truth and synthesized audio&amp;quot;.&lt;/p&gt;

&lt;p&gt;Now, when i try to calculate the DTW with &lt;a href=""https://dynamictimewarping.github.io/python/""&gt;this&lt;/a&gt; python package i seem to get strange results. In the fast speech 2 paper their DTW distances are between 24-26. Mine are all between 9000-10000 when i try to do this.&lt;/p&gt;

&lt;p&gt;the way i try to do this is as followed(in a short pseudo code):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for all gt and prediction:
    distance = DTW(gt,pred)
    total += distance
result = total/number_of_samples
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&amp;#39;ve tried several distances from the &lt;a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html""&gt;scipy page&lt;/a&gt; but none of them seem to get close to the range of 24-26 (when i use the seuclidean distance i get a total of 96, but this still seems too high)&lt;/p&gt;

&lt;p&gt;the only way i can come close to the 24-26 range is by also averaging the dtw (with normal euclidean distance) by the time series length as followed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for all gt and prediction:
    distance = DTW(gt,pred)/length_of_prediction
    total += distance
result = total/number_of_samples
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;can someone tell me if this is correct or how i should calculate the average DTW distance instead? Maybe any links (i tried to read the refrence from the paper to Meinard Muller&amp;#39;s chapter on this, but i couldnt figure out which distance to use from there).&lt;/p&gt;

&lt;p&gt;Any other metrics to evaluate speech is also always welcome :)&lt;/p&gt;

&lt;p&gt;Thank you and kind regards!&lt;/p&gt;

&lt;p&gt;(i&amp;#39;m tagging &lt;a href=""/u/rayeren""&gt;u/rayeren&lt;/a&gt; since he posted the FS1/FS2 papers on &lt;a href=""/r/MachineLearning""&gt;r/MachineLearning&lt;/a&gt; and i think he&amp;#39;s one of the authors. if so, can you comment on this please? :) )&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nn272c,True,,PlatoTheSloth,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nn272c/fastspeech2_dtw_computing_and_other_tts/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nn272c/fastspeech2_dtw_computing_and_other_tts/,30199,1622220891.0,3,,False,,,,,,2011
1100,,LanguageTechnology,,t2_50ul4xke,False,,0,False,[P] Do Context-Aware Translation Models Pay the Right Attention?,[],r/LanguageTechnology,False,6,,0,,False,t3_nmuhni,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1622225214.0,text,6,,,text,self.DeepLearningPapers,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nmuhni,True,,DL_updates,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nmuhni/p_do_contextaware_translation_models_pay_the/,all_ads,False,/r/DeepLearningPapers/comments/nmhyfr/p_do_contextaware_translation_models_pay_the/,30199,1622196414.0,0,,False,/r/DeepLearningPapers/comments/nmhyfr/p_do_contextaware_translation_models_pay_the/,"[{'approved_at_utc': None, 'subreddit': 'DeepLearningPapers', 'selftext': '🔗 Paper: [https://arxiv.org/abs/2105.06977v2](https://arxiv.org/abs/2105.06977v2)\n\n👫 Authors: Kayo Yin, Patrick Fernandes, Danish Pruthi, Aditi Chaudhary, André F. T. Martins, Graham Neubig\n\nIt is interesting to see how the attention mechanism has been hyper-investigated in ACL 2021. This study on human-machine behavior seems interesting. What do you think?\n\n60sec highlights: [https://www.youtube.com/watch?v=9e3thC4U\\_sU](https://www.youtube.com/watch?v=9e3thC4U_sU)\n\nJoin Telegram Channel: [https://t.me/deeplearning\\_updates](https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbThsNFpCczRnX0Q4bHNna1ZEZUo0Y3JMUXp1QXxBQ3Jtc0trem9rSWJLYWxRRmFTamtDSHlCVm5FMG5RT2FYYkl3LW9BV1VDMW51RHBSSkdaNnloOEVXbnJhVW5ndTJ6RmQ3Rnd5WFdqUVVHNU9UNFZJcThJekZ4M3ZIUkw0ZEVZMmJOMEhSTzRwNkdyLWhGSmVsYw&amp;q=https%3A%2F%2Ft.me%2Fdeeplearning_updates)', 'author_fullname': 't2_50ul4xke', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[P] Do Context-Aware Translation Models Pay the Right Attention?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/DeepLearningPapers', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_nmhyfr', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1622179263.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.DeepLearningPapers', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;🔗 Paper: &lt;a href=""https://arxiv.org/abs/2105.06977v2""&gt;https://arxiv.org/abs/2105.06977v2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;👫 Authors: Kayo Yin, Patrick Fernandes, Danish Pruthi, Aditi Chaudhary, André F. T. Martins, Graham Neubig&lt;/p&gt;\n\n&lt;p&gt;It is interesting to see how the attention mechanism has been hyper-investigated in ACL 2021. This study on human-machine behavior seems interesting. What do you think?&lt;/p&gt;\n\n&lt;p&gt;60sec highlights: &lt;a href=""https://www.youtube.com/watch?v=9e3thC4U_sU""&gt;https://www.youtube.com/watch?v=9e3thC4U_sU&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Join Telegram Channel: &lt;a href=""https://www.youtube.com/redirect?event=video_description&amp;amp;redir_token=QUFFLUhqbThsNFpCczRnX0Q4bHNna1ZEZUo0Y3JMUXp1QXxBQ3Jtc0trem9rSWJLYWxRRmFTamtDSHlCVm5FMG5RT2FYYkl3LW9BV1VDMW51RHBSSkdaNnloOEVXbnJhVW5ndTJ6RmQ3Rnd5WFdqUVVHNU9UNFZJcThJekZ4M3ZIUkw0ZEVZMmJOMEhSTzRwNkdyLWhGSmVsYw&amp;amp;q=https%3A%2F%2Ft.me%2Fdeeplearning_updates""&gt;https://t.me/deeplearning_updates&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_38ri8', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'nmhyfr', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'DL_updates', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/DeepLearningPapers/comments/nmhyfr/p_do_contextaware_translation_models_pay_the/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/DeepLearningPapers/comments/nmhyfr/p_do_contextaware_translation_models_pay_the/', 'subreddit_subscribers': 15753, 'created_utc': 1622150463.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_nmhyfr,,,0
1101,,LanguageTechnology,"I using the BERT transformer to solve the classification problem.

I am confused with the size of the learning rate of the BERT

The author suggests of using one of the following parameters

    learning rates: 3e-4, 1e-4, 5e-5, 3e-5
    
     

I know that a small learning rate makes our model learn very slow, however it also helps prevent overfitting, in contrast to big learning which learns faster but it can lead to overfitting.

&amp;#x200B;

When  I use a learning rate of 1e-5 I get the following result

    {""train"": {""eval_examples_count"": 8548, ""metrics"": {""f1_weighted"": 0.721, ""f1_macro"": 0.7201, ""accuracy"": 0.7255, ""roc_auc"": 0.8883}, ""time_spent"": ""0:02:06""}}
    {""valid"": {""eval_examples_count"": 2849, ""metrics"": {""f1_weighted"": 0.6766, ""f1_macro"": 0.6784, ""accuracy"": 0.6816, ""roc_auc"": 0.8545}, ""time_spent"": ""0:00:42""}}
    {""test"": {""eval_examples_count"": 2850, ""metrics"": {""f1_weighted"": 0.7003, ""f1_macro"": 0.7008, ""accuracy"": 0.7046, ""roc_auc"": 0.8685}, ""time_spent"": ""0:00:42""}}

However when I use 5e-5:

    {""train"": {""eval_examples_count"": 8548, ""metrics"": {""f1_weighted"": 0.1617, ""f1_macro"": 0.1647, ""accuracy"": 0.3269, ""roc_auc"": 0.5159}, ""time_spent"": ""0:02:07""}}
    {""valid"": {""eval_examples_count"": 2849, ""metrics"": {""f1_weighted"": 0.1743, ""f1_macro"": 0.1704, ""accuracy"": 0.3412, ""roc_auc"": 0.5321}, ""time_spent"": ""0:00:42""}}
    {""test"": {""eval_examples_count"": 2850, ""metrics"": {""f1_weighted"": 0.1758, ""f1_macro"": 0.1705, ""accuracy"": 0.3435, ""roc_auc"": 0.5208}, ""time_spent"": ""0:00:42""}}

IT DOES NOT LEARN ANYTHING

&amp;#x200B;

So I thought 1e-5 is small and 5e-5 is the big learning rate, am I right? 

What is the ascending order of these learning rates?

Which one considered as big and which is considered as small?",t2_6c0lef9b,False,,0,False,What is considered as a small learning rate?,[],r/LanguageTechnology,False,6,,0,,False,t3_nmljji,False,dark,1.0,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1622190286.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I using the BERT transformer to solve the classification problem.&lt;/p&gt;

&lt;p&gt;I am confused with the size of the learning rate of the BERT&lt;/p&gt;

&lt;p&gt;The author suggests of using one of the following parameters&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;learning rates: 3e-4, 1e-4, 5e-5, 3e-5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I know that a small learning rate makes our model learn very slow, however it also helps prevent overfitting, in contrast to big learning which learns faster but it can lead to overfitting.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;When  I use a learning rate of 1e-5 I get the following result&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;train&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 8548, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.721, &amp;quot;f1_macro&amp;quot;: 0.7201, &amp;quot;accuracy&amp;quot;: 0.7255, &amp;quot;roc_auc&amp;quot;: 0.8883}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:02:06&amp;quot;}}
{&amp;quot;valid&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 2849, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.6766, &amp;quot;f1_macro&amp;quot;: 0.6784, &amp;quot;accuracy&amp;quot;: 0.6816, &amp;quot;roc_auc&amp;quot;: 0.8545}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:00:42&amp;quot;}}
{&amp;quot;test&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 2850, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.7003, &amp;quot;f1_macro&amp;quot;: 0.7008, &amp;quot;accuracy&amp;quot;: 0.7046, &amp;quot;roc_auc&amp;quot;: 0.8685}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:00:42&amp;quot;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However when I use 5e-5:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;train&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 8548, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.1617, &amp;quot;f1_macro&amp;quot;: 0.1647, &amp;quot;accuracy&amp;quot;: 0.3269, &amp;quot;roc_auc&amp;quot;: 0.5159}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:02:07&amp;quot;}}
{&amp;quot;valid&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 2849, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.1743, &amp;quot;f1_macro&amp;quot;: 0.1704, &amp;quot;accuracy&amp;quot;: 0.3412, &amp;quot;roc_auc&amp;quot;: 0.5321}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:00:42&amp;quot;}}
{&amp;quot;test&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 2850, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.1758, &amp;quot;f1_macro&amp;quot;: 0.1705, &amp;quot;accuracy&amp;quot;: 0.3435, &amp;quot;roc_auc&amp;quot;: 0.5208}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:00:42&amp;quot;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;IT DOES NOT LEARN ANYTHING&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So I thought 1e-5 is small and 5e-5 is the big learning rate, am I right? &lt;/p&gt;

&lt;p&gt;What is the ascending order of these learning rates?&lt;/p&gt;

&lt;p&gt;Which one considered as big and which is considered as small?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nmljji,True,,strangeguy111,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nmljji/what_is_considered_as_a_small_learning_rate/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nmljji/what_is_considered_as_a_small_learning_rate/,30199,1622161486.0,0,,False,,,,,,1763
1102,,LanguageTechnology,"Although BERT became really popular after its release, it did have some limitations. And there were certain limitations associated with autoregressive methods like ELMo and GPT as well. XLNet was introduced to get the best of both worlds while at the same time not include their weaknesses.

In continuation of my Paper Notes series, I have written an informative summary of the paper. Personally, reading the XLNet paper was a very fun experience. I was amazed at every step, how they were including stuff to make the whole model work so well. The paper contained many interesting concepts that I had to give time to understand. So don't worry if you don't get it on the first go. Check out the links below and happy reading!

Paper Summary - [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/)

Annotated Paper - [https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/XLNet.pdf](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/XLNet.pdf)",t2_5xzd9om,False,,0,False,XLNet - Annotated Paper + Paper Summary,[],r/LanguageTechnology,False,6,,0,,False,t3_nmr2qi,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1622210163.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Although BERT became really popular after its release, it did have some limitations. And there were certain limitations associated with autoregressive methods like ELMo and GPT as well. XLNet was introduced to get the best of both worlds while at the same time not include their weaknesses.&lt;/p&gt;

&lt;p&gt;In continuation of my Paper Notes series, I have written an informative summary of the paper. Personally, reading the XLNet paper was a very fun experience. I was amazed at every step, how they were including stuff to make the whole model work so well. The paper contained many interesting concepts that I had to give time to understand. So don&amp;#39;t worry if you don&amp;#39;t get it on the first go. Check out the links below and happy reading!&lt;/p&gt;

&lt;p&gt;Paper Summary - &lt;a href=""https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/""&gt;XLNet: Generalized Autoregressive Pretraining for Language Understanding&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Annotated Paper - &lt;a href=""https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/XLNet.pdf""&gt;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/XLNet.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nmr2qi,True,,shreyansh26,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nmr2qi/xlnet_annotated_paper_paper_summary/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nmr2qi/xlnet_annotated_paper_paper_summary/,30199,1622181363.0,0,,False,,,,,,1075
1103,,LanguageTechnology,"I need a good way to generate paraphrases from given input sentences. The input sentences already have the same meaning/are paraphrases. But I want more paraphrases (for example with different syntactic structures, with synonyms etc.). Ideally, I want to combine lexical and syntactic paraphrasing. What could be the state of the art solution for this problem? I read in some paper about phrase based translation methods but I don't think this suits my task. I am open for paper suggestions!",t2_6fr49wdr,False,,0,False,Paraphrase Generation given Input Sentences,[],r/LanguageTechnology,False,6,,0,,False,t3_nmcowu,False,dark,1.0,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1622165383.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I need a good way to generate paraphrases from given input sentences. The input sentences already have the same meaning/are paraphrases. But I want more paraphrases (for example with different syntactic structures, with synonyms etc.). Ideally, I want to combine lexical and syntactic paraphrasing. What could be the state of the art solution for this problem? I read in some paper about phrase based translation methods but I don&amp;#39;t think this suits my task. I am open for paper suggestions!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nmcowu,True,,LargeBrick7,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nmcowu/paraphrase_generation_given_input_sentences/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nmcowu/paraphrase_generation_given_input_sentences/,30199,1622136583.0,0,,False,,,,,,491
1104,,LanguageTechnology,,t2_21qaqh1p,False,,0,False,English Paraphrasing in Python with Parrot,[],r/LanguageTechnology,False,6,,0,,False,t3_nmizd0,False,dark,0.67,,public,2,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/FFjpQiVFPZo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'English Paraphrasing in Python for NLU Text Augmentation - Chatbots Training | Applied NLP in Python', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/FFjpQiVFPZo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/FFjpQiVFPZo/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/FFjpQiVFPZo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nmizd0', 'height': 200}",,False,2,,False,False,,False,,[],{},,False,,1622182123.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nmizd0,True,,dulldata,,0,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/nmizd0/english_paraphrasing_in_python_with_parrot/,all_ads,False,https://youtu.be/FFjpQiVFPZo,30199,1622153323.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'English Paraphrasing in Python for NLU Text Augmentation - Chatbots Training | Applied NLP in Python', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/FFjpQiVFPZo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/FFjpQiVFPZo/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,https://youtu.be/FFjpQiVFPZo,,,,,0
1105,,LanguageTechnology,,t2_hkv9s,False,,0,False,Summarizing NLP Research Papers,[],r/LanguageTechnology,False,6,,0,,False,t3_nmacyt,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,False,,1622159175.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nmacyt,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nmacyt/summarizing_nlp_research_papers/,all_ads,False,https://link.medium.com/82UC06vuBgb,30199,1622130375.0,0,,False,https://link.medium.com/82UC06vuBgb,,,,,0
1106,,LanguageTechnology,"The GPT-2 model was a major breakthrough in the path of creating a general multitask NLP system that was totally unsupervised. It demonstrated that given a large training corpus and a large model size, the language model was capable of learning the knowledge required for solving these tasks. It was not perfect, however, and performed poorly on some tasks as well.

I went through the paper and have written an informative summary of the paper. The paper was quite easy to follow and the experimentation section had interesting observations. Check out the links below and happy reading!

Paper Summary -  [Language Models are Unsupervised Multitask Learners](https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/)

Annotated Paper -  [https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT2.pdf](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT2.pdf)",t2_5xzd9om,False,,0,False,GPT-2 - Annotated Paper + Paper Summary,[],r/LanguageTechnology,False,6,,0,,False,t3_nm39gt,False,dark,0.93,,public,22,0,{},,False,[],,False,False,,{},,False,22,,False,False,,False,,[],{},,True,,1622134886.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The GPT-2 model was a major breakthrough in the path of creating a general multitask NLP system that was totally unsupervised. It demonstrated that given a large training corpus and a large model size, the language model was capable of learning the knowledge required for solving these tasks. It was not perfect, however, and performed poorly on some tasks as well.&lt;/p&gt;

&lt;p&gt;I went through the paper and have written an informative summary of the paper. The paper was quite easy to follow and the experimentation section had interesting observations. Check out the links below and happy reading!&lt;/p&gt;

&lt;p&gt;Paper Summary -  &lt;a href=""https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/""&gt;Language Models are Unsupervised Multitask Learners&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Annotated Paper -  &lt;a href=""https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT2.pdf""&gt;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT2.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nm39gt,True,,shreyansh26,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nm39gt/gpt2_annotated_paper_paper_summary/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nm39gt/gpt2_annotated_paper_paper_summary/,30199,1622106086.0,0,,False,,,,,,923
1107,,LanguageTechnology,How long was it and what was it about?,t2_be2v1j11,False,,0,False,For GPT3 users - What's the longest output you've had it write?,[],r/LanguageTechnology,False,6,,0,,False,t3_nm53h3,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1622142844.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How long was it and what was it about?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nm53h3,True,,ricksElar,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nm53h3/for_gpt3_users_whats_the_longest_output_youve_had/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nm53h3/for_gpt3_users_whats_the_longest_output_youve_had/,30199,1622114044.0,0,,False,,,,,,38
1108,,LanguageTechnology,"My first youtube video discussing a research paper -

**Transformer-XL: Attentive Language Models Beyond a Fixed Length Context** [https://youtu.be/2cmVnQNWQt8](https://youtu.be/2cmVnQNWQt8) 

Check it out!",t2_ccnld7oq,False,,0,False,Transformer-XL: Attentive Language Models Beyond a Fixed Length Context,[],r/LanguageTechnology,False,6,,0,,False,t3_nlqr66,False,dark,0.92,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1622089911.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My first youtube video discussing a research paper -&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Transformer-XL: Attentive Language Models Beyond a Fixed Length Context&lt;/strong&gt; &lt;a href=""https://youtu.be/2cmVnQNWQt8""&gt;https://youtu.be/2cmVnQNWQt8&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Check it out!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nlqr66,True,,abhinandy,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nlqr66/transformerxl_attentive_language_models_beyond_a/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nlqr66/transformerxl_attentive_language_models_beyond_a/,30199,1622061111.0,0,,False,,,,,,206
1109,,LanguageTechnology,"Hi there,

I recently wrote a blog post about zero-shot knowledge distillation from big language models with a method called DINO (Datasets from Instructions 🦕) that does not require any (labeled or unlabeled) data or access to model internals. The key idea is to prompt the LM to generate an entire dataset from scratch on which much smaller models can then be trained.

 I'd be very happy to hear your thoughts (both on the blog post and on the method) 😊

📝 Link: http://timoschick.com/research/2021/05/19/dino.html",t2_383etasr,False,,0,False,Zero-Shot Knowledge Distillation From GPT-3,[],r/LanguageTechnology,False,6,,0,,False,t3_nlcx98,False,dark,0.94,,public,25,0,{},,False,[],,False,False,,{},,False,25,,False,False,,False,,[],{},,True,,1622050788.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there,&lt;/p&gt;

&lt;p&gt;I recently wrote a blog post about zero-shot knowledge distillation from big language models with a method called DINO (Datasets from Instructions 🦕) that does not require any (labeled or unlabeled) data or access to model internals. The key idea is to prompt the LM to generate an entire dataset from scratch on which much smaller models can then be trained.&lt;/p&gt;

&lt;p&gt;I&amp;#39;d be very happy to hear your thoughts (both on the blog post and on the method) 😊&lt;/p&gt;

&lt;p&gt;📝 Link: &lt;a href=""http://timoschick.com/research/2021/05/19/dino.html""&gt;http://timoschick.com/research/2021/05/19/dino.html&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nlcx98,True,,timoschick,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nlcx98/zeroshot_knowledge_distillation_from_gpt3/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nlcx98/zeroshot_knowledge_distillation_from_gpt3/,30199,1622021988.0,0,,False,,,,,,517
1110,,LanguageTechnology,"Hi there!

I'm working on a project in which I need to extract the topics from a collection of 250K facebook posts. 

Can someone recommend (in high level) what's the best approach to do something like that? Ways that will generate the best results.

Thanks!",t2_4v9mxawe,False,,0,False,Extracting topics from 250k facebook posts,[],r/LanguageTechnology,False,6,,0,,False,t3_nlfysm,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1622061427.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working on a project in which I need to extract the topics from a collection of 250K facebook posts. &lt;/p&gt;

&lt;p&gt;Can someone recommend (in high level) what&amp;#39;s the best approach to do something like that? Ways that will generate the best results.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nlfysm,True,,itreallyreallydoesnt,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nlfysm/extracting_topics_from_250k_facebook_posts/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nlfysm/extracting_topics_from_250k_facebook_posts/,30199,1622032627.0,0,,False,,,,,,258
1111,,LanguageTechnology,,t2_hkv9s,False,,0,False,Efficient System for Grammar Error Correction on Mobile Devices (Research Paper Summary),[],r/LanguageTechnology,False,6,,0,,False,t3_nlau0l,False,dark,1.0,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,False,,1622042197.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nlau0l,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nlau0l/efficient_system_for_grammar_error_correction_on/,all_ads,False,https://link.medium.com/tVSykpafzgb,30199,1622013397.0,0,,False,https://link.medium.com/tVSykpafzgb,,,,,0
1112,,LanguageTechnology,,t2_icvu8,False,,0,False,Can you classify texts without any labeled examples? Putting zero-shot classification to the test.,[],r/LanguageTechnology,False,6,,0,,False,t3_nkr5gr,False,dark,0.99,,public,35,0,{},,False,[],,False,False,,{},,False,35,,False,False,,False,,[],{},,False,,1621981972.0,text,6,,,text,nlp.town,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nkr5gr,True,,yvespeirsman,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nkr5gr/can_you_classify_texts_without_any_labeled/,all_ads,False,https://nlp.town/blog/zero-shot-classification/,30199,1621953172.0,0,,False,https://nlp.town/blog/zero-shot-classification/,,,,,0
1113,,LanguageTechnology,"In today’s era, all top benchmarks in natural language processing are dominated by Transformer-based models. In a machine learning model, the most critical elements of the training process are the model code, training data, and available computing resources.

With the Transformer family of models, researchers have now finally come up with a way to increase the performance of a model infinitely by increasing the amount of training data and compute power.

OpenAI did this with GPT-2 and with GPT-3. They used a private corpus of 500 billion tokens for training the model and spent $50 million in computing costs.

Full Article: [https://www.marktechpost.com/2021/05/24/eleutherai-develops-gpt-3s-free-alternative-gpt-neo/](https://www.marktechpost.com/2021/05/24/eleutherai-develops-gpt-3s-free-alternative-gpt-neo/?_ga=2.62220524.1924646600.1621739878-488125022.1618729090)

Github: [https://github.com/EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo)",t2_4wudjgid,False,,0,False,EleutherAI Develops GPT-3’s Free Alternative: GPT-Neo,[],r/LanguageTechnology,False,6,,0,,False,t3_njzmmr,False,dark,0.95,,public,33,0,{},,False,[],,False,False,,{},,False,33,,False,False,,False,,[],{},,True,,1621896370.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In today’s era, all top benchmarks in natural language processing are dominated by Transformer-based models. In a machine learning model, the most critical elements of the training process are the model code, training data, and available computing resources.&lt;/p&gt;

&lt;p&gt;With the Transformer family of models, researchers have now finally come up with a way to increase the performance of a model infinitely by increasing the amount of training data and compute power.&lt;/p&gt;

&lt;p&gt;OpenAI did this with GPT-2 and with GPT-3. They used a private corpus of 500 billion tokens for training the model and spent $50 million in computing costs.&lt;/p&gt;

&lt;p&gt;Full Article: &lt;a href=""https://www.marktechpost.com/2021/05/24/eleutherai-develops-gpt-3s-free-alternative-gpt-neo/?_ga=2.62220524.1924646600.1621739878-488125022.1618729090""&gt;https://www.marktechpost.com/2021/05/24/eleutherai-develops-gpt-3s-free-alternative-gpt-neo/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Github: &lt;a href=""https://github.com/EleutherAI/gpt-neo""&gt;https://github.com/EleutherAI/gpt-neo&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,njzmmr,True,,techsucker,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/njzmmr/eleutherai_develops_gpt3s_free_alternative_gptneo/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/njzmmr/eleutherai_develops_gpt3s_free_alternative_gptneo/,30199,1621867570.0,0,,False,,,,,,965
1114,,LanguageTechnology,"Using Python.

Having trouble putting this question into words.

Is there a way to score the structure of a sentence against a corpus of other sentences?

So if a sentence if badly written in terms of grammar/word order, the score would be low.

I guess one would need to know the most common sentence structure within the corpus and work back from there?

Any ideas much appreciated and apologies for the vagueness of the question.",t2_4s6pt7r6,False,,0,False,Is there a way to score the structure of a sentence against a corpus of other sentences?,[],r/LanguageTechnology,False,6,,0,,False,t3_njvtsw,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1621884763.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Using Python.&lt;/p&gt;

&lt;p&gt;Having trouble putting this question into words.&lt;/p&gt;

&lt;p&gt;Is there a way to score the structure of a sentence against a corpus of other sentences?&lt;/p&gt;

&lt;p&gt;So if a sentence if badly written in terms of grammar/word order, the score would be low.&lt;/p&gt;

&lt;p&gt;I guess one would need to know the most common sentence structure within the corpus and work back from there?&lt;/p&gt;

&lt;p&gt;Any ideas much appreciated and apologies for the vagueness of the question.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,njvtsw,True,,breadncheesetheking1,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/njvtsw/is_there_a_way_to_score_the_structure_of_a/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/njvtsw/is_there_a_way_to_score_the_structure_of_a/,30199,1621855963.0,0,,False,,,,,,432
1115,,LanguageTechnology,"Hi all, so I'm doing some constituency parsing with SpaCy and benepar, and i'd ideally like to be able to tag the words in a sentence with their corresponding constituents. 

Currently however, Spacy allows you to print the parse string, but I'd like to be able to attach the actual syntactic structure to each word, as a POS tagger would do. So for the following code; 

    import spacy
    import benepar
    
    nlp = spacy.load('en_core_web_md')
    
    if spacy.__version__.startswith('2'):
      nlp.add_pipe('benepar', config={'model': 'benepar_en3'})
    else:
      nlp.add_pipe(""benepar"", config={""model"": ""benepar_en3""})
    
    doc = nlp(""Last Tuesday, I thought to myself that I saw a cat."")
    sent = list(doc.sents)[0]
    
    constituents = sent._.parse_string
    print(constituents)
    
    &gt;&gt;&gt; (S (NP (JJ Last) (NNP Tuesday)) (, ,) (NP (PRP I)) (VP (VBD thought) (PP (IN to) (NP (PRP myself))) (SBAR (IN that) (S (NP (PRP I)) (VP (VBD saw) (NP (DT a) (NN cat)))))) (. .))
    

I'd like for each word to be tagged with it's corresponding grammatical structure, but currently this is just one string. Any ideas on how to achieve this?",t2_48eyl33r,False,,0,False,Tokenising SpaCy constituency parse output,[],r/LanguageTechnology,False,6,,0,,False,t3_njudn4,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1621879400.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, so I&amp;#39;m doing some constituency parsing with SpaCy and benepar, and i&amp;#39;d ideally like to be able to tag the words in a sentence with their corresponding constituents. &lt;/p&gt;

&lt;p&gt;Currently however, Spacy allows you to print the parse string, but I&amp;#39;d like to be able to attach the actual syntactic structure to each word, as a POS tagger would do. So for the following code; &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import spacy
import benepar

nlp = spacy.load(&amp;#39;en_core_web_md&amp;#39;)

if spacy.__version__.startswith(&amp;#39;2&amp;#39;):
  nlp.add_pipe(&amp;#39;benepar&amp;#39;, config={&amp;#39;model&amp;#39;: &amp;#39;benepar_en3&amp;#39;})
else:
  nlp.add_pipe(&amp;quot;benepar&amp;quot;, config={&amp;quot;model&amp;quot;: &amp;quot;benepar_en3&amp;quot;})

doc = nlp(&amp;quot;Last Tuesday, I thought to myself that I saw a cat.&amp;quot;)
sent = list(doc.sents)[0]

constituents = sent._.parse_string
print(constituents)

&amp;gt;&amp;gt;&amp;gt; (S (NP (JJ Last) (NNP Tuesday)) (, ,) (NP (PRP I)) (VP (VBD thought) (PP (IN to) (NP (PRP myself))) (SBAR (IN that) (S (NP (PRP I)) (VP (VBD saw) (NP (DT a) (NN cat)))))) (. .))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&amp;#39;d like for each word to be tagged with it&amp;#39;s corresponding grammatical structure, but currently this is just one string. Any ideas on how to achieve this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,njudn4,True,,crowpup783,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/njudn4/tokenising_spacy_constituency_parse_output/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/njudn4/tokenising_spacy_constituency_parse_output/,30199,1621850600.0,0,,False,,,,,,1168
1116,,LanguageTechnology,"EDIT - solved 

I have a list of strings, which I have turned into SpaCy documents in order to be able to POS tag each string as such; 

    example_sents = [nlp(sent) for sent in example_sents[0:2]]
    print(example_sents)
    
    &gt;&gt;&gt; [well as long as you accept that it is  testing uhyou know  alleviate, she cant accept that we want to be the care givers]

However, when I try and POS tag multiple strings, I get the error 'AttributeError: 'spacy.tokens.doc.Doc' object has no attribute 'pos\_''

Does anyone know why this is happening when I try to iterate over each spacy document as such; 

    for token in example_sents[1:2]:
      print(token.text, token.pos_)

Thanks!",t2_48eyl33r,False,,0,False,Get POS tags for multiple spacy documents,[],r/LanguageTechnology,False,6,,0,,False,t3_njzz7v,False,dark,0.67,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,1621878316.0,,[],{},,True,,1621897312.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;EDIT - solved &lt;/p&gt;

&lt;p&gt;I have a list of strings, which I have turned into SpaCy documents in order to be able to POS tag each string as such; &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;example_sents = [nlp(sent) for sent in example_sents[0:2]]
print(example_sents)

&amp;gt;&amp;gt;&amp;gt; [well as long as you accept that it is  testing uhyou know  alleviate, she cant accept that we want to be the care givers]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, when I try and POS tag multiple strings, I get the error &amp;#39;AttributeError: &amp;#39;spacy.tokens.doc.Doc&amp;#39; object has no attribute &amp;#39;pos_&amp;#39;&amp;#39;&lt;/p&gt;

&lt;p&gt;Does anyone know why this is happening when I try to iterate over each spacy document as such; &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for token in example_sents[1:2]:
  print(token.text, token.pos_)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,njzz7v,True,,crowpup783,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/njzz7v/get_pos_tags_for_multiple_spacy_documents/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/njzz7v/get_pos_tags_for_multiple_spacy_documents/,30199,1621868512.0,0,,False,,,,,,689
1117,,LanguageTechnology,,t2_3fhicx1x,False,,0,False,Analyzing the vast coronavirus literature with CoronaCentral,[],r/LanguageTechnology,False,6,,0,,False,t3_nju4az,False,dark,0.99,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,False,,1621878360.0,text,6,,,text,pnas.org,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nju4az,True,,jakelikestextmining,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nju4az/analyzing_the_vast_coronavirus_literature_with/,all_ads,False,https://www.pnas.org/content/118/23/e2100766118,30199,1621849560.0,0,,False,https://www.pnas.org/content/118/23/e2100766118,,,,,0
1118,,LanguageTechnology,,t2_21qaqh1p,False,,0,False,Controllable Text Summarization in Python based on CtrlSum,[],r/LanguageTechnology,False,6,,0,,False,t3_nk1w7q,False,dark,0.6,,public,1,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/P8CqGqR1Zr4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Controllable Text Summarization in Python based on CtrlSum | Applied NLP in Python', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/P8CqGqR1Zr4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/P8CqGqR1Zr4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/P8CqGqR1Zr4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nk1w7q', 'height': 200}",,False,1,,False,False,,False,,[],{},,False,,1621902346.0,text,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nk1w7q,True,,dulldata,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nk1w7q/controllable_text_summarization_in_python_based/,all_ads,False,https://www.youtube.com/watch?v=P8CqGqR1Zr4,30199,1621873546.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Controllable Text Summarization in Python based on CtrlSum | Applied NLP in Python', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/P8CqGqR1Zr4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/P8CqGqR1Zr4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,https://www.youtube.com/watch?v=P8CqGqR1Zr4,,,,,0
1119,,LanguageTechnology,"Here is the list of all NAACL-2021 (Annual Conference of the North American Chapter of the Association for Computational Linguistics) papers, and a one sentence highlight for each of them. The proceeding was released today.

[https://www.paperdigest.org/2021/05/naacl-2021-highlights/](https://www.paperdigest.org/2021/05/naacl-2021-highlights/)",t2_2niqx8mz,False,,0,False,One sentence highlight for every NAACL-2021 Paper,[],r/LanguageTechnology,False,6,,0,,False,t3_njkuau,False,dark,0.99,,public,14,1,{},,False,[],,False,False,,{},,False,14,,False,False,,False,,[],{'gid_1': 1},,True,,1621843442.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Here is the list of all NAACL-2021 (Annual Conference of the North American Chapter of the Association for Computational Linguistics) papers, and a one sentence highlight for each of them. The proceeding was released today.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.paperdigest.org/2021/05/naacl-2021-highlights/""&gt;https://www.paperdigest.org/2021/05/naacl-2021-highlights/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,njkuau,True,,biandangou,,2,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/njkuau/one_sentence_highlight_for_every_naacl2021_paper/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/njkuau/one_sentence_highlight_for_every_naacl2021_paper/,30199,1621814642.0,0,,False,,,,,,345
1120,,LanguageTechnology,"I am in my last year of undergraduate studies and want to use my electives on math. Other than basic math like Probability theory, Statistics, Linear Algebra, or Multivariate Calculus, are there any other important math classes that would be beneficial for a career or research in NLP in 2021-2022?

I assume ODEs can help with Neural Nets and higher statistics are always beneficial but there must be more.",t2_7kpfukab,False,,0,False,Advanced Math for NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_njibcy,False,dark,1.0,,public,15,0,{},,False,[],,False,False,,{},,False,15,,False,False,,False,,[],{},,True,,1621835693.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am in my last year of undergraduate studies and want to use my electives on math. Other than basic math like Probability theory, Statistics, Linear Algebra, or Multivariate Calculus, are there any other important math classes that would be beneficial for a career or research in NLP in 2021-2022?&lt;/p&gt;

&lt;p&gt;I assume ODEs can help with Neural Nets and higher statistics are always beneficial but there must be more.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,njibcy,True,,throwaway1287odc,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/njibcy/advanced_math_for_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/njibcy/advanced_math_for_nlp/,30199,1621806893.0,0,,False,,,,,,407
1121,,LanguageTechnology,"What would be the way to go for a search engine to search on similar words. When I check txtai or similar implementations they use pooling (averaging) on words. What I am looking for is that if i search on a query of 3 words for example that it searches for the 100 closest words (cosine similarity) for each word and creates some rank. But this rank should not be compromised by matching words of only one semantic meaning. 
Example if I search for:  “great iphone tutorials” this ranking should not be compromised by documents with a lot of matching similarities for “great” with no mentioning for iphone and tutorials.",t2_7tb2j,False,,0,False,Search on vector embeddings,[],r/LanguageTechnology,False,6,,0,,False,t3_njc3wk,False,dark,1.0,,public,15,0,{},,False,[],,False,False,,{},,False,15,,False,False,,False,,[],{},,True,,1621818601.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What would be the way to go for a search engine to search on similar words. When I check txtai or similar implementations they use pooling (averaging) on words. What I am looking for is that if i search on a query of 3 words for example that it searches for the 100 closest words (cosine similarity) for each word and creates some rank. But this rank should not be compromised by matching words of only one semantic meaning. 
Example if I search for:  “great iphone tutorials” this ranking should not be compromised by documents with a lot of matching similarities for “great” with no mentioning for iphone and tutorials.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,njc3wk,True,,gevezex,,13,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/njc3wk/search_on_vector_embeddings/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/njc3wk/search_on_vector_embeddings/,30199,1621789801.0,0,,False,,,,,,621
1122,,LanguageTechnology,"I am new to machine learning.  I am trying to build a text classifier.

I have the following code:

    title_tfidf = TfidfVectorizer(use_idf=True, max_df=0.12, stop_words='english', preprocessor=custom_preprocessor) 
    text_tfidf = TfidfVectorizer(use_idf=True, max_df=0.12, stop_words='english', preprocessor=custom_preprocessor)  
    
    preprocess = ColumnTransformer([('title_tfidf', title_tfidf, 'Title'), ('text_tfidf', text_tfidf, 'Text')])  
    
    model = make_pipeline(preprocess, LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000))  
    
    model.fit(x_train, y_train)

I access the feature-names list and coefficients in the following way:

    regressor = model.named_steps['logisticregression'] 
    print (""Coefficients: "", regressor.coef_) 
    feature_names = preprocess.get_feature_names()

how can one retrieve the feature vector?

Would be grateful for any inputs!",t2_a40fenez,False,,0,False,NLP : Access feature vector from Pipeline object,[],r/LanguageTechnology,False,6,,0,,False,t3_njs41v,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,1621853146.0,,[],{},,True,,1621869803.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am new to machine learning.  I am trying to build a text classifier.&lt;/p&gt;

&lt;p&gt;I have the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;title_tfidf = TfidfVectorizer(use_idf=True, max_df=0.12, stop_words=&amp;#39;english&amp;#39;, preprocessor=custom_preprocessor) 
text_tfidf = TfidfVectorizer(use_idf=True, max_df=0.12, stop_words=&amp;#39;english&amp;#39;, preprocessor=custom_preprocessor)  

preprocess = ColumnTransformer([(&amp;#39;title_tfidf&amp;#39;, title_tfidf, &amp;#39;Title&amp;#39;), (&amp;#39;text_tfidf&amp;#39;, text_tfidf, &amp;#39;Text&amp;#39;)])  

model = make_pipeline(preprocess, LogisticRegression(verbose=1, solver=&amp;#39;liblinear&amp;#39;,random_state=0, C=5, penalty=&amp;#39;l2&amp;#39;,max_iter=1000))  

model.fit(x_train, y_train)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I access the feature-names list and coefficients in the following way:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;regressor = model.named_steps[&amp;#39;logisticregression&amp;#39;] 
print (&amp;quot;Coefficients: &amp;quot;, regressor.coef_) 
feature_names = preprocess.get_feature_names()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;how can one retrieve the feature vector?&lt;/p&gt;

&lt;p&gt;Would be grateful for any inputs!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,njs41v,True,,puzzled-cognition,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/njs41v/nlp_access_feature_vector_from_pipeline_object/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/njs41v/nlp_access_feature_vector_from_pipeline_object/,30199,1621841003.0,0,,False,,,,,,940
1123,,LanguageTechnology,"I have to attend a quota of seminars as part of my degree, but the offerings at my university are a bit limited. Is there an equivalent to [http://researchseminars.org/](http://researchseminars.org/) for language technology / computation linguistics / that sort of space?

Or, if you don't know of a large universal one, does your research group have a website where you publish your upcoming seminars?",t2_bz3u6,False,,0,False,Interesting seminars and talks?,[],r/LanguageTechnology,False,6,,0,,False,t3_njoaqt,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1621854999.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have to attend a quota of seminars as part of my degree, but the offerings at my university are a bit limited. Is there an equivalent to &lt;a href=""http://researchseminars.org/""&gt;http://researchseminars.org/&lt;/a&gt; for language technology / computation linguistics / that sort of space?&lt;/p&gt;

&lt;p&gt;Or, if you don&amp;#39;t know of a large universal one, does your research group have a website where you publish your upcoming seminars?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,njoaqt,True,,solresol,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/njoaqt/interesting_seminars_and_talks/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/njoaqt/interesting_seminars_and_talks/,30199,1621826199.0,0,,False,,,,,,402
1124,,LanguageTechnology,,t2_hkv9s,False,,0,False,A Graph-based Text Similarity Method with Named Entity Information in NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_nj4xc6,False,dark,0.89,,public,13,0,{},,False,[],,False,False,,{},,False,13,,False,False,,False,,[],{},,False,,1621793927.0,text,6,,,text,medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nj4xc6,True,,prakhar21,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nj4xc6/a_graphbased_text_similarity_method_with_named/,all_ads,False,https://medium.com/p/abc7f1201d96?source=linkShare-bcb8dddfcc90-1621765097,30199,1621765127.0,0,,False,https://medium.com/p/abc7f1201d96?source=linkShare-bcb8dddfcc90-1621765097,,,,,0
1125,,LanguageTechnology,"Hey guys am creating a keyword extraction model to get keywords from text, for Spanish language. Any and all resources or ideas are welcome. Thanks 😊",t2_9v2jnqq5,False,,0,False,Spanish keyword extraction [D],[],r/LanguageTechnology,False,6,,0,,False,t3_njgmwd,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1621831091.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys am creating a keyword extraction model to get keywords from text, for Spanish language. Any and all resources or ideas are welcome. Thanks 😊&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,njgmwd,True,,Sensitive-Loss1522,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/njgmwd/spanish_keyword_extraction_d/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/njgmwd/spanish_keyword_extraction_d/,30199,1621802291.0,0,,False,,,,,,149
1126,,LanguageTechnology,"Hello,

What is the best laptops for deep learning under 1000$?

Thank you",t2_9e6mmc22,False,,0,False,Laptop for deep learning,[],r/LanguageTechnology,False,6,,0,,False,t3_nj4xvz,False,dark,0.9,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1621793993.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;What is the best laptops for deep learning under 1000$?&lt;/p&gt;

&lt;p&gt;Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nj4xvz,True,,Ok_Inspection_5208,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nj4xvz/laptop_for_deep_learning/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nj4xvz/laptop_for_deep_learning/,30199,1621765193.0,0,,False,,,,,,74
1127,,LanguageTechnology,"Hello everybody, I am a student working on his bachelor thesis.. I have a some ML background but I am quite new to NLP.

Shortly, the goal of my thesis is to extract text from CSV bank transactions files and EBICS exports and categorize them into about 20 categories.. So far I have tried with a basic Naive-Bayes classificator getting +- 65% accuracy but nothing more.. do you have any tips on some specific models or papers that could help me achieve a higher precision?

To add some more details, the bank statements that I have so far are not that big in terms of transactions, I have about 400 rows of data right now.. this is probably impacting the accuracy but it's kinda a long process to gather those kind of files.

Additionally, the data from the CVS and EBIC files are quite short sentences.. around 60/80 characters per line.

Thanks in advance for any suggestion! have a nice day :)",t2_11j08j,False,,0,False,Classification of bank statement transactions,[],r/LanguageTechnology,False,6,,0,,False,t3_nje9nc,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1621824557.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everybody, I am a student working on his bachelor thesis.. I have a some ML background but I am quite new to NLP.&lt;/p&gt;

&lt;p&gt;Shortly, the goal of my thesis is to extract text from CSV bank transactions files and EBICS exports and categorize them into about 20 categories.. So far I have tried with a basic Naive-Bayes classificator getting +- 65% accuracy but nothing more.. do you have any tips on some specific models or papers that could help me achieve a higher precision?&lt;/p&gt;

&lt;p&gt;To add some more details, the bank statements that I have so far are not that big in terms of transactions, I have about 400 rows of data right now.. this is probably impacting the accuracy but it&amp;#39;s kinda a long process to gather those kind of files.&lt;/p&gt;

&lt;p&gt;Additionally, the data from the CVS and EBIC files are quite short sentences.. around 60/80 characters per line.&lt;/p&gt;

&lt;p&gt;Thanks in advance for any suggestion! have a nice day :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nje9nc,True,,thisunamewasfree,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nje9nc/classification_of_bank_statement_transactions/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nje9nc/classification_of_bank_statement_transactions/,30199,1621795757.0,0,,False,,,,,,896
1128,,LanguageTechnology,"To clarify, is it difficult for a non-programmer to write a code or mini program that would collect articles based on several simple criteria or maybe there are some other solutions for this purpose?

Example: I want to collect and gather all articles published on BBC website between January 2021 and April 2021 that contain the word ""doggo"" for the purposes of corpus linguistics. 

I am aware of BootCat but I never managed to get it work properly, though",t2_6g45m2tc,False,,0,False,How to write a code/mini program that collects articles?,[],r/LanguageTechnology,False,6,,0,,False,t3_nimrip,False,dark,0.85,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1621730971.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;To clarify, is it difficult for a non-programmer to write a code or mini program that would collect articles based on several simple criteria or maybe there are some other solutions for this purpose?&lt;/p&gt;

&lt;p&gt;Example: I want to collect and gather all articles published on BBC website between January 2021 and April 2021 that contain the word &amp;quot;doggo&amp;quot; for the purposes of corpus linguistics. &lt;/p&gt;

&lt;p&gt;I am aware of BootCat but I never managed to get it work properly, though&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nimrip,True,,Sedulas,,13,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nimrip/how_to_write_a_codemini_program_that_collects/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nimrip/how_to_write_a_codemini_program_that_collects/,30199,1621702171.0,0,,False,,,,,,458
1129,,LanguageTechnology,,t2_hkv9s,False,,0,False,An Efficient System for Grammatical Error Correction on Mobile Devices (Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_nidynd,False,dark,0.92,,public,18,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3rVn14m8zaM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'An Efficient System for Grammatical Error Correction on Mobile Devices (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3rVn14m8zaM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3rVn14m8zaM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3rVn14m8zaM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nidynd', 'height': 200}",,False,18,,False,False,,False,,[],{},,False,,1621700579.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nidynd,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nidynd/an_efficient_system_for_grammatical_error/,all_ads,False,https://youtu.be/3rVn14m8zaM,30199,1621671779.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'An Efficient System for Grammatical Error Correction on Mobile Devices (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3rVn14m8zaM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3rVn14m8zaM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/3rVn14m8zaM,,,,,0
1130,,LanguageTechnology,"I'm working with some older texts, were the letters are sometimes a little smudgy.

The letters and words get recognized near-perfectly, and when I look at the hOCR or html file, the text looks perfect.

But when I **export to PDF** with an invisible text layer the spaces between words frequently go missing, for paragraphs at a time. This is annoying when trying to highlight parts of the text and then copy-paste those excerpts.

Any advice?

Other than these old texts, gImageReader is absolutely amazing and does exactly what I want.",t2_5lrs9ing,False,,0,False,gImageReader/Tesseract: Having trouble with old texts and spaces between words,[],r/LanguageTechnology,False,6,,0,,False,t3_nidwks,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1621700317.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working with some older texts, were the letters are sometimes a little smudgy.&lt;/p&gt;

&lt;p&gt;The letters and words get recognized near-perfectly, and when I look at the hOCR or html file, the text looks perfect.&lt;/p&gt;

&lt;p&gt;But when I &lt;strong&gt;export to PDF&lt;/strong&gt; with an invisible text layer the spaces between words frequently go missing, for paragraphs at a time. This is annoying when trying to highlight parts of the text and then copy-paste those excerpts.&lt;/p&gt;

&lt;p&gt;Any advice?&lt;/p&gt;

&lt;p&gt;Other than these old texts, gImageReader is absolutely amazing and does exactly what I want.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nidwks,True,,Party-Permission,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nidwks/gimagereadertesseract_having_trouble_with_old/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nidwks/gimagereadertesseract_having_trouble_with_old/,30199,1621671517.0,0,,False,,,,,,538
1131,,LanguageTechnology,"I'm trying to automate the process of web scraping. I have a dataset of real-estate data with fields like Title, Price, Street, Postcode, etc. I want to make a model that takes the content of a webpage and returns words/phrases that are similar to/match those in the dataset. This way I won't have to write a custom web scraper for every website. I think this is similar to keyword extraction except that I want the model to learn a dataset of keywords and then extract those keywords from a webpage.

Can anyone guide me on how to approach this problem?",t2_c7r1mlx,False,,0,False,Automating a Web Scraper,[],r/LanguageTechnology,False,6,,0,,False,t3_nie7t0,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1621701729.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to automate the process of web scraping. I have a dataset of real-estate data with fields like Title, Price, Street, Postcode, etc. I want to make a model that takes the content of a webpage and returns words/phrases that are similar to/match those in the dataset. This way I won&amp;#39;t have to write a custom web scraper for every website. I think this is similar to keyword extraction except that I want the model to learn a dataset of keywords and then extract those keywords from a webpage.&lt;/p&gt;

&lt;p&gt;Can anyone guide me on how to approach this problem?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nie7t0,True,,Jack7heRapper,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nie7t0/automating_a_web_scraper/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nie7t0/automating_a_web_scraper/,30199,1621672929.0,0,,False,,,,,,554
1132,,LanguageTechnology,"Can anyone recommend any NLU services, libraries, or methods that can perform NLU tasks using both language tokens and simple document formatting such as indentation level. For example, in lecture notes, indentations could help refine coreference resolution by understanding that there’s a good chance indented text is highly related to less indented text on an earlier document line or paragraph.",t2_3ardsqrn,False,,0,False,Using document formatting for NLU,[],r/LanguageTechnology,False,6,,0,,False,t3_ni48rc,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1621663872.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can anyone recommend any NLU services, libraries, or methods that can perform NLU tasks using both language tokens and simple document formatting such as indentation level. For example, in lecture notes, indentations could help refine coreference resolution by understanding that there’s a good chance indented text is highly related to less indented text on an earlier document line or paragraph.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ni48rc,True,,GreenOnGray,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ni48rc/using_document_formatting_for_nlu/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ni48rc/using_document_formatting_for_nlu/,30199,1621635072.0,0,,False,,,,,,397
1133,,LanguageTechnology,"I recently published a blog post about how the original WSD task formulation is not suitable for modern domain-specific and enterprise disambiguation settings and how Target Sense Verification can improve this situation. Here is the link: [https://annabreit.medium.com/is-word-sense-disambiguation-outdated-ef05a139576](https://annabreit.medium.com/is-word-sense-disambiguation-outdated-ef05a139576)  


Would be glad for any feedback and am happy to discuss!",t2_c3u2hpml,False,,0,False,Is Word Sense Disambiguation outdated?,[],r/LanguageTechnology,False,6,,0,,False,t3_nhohkx,False,dark,0.96,,public,17,1,{},,False,[],,False,False,,{},,False,17,,False,False,,False,,[],{},,True,,1621618542.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I recently published a blog post about how the original WSD task formulation is not suitable for modern domain-specific and enterprise disambiguation settings and how Target Sense Verification can improve this situation. Here is the link: &lt;a href=""https://annabreit.medium.com/is-word-sense-disambiguation-outdated-ef05a139576""&gt;https://annabreit.medium.com/is-word-sense-disambiguation-outdated-ef05a139576&lt;/a&gt;  &lt;/p&gt;

&lt;p&gt;Would be glad for any feedback and am happy to discuss!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nhohkx,True,,anna_with_b,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nhohkx/is_word_sense_disambiguation_outdated/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nhohkx/is_word_sense_disambiguation_outdated/,30199,1621589742.0,0,,False,,,,,,459
1134,,LanguageTechnology,New to NLP. Can anyone suggest me how I can use NLP to extract important details from large documents? Thanks in advance.,t2_b3gv09ny,False,,0,False,Text Extraction,[],r/LanguageTechnology,False,6,,0,,False,t3_nhnpyv,False,dark,0.81,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1621615340.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;New to NLP. Can anyone suggest me how I can use NLP to extract important details from large documents? Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nhnpyv,True,,Key-Feature-1228,,11,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nhnpyv/text_extraction/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nhnpyv/text_extraction/,30199,1621586540.0,0,,False,,,,,,121
1135,,LanguageTechnology,,t2_hkv9s,False,,0,False,Automatic Extraction of Hypernym Relations from Text,[],r/LanguageTechnology,False,6,,0,,False,t3_nhlyzg,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,False,,1621607930.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nhlyzg,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nhlyzg/automatic_extraction_of_hypernym_relations_from/,all_ads,False,https://link.medium.com/r22yL4WTqgb,30199,1621579130.0,0,,False,https://link.medium.com/r22yL4WTqgb,,,,,0
1136,,LanguageTechnology,"# NLU 3.0.1 Release Notes
We are very excited to announce NLU 3.0.1 has been released!
This is one of the most visually appealing releases, with the integration of the [Spark-NLP-Display](https://nlp.johnsnowlabs.com/docs/en/display) library and visualizations for `dependency trees`, `entity resolution`, `entity assertion`, `relationship between entities` and `named
entity recognition`. In addition to this, the schema of how columns are named by NLU has been reworked and all 140+ tutorial notebooks have been updated to reflect the latest changes in NLU 3.0.0+
Finally, new multilingual models for `Afrikaans`, `Welsh`, `Maltese`, `Tamil`, and`Vietnamese` are now available.




# New Features and Enhancements
- 1 line to visualization for `NER`, `Dependency`, `Resolution`, `Assertion` and `Relation` via [Spark-NLP-Display](https://nlp.johnsnowlabs.com/docs/en/display) integration
- Improved column naming schema
- [Over 140 + NLU tutorial Notebooks updated](https://github.com/JohnSnowLabs/nlu/tree/master/examples) and improved to reflect latest changes in NLU 3.0.0 +
- New multilingual models for `Afrikaans`, `Welsh`, `Maltese`, `Tamil`, and`Vietnamese`
- Enhanced offline loading


## NLU visualization
The latest NLU release integrated the beautiful Spark-NLP-Display package visualizations. You do not need to worry about installing it, when you try to visualize something, NLU will check if
Spark-NLP-Display is installed, if it is missing it will be dynamically installed into your python executable environment, so you don't need to worry about anything!

See the [visualization tutorial notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/visualization/NLU_visualizations_tutorial.ipynb)  and [visualization docs](https://nlu.johnsnowlabs.com/docs/en/viz_examples) for more info.

![Cheat Sheet visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/cheat_sheet.png)

## NER visualization
Applicable to any of the [100+ NER models! See here for an overview](https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition)
```python
nlu.load('ner').viz(""Donald Trump from America and Angela Merkel from Germany don't share many oppinions."")
```
![NER visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/NER.png)

## Dependency tree visualization
Visualizes the structure of the labeled dependency tree and part of speech tags
```python
nlu.load('dep.typed').viz(""Billy went to the mall"")
```

![Dependency Tree visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/DEP.png)

```python
#Bigger Example
nlu.load('dep.typed').viz(""Donald Trump from America and Angela Merkel from Germany don't share many oppinions but they both love John Snow Labs software"")
```
![Dependency Tree visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/DEP_big.png)

## Assertion status visualization
Visualizes asserted statuses and entities.        
Applicable to any of the [10 + Assertion models! See here for an overview](https://nlp.johnsnowlabs.com/models?task=Assertion+Status)
```python
nlu.load('med_ner.clinical assert').viz(""The MRI scan showed no signs of cancer in the left lung"")
```


![Assert visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/assertion.png)

```python
#bigger example
data ='This is the case of a very pleasant 46-year-old Caucasian female, seen in clinic on 12/11/07 during which time MRI of the left shoulder showed no evidence of rotator cuff tear. She did have a previous MRI of the cervical spine that did show an osteophyte on the left C6-C7 level. Based on this, negative MRI of the shoulder, the patient was recommended to have anterior cervical discectomy with anterior interbody fusion at C6-C7 level. Operation, expected outcome, risks, and benefits were discussed with her. Risks include, but not exclusive of bleeding and infection, bleeding could be soft tissue bleeding, which may compromise airway and may result in return to the operating room emergently for evacuation of said hematoma. There is also the possibility of bleeding into the epidural space, which can compress the spinal cord and result in weakness and numbness of all four extremities as well as impairment of bowel and bladder function. However, the patient may develop deeper-seated infection, which may require return to the operating room. Should the infection be in the area of the spinal instrumentation, this will cause a dilemma since there might be a need to remove the spinal instrumentation and/or allograft. There is also the possibility of potential injury to the esophageus, the trachea, and the carotid artery. There is also the risks of stroke on the right cerebral circulation should an undiagnosed plaque be propelled from the right carotid. She understood all of these risks and agreed to have the procedure performed.'
nlu.load('med_ner.clinical assert').viz(data)
```
![Assert visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/assertion_big.png)


## Relationship between entities visualization
Visualizes the extracted entities between relationship.    
Applicable to any of the [20 + Relation Extractor models See here for an overview](https://nlp.johnsnowlabs.com/models?task=Relation+Extraction)
```python
nlu.load('med_ner.jsl.wip.clinical relation.temporal_events').viz('The patient developed cancer after a mercury poisoning in 1999 ')
```
![Entity Relation visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/relation.png)

```python
# bigger example
data = 'This is the case of a very pleasant 46-year-old Caucasian female, seen in clinic on 12/11/07 during which time MRI of the left shoulder showed no evidence of rotator cuff tear. She did have a previous MRI of the cervical spine that did show an osteophyte on the left C6-C7 level. Based on this, negative MRI of the shoulder, the patient was recommended to have anterior cervical discectomy with anterior interbody fusion at C6-C7 level. Operation, expected outcome, risks, and benefits were discussed with her. Risks include, but not exclusive of bleeding and infection, bleeding could be soft tissue bleeding, which may compromise airway and may result in return to the operating room emergently for evacuation of said hematoma. There is also the possibility of bleeding into the epidural space, which can compress the spinal cord and result in weakness and numbness of all four extremities as well as impairment of bowel and bladder function. However, the patient may develop deeper-seated infection, which may require return to the operating room. Should the infection be in the area of the spinal instrumentation, this will cause a dilemma since there might be a need to remove the spinal instrumentation and/or allograft. There is also the possibility of potential injury to the esophageus, the trachea, and the carotid artery. There is also the risks of stroke on the right cerebral circulation should an undiagnosed plaque be propelled from the right carotid. She understood all of these risks and agreed to have the procedure performed'
pipe = nlu.load('med_ner.jsl.wip.clinical relation.clinical').viz(data)
```
![Entity Relation visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/relation_big.png)


## Entity Resolution visualization for chunks
Visualizes resolutions of entities
Applicable to any of the [100+ Resolver models See here for an overview](https://nlp.johnsnowlabs.com/models?task=Entity+Resolution)
```python
nlu.load('med_ner.jsl.wip.clinical resolve_chunk.rxnorm.in').viz(""He took Prevacid 30 mg  daily"")
```
![Chunk Resolution visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_chunk.png)

```python
# bigger example
data = ""This is an 82 - year-old male with a history of prior tobacco use , hypertension , chronic renal insufficiency , COPD , gastritis , and TIA who initially presented to Braintree with a non-ST elevation MI and Guaiac positive stools , transferred to St . Margaret\'s Center for Women &amp; Infants for cardiac catheterization with PTCA to mid LAD lesion complicated by hypotension and bradycardia requiring Atropine , IV fluids and transient dopamine possibly secondary to vagal reaction , subsequently transferred to CCU for close monitoring , hemodynamically stable at the time of admission to the CCU .""
nlu.load('med_ner.jsl.wip.clinical resolve_chunk.rxnorm.in').viz(data)
```

![Chunk Resolution visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_chunk_big.png)


## Entity Resolution visualization for sentences
Visualizes resolutions of entities in sentences
Applicable to any of the [100+ Resolver models See here for an overview](https://nlp.johnsnowlabs.com/models?task=Entity+Resolution)
```python
nlu.load('med_ner.jsl.wip.clinical resolve.icd10cm').viz('She was diagnosed with a respiratory congestion')
```
![Sentence Resolution visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_sentence.png)

```python
# bigger example
data = 'The patient is a 5-month-old infant who presented initially on Monday with a cold, cough, and runny nose for 2 days. Mom states she had no fever. Her appetite was good but she was spitting up a lot. She had no difficulty breathing and her cough was described as dry and hacky. At that time, physical exam showed a right TM, which was red. Left TM was okay. She was fairly congested but looked happy and playful. She was started on Amoxil and Aldex and we told to recheck in 2 weeks to recheck her ear. Mom returned to clinic again today because she got much worse overnight. She was having difficulty breathing. She was much more congested and her appetite had decreased significantly today. She also spiked a temperature yesterday of 102.6 and always having trouble sleeping secondary to congestion'
nlu.load('med_ner.jsl.wip.clinical resolve.icd10cm').viz(data)
```
![Sentence Resolution visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_sentence_big.png)

## Configure visualizations
### Define custom colors for labels
Some entity and relation labels will be highlighted with a pre-defined color, which you [can find here](https://github.com/JohnSnowLabs/spark-nlp-display/tree/main/sparknlp_display/label_colors).    
For labels that have no color defined, a random color will be generated.     
You can define colors for labels manually, by specifying via the `viz_colors` parameter
and defining `hex color codes` in a dictionary that maps `labels` to `colors` .
```python
data = 'Dr. John Snow suggested that Fritz takes 5mg penicilin for his cough'
# Define custom colors for labels
viz_colors={'STRENGTH':'#800080', 'DRUG_BRANDNAME':'#77b5fe', 'GENDER':'#77ffe'}
nlu.load('med_ner.jsl.wip.clinical').viz(data,viz_colors =viz_colors)
```
![define colors labels](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/define_colors.png)


### Filter entities that get highlighted
By default every entity class will be visualized.    
The `labels_to_viz` can be used to define a set of labels to highlight.       
Applicable for ner, resolution and assert.
```python
data = 'Dr. John Snow suggested that Fritz takes 5mg penicilin for his cough'
# Filter wich NER label to viz
labels_to_viz=['SYMPTOM']
nlu.load('med_ner.jsl.wip.clinical').viz(data,labels_to_viz=labels_to_viz)
```
![filter labels](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/filter_labels.png)


## New models
New multilingual models for `Afrikaans`, `Welsh`, `Maltese`, `Tamil`, and`Vietnamese`

| nlu.load() Refrence                                          | Spark NLP Refrence                                           |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [vi.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_vi.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_vi.html) |
| [mt.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_mt.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_mt.html) |
| [ta.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_ta.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_ta.html) |
| [af.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_af.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_af.html) |
| [af.pos](https://nlp.johnsnowlabs.com/2021/04/06/pos_afribooms_af.html) | [pos_afribooms](https://nlp.johnsnowlabs.com/2021/04/06/pos_afribooms_af.html) |
| [cy.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_cy.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_cy.html) |

## Reworked and updated NLU tutorial notebooks

All of the [140+ NLU tutorial Notebooks](https://github.com/JohnSnowLabs/nlu/tree/master/examples) have been updated and reworked to reflect the latest changes in NLU 3.0.0+


## Improved Column Name generation
- NLU categorized each internal component now with boolean labels for `name_deductable` and `always_name_deductable` .
- Before generating column names, NLU checks wether each component is of unique in the pipeline or not. If a component is not unique in the
  pipe and there are multiple components of same type, i.e. multiple `NER` models, NLU will deduct a base name for the final output columns from the
  NLU reference each NER model is pointing to.
- If on the other hand, there is only one `NER` model in the pipeline, only the default `ner` column prefixed will be generated.
- For some components, like `embeddings` and `classifiers` are now defined as `always_name_deductable`, for those NLU will always try to infer a meaningful base name for the output columns.
- Newly trained component output columns will now be prefixed with `trained_&lt;type&gt;` , for types `pos` , `ner`, `cLassifier`, `sentiment` and `multi_classifier`

## Enhanced offline mode
- You can still load a model from a path as usual with `nlu.load(path=model_path)` and output columns will be suffixed with `from_disk`
- You can now optionally also specify `request` parameter during  load a model from HDD, it will be used to deduct more meaningful column name suffixes, instead of `from_disk`, i.e. by calling `nlu.load(request ='en.embed_sentence.biobert.pubmed_pmc_base_cased', path=model_path)`


### Bugfixes
- Fixed a bug that caused  resolution algorithms output level to be inferred incorrectly
- Fixed a bug that caused stranger cols got dropped
- Fixed a bug that caused endings to miss when  .predict(position=True) was specified
- Fixed a bug that caused pd.Series to be converted incorrectly internally
- Fixed a bug that caused output level transformations to crash
- Fixed a bug that caused verbose mode not to turn of properly after turning it on.
- fixed a bug that caused some models to crash when loaded for HDD

# Additional NLU resources
* [140+ updates tutorials](https://github.com/JohnSnowLabs/nlu/tree/master/examples)
* [Updated visualization docs](https://nlu.johnsnowlabs.com/docs/en/viz_examples)
* [Models Hub](https://nlp.johnsnowlabs.com/models) with new models
* [Spark NLP publications](https://medium.com/spark-nlp)
* [NLU in Action](https://nlp.johnsnowlabs.com/demo)
* [NLU documentation](https://nlu.johnsnowlabs.com/docs/en/install)
* [Discussions](https://github.com/JohnSnowLabs/spark-nlp/discussions) Engage with other community members, share ideas, and show off how you use Spark NLP and NLU!

# 1 line Install NLU on Google Colab
```!wget https://setup.johnsnowlabs.com/nlu/colab.sh  -O - | bash```
# 1 line Install NLU on Kaggle
```!wget https://setup.johnsnowlabs.com/nlu/kaggle.sh  -O - | bash```
# Install via PIP
```! pip install nlu pyspark==3.0.1```",t2_53n73cus,False,,0,False,"1 line to visualizations for dependency trees, entity relationships, resolution, assertion, NER and new models for Afrikaans, Welsh, Maltese, Tamil, and Vietnamese - John Snow Labs NLU 3.0.1 for Python",[],r/LanguageTechnology,False,6,,0,,False,t3_nh4qzh,False,dark,0.95,,public,16,0,{},,False,[],,False,False,,{},,False,16,,False,False,,False,,[],{},,True,,1621557047.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;h1&gt;NLU 3.0.1 Release Notes&lt;/h1&gt;

&lt;p&gt;We are very excited to announce NLU 3.0.1 has been released!
This is one of the most visually appealing releases, with the integration of the &lt;a href=""https://nlp.johnsnowlabs.com/docs/en/display""&gt;Spark-NLP-Display&lt;/a&gt; library and visualizations for &lt;code&gt;dependency trees&lt;/code&gt;, &lt;code&gt;entity resolution&lt;/code&gt;, &lt;code&gt;entity assertion&lt;/code&gt;, &lt;code&gt;relationship between entities&lt;/code&gt; and &lt;code&gt;named
entity recognition&lt;/code&gt;. In addition to this, the schema of how columns are named by NLU has been reworked and all 140+ tutorial notebooks have been updated to reflect the latest changes in NLU 3.0.0+
Finally, new multilingual models for &lt;code&gt;Afrikaans&lt;/code&gt;, &lt;code&gt;Welsh&lt;/code&gt;, &lt;code&gt;Maltese&lt;/code&gt;, &lt;code&gt;Tamil&lt;/code&gt;, and&lt;code&gt;Vietnamese&lt;/code&gt; are now available.&lt;/p&gt;

&lt;h1&gt;New Features and Enhancements&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;1 line to visualization for &lt;code&gt;NER&lt;/code&gt;, &lt;code&gt;Dependency&lt;/code&gt;, &lt;code&gt;Resolution&lt;/code&gt;, &lt;code&gt;Assertion&lt;/code&gt; and &lt;code&gt;Relation&lt;/code&gt; via &lt;a href=""https://nlp.johnsnowlabs.com/docs/en/display""&gt;Spark-NLP-Display&lt;/a&gt; integration&lt;/li&gt;
&lt;li&gt;Improved column naming schema&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/tree/master/examples""&gt;Over 140 + NLU tutorial Notebooks updated&lt;/a&gt; and improved to reflect latest changes in NLU 3.0.0 +&lt;/li&gt;
&lt;li&gt;New multilingual models for &lt;code&gt;Afrikaans&lt;/code&gt;, &lt;code&gt;Welsh&lt;/code&gt;, &lt;code&gt;Maltese&lt;/code&gt;, &lt;code&gt;Tamil&lt;/code&gt;, and&lt;code&gt;Vietnamese&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Enhanced offline loading&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;NLU visualization&lt;/h2&gt;

&lt;p&gt;The latest NLU release integrated the beautiful Spark-NLP-Display package visualizations. You do not need to worry about installing it, when you try to visualize something, NLU will check if
Spark-NLP-Display is installed, if it is missing it will be dynamically installed into your python executable environment, so you don&amp;#39;t need to worry about anything!&lt;/p&gt;

&lt;p&gt;See the &lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/visualization/NLU_visualizations_tutorial.ipynb""&gt;visualization tutorial notebook&lt;/a&gt;  and &lt;a href=""https://nlu.johnsnowlabs.com/docs/en/viz_examples""&gt;visualization docs&lt;/a&gt; for more info.&lt;/p&gt;

&lt;p&gt;![Cheat Sheet visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/cheat_sheet.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/cheat_sheet.png&lt;/a&gt;)&lt;/p&gt;

&lt;h2&gt;NER visualization&lt;/h2&gt;

&lt;p&gt;Applicable to any of the &lt;a href=""https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition""&gt;100+ NER models! See here for an overview&lt;/a&gt;
&lt;code&gt;python
nlu.load(&amp;#39;ner&amp;#39;).viz(&amp;quot;Donald Trump from America and Angela Merkel from Germany don&amp;#39;t share many oppinions.&amp;quot;)
&lt;/code&gt;
![NER visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/NER.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/NER.png&lt;/a&gt;)&lt;/p&gt;

&lt;h2&gt;Dependency tree visualization&lt;/h2&gt;

&lt;p&gt;Visualizes the structure of the labeled dependency tree and part of speech tags
&lt;code&gt;python
nlu.load(&amp;#39;dep.typed&amp;#39;).viz(&amp;quot;Billy went to the mall&amp;quot;)
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;![Dependency Tree visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/DEP.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/DEP.png&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;Bigger Example&lt;/h1&gt;

&lt;p&gt;nlu.load(&amp;#39;dep.typed&amp;#39;).viz(&amp;quot;Donald Trump from America and Angela Merkel from Germany don&amp;#39;t share many oppinions but they both love John Snow Labs software&amp;quot;)
```
![Dependency Tree visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/DEP_big.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/DEP_big.png&lt;/a&gt;)&lt;/p&gt;

&lt;h2&gt;Assertion status visualization&lt;/h2&gt;

&lt;p&gt;Visualizes asserted statuses and entities.&lt;br/&gt;
Applicable to any of the &lt;a href=""https://nlp.johnsnowlabs.com/models?task=Assertion+Status""&gt;10 + Assertion models! See here for an overview&lt;/a&gt;
&lt;code&gt;python
nlu.load(&amp;#39;med_ner.clinical assert&amp;#39;).viz(&amp;quot;The MRI scan showed no signs of cancer in the left lung&amp;quot;)
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;![Assert visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/assertion.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/assertion.png&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;bigger example&lt;/h1&gt;

&lt;p&gt;data =&amp;#39;This is the case of a very pleasant 46-year-old Caucasian female, seen in clinic on 12/11/07 during which time MRI of the left shoulder showed no evidence of rotator cuff tear. She did have a previous MRI of the cervical spine that did show an osteophyte on the left C6-C7 level. Based on this, negative MRI of the shoulder, the patient was recommended to have anterior cervical discectomy with anterior interbody fusion at C6-C7 level. Operation, expected outcome, risks, and benefits were discussed with her. Risks include, but not exclusive of bleeding and infection, bleeding could be soft tissue bleeding, which may compromise airway and may result in return to the operating room emergently for evacuation of said hematoma. There is also the possibility of bleeding into the epidural space, which can compress the spinal cord and result in weakness and numbness of all four extremities as well as impairment of bowel and bladder function. However, the patient may develop deeper-seated infection, which may require return to the operating room. Should the infection be in the area of the spinal instrumentation, this will cause a dilemma since there might be a need to remove the spinal instrumentation and/or allograft. There is also the possibility of potential injury to the esophageus, the trachea, and the carotid artery. There is also the risks of stroke on the right cerebral circulation should an undiagnosed plaque be propelled from the right carotid. She understood all of these risks and agreed to have the procedure performed.&amp;#39;
nlu.load(&amp;#39;med_ner.clinical assert&amp;#39;).viz(data)
```
![Assert visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/assertion_big.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/assertion_big.png&lt;/a&gt;)&lt;/p&gt;

&lt;h2&gt;Relationship between entities visualization&lt;/h2&gt;

&lt;p&gt;Visualizes the extracted entities between relationship.&lt;br/&gt;
Applicable to any of the &lt;a href=""https://nlp.johnsnowlabs.com/models?task=Relation+Extraction""&gt;20 + Relation Extractor models See here for an overview&lt;/a&gt;
&lt;code&gt;python
nlu.load(&amp;#39;med_ner.jsl.wip.clinical relation.temporal_events&amp;#39;).viz(&amp;#39;The patient developed cancer after a mercury poisoning in 1999 &amp;#39;)
&lt;/code&gt;
![Entity Relation visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/relation.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/relation.png&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;bigger example&lt;/h1&gt;

&lt;p&gt;data = &amp;#39;This is the case of a very pleasant 46-year-old Caucasian female, seen in clinic on 12/11/07 during which time MRI of the left shoulder showed no evidence of rotator cuff tear. She did have a previous MRI of the cervical spine that did show an osteophyte on the left C6-C7 level. Based on this, negative MRI of the shoulder, the patient was recommended to have anterior cervical discectomy with anterior interbody fusion at C6-C7 level. Operation, expected outcome, risks, and benefits were discussed with her. Risks include, but not exclusive of bleeding and infection, bleeding could be soft tissue bleeding, which may compromise airway and may result in return to the operating room emergently for evacuation of said hematoma. There is also the possibility of bleeding into the epidural space, which can compress the spinal cord and result in weakness and numbness of all four extremities as well as impairment of bowel and bladder function. However, the patient may develop deeper-seated infection, which may require return to the operating room. Should the infection be in the area of the spinal instrumentation, this will cause a dilemma since there might be a need to remove the spinal instrumentation and/or allograft. There is also the possibility of potential injury to the esophageus, the trachea, and the carotid artery. There is also the risks of stroke on the right cerebral circulation should an undiagnosed plaque be propelled from the right carotid. She understood all of these risks and agreed to have the procedure performed&amp;#39;
pipe = nlu.load(&amp;#39;med_ner.jsl.wip.clinical relation.clinical&amp;#39;).viz(data)
```
![Entity Relation visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/relation_big.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/relation_big.png&lt;/a&gt;)&lt;/p&gt;

&lt;h2&gt;Entity Resolution visualization for chunks&lt;/h2&gt;

&lt;p&gt;Visualizes resolutions of entities
Applicable to any of the &lt;a href=""https://nlp.johnsnowlabs.com/models?task=Entity+Resolution""&gt;100+ Resolver models See here for an overview&lt;/a&gt;
&lt;code&gt;python
nlu.load(&amp;#39;med_ner.jsl.wip.clinical resolve_chunk.rxnorm.in&amp;#39;).viz(&amp;quot;He took Prevacid 30 mg  daily&amp;quot;)
&lt;/code&gt;
![Chunk Resolution visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_chunk.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_chunk.png&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;bigger example&lt;/h1&gt;

&lt;p&gt;data = &amp;quot;This is an 82 - year-old male with a history of prior tobacco use , hypertension , chronic renal insufficiency , COPD , gastritis , and TIA who initially presented to Braintree with a non-ST elevation MI and Guaiac positive stools , transferred to St . Margaret\&amp;#39;s Center for Women &amp;amp; Infants for cardiac catheterization with PTCA to mid LAD lesion complicated by hypotension and bradycardia requiring Atropine , IV fluids and transient dopamine possibly secondary to vagal reaction , subsequently transferred to CCU for close monitoring , hemodynamically stable at the time of admission to the CCU .&amp;quot;
nlu.load(&amp;#39;med_ner.jsl.wip.clinical resolve_chunk.rxnorm.in&amp;#39;).viz(data)
```&lt;/p&gt;

&lt;p&gt;![Chunk Resolution visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_chunk_big.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_chunk_big.png&lt;/a&gt;)&lt;/p&gt;

&lt;h2&gt;Entity Resolution visualization for sentences&lt;/h2&gt;

&lt;p&gt;Visualizes resolutions of entities in sentences
Applicable to any of the &lt;a href=""https://nlp.johnsnowlabs.com/models?task=Entity+Resolution""&gt;100+ Resolver models See here for an overview&lt;/a&gt;
&lt;code&gt;python
nlu.load(&amp;#39;med_ner.jsl.wip.clinical resolve.icd10cm&amp;#39;).viz(&amp;#39;She was diagnosed with a respiratory congestion&amp;#39;)
&lt;/code&gt;
![Sentence Resolution visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_sentence.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_sentence.png&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;```python&lt;/p&gt;

&lt;h1&gt;bigger example&lt;/h1&gt;

&lt;p&gt;data = &amp;#39;The patient is a 5-month-old infant who presented initially on Monday with a cold, cough, and runny nose for 2 days. Mom states she had no fever. Her appetite was good but she was spitting up a lot. She had no difficulty breathing and her cough was described as dry and hacky. At that time, physical exam showed a right TM, which was red. Left TM was okay. She was fairly congested but looked happy and playful. She was started on Amoxil and Aldex and we told to recheck in 2 weeks to recheck her ear. Mom returned to clinic again today because she got much worse overnight. She was having difficulty breathing. She was much more congested and her appetite had decreased significantly today. She also spiked a temperature yesterday of 102.6 and always having trouble sleeping secondary to congestion&amp;#39;
nlu.load(&amp;#39;med_ner.jsl.wip.clinical resolve.icd10cm&amp;#39;).viz(data)
```
![Sentence Resolution visualization](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_sentence_big.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_sentence_big.png&lt;/a&gt;)&lt;/p&gt;

&lt;h2&gt;Configure visualizations&lt;/h2&gt;

&lt;h3&gt;Define custom colors for labels&lt;/h3&gt;

&lt;p&gt;Some entity and relation labels will be highlighted with a pre-defined color, which you &lt;a href=""https://github.com/JohnSnowLabs/spark-nlp-display/tree/main/sparknlp_display/label_colors""&gt;can find here&lt;/a&gt;.&lt;br/&gt;
For labels that have no color defined, a random color will be generated.&lt;br/&gt;
You can define colors for labels manually, by specifying via the &lt;code&gt;viz_colors&lt;/code&gt; parameter
and defining &lt;code&gt;hex color codes&lt;/code&gt; in a dictionary that maps &lt;code&gt;labels&lt;/code&gt; to &lt;code&gt;colors&lt;/code&gt; .
```python
data = &amp;#39;Dr. John Snow suggested that Fritz takes 5mg penicilin for his cough&amp;#39;&lt;/p&gt;

&lt;h1&gt;Define custom colors for labels&lt;/h1&gt;

&lt;p&gt;viz_colors={&amp;#39;STRENGTH&amp;#39;:&amp;#39;#800080&amp;#39;, &amp;#39;DRUG_BRANDNAME&amp;#39;:&amp;#39;#77b5fe&amp;#39;, &amp;#39;GENDER&amp;#39;:&amp;#39;#77ffe&amp;#39;}
nlu.load(&amp;#39;med_ner.jsl.wip.clinical&amp;#39;).viz(data,viz_colors =viz_colors)
```
![define colors labels](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/define_colors.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/define_colors.png&lt;/a&gt;)&lt;/p&gt;

&lt;h3&gt;Filter entities that get highlighted&lt;/h3&gt;

&lt;p&gt;By default every entity class will be visualized.&lt;br/&gt;
The &lt;code&gt;labels_to_viz&lt;/code&gt; can be used to define a set of labels to highlight.&lt;br/&gt;
Applicable for ner, resolution and assert.
```python
data = &amp;#39;Dr. John Snow suggested that Fritz takes 5mg penicilin for his cough&amp;#39;&lt;/p&gt;

&lt;h1&gt;Filter wich NER label to viz&lt;/h1&gt;

&lt;p&gt;labels_to_viz=[&amp;#39;SYMPTOM&amp;#39;]
nlu.load(&amp;#39;med_ner.jsl.wip.clinical&amp;#39;).viz(data,labels_to_viz=labels_to_viz)
```
![filter labels](&lt;a href=""https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/filter_labels.png""&gt;https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/filter_labels.png&lt;/a&gt;)&lt;/p&gt;

&lt;h2&gt;New models&lt;/h2&gt;

&lt;p&gt;New multilingual models for &lt;code&gt;Afrikaans&lt;/code&gt;, &lt;code&gt;Welsh&lt;/code&gt;, &lt;code&gt;Maltese&lt;/code&gt;, &lt;code&gt;Tamil&lt;/code&gt;, and&lt;code&gt;Vietnamese&lt;/code&gt;&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;nlu.load() Refrence&lt;/th&gt;
&lt;th&gt;Spark NLP Refrence&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/lemma_vi.html""&gt;vi.lemma&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/lemma_vi.html""&gt;lemma&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/lemma_mt.html""&gt;mt.lemma&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/lemma_mt.html""&gt;lemma&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/lemma_ta.html""&gt;ta.lemma&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/lemma_ta.html""&gt;lemma&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/lemma_af.html""&gt;af.lemma&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/lemma_af.html""&gt;lemma&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/06/pos_afribooms_af.html""&gt;af.pos&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/06/pos_afribooms_af.html""&gt;pos_afribooms&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/lemma_cy.html""&gt;cy.lemma&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/lemma_cy.html""&gt;lemma&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;Reworked and updated NLU tutorial notebooks&lt;/h2&gt;

&lt;p&gt;All of the &lt;a href=""https://github.com/JohnSnowLabs/nlu/tree/master/examples""&gt;140+ NLU tutorial Notebooks&lt;/a&gt; have been updated and reworked to reflect the latest changes in NLU 3.0.0+&lt;/p&gt;

&lt;h2&gt;Improved Column Name generation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;NLU categorized each internal component now with boolean labels for &lt;code&gt;name_deductable&lt;/code&gt; and &lt;code&gt;always_name_deductable&lt;/code&gt; .&lt;/li&gt;
&lt;li&gt;Before generating column names, NLU checks wether each component is of unique in the pipeline or not. If a component is not unique in the
pipe and there are multiple components of same type, i.e. multiple &lt;code&gt;NER&lt;/code&gt; models, NLU will deduct a base name for the final output columns from the
NLU reference each NER model is pointing to.&lt;/li&gt;
&lt;li&gt;If on the other hand, there is only one &lt;code&gt;NER&lt;/code&gt; model in the pipeline, only the default &lt;code&gt;ner&lt;/code&gt; column prefixed will be generated.&lt;/li&gt;
&lt;li&gt;For some components, like &lt;code&gt;embeddings&lt;/code&gt; and &lt;code&gt;classifiers&lt;/code&gt; are now defined as &lt;code&gt;always_name_deductable&lt;/code&gt;, for those NLU will always try to infer a meaningful base name for the output columns.&lt;/li&gt;
&lt;li&gt;Newly trained component output columns will now be prefixed with &lt;code&gt;trained_&amp;lt;type&amp;gt;&lt;/code&gt; , for types &lt;code&gt;pos&lt;/code&gt; , &lt;code&gt;ner&lt;/code&gt;, &lt;code&gt;cLassifier&lt;/code&gt;, &lt;code&gt;sentiment&lt;/code&gt; and &lt;code&gt;multi_classifier&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Enhanced offline mode&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;You can still load a model from a path as usual with &lt;code&gt;nlu.load(path=model_path)&lt;/code&gt; and output columns will be suffixed with &lt;code&gt;from_disk&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;You can now optionally also specify &lt;code&gt;request&lt;/code&gt; parameter during  load a model from HDD, it will be used to deduct more meaningful column name suffixes, instead of &lt;code&gt;from_disk&lt;/code&gt;, i.e. by calling &lt;code&gt;nlu.load(request =&amp;#39;en.embed_sentence.biobert.pubmed_pmc_base_cased&amp;#39;, path=model_path)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Bugfixes&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Fixed a bug that caused  resolution algorithms output level to be inferred incorrectly&lt;/li&gt;
&lt;li&gt;Fixed a bug that caused stranger cols got dropped&lt;/li&gt;
&lt;li&gt;Fixed a bug that caused endings to miss when  .predict(position=True) was specified&lt;/li&gt;
&lt;li&gt;Fixed a bug that caused pd.Series to be converted incorrectly internally&lt;/li&gt;
&lt;li&gt;Fixed a bug that caused output level transformations to crash&lt;/li&gt;
&lt;li&gt;Fixed a bug that caused verbose mode not to turn of properly after turning it on.&lt;/li&gt;
&lt;li&gt;fixed a bug that caused some models to crash when loaded for HDD&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;Additional NLU resources&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/tree/master/examples""&gt;140+ updates tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/docs/en/viz_examples""&gt;Updated visualization docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlp.johnsnowlabs.com/models""&gt;Models Hub&lt;/a&gt; with new models&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://medium.com/spark-nlp""&gt;Spark NLP publications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlp.johnsnowlabs.com/demo""&gt;NLU in Action&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/docs/en/install""&gt;NLU documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/spark-nlp/discussions""&gt;Discussions&lt;/a&gt; Engage with other community members, share ideas, and show off how you use Spark NLP and NLU!&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;1 line Install NLU on Google Colab&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;!wget https://setup.johnsnowlabs.com/nlu/colab.sh  -O - | bash&lt;/code&gt;&lt;/p&gt;

&lt;h1&gt;1 line Install NLU on Kaggle&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;!wget https://setup.johnsnowlabs.com/nlu/kaggle.sh  -O - | bash&lt;/code&gt;&lt;/p&gt;

&lt;h1&gt;Install via PIP&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;! pip install nlu pyspark==3.0.1&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nh4qzh,True,,CKL-IT,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nh4qzh/1_line_to_visualizations_for_dependency_trees/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nh4qzh/1_line_to_visualizations_for_dependency_trees/,30199,1621528247.0,0,,False,,,,,,16375
1137,,LanguageTechnology," I have to study the structure of questions comparing electronic products, e.g., Computers, Laptops, Tablets, etc.

Example questions:

1. What are the disadvantages of a Chromebook as opposed to a regular laptop computer?
2. what are the pros and cons of buying a dell laptop?
3. what can a $1000 apple laptop do which $500 Lenovo laptop not? is an expensive one worth it?
4. are hp laptops good for students?

I would be grateful If anyone can provide a lead to such a dataset. Currently, I am looking at questions on Quora, Reddit, and other forums manually.",t2_7nrnwrud,False,,0,False,Dataset to study structure of comparison questions,[],r/LanguageTechnology,False,6,,0,,False,t3_nh2flv,False,dark,1.0,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1621551522.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have to study the structure of questions comparing electronic products, e.g., Computers, Laptops, Tablets, etc.&lt;/p&gt;

&lt;p&gt;Example questions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;What are the disadvantages of a Chromebook as opposed to a regular laptop computer?&lt;/li&gt;
&lt;li&gt;what are the pros and cons of buying a dell laptop?&lt;/li&gt;
&lt;li&gt;what can a $1000 apple laptop do which $500 Lenovo laptop not? is an expensive one worth it?&lt;/li&gt;
&lt;li&gt;are hp laptops good for students?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I would be grateful If anyone can provide a lead to such a dataset. Currently, I am looking at questions on Quora, Reddit, and other forums manually.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nh2flv,True,,linuxjain,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nh2flv/dataset_to_study_structure_of_comparison_questions/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nh2flv/dataset_to_study_structure_of_comparison_questions/,30199,1621522722.0,0,,False,,,,,,561
1138,,LanguageTechnology,"If you have a specific use case that can benefit from knowledge graphs, please share below, we would love to learn about the different applications of NLP. If you have any question, please reply below or send an email at [admin@ubiai.tools](mailto:admin@ubiai.tools)

&amp;#x200B;

[https://walidamamou.medium.com/building-a-knowledge-graph-for-job-search-using-bert-transformer-8677c8b3a2e7](https://walidamamou.medium.com/building-a-knowledge-graph-for-job-search-using-bert-transformer-8677c8b3a2e7)",t2_32tnavmg,False,,0,False,Looking to extract insights from your unstructured text? Check out this article on how to combine NLP and knowledge graphs to uncover new information.,[],r/LanguageTechnology,False,6,,0,,False,t3_nhhp3y,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1621592178.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If you have a specific use case that can benefit from knowledge graphs, please share below, we would love to learn about the different applications of NLP. If you have any question, please reply below or send an email at [&lt;a href=""mailto:admin@ubiai.tools""&gt;admin@ubiai.tools&lt;/a&gt;](mailto:&lt;a href=""mailto:admin@ubiai.tools""&gt;admin@ubiai.tools&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://walidamamou.medium.com/building-a-knowledge-graph-for-job-search-using-bert-transformer-8677c8b3a2e7""&gt;https://walidamamou.medium.com/building-a-knowledge-graph-for-job-search-using-bert-transformer-8677c8b3a2e7&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nhhp3y,True,,UBIAI,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nhhp3y/looking_to_extract_insights_from_your/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nhhp3y/looking_to_extract_insights_from_your/,30199,1621563378.0,0,,False,,,,,,502
1139,,LanguageTechnology,"I'm talking about the little translate button at the top left (can't put images).

&amp;#x200B;

As I understood it takes the html and translates the text in it. What I can't comprehend is it fits the correct words perfectly, for example in the &lt;strong&gt; tag.

For example &lt;p&gt;there is &lt;strong&gt;some text here&lt;/strong&gt; google will translate it&lt;/p&gt;

After the translation google finds the exact match to put inside the &lt;strong&gt; tag. They can't translate it separately it would distort the meaning. Even sentence is complex and inside the tag does not make a sentence on its own they handle it somehow.

I tried to make it as clear as possible. I've been searching but I couldn't come up with a solution. Do you have any ideas? Please share.

&amp;#x200B;

Edit (Clarification)

&amp;#x200B;

We have a sentence: ""It’s recommended that you do that in a [virtual environment using virtualenv](https://pythonbasics.org/virtualenv/).""

&amp;#x200B;

as you can see ""virtual environment using virtualenv"" is let's say tagged. After translating this sentence google is able to tag the same piece in the target language. For example

&amp;#x200B;

Es wird empfohlen, dies in einer [virtuellen Umgebung mit virtualenv zu tun](https://pythonbasics.org/virtualenv/) .

&amp;#x200B;

How they can match the meaning and ""tag"" the correct positions? Obviously translation of the tagged part won't be same if it's done independently from the original sentence. So how they match it?",t2_3q5i07qa,False,,0,False,How does Google Translate's add-on translates a web page so perfectly?,[],r/LanguageTechnology,False,6,,0,,False,t3_nhbztd,False,dark,0.4,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,1621589300.0,,[],{},,True,,1621574794.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m talking about the little translate button at the top left (can&amp;#39;t put images).&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;As I understood it takes the html and translates the text in it. What I can&amp;#39;t comprehend is it fits the correct words perfectly, for example in the &amp;lt;strong&amp;gt; tag.&lt;/p&gt;

&lt;p&gt;For example &amp;lt;p&amp;gt;there is &amp;lt;strong&amp;gt;some text here&amp;lt;/strong&amp;gt; google will translate it&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;After the translation google finds the exact match to put inside the &amp;lt;strong&amp;gt; tag. They can&amp;#39;t translate it separately it would distort the meaning. Even sentence is complex and inside the tag does not make a sentence on its own they handle it somehow.&lt;/p&gt;

&lt;p&gt;I tried to make it as clear as possible. I&amp;#39;ve been searching but I couldn&amp;#39;t come up with a solution. Do you have any ideas? Please share.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Edit (Clarification)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;We have a sentence: &amp;quot;It’s recommended that you do that in a &lt;a href=""https://pythonbasics.org/virtualenv/""&gt;virtual environment using virtualenv&lt;/a&gt;.&amp;quot;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;as you can see &amp;quot;virtual environment using virtualenv&amp;quot; is let&amp;#39;s say tagged. After translating this sentence google is able to tag the same piece in the target language. For example&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Es wird empfohlen, dies in einer &lt;a href=""https://pythonbasics.org/virtualenv/""&gt;virtuellen Umgebung mit virtualenv zu tun&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;How they can match the meaning and &amp;quot;tag&amp;quot; the correct positions? Obviously translation of the tagged part won&amp;#39;t be same if it&amp;#39;s done independently from the original sentence. So how they match it?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nhbztd,True,,DoIHAVeaNIdenTItY,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nhbztd/how_does_google_translates_addon_translates_a_web/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nhbztd/how_does_google_translates_addon_translates_a_web/,30199,1621545994.0,0,,False,,,,,,1500
1140,,LanguageTechnology,"Hi, hi!

Basically, my case is the opposite as the one archived here:

https://www.reddit.com/r/LanguageTechnology/comments/8w06zn/msc_computational_linguistics_in_germany/

I come from a theoretical/computational physics background and would like to properly educate myself in computational linguistics. Linguistics has been much of a hobby for me, while math and programming were part of my curriculum. 

Now I am finishing my master's on physics of complex systems and had one introduction to speech and text analysis, as well as a deep learning course.

This last information is also relevant, since I wouldn't like paying the tution fees for a second master (Zweitstudiengebühren) which would be applied in BaWü (Heidelberg, Tübingen, Stuttgart)

Any recommendations here?",t2_62v3m7yv,False,,0,False,M.Sc. Computer linguistics in Germany,[],r/LanguageTechnology,False,6,,0,,False,t3_ngyzvo,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1621542828.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, hi!&lt;/p&gt;

&lt;p&gt;Basically, my case is the opposite as the one archived here:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.reddit.com/r/LanguageTechnology/comments/8w06zn/msc_computational_linguistics_in_germany/""&gt;https://www.reddit.com/r/LanguageTechnology/comments/8w06zn/msc_computational_linguistics_in_germany/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I come from a theoretical/computational physics background and would like to properly educate myself in computational linguistics. Linguistics has been much of a hobby for me, while math and programming were part of my curriculum. &lt;/p&gt;

&lt;p&gt;Now I am finishing my master&amp;#39;s on physics of complex systems and had one introduction to speech and text analysis, as well as a deep learning course.&lt;/p&gt;

&lt;p&gt;This last information is also relevant, since I wouldn&amp;#39;t like paying the tution fees for a second master (Zweitstudiengebühren) which would be applied in BaWü (Heidelberg, Tübingen, Stuttgart)&lt;/p&gt;

&lt;p&gt;Any recommendations here?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ngyzvo,True,,GokuPotenciaExtrema,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ngyzvo/msc_computer_linguistics_in_germany/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ngyzvo/msc_computer_linguistics_in_germany/,30199,1621514028.0,0,,False,,,,,,777
1141,,LanguageTechnology,,t2_2wsvqwhg,False,,0,False,A New Google Research Introduces FNet By Replacing Self-Attention Sublayers With Simple Linear Transformations Achieving 92% Accuracy and Runs Up To Seven Times Faster On GPUs And Twice As Fast On TPUs (Paper link in comments),[],r/LanguageTechnology,False,6,,0,,False,t3_ngsmcs,False,dark,0.63,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1621520210.0,text,6,,,text,marktechpost.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ngsmcs,True,,ai-lover,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ngsmcs/a_new_google_research_introduces_fnet_by/,all_ads,False,https://www.marktechpost.com/2021/05/19/a-new-google-research-introduces-fnet-by-replacing-self-attention-sublayers-with-simple-linear-transformations-achieving-92-accuracy-and-runs-up-to-seven-times-faster-on-gpus-and-twice-as-fast-on-tpus/,30199,1621491410.0,0,,False,https://www.marktechpost.com/2021/05/19/a-new-google-research-introduces-fnet-by-replacing-self-attention-sublayers-with-simple-linear-transformations-achieving-92-accuracy-and-runs-up-to-seven-times-faster-on-gpus-and-twice-as-fast-on-tpus/,,,,,0
1142,,LanguageTechnology,"Hello! Been playing around with summary generators, and I'm pretty sure I came up with a novel approach, which should produce a summary that has the most resemblance to an author's typical writing style. Basically I'm just trying to find out if there's a word for this algo yet or not. Project is called Bite, and can be found [here](https://github.com/cyberrumor/bite). 

&amp;#x200B;

Say we have the following corpus:  


""Mary had a little lamb.""  


Bite will tokenize the sentence like so:  


* mary had
* mary had a 
* mary had a little
* mary had a little lamb
* had a
* had a little
* had a little lamb
* a little
* a little lamb
* a lamb

Next, it creates a frequency map to count all occurrences of each token, assigning scores to sentences by adding up the sum of token scores. 

&amp;#x200B;

Is there a word for this type of categorization?",t2_kulz0,False,,0,False,Is this approach to summary production novel?,[],r/LanguageTechnology,False,6,,0,,False,t3_nggaag,False,dark,0.75,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1621485211.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! Been playing around with summary generators, and I&amp;#39;m pretty sure I came up with a novel approach, which should produce a summary that has the most resemblance to an author&amp;#39;s typical writing style. Basically I&amp;#39;m just trying to find out if there&amp;#39;s a word for this algo yet or not. Project is called Bite, and can be found &lt;a href=""https://github.com/cyberrumor/bite""&gt;here&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Say we have the following corpus:  &lt;/p&gt;

&lt;p&gt;&amp;quot;Mary had a little lamb.&amp;quot;  &lt;/p&gt;

&lt;p&gt;Bite will tokenize the sentence like so:  &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;mary had&lt;/li&gt;
&lt;li&gt;mary had a &lt;/li&gt;
&lt;li&gt;mary had a little&lt;/li&gt;
&lt;li&gt;mary had a little lamb&lt;/li&gt;
&lt;li&gt;had a&lt;/li&gt;
&lt;li&gt;had a little&lt;/li&gt;
&lt;li&gt;had a little lamb&lt;/li&gt;
&lt;li&gt;a little&lt;/li&gt;
&lt;li&gt;a little lamb&lt;/li&gt;
&lt;li&gt;a lamb&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Next, it creates a frequency map to count all occurrences of each token, assigning scores to sentences by adding up the sum of token scores. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Is there a word for this type of categorization?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nggaag,True,,cyberrumor,,4,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/nggaag/is_this_approach_to_summary_production_novel/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nggaag/is_this_approach_to_summary_production_novel/,30199,1621456411.0,0,,False,,,,,,855
1143,,LanguageTechnology,,t2_hkv9s,False,,0,False,Data Augmentation Techniques in NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_nfzwl5,False,dark,0.9,,public,16,0,{},,False,[],,False,False,,{},,False,16,,False,False,,False,,[],{},,False,,1621440834.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nfzwl5,True,,prakhar21,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nfzwl5/data_augmentation_techniques_in_nlp/,all_ads,False,https://youtube.com/playlist?list=PLsAqq9lZFOtUg63g_95OuV-R2GhV1UiIZ,30199,1621412034.0,0,,False,https://youtube.com/playlist?list=PLsAqq9lZFOtUg63g_95OuV-R2GhV1UiIZ,,,,,0
1144,,LanguageTechnology,,t2_oro2l,False,,0,False,What are some resources to learn about incorporating images into word vectorization model?,[],r/LanguageTechnology,False,6,,0,,False,t3_ngcis0,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1621475888.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ngcis0,True,,krews2,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ngcis0/what_are_some_resources_to_learn_about/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ngcis0/what_are_some_resources_to_learn_about/,30199,1621447088.0,0,,False,,,,,,0
1145,,LanguageTechnology,,t2_79ir5cir,False,,0,False,Neural nets for word sense disambiguation?,[],r/LanguageTechnology,False,6,,0,,False,t3_ng6e8v,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,False,,1621461033.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ng6e8v,True,,c_metaphorique,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ng6e8v/neural_nets_for_word_sense_disambiguation/,all_ads,False,/r/learnmachinelearning/comments/ng6dfp/neural_nets_for_word_sense_disambiguation/,30199,1621432233.0,0,,False,/r/learnmachinelearning/comments/ng6dfp/neural_nets_for_word_sense_disambiguation/,"[{'approved_at_utc': None, 'subreddit': 'learnmachinelearning', 'selftext': ""Hello. \n\nDoes anyone know of any recent work in trying to use artificial neural networks to disambiguate different senses of function words? E.g., the *of* in `Mother of the bride` is different from the *of* in `Eliza is of Dutch heritage` is different from the *of* in `The Brooklyn Borough of New York City` is different from the *of* in `The New York City borough of Brooklyn`. \n\nI've found some papers that deal with disambiguating content words (e.g., *bank*, the financial institution, vs *bank*, a feature of geography), but none that deal with function words. \n\nThanks in advance."", 'author_fullname': 't2_79ir5cir', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Neural nets for word sense disambiguation?', 'link_flair_richtext': [{'e': 'text', 't': 'Question'}], 'subreddit_name_prefixed': 'r/learnmachinelearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_ng6dfp', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Question', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1621460980.0, 'link_flair_type': 'richtext', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.learnmachinelearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello. &lt;/p&gt;\n\n&lt;p&gt;Does anyone know of any recent work in trying to use artificial neural networks to disambiguate different senses of function words? E.g., the &lt;em&gt;of&lt;/em&gt; in &lt;code&gt;Mother of the bride&lt;/code&gt; is different from the &lt;em&gt;of&lt;/em&gt; in &lt;code&gt;Eliza is of Dutch heritage&lt;/code&gt; is different from the &lt;em&gt;of&lt;/em&gt; in &lt;code&gt;The Brooklyn Borough of New York City&lt;/code&gt; is different from the &lt;em&gt;of&lt;/em&gt; in &lt;code&gt;The New York City borough of Brooklyn&lt;/code&gt;. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve found some papers that deal with disambiguating content words (e.g., &lt;em&gt;bank&lt;/em&gt;, the financial institution, vs &lt;em&gt;bank&lt;/em&gt;, a feature of geography), but none that deal with function words. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'ec81b8ee-accf-11e9-b8f8-0ebea2df7d78', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_3cqa1', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ffb000', 'id': 'ng6dfp', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'c_metaphorique', 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/learnmachinelearning/comments/ng6dfp/neural_nets_for_word_sense_disambiguation/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/learnmachinelearning/comments/ng6dfp/neural_nets_for_word_sense_disambiguation/', 'subreddit_subscribers': 232286, 'created_utc': 1621432180.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_ng6dfp,,,0
1146,,LanguageTechnology,"I'm curious as to the best way to tag the syntactic structures of some text. So not just POS tags, but things like complement and relative clauses? Are there taggers specifically for this?",t2_48eyl33r,False,,0,False,Automatically label syntactic structures in text,[],r/LanguageTechnology,False,6,,0,,False,t3_ng3bic,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1621452665.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m curious as to the best way to tag the syntactic structures of some text. So not just POS tags, but things like complement and relative clauses? Are there taggers specifically for this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ng3bic,True,,crowpup783,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ng3bic/automatically_label_syntactic_structures_in_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ng3bic/automatically_label_syntactic_structures_in_text/,30199,1621423865.0,0,,False,,,,,,188
1147,,LanguageTechnology,,t2_cwydf,False,,0,False,MUM: a new AI milestone for understanding information,[],r/LanguageTechnology,False,6,,0,,False,t3_nfjtj1,False,dark,0.92,,public,30,0,{},,False,[],,False,False,,{},,False,30,,False,False,,False,,[],{},,False,,1621394992.0,text,6,,,text,blog.google,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nfjtj1,True,,hedekar,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nfjtj1/mum_a_new_ai_milestone_for_understanding/,all_ads,False,https://blog.google/products/search/introducing-mum/,30199,1621366192.0,0,,False,https://blog.google/products/search/introducing-mum/,,,,,0
1148,,LanguageTechnology,,t2_97mkm0fp,False,,0,False,Māori are trying to save their language from Big Tech,[],r/LanguageTechnology,False,6,,0,,False,t3_nfkiao,False,dark,0.83,,public,19,0,{},,False,[],,False,False,,{},,False,19,,False,True,,False,,[],{},,False,,1621396622.0,text,6,,,text,wired.co.uk,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nfkiao,True,,Madame_President_,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nfkiao/māori_are_trying_to_save_their_language_from_big/,all_ads,False,https://www.wired.co.uk/article/maori-language-tech,30199,1621367822.0,0,,False,https://www.wired.co.uk/article/maori-language-tech,,,,,0
1149,,LanguageTechnology,"Hi all,

I have trained a simple multi-input NN. I have 4 inputs ( one text field &amp; other 3 categorical variables : cat1, cat2 , cat3).

It is a classification model.

For the text field, I use Glove embeddings in the Embedding layer, followed by LSTM layer.

For the other 3 categorical fields, I just encode them in a dense layer. Finally, I concatenate these 2 layers followed by softmax for classification. Below is the main block of code :

&amp;#x200B;

    embeddings_dictionary = dict()
    glove_file = open('glove.6B.100d.txt', encoding=""utf8"")
    for line in glove_file:
        records = line.split()
        word = records[0]
        vector_dimensions = asarray(records[1:], dtype='float32')
        embeddings_dictionary[word] = vector_dimensions
    glove_file.close()
    embedding_matrix = zeros((vocab_size, 100))
    for word, index in tokenizer.word_index.items():
        embedding_vector = embeddings_dictionary.get(word)
        if embedding_vector is not None:
            embedding_matrix[index] = embedding_vector
    
    # Text input handling
    text_input_1 = Input(shape=(maxlen,))
    embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(text_input_1)
    LSTM_Layer_1 = LSTM(128)(embedding_layer)
    
    
    # categorical variables handling
    layout_input_2 = Input(shape=(3,))
    dense_layer_1 = Dense(10, activation='relu')(layout_input_2)
    dense_layer_2 = Dense(10, activation='relu')(dense_layer_1)
    
    # Concatenating the above 2
    concat_layer = Concatenate()([LSTM_Layer_1, dense_layer_2])
    dense_layer_3 = Dense(10, activation='relu')(concat_layer)
    output = Dense(3, activation='softmax')(dense_layer_3)
    model = Model(inputs=[text_input_1, layout_input_2], outputs=output)
    
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
    print(model.summary())

The problem is , when I infer/predict using the above model weights, the respose is like all the weightage is being given to the categorical variables only , irrespective of the text input.

e.g. predict (text1, cat1, cat2, cat3)  &amp;  predict (text2, cat1, cat2, cat3)  is exactly same.

Even if I provide blank text as input, the output remains same if I don't change the categorical variable values.

Can anybody help troubleshooting this ?",t2_3ckj43a7,False,,0,False,Debugging a NN model,[],r/LanguageTechnology,False,6,,0,,False,t3_nfxmqa,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1621432783.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I have trained a simple multi-input NN. I have 4 inputs ( one text field &amp;amp; other 3 categorical variables : cat1, cat2 , cat3).&lt;/p&gt;

&lt;p&gt;It is a classification model.&lt;/p&gt;

&lt;p&gt;For the text field, I use Glove embeddings in the Embedding layer, followed by LSTM layer.&lt;/p&gt;

&lt;p&gt;For the other 3 categorical fields, I just encode them in a dense layer. Finally, I concatenate these 2 layers followed by softmax for classification. Below is the main block of code :&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;embeddings_dictionary = dict()
glove_file = open(&amp;#39;glove.6B.100d.txt&amp;#39;, encoding=&amp;quot;utf8&amp;quot;)
for line in glove_file:
    records = line.split()
    word = records[0]
    vector_dimensions = asarray(records[1:], dtype=&amp;#39;float32&amp;#39;)
    embeddings_dictionary[word] = vector_dimensions
glove_file.close()
embedding_matrix = zeros((vocab_size, 100))
for word, index in tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

# Text input handling
text_input_1 = Input(shape=(maxlen,))
embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(text_input_1)
LSTM_Layer_1 = LSTM(128)(embedding_layer)


# categorical variables handling
layout_input_2 = Input(shape=(3,))
dense_layer_1 = Dense(10, activation=&amp;#39;relu&amp;#39;)(layout_input_2)
dense_layer_2 = Dense(10, activation=&amp;#39;relu&amp;#39;)(dense_layer_1)

# Concatenating the above 2
concat_layer = Concatenate()([LSTM_Layer_1, dense_layer_2])
dense_layer_3 = Dense(10, activation=&amp;#39;relu&amp;#39;)(concat_layer)
output = Dense(3, activation=&amp;#39;softmax&amp;#39;)(dense_layer_3)
model = Model(inputs=[text_input_1, layout_input_2], outputs=output)

model.compile(loss=&amp;#39;categorical_crossentropy&amp;#39;, optimizer=&amp;#39;adam&amp;#39;, metrics=[&amp;#39;acc&amp;#39;])
print(model.summary())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The problem is , when I infer/predict using the above model weights, the respose is like all the weightage is being given to the categorical variables only , irrespective of the text input.&lt;/p&gt;

&lt;p&gt;e.g. predict (text1, cat1, cat2, cat3)  &amp;amp;  predict (text2, cat1, cat2, cat3)  is exactly same.&lt;/p&gt;

&lt;p&gt;Even if I provide blank text as input, the output remains same if I don&amp;#39;t change the categorical variable values.&lt;/p&gt;

&lt;p&gt;Can anybody help troubleshooting this ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nfxmqa,True,,lonewolf_9,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nfxmqa/debugging_a_nn_model/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nfxmqa/debugging_a_nn_model/,30199,1621403983.0,0,,False,,,,,,2347
1150,,LanguageTechnology,"Hello all,

Recently I wrote an article about deploying spaCy with FastAPI for NER. As many people told me it was helpful, I did a new article about deploying transformer-based models with FastAPI for text classification (using Facebook's Bart Large MNLI model).

FastAPI  is a great is great framework for API development in Python in my opinion. It helped me save a lot of troubles when developing the [NLPCloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=d13c16ae-b7d8-11eb-8529-0242ac130003) API.

Here's the article:

[https://nlpcloud.io/nlp-machine-learning-classification-api-production-fastapi-transformers-nlpcloud.html](https://nlpcloud.io/nlp-machine-learning-classification-api-production-fastapi-transformers-nlpcloud.html?utm_source=reddit&amp;utm_campaign=d13c16ae-b7d8-11eb-8529-0242ac130003)

I'd love to have your feedback on this! Have you ever deployed transformer-based models to production? If so, which tools did you use?

Thanks!",t2_4z4m2qcs,False,,0,False,Deploying a transformer-based text classification NLP model with FastAPI,[],r/LanguageTechnology,False,6,,0,,False,t3_nfa5ek,False,dark,0.94,,public,16,0,{},,False,[],,False,False,,{},,False,16,,False,False,,1621344253.0,,[],{},,True,,1621371728.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all,&lt;/p&gt;

&lt;p&gt;Recently I wrote an article about deploying spaCy with FastAPI for NER. As many people told me it was helpful, I did a new article about deploying transformer-based models with FastAPI for text classification (using Facebook&amp;#39;s Bart Large MNLI model).&lt;/p&gt;

&lt;p&gt;FastAPI  is a great is great framework for API development in Python in my opinion. It helped me save a lot of troubles when developing the &lt;a href=""https://nlpcloud.io/?utm_source=reddit&amp;amp;utm_campaign=d13c16ae-b7d8-11eb-8529-0242ac130003""&gt;NLPCloud.io&lt;/a&gt; API.&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s the article:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://nlpcloud.io/nlp-machine-learning-classification-api-production-fastapi-transformers-nlpcloud.html?utm_source=reddit&amp;amp;utm_campaign=d13c16ae-b7d8-11eb-8529-0242ac130003""&gt;https://nlpcloud.io/nlp-machine-learning-classification-api-production-fastapi-transformers-nlpcloud.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;d love to have your feedback on this! Have you ever deployed transformer-based models to production? If so, which tools did you use?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nfa5ek,True,,juliensalinas,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nfa5ek/deploying_a_transformerbased_text_classification/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nfa5ek/deploying_a_transformerbased_text_classification/,30199,1621342928.0,0,,False,,,,,,970
1151,,LanguageTechnology,,t2_hkv9s,False,,0,False,Explanability for Transformers with Transformers-Interpret — A Model Explainability Tool,[],r/LanguageTechnology,False,6,,0,,False,t3_nf5o8o,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,False,,1621357102.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nf5o8o,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nf5o8o/explanability_for_transformers_with/,all_ads,False,https://link.medium.com/cNDQwZ84lgb,30199,1621328302.0,0,,False,https://link.medium.com/cNDQwZ84lgb,,,,,0
1152,,LanguageTechnology,"So I'm looking to create a subtitle file from a film that is only in Portuguese. 

I can do it by hand but that would take sooo long. 

I figure that I can probably extract the audio and use Google's transcription API to do a majority of the work for me, but I'm not sure how to go about doing that with a long length file.

Any tips?",t2_4un59,False,,0,False,Long Form Automatic Transcription from Audio (Transcribe Movie Dialogue to Text) - How Do I Do It?,[],r/LanguageTechnology,False,6,,0,,False,t3_nf3vmz,False,dark,1.0,,public,8,1,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{'gid_1': 1},,True,,1621350363.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I&amp;#39;m looking to create a subtitle file from a film that is only in Portuguese. &lt;/p&gt;

&lt;p&gt;I can do it by hand but that would take sooo long. &lt;/p&gt;

&lt;p&gt;I figure that I can probably extract the audio and use Google&amp;#39;s transcription API to do a majority of the work for me, but I&amp;#39;m not sure how to go about doing that with a long length file.&lt;/p&gt;

&lt;p&gt;Any tips?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nf3vmz,True,,MattyXarope,,14,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nf3vmz/long_form_automatic_transcription_from_audio/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nf3vmz/long_form_automatic_transcription_from_audio/,30199,1621321563.0,0,,False,,,,,,334
1153,,LanguageTechnology,,t2_hkv9s,False,,0,False,Text Summarisation Techniques in NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_neo5lz,False,dark,0.92,,public,20,0,{},,False,[],,False,False,,{},,False,20,,False,False,,False,,[],{},,False,,1621305562.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,neo5lz,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/neo5lz/text_summarisation_techniques_in_nlp/,all_ads,False,https://youtube.com/playlist?list=PLsAqq9lZFOtV8jYq3JlkqPQUN5QxcWq0f,30199,1621276762.0,0,,False,https://youtube.com/playlist?list=PLsAqq9lZFOtV8jYq3JlkqPQUN5QxcWq0f,,,,,0
1154,,LanguageTechnology,,t2_y7ny0,False,,0,False,The importance of language technologies for the future of the public health system (in Spanish),[],r/LanguageTechnology,False,6,,0,,False,t3_nf73gw,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,False,,1621362322.0,text,6,,,text,theconversation.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nf73gw,True,,luisgasco,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nf73gw/the_importance_of_language_technologies_for_the/,all_ads,False,https://theconversation.com/el-desarrollo-de-las-tecnologias-del-lenguaje-para-el-futuro-de-la-sanidad-159013,30199,1621333522.0,0,,False,https://theconversation.com/el-desarrollo-de-las-tecnologias-del-lenguaje-para-el-futuro-de-la-sanidad-159013,,,,,0
1155,,LanguageTechnology,"If anyone has any experience with this I would love to pick your brain / ask you a little assistance. 

Been struggling to correctly initialize my model.",t2_44md8gjv,False,,0,False,Spacy 3.0 multi class Textcat,[],r/LanguageTechnology,False,6,,0,,False,t3_nerxde,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1621314460.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If anyone has any experience with this I would love to pick your brain / ask you a little assistance. &lt;/p&gt;

&lt;p&gt;Been struggling to correctly initialize my model.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nerxde,True,,washknoxnash2255,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nerxde/spacy_30_multi_class_textcat/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nerxde/spacy_30_multi_class_textcat/,30199,1621285660.0,0,,False,,,,,,153
1156,,LanguageTechnology,"Hello all! Currently have a Spacy NER model that looks for 6 custom entities, while only two (Person and Organization) are truly important.

Would it be beneficial to cut out the other four entities in the model? My boss added them because he thought they would help with accuracy essentially through lessening the chance of false positives for person and organization. 

I personally believe the model would perform fine with only Person and organization, but am new to data science so just wanted to verify my hunch.",t2_44md8gjv,False,,0,False,Spacy NER Model 2 Entities versus 6,[],r/LanguageTechnology,False,6,,0,,False,t3_nerh99,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1621313385.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all! Currently have a Spacy NER model that looks for 6 custom entities, while only two (Person and Organization) are truly important.&lt;/p&gt;

&lt;p&gt;Would it be beneficial to cut out the other four entities in the model? My boss added them because he thought they would help with accuracy essentially through lessening the chance of false positives for person and organization. &lt;/p&gt;

&lt;p&gt;I personally believe the model would perform fine with only Person and organization, but am new to data science so just wanted to verify my hunch.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nerh99,True,,washknoxnash2255,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nerh99/spacy_ner_model_2_entities_versus_6/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nerh99/spacy_ner_model_2_entities_versus_6/,30199,1621284585.0,0,,False,,,,,,518
1157,,LanguageTechnology,"My final year project is on Multi-class text classification and simply explores existing techniques (TF-IDF and Word2Vec) on a new and different dataset. Pipeline is standard: Data Pre-processing and Cleaning, Vectorization and Dimensionality Reduction, Model splitting and Training, Hyperparameter tuning, model re-training and finally model evaluation.

Is it worthy of sending it as a paper to conferences even though it is nothing novel technique-wise but is on a different dataset?

Any guidance would be appreciated!",t2_581q2gxy,False,,0,False,Is it conference paper worthy?,[],r/LanguageTechnology,False,6,,0,,False,t3_nehdv5,False,dark,0.88,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1621289684.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My final year project is on Multi-class text classification and simply explores existing techniques (TF-IDF and Word2Vec) on a new and different dataset. Pipeline is standard: Data Pre-processing and Cleaning, Vectorization and Dimensionality Reduction, Model splitting and Training, Hyperparameter tuning, model re-training and finally model evaluation.&lt;/p&gt;

&lt;p&gt;Is it worthy of sending it as a paper to conferences even though it is nothing novel technique-wise but is on a different dataset?&lt;/p&gt;

&lt;p&gt;Any guidance would be appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nehdv5,True,,jgj0707,,10,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nehdv5/is_it_conference_paper_worthy/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nehdv5/is_it_conference_paper_worthy/,30199,1621260884.0,0,,False,,,,,,522
1158,,LanguageTechnology,"I am doing my individual research on automatic question generation from the paragraphs which will be input in to the system. I have divided this idea into three steps,

1. sentence selection
2. complex sentence simplification
3. sentence classification based on POS and NE tagged information

I would be grateful to hear your opinions based on the methodology which i am going to use. Please be kind enough to mention if i need to upgrade the methodology or any other changes that i should consider.",t2_8ve5eb9h,False,,0,False,Automatic question generation from input paragraph,[],r/LanguageTechnology,False,6,,0,,False,t3_neav37,False,dark,1.0,,public,14,0,{},,False,[],,False,False,,{},,False,14,,False,False,,False,,[],{},,True,,1621269815.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am doing my individual research on automatic question generation from the paragraphs which will be input in to the system. I have divided this idea into three steps,&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;sentence selection&lt;/li&gt;
&lt;li&gt;complex sentence simplification&lt;/li&gt;
&lt;li&gt;sentence classification based on POS and NE tagged information&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I would be grateful to hear your opinions based on the methodology which i am going to use. Please be kind enough to mention if i need to upgrade the methodology or any other changes that i should consider.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,neav37,True,,Visual-Milk8009,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/neav37/automatic_question_generation_from_input_paragraph/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/neav37/automatic_question_generation_from_input_paragraph/,30199,1621241015.0,0,,False,,,,,,499
1159,,LanguageTechnology,,t2_atcn4bta,False,,0,False,IBM's Project Codenet will teach AI to code in dozens of programming languages. Extremely vast dataset,[],r/LanguageTechnology,False,6,,0,,False,t3_nep2tj,False,dark,0.75,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1621307731.0,text,6,,,text,artificialintelligence-news.com,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nep2tj,True,,abbumm,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nep2tj/ibms_project_codenet_will_teach_ai_to_code_in/,all_ads,False,https://artificialintelligence-news.com/2021/05/11/ibm-project-codenet-wants-teach-ai-how-code/,30199,1621278931.0,0,,False,https://artificialintelligence-news.com/2021/05/11/ibm-project-codenet-wants-teach-ai-how-code/,,,,,0
1160,,LanguageTechnology,"Are there any tools that determine a word's difficulty? GPT3 has this thing where it can simplify things so that a 2nd grader would understand, how would it distinguish a word's level of complexity?",t2_be2v1j11,False,,0,False,Are there tools that tell a word's difficulty?,[],r/LanguageTechnology,False,6,,0,,False,t3_nehs9k,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1621290646.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Are there any tools that determine a word&amp;#39;s difficulty? GPT3 has this thing where it can simplify things so that a 2nd grader would understand, how would it distinguish a word&amp;#39;s level of complexity?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nehs9k,True,,ricksElar,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nehs9k/are_there_tools_that_tell_a_words_difficulty/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nehs9k/are_there_tools_that_tell_a_words_difficulty/,30199,1621261846.0,0,,False,,,,,,198
1161,,LanguageTechnology,"Hello. I recently decided to start exploring the use of NLP for solving language and machine translation problems.

I'm familiar with Python but don't really know my way around what NLP libraries are out there as of 2021. **Does anyone have a list of must-know Python-based NLP libraries (I assume Python is the best tool for NLP) and NLP libraries specific to machine translation?**

Thank you in advance.",t2_hwnig,False,,0,False,New to NLP. Looking for library recommendations.,[],r/LanguageTechnology,False,6,,0,,False,t3_ndz50q,False,dark,0.94,,public,14,0,{},,False,[],,False,False,,{},,False,14,,False,False,,False,,[],{},,True,,1621231271.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello. I recently decided to start exploring the use of NLP for solving language and machine translation problems.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m familiar with Python but don&amp;#39;t really know my way around what NLP libraries are out there as of 2021. &lt;strong&gt;Does anyone have a list of must-know Python-based NLP libraries (I assume Python is the best tool for NLP) and NLP libraries specific to machine translation?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Thank you in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ndz50q,True,,PeleMaradona,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ndz50q/new_to_nlp_looking_for_library_recommendations/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ndz50q/new_to_nlp_looking_for_library_recommendations/,30199,1621202471.0,0,,False,,,,,,406
1162,,LanguageTechnology,"I am a sophomore and have studied ML and DL for last 6 months. Recently I started working on NLP. I was wondering how these models (BERT, GPT) are created. Following are some questions

1. Is it possible to create my own model? I was thinking of making a hybrid of encoders and decoders of transformers. 

2. Is it even feasible to create a model at my stage? 

3. How much time would it take to create it?",t2_89k44e4d,False,,0,False,How to create a model like BERT or GPT?,[],r/LanguageTechnology,False,6,,0,,False,t3_ne5mkc,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1621251150.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am a sophomore and have studied ML and DL for last 6 months. Recently I started working on NLP. I was wondering how these models (BERT, GPT) are created. Following are some questions&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Is it possible to create my own model? I was thinking of making a hybrid of encoders and decoders of transformers. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is it even feasible to create a model at my stage? &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How much time would it take to create it?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ne5mkc,True,,maisterdetemplar,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ne5mkc/how_to_create_a_model_like_bert_or_gpt/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ne5mkc/how_to_create_a_model_like_bert_or_gpt/,30199,1621222350.0,0,,False,,,,,,406
1163,,LanguageTechnology,"Hi, I'm currently using a dataset, that has 6 different target labels. The number of instance of target 1-6 is 20k, 16k, 3k, 2k, 1.5k, 1.2k.

I'm making use of fastText to classify. The model gets over-fitted and for most of times gives the targets that have 20k (target 1) or 16k (target 2) instances.

The accuracy given by fastText is 90% but that is because of over fitting.
In another try, I provided (almost) equal amount of all instances ( &lt;= 2k ) for each target label and the accuracy had become 78%.

Is there any better way of handling this problem without neglecting thousands of instances?

Any suggestions would be helpful!",t2_8n5d56qx,False,,0,False,Techniques to handle over-fitting with text classifier,[],r/LanguageTechnology,False,6,,0,,False,t3_ndw6ys,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1621223024.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m currently using a dataset, that has 6 different target labels. The number of instance of target 1-6 is 20k, 16k, 3k, 2k, 1.5k, 1.2k.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m making use of fastText to classify. The model gets over-fitted and for most of times gives the targets that have 20k (target 1) or 16k (target 2) instances.&lt;/p&gt;

&lt;p&gt;The accuracy given by fastText is 90% but that is because of over fitting.
In another try, I provided (almost) equal amount of all instances ( &amp;lt;= 2k ) for each target label and the accuracy had become 78%.&lt;/p&gt;

&lt;p&gt;Is there any better way of handling this problem without neglecting thousands of instances?&lt;/p&gt;

&lt;p&gt;Any suggestions would be helpful!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ndw6ys,True,,ChandlerBingggggggg,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ndw6ys/techniques_to_handle_overfitting_with_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ndw6ys/techniques_to_handle_overfitting_with_text/,30199,1621194224.0,0,,False,,,,,,640
1164,,LanguageTechnology,"This question might be inappropriate here, but IDK where else to ask it. I have trouble getting a hugging face model to work, but I do have sufficient data and not enough time. I want to 'predict' a sentence from a previous one, or a sentence fragment. What is the simplest thing that I can do with NLTK and counters that is still useful?",t2_7bzamk8n,False,,0,False,Is a primitive method using NLTK and counters viable?,[],r/LanguageTechnology,False,6,,0,,False,t3_ndq9fq,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1621206340.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This question might be inappropriate here, but IDK where else to ask it. I have trouble getting a hugging face model to work, but I do have sufficient data and not enough time. I want to &amp;#39;predict&amp;#39; a sentence from a previous one, or a sentence fragment. What is the simplest thing that I can do with NLTK and counters that is still useful?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ndq9fq,True,,reniairtanitram,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ndq9fq/is_a_primitive_method_using_nltk_and_counters/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ndq9fq/is_a_primitive_method_using_nltk_and_counters/,30199,1621177540.0,0,,False,,,,,,338
1165,,LanguageTechnology,"This blog lists out (All?) popular Unsupervised Keyword Extraction Algorithms in NLP. 

Here, I summarize almost 10 papers w.r.t all these techniques. Enjoy the read! 🎉 


https://link.medium.com/4ah0jdgXhgb",t2_hkv9s,False,,0,False,10 popular Keyword Extraction Techniques in NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_nd8q5a,False,dark,0.86,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1621142252.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This blog lists out (All?) popular Unsupervised Keyword Extraction Algorithms in NLP. &lt;/p&gt;

&lt;p&gt;Here, I summarize almost 10 papers w.r.t all these techniques. Enjoy the read! 🎉 &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://link.medium.com/4ah0jdgXhgb""&gt;https://link.medium.com/4ah0jdgXhgb&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nd8q5a,True,,prakhar21,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nd8q5a/10_popular_keyword_extraction_techniques_in_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nd8q5a/10_popular_keyword_extraction_techniques_in_nlp/,30199,1621113452.0,0,,False,,,,,,207
1166,,LanguageTechnology,"Has anyone here gotten LexiDB to work? I recently found LexiDB as a potenialy corpus querying alternative to CWB's CQP.

These tools allow you to query large corpora (my current largest corpus has 800 million tokens in it) for patterns. For example, you could search for ""the &lt;adjective&gt; *"" and the tool will quickly return a list of matches.

I have CQP working already but LexiDB sounds a little better for my needs. Unfortunately so far I can't get it to work.",t2_2o4k3icq,False,,0,False,CQP vs LexiDB,[],r/LanguageTechnology,False,6,,0,,False,t3_nd8ra2,False,dark,1.0,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1621142343.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Has anyone here gotten LexiDB to work? I recently found LexiDB as a potenialy corpus querying alternative to CWB&amp;#39;s CQP.&lt;/p&gt;

&lt;p&gt;These tools allow you to query large corpora (my current largest corpus has 800 million tokens in it) for patterns. For example, you could search for &amp;quot;the &amp;lt;adjective&amp;gt; *&amp;quot; and the tool will quickly return a list of matches.&lt;/p&gt;

&lt;p&gt;I have CQP working already but LexiDB sounds a little better for my needs. Unfortunately so far I can&amp;#39;t get it to work.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nd8ra2,True,,anlinguist,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nd8ra2/cqp_vs_lexidb/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nd8ra2/cqp_vs_lexidb/,30199,1621113543.0,0,,False,,,,,,469
1167,,LanguageTechnology,"In short: I listen to lots of interesting podcasts, but with my limited human memory, I tend to forget some of the juicier details that I would like to remember.. My solution to this would be to summarize podcast episodes that I find memorable so that a quick glimpse at the highlights would bring the topics back to memory. However, instead of spending time doing manual summations, I would like to practice some ML/NLP applications by making my own podcast summarizer.

My thought is to divide the problem into two main steps:

1. Audio to transcript
2. Transcript summation/extraction of main topics

What ideas and thoughts do you all having regarding which models/libraries to use for the two steps? Or any other inputs on how to approach it?

Super excited to hear your thoughts!",t2_uvm35oi,False,,0,False,Ides for making a podcast summarizer?,[],r/LanguageTechnology,False,6,,0,,False,t3_nd3pff,False,dark,0.91,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1621127744.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In short: I listen to lots of interesting podcasts, but with my limited human memory, I tend to forget some of the juicier details that I would like to remember.. My solution to this would be to summarize podcast episodes that I find memorable so that a quick glimpse at the highlights would bring the topics back to memory. However, instead of spending time doing manual summations, I would like to practice some ML/NLP applications by making my own podcast summarizer.&lt;/p&gt;

&lt;p&gt;My thought is to divide the problem into two main steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Audio to transcript&lt;/li&gt;
&lt;li&gt;Transcript summation/extraction of main topics&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;What ideas and thoughts do you all having regarding which models/libraries to use for the two steps? Or any other inputs on how to approach it?&lt;/p&gt;

&lt;p&gt;Super excited to hear your thoughts!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nd3pff,True,,ErnieBernie2017,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nd3pff/ides_for_making_a_podcast_summarizer/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nd3pff/ides_for_making_a_podcast_summarizer/,30199,1621098944.0,0,,False,,,,,,785
1168,,LanguageTechnology,"Hello,

I am a Computer science Master graduate from a top EU university.  During my Masters I focused mainly on NLP and I did 2 internships as an NLP engineer. I am currently looking(not actively looking) for opportunities in NLP in Europe(mainly London, Switzerland, and maybe Germany). 

But I am having trouble finding opportunities that match my profile and that I like. The main issue that I have is that a lot of big companies(not start ups) say that they do NLP but in reality most of the time you end up working on software engineering tasks. The other problem is that all the posts that I like require a PhD.  I also applied in FAANG companies but they only accepted me to interview for a Software engineering roles. 

1. Do any of you work at a FAANG company as a Software engineer and work on NLP projects ? What exactly is your job, do you come up with architecture of the model or do you just build the infrastructure around the model ... ? 
2. For those who work in NLP but don't have a PhD can you give me an example of NLP projects that you do at work and if possible tell me for which company you work. 

Any recommendations of companies that do NLP and hire people without a PhD are welcomed.",t2_gz4ps3x,False,,0,False,NLP job opportunities for Master graduates(no PhD),[],r/LanguageTechnology,False,6,,0,,False,t3_ncue0g,False,dark,0.97,,public,22,0,{},,False,[],,False,False,,{},,False,22,,False,False,,False,,[],{},,True,,1621097488.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am a Computer science Master graduate from a top EU university.  During my Masters I focused mainly on NLP and I did 2 internships as an NLP engineer. I am currently looking(not actively looking) for opportunities in NLP in Europe(mainly London, Switzerland, and maybe Germany). &lt;/p&gt;

&lt;p&gt;But I am having trouble finding opportunities that match my profile and that I like. The main issue that I have is that a lot of big companies(not start ups) say that they do NLP but in reality most of the time you end up working on software engineering tasks. The other problem is that all the posts that I like require a PhD.  I also applied in FAANG companies but they only accepted me to interview for a Software engineering roles. &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Do any of you work at a FAANG company as a Software engineer and work on NLP projects ? What exactly is your job, do you come up with architecture of the model or do you just build the infrastructure around the model ... ? &lt;/li&gt;
&lt;li&gt;For those who work in NLP but don&amp;#39;t have a PhD can you give me an example of NLP projects that you do at work and if possible tell me for which company you work. &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Any recommendations of companies that do NLP and hire people without a PhD are welcomed.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ncue0g,True,,angular-calendar,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ncue0g/nlp_job_opportunities_for_master_graduatesno_phd/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ncue0g/nlp_job_opportunities_for_master_graduatesno_phd/,30199,1621068688.0,0,,False,,,,,,1211
1169,,LanguageTechnology,"When I train mobileBERT on GTX 1070, I get 3.8 it/sec. However, when I train DistilBERT on the same GPU, I get 15 it/sec. Am I missing something? The paper on MobileBERT states that MobileBERT should be faster.",t2_2w70t3st,False,,0,False,Is it just me or is mobileBERT is much slower than DistilBERT on Huggingface,[],r/LanguageTechnology,False,6,,0,,False,t3_nczb5s,False,dark,1.0,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1621115374.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;When I train mobileBERT on GTX 1070, I get 3.8 it/sec. However, when I train DistilBERT on the same GPU, I get 15 it/sec. Am I missing something? The paper on MobileBERT states that MobileBERT should be faster.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nczb5s,True,,SiegeMemeLord,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nczb5s/is_it_just_me_or_is_mobilebert_is_much_slower/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nczb5s/is_it_just_me_or_is_mobilebert_is_much_slower/,30199,1621086574.0,0,,False,,,,,,210
1170,,LanguageTechnology,"Title, basically sums up my question.",t2_dwsy8,False,,0,False,"Is there a network available for download that has a very long attention span? Something that could summarize a book, for instance.",[],r/LanguageTechnology,False,6,,0,,False,t3_ncb54r,False,dark,0.89,,public,13,0,{},,False,[],,False,False,,{},,False,13,,False,True,,False,,[],{},,True,,1621034508.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Title, basically sums up my question.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ncb54r,True,,urammar,,13,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ncb54r/is_there_a_network_available_for_download_that/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ncb54r/is_there_a_network_available_for_download_that/,30199,1621005708.0,0,,False,,,,,,37
1171,,LanguageTechnology,"Hi everyone! I'm wrapping my head around regular expressions and I managed to write a simple line of code that detects age. I tried my best, unfortunately that doesn't work every time because there are several ways people can express their age.

1) ""I'm 25""
2) ""I'm 25yo""
3) ""I'm 25 years old""
4) ""I'm 25M""

My regex is very simple re.search(r'\d+\w+', text) and it works fine with 2 and 4 since there's no space between them. Do you have any idea to improve this regex so that it works with all of them?

Thanks for your time!",t2_b31us5mn,False,,0,False,Regex to detect age in a sting literal on Python,[],r/LanguageTechnology,False,6,,0,,False,t3_ncczd0,False,dark,0.67,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1621039259.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone! I&amp;#39;m wrapping my head around regular expressions and I managed to write a simple line of code that detects age. I tried my best, unfortunately that doesn&amp;#39;t work every time because there are several ways people can express their age.&lt;/p&gt;

&lt;p&gt;1) &amp;quot;I&amp;#39;m 25&amp;quot;
2) &amp;quot;I&amp;#39;m 25yo&amp;quot;
3) &amp;quot;I&amp;#39;m 25 years old&amp;quot;
4) &amp;quot;I&amp;#39;m 25M&amp;quot;&lt;/p&gt;

&lt;p&gt;My regex is very simple re.search(r&amp;#39;\d+\w+&amp;#39;, text) and it works fine with 2 and 4 since there&amp;#39;s no space between them. Do you have any idea to improve this regex so that it works with all of them?&lt;/p&gt;

&lt;p&gt;Thanks for your time!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ncczd0,True,,Dr_Funkmachine,,13,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ncczd0/regex_to_detect_age_in_a_sting_literal_on_python/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ncczd0/regex_to_detect_age_in_a_sting_literal_on_python/,30199,1621010459.0,0,,False,,,,,,527
1172,,LanguageTechnology,,t2_hkv9s,False,,0,False,EmbedRank: Simple Unsupervised Keyphrase Extraction using Sentence Embeddings,[],r/LanguageTechnology,False,6,,0,,False,t3_ncbjs8,False,dark,0.83,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,False,,1621035562.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ncbjs8,True,,prakhar21,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ncbjs8/embedrank_simple_unsupervised_keyphrase/,all_ads,False,https://link.medium.com/zaWf1MfUfgb,30199,1621006762.0,0,,False,https://link.medium.com/zaWf1MfUfgb,,,,,0
1173,,LanguageTechnology,"I'm looking for a large (more than 100K records) corpus dataset for a summarization task in Portuguese. Something like the CNN/DailyMail dataset ([https://huggingface.co/datasets/cnn\_dailymail](https://huggingface.co/datasets/cnn_dailymail)) but in Portuguese.

Does anyone know of such a dataset?",t2_7gloh8o4,False,,0,False,Large summarization dataset in Portuguese,[],r/LanguageTechnology,False,6,,0,,False,t3_nc6mku,False,dark,1.0,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1621021452.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m looking for a large (more than 100K records) corpus dataset for a summarization task in Portuguese. Something like the CNN/DailyMail dataset (&lt;a href=""https://huggingface.co/datasets/cnn_dailymail""&gt;https://huggingface.co/datasets/cnn_dailymail&lt;/a&gt;) but in Portuguese.&lt;/p&gt;

&lt;p&gt;Does anyone know of such a dataset?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nc6mku,True,,GlitteringPitch7989,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nc6mku/large_summarization_dataset_in_portuguese/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nc6mku/large_summarization_dataset_in_portuguese/,30199,1620992652.0,0,,False,,,,,,298
1174,,LanguageTechnology,,t2_auwgbh53,False,,0,False,Evolution of NLP search methods and the latest method - Neural Search. What is it and how to get started with it,[],r/LanguageTechnology,False,6,,0,,False,t3_nbzarl,False,dark,0.65,,public,5,1,{},,False,[],,False,False,,{},,False,5,,False,True,,False,,[],{'gid_1': 1},,False,,1620991793.0,text,6,,,text,medium.com,False,,,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbzarl,True,,opensourcecolumbus,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbzarl/evolution_of_nlp_search_methods_and_the_latest/,all_ads,False,https://medium.com/nerd-for-tech/what-is-neural-search-537853f3d628,30199,1620962993.0,2,,False,https://medium.com/nerd-for-tech/what-is-neural-search-537853f3d628,,,,,0
1175,,LanguageTechnology,"I've developed a conversational architecture for my robots that works fairly well in an unsupervised setting.  The robots recognize the folks they talk to (if they've been introduced) and save the interactions and the full text of the dialog between them by date.  

I am looking for something that can summarize the primary subjects of the conversations after the fact, so that I can sort of ""categorize"" the subjects that interest various folks that the robots talk with.  

I've found things like this:  https://convokit.cornell.edu/documentation/tutorial.html

These seem to offer some useful direction - just wondering if anyone else has any suggestions to summarize a given interaction?  Thanks.",t2_16rqfm,False,,0,False,After-the-fact conversational topic analysis?,[],r/LanguageTechnology,False,6,,0,,False,t3_nbsuph,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1620972728.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve developed a conversational architecture for my robots that works fairly well in an unsupervised setting.  The robots recognize the folks they talk to (if they&amp;#39;ve been introduced) and save the interactions and the full text of the dialog between them by date.  &lt;/p&gt;

&lt;p&gt;I am looking for something that can summarize the primary subjects of the conversations after the fact, so that I can sort of &amp;quot;categorize&amp;quot; the subjects that interest various folks that the robots talk with.  &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve found things like this:  &lt;a href=""https://convokit.cornell.edu/documentation/tutorial.html""&gt;https://convokit.cornell.edu/documentation/tutorial.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;These seem to offer some useful direction - just wondering if anyone else has any suggestions to summarize a given interaction?  Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbsuph,True,,DelosBoard2052,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbsuph/afterthefact_conversational_topic_analysis/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nbsuph/afterthefact_conversational_topic_analysis/,30199,1620943928.0,0,,False,,,,,,701
1176,,LanguageTechnology,"I mean is there already a database that I can use. If not, how do I get their website address without looking each one up on google and how to automate the process of collecting information. I use python if you need to know. Any suggestion helps. Thankyou",t2_5r4mo7fh,False,,0,False,I have a list of names of publicly listed companies (around 1000) and want to do textual analysis of information on their websites. Where do I start?,[],r/LanguageTechnology,False,6,,0,,False,t3_nbyu2x,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620990178.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I mean is there already a database that I can use. If not, how do I get their website address without looking each one up on google and how to automate the process of collecting information. I use python if you need to know. Any suggestion helps. Thankyou&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbyu2x,True,,Epiphany925,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbyu2x/i_have_a_list_of_names_of_publicly_listed/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nbyu2x/i_have_a_list_of_names_of_publicly_listed/,30199,1620961378.0,0,,False,,,,,,255
1177,,LanguageTechnology,Anyone have experience using Spacy 3.0 Text classifier for multi class classification? I have 6 classes but after running the model get all 0s for the accuracy score. Code is at work but I can provide it tomorrow if anyone has had a similar use case! I have a feeling I’m making a mistake regarding the multiple classes,t2_44md8gjv,False,,0,False,SpaCy 3.0 Text Classifier,[],r/LanguageTechnology,False,6,,0,,False,t3_nbvv0h,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1620980904.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Anyone have experience using Spacy 3.0 Text classifier for multi class classification? I have 6 classes but after running the model get all 0s for the accuracy score. Code is at work but I can provide it tomorrow if anyone has had a similar use case! I have a feeling I’m making a mistake regarding the multiple classes&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbvv0h,True,,washknoxnash2255,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbvv0h/spacy_30_text_classifier/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nbvv0h/spacy_30_text_classifier/,30199,1620952104.0,0,,False,,,,,,319
1178,,LanguageTechnology,"Hi everyone!

What would you recommend for an efficient preprocessing pipeline these days? I was checking spaCy but they don't offer bi-gramization out of the box right?

So to include bi-grams I was thinking on gensim.

But there things get hairy. Gensim needs tokens per sentence per doc to learn the phrase model, then I need to tokenize/segment first.

I don't like this since I wanted to use spaCy for the tokenization and lemmatization as a final and single step.

Also, I guess spaCy's lemmatization is much better than NLTK's

All seems like a mess and perhaps can shade some light on this.

Finally, will bi-grams affect lemmatization? Which is the correct order?

Thanks for your insights!!



Edit 1: And how about frequency filtering and min/max length filtering?

Edit 2: Is it better to TF-IDF or just stop words removal?

Edit 3: This preprocessing if for topic modelling",t2_5m0mxfui,False,,0,False,Best approach for preprocessing in 2021,[],r/LanguageTechnology,False,6,,0,,False,t3_nbn0rw,False,dark,1.0,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,1620939743.0,,[],{},,True,,1620957844.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;

&lt;p&gt;What would you recommend for an efficient preprocessing pipeline these days? I was checking spaCy but they don&amp;#39;t offer bi-gramization out of the box right?&lt;/p&gt;

&lt;p&gt;So to include bi-grams I was thinking on gensim.&lt;/p&gt;

&lt;p&gt;But there things get hairy. Gensim needs tokens per sentence per doc to learn the phrase model, then I need to tokenize/segment first.&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t like this since I wanted to use spaCy for the tokenization and lemmatization as a final and single step.&lt;/p&gt;

&lt;p&gt;Also, I guess spaCy&amp;#39;s lemmatization is much better than NLTK&amp;#39;s&lt;/p&gt;

&lt;p&gt;All seems like a mess and perhaps can shade some light on this.&lt;/p&gt;

&lt;p&gt;Finally, will bi-grams affect lemmatization? Which is the correct order?&lt;/p&gt;

&lt;p&gt;Thanks for your insights!!&lt;/p&gt;

&lt;p&gt;Edit 1: And how about frequency filtering and min/max length filtering?&lt;/p&gt;

&lt;p&gt;Edit 2: Is it better to TF-IDF or just stop words removal?&lt;/p&gt;

&lt;p&gt;Edit 3: This preprocessing if for topic modelling&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbn0rw,True,,iblysa,,15,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbn0rw/best_approach_for_preprocessing_in_2021/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nbn0rw/best_approach_for_preprocessing_in_2021/,30199,1620929044.0,0,,False,,,,,,886
1179,,LanguageTechnology,"This research talks about using Random Walk inspired Anonymous Walks as graph units to derive feature-based and data-driven Graph Embeddings in an unsupervised fashion. 🔥 

https://youtu.be/VVml3nDiM3E",t2_hkv9s,False,,0,False,Anonymous Walk Embeddings (Graph ML Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_nbk2ja,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1620950566.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This research talks about using Random Walk inspired Anonymous Walks as graph units to derive feature-based and data-driven Graph Embeddings in an unsupervised fashion. 🔥 &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://youtu.be/VVml3nDiM3E""&gt;https://youtu.be/VVml3nDiM3E&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbk2ja,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbk2ja/anonymous_walk_embeddings_graph_ml_research_paper/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nbk2ja/anonymous_walk_embeddings_graph_ml_research_paper/,30199,1620921766.0,0,,False,,,,,,201
1180,,LanguageTechnology,"Hi All, I’m looking for a framework that allows for the extraction of knowledge claims/factual statements from a given text. For example…

Input: “Joe Biden won the election, making him the 46th President of the United States”

Output: “Joe Biden won the election”, “Joe Biden is the 46th President of the United States”

I know this has some overlap with sentence entailment, but that’s not exactly what I’m looking for.

Does anyone know of anything like this? Thanks! :)",t2_21v8yh89,False,,0,False,Looking for a Claim Extraction/Sentence Segmentation Framework,[],r/LanguageTechnology,False,6,,0,,False,t3_nbp2ec,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620962814.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All, I’m looking for a framework that allows for the extraction of knowledge claims/factual statements from a given text. For example…&lt;/p&gt;

&lt;p&gt;Input: “Joe Biden won the election, making him the 46th President of the United States”&lt;/p&gt;

&lt;p&gt;Output: “Joe Biden won the election”, “Joe Biden is the 46th President of the United States”&lt;/p&gt;

&lt;p&gt;I know this has some overlap with sentence entailment, but that’s not exactly what I’m looking for.&lt;/p&gt;

&lt;p&gt;Does anyone know of anything like this? Thanks! :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbp2ec,True,,flakessss,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbp2ec/looking_for_a_claim_extractionsentence/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nbp2ec/looking_for_a_claim_extractionsentence/,30199,1620934014.0,0,,False,,,,,,473
1181,,LanguageTechnology,,t2_hkv9s,False,,0,False,Beyond Accuracy: Behavioral Testing of NLP Models with CheckList (BEST PAPER ACL),[],r/LanguageTechnology,False,6,,0,,False,t3_nbbbiy,False,dark,0.94,,public,13,0,{},,False,[],,False,False,,{},,False,13,,False,False,,False,,[],{},,False,,1620921142.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbbbiy,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbbbiy/beyond_accuracy_behavioral_testing_of_nlp_models/,all_ads,False,https://link.medium.com/XUG13JXHdgb,30199,1620892342.0,0,,False,https://link.medium.com/XUG13JXHdgb,,,,,0
1182,,LanguageTechnology,,t2_apv7o2y8,False,,0,False,Parrot: Paraphrase based utterance augmentation framework | Python #NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_nbhb5l,False,dark,1.0,,public,4,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/7rgvS1qMePo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Parrot: Paraphrase based utterance augmentation framework | Python #NLP', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/7rgvS1qMePo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/7rgvS1qMePo/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/7rgvS1qMePo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nbhb5l', 'height': 200}",,False,4,,False,False,,False,,[],{},,False,,1620943237.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbhb5l,True,,rsree123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbhb5l/parrot_paraphrase_based_utterance_augmentation/,all_ads,False,https://youtu.be/7rgvS1qMePo,30199,1620914437.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Parrot: Paraphrase based utterance augmentation framework | Python #NLP', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/7rgvS1qMePo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/7rgvS1qMePo/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,https://youtu.be/7rgvS1qMePo,,,,,0
1183,,LanguageTechnology,,t2_2dwso7l3,False,,0,False,Sentiment Analysis Recommendations on Review data,[],r/LanguageTechnology,False,6,,0,,False,t3_nbj7u6,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,False,,1620948349.0,text,6,,,text,self.datascience,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbj7u6,True,,Jaypal17,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbj7u6/sentiment_analysis_recommendations_on_review_data/,all_ads,False,/r/datascience/comments/nbj29s/sentiment_analysis_recommendations_on_review_data/,30199,1620919549.0,0,,False,/r/datascience/comments/nbj29s/sentiment_analysis_recommendations_on_review_data/,"[{'approved_at_utc': None, 'subreddit': 'datascience', 'selftext': ""I'm looking for recommendations on my project, currently, we have a couple of hundred rows of health care review data. My project manager wants me to find a sentiment analysis tool that gives a compound score that correlates accurately to the stars given for the review. My first attempt I used vaderSentiment and it was around 55% accurate at the score to start rating, my second attempt I used texBlob and that was less accurate (35%). I want to know if there is any off the shelf models or other libraries I can use with python, especially if it understand Healthcare lingo. We hope that we can find something that is about 60-70% accurate from compound score to star rating. Eventually, we will build our own model once we have more data and time. For now, we just want to demo the data we have. Also if you think I'm going about this all wrong please let me know. I am relatively new to data science and this is a part-time project for my job."", 'author_fullname': 't2_2dwso7l3', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Sentiment Analysis Recommendations on Review data', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/datascience', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'discussion', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_nbj29s', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1620947960.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.datascience', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m looking for recommendations on my project, currently, we have a couple of hundred rows of health care review data. My project manager wants me to find a sentiment analysis tool that gives a compound score that correlates accurately to the stars given for the review. My first attempt I used vaderSentiment and it was around 55% accurate at the score to start rating, my second attempt I used texBlob and that was less accurate (35%). I want to know if there is any off the shelf models or other libraries I can use with python, especially if it understand Healthcare lingo. We hope that we can find something that is about 60-70% accurate from compound score to star rating. Eventually, we will build our own model once we have more data and time. For now, we just want to demo the data we have. Also if you think I&amp;#39;m going about this all wrong please let me know. I am relatively new to data science and this is a part-time project for my job.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': 'confidence', 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '4fad7108-d77d-11e7-b0c6-0ee69f155af2', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2sptq', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'nbj29s', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Jaypal17', 'discussion_type': None, 'num_comments': 10, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/datascience/comments/nbj29s/sentiment_analysis_recommendations_on_review_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/datascience/comments/nbj29s/sentiment_analysis_recommendations_on_review_data/', 'subreddit_subscribers': 512462, 'created_utc': 1620919160.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_nbj29s,,,0
1184,,LanguageTechnology,"I am an undergraduate in my third year, I've been working in NLP research for over a year now. My university however, is not very research inclined and there is very little good guidance on good poster submission/publication venues. While I've researched and found places to submit complete research papers, there's very little information on poster submission venues. Any information would be helpful!",t2_8tcci4yd,False,,0,False,What are some good Poster Submission venues for NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_nbpox4,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1620964314.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am an undergraduate in my third year, I&amp;#39;ve been working in NLP research for over a year now. My university however, is not very research inclined and there is very little good guidance on good poster submission/publication venues. While I&amp;#39;ve researched and found places to submit complete research papers, there&amp;#39;s very little information on poster submission venues. Any information would be helpful!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbpox4,True,,Ninja_Similar,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbpox4/what_are_some_good_poster_submission_venues_for/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nbpox4/what_are_some_good_poster_submission_venues_for/,30199,1620935514.0,0,,False,,,,,,402
1185,,LanguageTechnology,"I'm faced with a task: Given the URL of webpage, that links to the homepage of a company, can you describe to me, in 1 to 10 sentences, what the company does?

Are there any pre-made solutions for this? I already have a massive amounts of supervised training data for the problem.",t2_ow7xp,False,,0,False,"Webpage summarization: Given the URL of a company webpage, what does the company do?",[],r/LanguageTechnology,False,6,,0,,False,t3_nbcwma,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1620927988.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m faced with a task: Given the URL of webpage, that links to the homepage of a company, can you describe to me, in 1 to 10 sentences, what the company does?&lt;/p&gt;

&lt;p&gt;Are there any pre-made solutions for this? I already have a massive amounts of supervised training data for the problem.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbcwma,True,,shackleshot,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbcwma/webpage_summarization_given_the_url_of_a_company/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nbcwma/webpage_summarization_given_the_url_of_a_company/,30199,1620899188.0,0,,False,,,,,,280
1186,,LanguageTechnology,,t2_86ukz5po,False,,0,False,"Graph-Based Framework for Structured Prediction Tasks in Sanskrit by Dr. Pawan Goyal. This is a search-based structured prediction framework, which expects a graph as input, where relevant linguistic info is encoded in the nodes, &amp; the edges are then used to indicate the association b/w these nodes.",[],r/LanguageTechnology,False,6,,0,,False,t3_nb4hty,False,dark,0.86,,public,10,1,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,False,,1620896404.0,text,6,,,text,asiainnovationsummit.com,False,,,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nb4hty,True,,LeagueRude6428,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nb4hty/graphbased_framework_for_structured_prediction/,all_ads,False,https://www.asiainnovationsummit.com/pawan-goyal,30199,1620867604.0,0,,False,https://www.asiainnovationsummit.com/pawan-goyal,,,,,0
1187,,LanguageTechnology,"Hello! I am currently a second-year (third) linguistics major student, and I am looking for an academic advice. I am looking forward intro declaring a minor in computer science or math. The problem is that I do not know what to commit to (and I can only choose one). I am aiming to apply to grad school in data science / natural language processing / computational linguistics. I have already developed some Data Science projects and I have developed pretty good coding skills by myself (Python, C++, Swift), and I am pretty much equally good at math. However, I am not entirely sure which minor would help me more career-wise. Can you guys help me out?",t2_9zny9grc,False,,0,False,Linguistics student looking for advice to advance in NLP/Computational Linguistics,[],r/LanguageTechnology,False,6,,0,,False,t3_nbcb2p,False,dark,0.75,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620925451.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! I am currently a second-year (third) linguistics major student, and I am looking for an academic advice. I am looking forward intro declaring a minor in computer science or math. The problem is that I do not know what to commit to (and I can only choose one). I am aiming to apply to grad school in data science / natural language processing / computational linguistics. I have already developed some Data Science projects and I have developed pretty good coding skills by myself (Python, C++, Swift), and I am pretty much equally good at math. However, I am not entirely sure which minor would help me more career-wise. Can you guys help me out?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nbcb2p,True,,kaisar_dauletbek,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nbcb2p/linguistics_student_looking_for_advice_to_advance/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nbcb2p/linguistics_student_looking_for_advice_to_advance/,30199,1620896651.0,0,,False,,,,,,653
1188,,LanguageTechnology,,t2_a0fp5ht8,False,,0,False,Cognitive Scientist Terrence Deacon Says Current AI Lacks Symbolic Representations &amp; True Language Comprehension Abilities,[],r/LanguageTechnology,False,6,,0,,False,t3_nas06j,False,dark,0.88,,public,33,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/gbWX--cpbYU?start=10&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'GC Clips: Can AI Comprehend Language Like Humans?', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/gbWX--cpbYU?start=10&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Nick Jikomes', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/gbWX--cpbYU/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC_dJ5ThfEhj39zkEPQbNdPg'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/gbWX--cpbYU?start=10&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/nas06j', 'height': 200}",,False,33,,False,False,,False,,[],{},,False,,1620863590.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nas06j,True,,laitnesba,,16,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nas06j/cognitive_scientist_terrence_deacon_says_current/,all_ads,False,https://www.youtube.com/watch?v=gbWX--cpbYU&amp;t=10s,30199,1620834790.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'GC Clips: Can AI Comprehend Language Like Humans?', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/gbWX--cpbYU?start=10&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Nick Jikomes', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/gbWX--cpbYU/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC_dJ5ThfEhj39zkEPQbNdPg'}}",False,https://www.youtube.com/watch?v=gbWX--cpbYU&amp;t=10s,,,,,0
1189,,LanguageTechnology,,t2_3rtgvxbs,False,,0,False,Flashcard Generator - using a multi-transformer pipeline to self-correct,[],r/LanguageTechnology,False,6,,0,,False,t3_nb6nem,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,False,,1620902925.0,text,6,,,text,revision.ai,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nb6nem,True,,dancingnightly,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nb6nem/flashcard_generator_using_a_multitransformer/,all_ads,False,http://www.revision.ai/quiz?cdf,30199,1620874125.0,0,,False,http://www.revision.ai/quiz?cdf,,,,,0
1190,,LanguageTechnology,"I'm reading a book 'Speech and Language Processing'. Here it is mentioned that without an end-symbol, the sentence probabilities for all sentences of a given length would sum to one.
I searched google to understand this statement but couldn't get satisfactory answers. Could anyone help me understand this.",t2_1xoxalgq,False,,0,False,Importance of end symbol in N gram,[],r/LanguageTechnology,False,6,,0,,False,t3_nav6mc,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620871491.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m reading a book &amp;#39;Speech and Language Processing&amp;#39;. Here it is mentioned that without an end-symbol, the sentence probabilities for all sentences of a given length would sum to one.
I searched google to understand this statement but couldn&amp;#39;t get satisfactory answers. Could anyone help me understand this.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nav6mc,True,,niteshbisht26,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nav6mc/importance_of_end_symbol_in_n_gram/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nav6mc/importance_of_end_symbol_in_n_gram/,30199,1620842691.0,0,,False,,,,,,306
1191,,LanguageTechnology,,t2_hkv9s,False,,0,False,Leveraging BERT for Extractive Text Summarization on Lectures,[],r/LanguageTechnology,False,6,,0,,False,t3_nalfmp,False,dark,0.9,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,False,,1620843981.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nalfmp,True,,prakhar21,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nalfmp/leveraging_bert_for_extractive_text_summarization/,all_ads,False,https://link.medium.com/hvJSmr6dcgb,30199,1620815181.0,0,,False,https://link.medium.com/hvJSmr6dcgb,,,,,0
1192,,LanguageTechnology,"Does anyone know where to find a dataset of adjectives used to describe humans in a text? I know named entity recognition exists, but I don't know how I would extend this to only extract human entities and then select adjectives which describe them.",t2_4c4ybmje,False,,0,False,Descriptions of Named Humans,[],r/LanguageTechnology,False,6,,0,,False,t3_naz8vi,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1620881935.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Does anyone know where to find a dataset of adjectives used to describe humans in a text? I know named entity recognition exists, but I don&amp;#39;t know how I would extend this to only extract human entities and then select adjectives which describe them.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,naz8vi,True,,Adolphins,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/naz8vi/descriptions_of_named_humans/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/naz8vi/descriptions_of_named_humans/,30199,1620853135.0,0,,False,,,,,,249
1193,,LanguageTechnology,The common techniques to do topic modelling (eg. SVD) depend on multiple documents. What's a good method for extracting topics for a single document?,t2_8py9vlu0,False,,0,False,Topic modelling for single document?,[],r/LanguageTechnology,False,6,,0,,False,t3_nat8fr,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1620866620.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The common techniques to do topic modelling (eg. SVD) depend on multiple documents. What&amp;#39;s a good method for extracting topics for a single document?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nat8fr,True,,nuvicc,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nat8fr/topic_modelling_for_single_document/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nat8fr/topic_modelling_for_single_document/,30199,1620837820.0,0,,False,,,,,,149
1194,,LanguageTechnology,,t2_3vcebtqk,False,,0,False,FactSumm: Factual Consistency Scorer for Abstractive Summarization,[],r/LanguageTechnology,False,6,,0,,False,t3_nakqet,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,False,,1620841169.0,text,6,,,text,github.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nakqet,True,,huffonism,,0,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/nakqet/factsumm_factual_consistency_scorer_for/,all_ads,False,https://github.com/Huffon/factsumm,30199,1620812369.0,0,,False,https://github.com/Huffon/factsumm,,,,,0
1195,,LanguageTechnology,"&amp;#x200B;

https://reddit.com/link/nacy0z/video/enf2j74sbly61/player

What if we want to extract and summarize text from documents, also handle translation, combine the  outputs and load it into an Embeddings index. Enter workflows! The demo above takes a list of GitHub project pages, extracts text from HTML, summarizes the text and builds a similarity search index. This same concept could be applied towards a list of company pages, wikipedia  pages and more. This is just one example of what txtai workflows can do.

txtai workflows are a simple yet powerful construct that takes a callable and returns elements. Workflows are streaming by nature and work on data in batches, allowing large volumes of data to be processed efficiently. The amount of functionality provided by machine learning models continues to grow rapidly. txtai provides an easy way to interface with these models. The following is a non-comprehensive list.

\- Questions - Extractive question-answering using a text context  
\- Labels - Apply labels to text using a zero-shot classification model  
\- Summary - Abstractive text summarization  
\- Text Extraction - Extract text from documents  
\- Transcription - Transcribe audio to text  
\- Translation - Machine translation

Workflows allows joining these models together to create powerful data transformations. Workflows can also be constructed in JavaScript, Go, Rust and Java via the API.

See the following links for more information.

[GitHub](https://github.com/neuml/txtai) | [Workflow builder](https://github.com/neuml/txtai/blob/master/examples/workflows.py) | [Documentation](https://neuml.github.io/txtai) | [Article](https://towardsdatascience.com/run-machine-learning-workflows-to-transform-data-and-build-ai-powered-text-indices-with-txtai-43d769b566a7)",t2_536lg1nv,False,,0,False,"Machine learning workflows to summarize, translate, transcribe and more",[],r/LanguageTechnology,False,6,,0,,False,t3_nacy0z,False,dark,1.0,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,True,,1620810991.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://reddit.com/link/nacy0z/video/enf2j74sbly61/player""&gt;https://reddit.com/link/nacy0z/video/enf2j74sbly61/player&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;What if we want to extract and summarize text from documents, also handle translation, combine the  outputs and load it into an Embeddings index. Enter workflows! The demo above takes a list of GitHub project pages, extracts text from HTML, summarizes the text and builds a similarity search index. This same concept could be applied towards a list of company pages, wikipedia  pages and more. This is just one example of what txtai workflows can do.&lt;/p&gt;

&lt;p&gt;txtai workflows are a simple yet powerful construct that takes a callable and returns elements. Workflows are streaming by nature and work on data in batches, allowing large volumes of data to be processed efficiently. The amount of functionality provided by machine learning models continues to grow rapidly. txtai provides an easy way to interface with these models. The following is a non-comprehensive list.&lt;/p&gt;

&lt;p&gt;- Questions - Extractive question-answering using a text context&lt;br/&gt;
- Labels - Apply labels to text using a zero-shot classification model&lt;br/&gt;
- Summary - Abstractive text summarization&lt;br/&gt;
- Text Extraction - Extract text from documents&lt;br/&gt;
- Transcription - Transcribe audio to text&lt;br/&gt;
- Translation - Machine translation&lt;/p&gt;

&lt;p&gt;Workflows allows joining these models together to create powerful data transformations. Workflows can also be constructed in JavaScript, Go, Rust and Java via the API.&lt;/p&gt;

&lt;p&gt;See the following links for more information.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/neuml/txtai""&gt;GitHub&lt;/a&gt; | &lt;a href=""https://github.com/neuml/txtai/blob/master/examples/workflows.py""&gt;Workflow builder&lt;/a&gt; | &lt;a href=""https://neuml.github.io/txtai""&gt;Documentation&lt;/a&gt; | &lt;a href=""https://towardsdatascience.com/run-machine-learning-workflows-to-transform-data-and-build-ai-powered-text-indices-with-txtai-43d769b566a7""&gt;Article&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,nacy0z,True,,davidmezzetti,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/nacy0z/machine_learning_workflows_to_summarize_translate/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/nacy0z/machine_learning_workflows_to_summarize_translate/,30199,1620782191.0,0,,False,,,,"{'enf2j74sbly61': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/nacy0z/asset/enf2j74sbly61/DASHPlaylist.mpd?a=1626418348%2CM2ViY2EwZjg4MDM5ZTAwMTJhZTBhODAzYTc5ODgxNmUyMWIwYTk2MTZhYjcwMmY3N2ZkMDgxNmE4NmM3NWI3ZQ%3D%3D&amp;v=1&amp;f=sd', 'x': 1127, 'y': 720, 'hlsUrl': 'https://v.redd.it/link/nacy0z/asset/enf2j74sbly61/HLSPlaylist.m3u8?a=1626418348%2CYzA2MjE0MmVkOTZlNDIwNTU4MDM1NjM4MzllZjc4N2IxMzRjMTZkZDZhM2NjZDViMGYxYjNiMDg2Y2QxNDIwMg%3D%3D&amp;v=1&amp;f=sd', 'id': 'enf2j74sbly61', 'isGif': False}}",,1804
1196,,LanguageTechnology,Are [IAAI](https://aaai.org/Conferences/IAAI/iaai.php) INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE and AAAI Conference on Artificial Intelligence considered as tier I conferences in AI? Both of these seem to be quite popular but I am not sure if these could be called tier I. How can I find out if a particular conference is a tier I conference?,t2_9ptho1m,False,,0,False,Are AAAI and IAAI tier I conferences?,[],r/LanguageTechnology,False,6,,0,,False,t3_napn21,False,dark,0.6,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1620857578.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Are &lt;a href=""https://aaai.org/Conferences/IAAI/iaai.php""&gt;IAAI&lt;/a&gt; INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE and AAAI Conference on Artificial Intelligence considered as tier I conferences in AI? Both of these seem to be quite popular but I am not sure if these could be called tier I. How can I find out if a particular conference is a tier I conference?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,napn21,True,,freaky_eater,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/napn21/are_aaai_and_iaai_tier_i_conferences/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/napn21/are_aaai_and_iaai_tier_i_conferences/,30199,1620828778.0,0,,False,,,,,,363
1197,,LanguageTechnology,,t2_hkv9s,False,,0,False,BERT-QE: Contextualized Query Expansion for Document Re-ranking,[],r/LanguageTechnology,False,6,,0,,False,t3_n9t3ii,False,dark,0.96,,public,22,0,{},,False,[],,False,False,,{},,False,22,,False,False,,False,,[],{},,False,,1620754336.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n9t3ii,True,,prakhar21,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n9t3ii/bertqe_contextualized_query_expansion_for/,all_ads,False,https://link.medium.com/LxXy8cjvagb,30199,1620725536.0,0,,False,https://link.medium.com/LxXy8cjvagb,,,,,0
1198,,LanguageTechnology,"Hello! I'd like to know your opinions about these two programs. I am applying to both, but I'm not sure which one is better. I have a background in Applied linguistics and I would like to move towards Computational linguistics.
I have many argumentd for each program, in fact, so far I have a tie. But what I haven't been able to consider are arguments about the quality and prestige of the programs, so I turn to you. Please, I want to read your opinions!

Thank you in advance!",t2_bbal3uqg,False,,0,False,¿Master in Digital text Analysis (UAntwerp-Belgium) vs Master in Computational linguistics (UStuttgart-Germany)?,[],r/LanguageTechnology,False,6,,0,,False,t3_na5jav,False,dark,0.8,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1620790480.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! I&amp;#39;d like to know your opinions about these two programs. I am applying to both, but I&amp;#39;m not sure which one is better. I have a background in Applied linguistics and I would like to move towards Computational linguistics.
I have many argumentd for each program, in fact, so far I have a tie. But what I haven&amp;#39;t been able to consider are arguments about the quality and prestige of the programs, so I turn to you. Please, I want to read your opinions!&lt;/p&gt;

&lt;p&gt;Thank you in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,na5jav,True,,Ravest_vale,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/na5jav/master_in_digital_text_analysis_uantwerpbelgium/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/na5jav/master_in_digital_text_analysis_uantwerpbelgium/,30199,1620761680.0,0,,False,,,,,,479
1199,,LanguageTechnology,,t2_21qaqh1p,False,,0,False,NLP Keywords to Sentences Text Generation with {keytotext},[],r/LanguageTechnology,False,6,,0,,False,t3_na71dg,False,dark,0.75,,public,2,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/I0iBzP-SxFY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'NLP Keywords to Sentences Text Generation with {keytotext} | Python NLP Library - Applied NLP', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/I0iBzP-SxFY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/I0iBzP-SxFY/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/I0iBzP-SxFY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/na71dg', 'height': 200}",,False,2,,False,False,,False,,[],{},,False,,1620794196.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,na71dg,True,,dulldata,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/na71dg/nlp_keywords_to_sentences_text_generation_with/,all_ads,False,https://youtu.be/I0iBzP-SxFY,30199,1620765396.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'NLP Keywords to Sentences Text Generation with {keytotext} | Python NLP Library - Applied NLP', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/I0iBzP-SxFY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/I0iBzP-SxFY/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,https://youtu.be/I0iBzP-SxFY,,,,,0
1200,,LanguageTechnology,"“keytotext” introduces the idea of building a model that would translate keywords into sentences using amazingly powerful T5 model. 

For example- 
Input: India, Capital, New Delhi
Output: The capital of India is New Delhi.

Interesting? Then read this walkthrough, 
Blog: https://lnkd.in/dS9_AH7
GitHub: https://github.com/gagan3012/keytotext",t2_hkv9s,False,,0,False,Generating Sentences from Keywords using Transformers,[],r/LanguageTechnology,False,6,,0,,False,t3_n92sfi,False,dark,0.99,,public,30,1,{},,False,[],,False,False,,{},,False,30,,False,False,,False,,[],{},,True,,1620676961.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;“keytotext” introduces the idea of building a model that would translate keywords into sentences using amazingly powerful T5 model. &lt;/p&gt;

&lt;p&gt;For example- 
Input: India, Capital, New Delhi
Output: The capital of India is New Delhi.&lt;/p&gt;

&lt;p&gt;Interesting? Then read this walkthrough, 
Blog: &lt;a href=""https://lnkd.in/dS9_AH7""&gt;https://lnkd.in/dS9_AH7&lt;/a&gt;
GitHub: &lt;a href=""https://github.com/gagan3012/keytotext""&gt;https://github.com/gagan3012/keytotext&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n92sfi,True,,prakhar21,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n92sfi/generating_sentences_from_keywords_using/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n92sfi/generating_sentences_from_keywords_using/,30199,1620648161.0,0,,False,,,,,,343
1201,,LanguageTechnology,"I have two text columns. 

The objective is to vectorize each column separately. And then pass them into a MLP, so that the model can also understand which column a word is coming from.

But I am confused as to how to actually implement this.

Any help or resources would be appreciated.",t2_3xirs0ww,False,,0,False,How to pass in two count vectorizers as separate features into MLP?,[],r/LanguageTechnology,False,6,,0,,False,t3_n9cnxk,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620700987.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have two text columns. &lt;/p&gt;

&lt;p&gt;The objective is to vectorize each column separately. And then pass them into a MLP, so that the model can also understand which column a word is coming from.&lt;/p&gt;

&lt;p&gt;But I am confused as to how to actually implement this.&lt;/p&gt;

&lt;p&gt;Any help or resources would be appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n9cnxk,True,,GetStuffTogether,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n9cnxk/how_to_pass_in_two_count_vectorizers_as_separate/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n9cnxk/how_to_pass_in_two_count_vectorizers_as_separate/,30199,1620672187.0,0,,False,,,,,,287
1202,,LanguageTechnology,"So far I could only find this one

https://www.aclweb.org/anthology/K18-1020/",t2_1bqxd32j,False,,0,False,Anyone know of any papers that look at latent NER? Entities which are not exactly mentioned in the text.,[],r/LanguageTechnology,False,6,,0,,False,t3_n9asbs,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620696456.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So far I could only find this one&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.aclweb.org/anthology/K18-1020/""&gt;https://www.aclweb.org/anthology/K18-1020/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n9asbs,True,,AdditionalWay,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n9asbs/anyone_know_of_any_papers_that_look_at_latent_ner/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n9asbs/anyone_know_of_any_papers_that_look_at_latent_ner/,30199,1620667656.0,0,,False,,,,,,77
1203,,LanguageTechnology,"I have been searching around online weekly for any news or interesting developments in the cryptosphere/LSP world.

Keywords like decentralization, verification, remote, payment, trustless, exchange, control ... to me sound like there is much room for overlap, but there seems to be very little out there besides conjectures of NMT going to the ""cloud"" then the cryptosphere, which doesn't mean anything... 

Crtain coins/platforms like to market themselves as being a translation-crypto coin/platform but it is merely replacing QC and payment with crypto-incentives (that are not very motivating for a career translator).

I would to hear any thoughts on this from fellow LSPs and interested parties who are into augmenting their capabilities with what technology has to offer.

I have cross-posted this across some relevant subreddits.
Thanks in advance!",t2_785fmg9x,False,,0,False,Any known applications/developments between decentralised ledger (crypto) systems and MT/CAT?,[],r/LanguageTechnology,False,6,,0,,False,t3_n90h0s,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620668223.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have been searching around online weekly for any news or interesting developments in the cryptosphere/LSP world.&lt;/p&gt;

&lt;p&gt;Keywords like decentralization, verification, remote, payment, trustless, exchange, control ... to me sound like there is much room for overlap, but there seems to be very little out there besides conjectures of NMT going to the &amp;quot;cloud&amp;quot; then the cryptosphere, which doesn&amp;#39;t mean anything... &lt;/p&gt;

&lt;p&gt;Crtain coins/platforms like to market themselves as being a translation-crypto coin/platform but it is merely replacing QC and payment with crypto-incentives (that are not very motivating for a career translator).&lt;/p&gt;

&lt;p&gt;I would to hear any thoughts on this from fellow LSPs and interested parties who are into augmenting their capabilities with what technology has to offer.&lt;/p&gt;

&lt;p&gt;I have cross-posted this across some relevant subreddits.
Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n90h0s,True,,neon_metaphors,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n90h0s/any_known_applicationsdevelopments_between/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n90h0s/any_known_applicationsdevelopments_between/,30199,1620639423.0,2,,False,,,,,,856
1204,,LanguageTechnology,"Hello,

I need to do resume parsing with French resumes. I will use Prodigy ([prodi.gy](https://prodi.gy)) to annotate my dataset and make my model.

My question is : is it better to annotate with the full resume or annotate line by line  ,

for me, the position of the text in the full document is an information but maybe spacy don't care.

Thanks !",t2_3llvpcr6,False,,0,False,NER for Resume parsing,[],r/LanguageTechnology,False,6,,0,,False,t3_n8yidr,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1620660165.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I need to do resume parsing with French resumes. I will use Prodigy (&lt;a href=""https://prodi.gy""&gt;prodi.gy&lt;/a&gt;) to annotate my dataset and make my model.&lt;/p&gt;

&lt;p&gt;My question is : is it better to annotate with the full resume or annotate line by line  ,&lt;/p&gt;

&lt;p&gt;for me, the position of the text in the full document is an information but maybe spacy don&amp;#39;t care.&lt;/p&gt;

&lt;p&gt;Thanks !&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n8yidr,True,,Ekkolo,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n8yidr/ner_for_resume_parsing/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n8yidr/ner_for_resume_parsing/,30199,1620631365.0,0,,False,,,,,,351
1205,,LanguageTechnology,"I've recently started working as a .NET web developer building services, but as a side project I'm working on a text summarization API.

I would like to serve [this model](https://huggingface.co/facebook/bart-large-cnn/tree/main) through the API. How should I go about it? I haven't found much after searching, is there any tutorials out there that I'm missing? Do I need to create the API in Python instead of C#?

I would really appreciate any help. Thank you very much and sorry about my English.",t2_6s740nk,False,,0,False,Best way of integrating a NLP model in Python with a .NET Core API?,[],r/LanguageTechnology,False,6,,0,,False,t3_n8nokg,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1620623434.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve recently started working as a .NET web developer building services, but as a side project I&amp;#39;m working on a text summarization API.&lt;/p&gt;

&lt;p&gt;I would like to serve &lt;a href=""https://huggingface.co/facebook/bart-large-cnn/tree/main""&gt;this model&lt;/a&gt; through the API. How should I go about it? I haven&amp;#39;t found much after searching, is there any tutorials out there that I&amp;#39;m missing? Do I need to create the API in Python instead of C#?&lt;/p&gt;

&lt;p&gt;I would really appreciate any help. Thank you very much and sorry about my English.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n8nokg,True,,Wufi,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n8nokg/best_way_of_integrating_a_nlp_model_in_python/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n8nokg/best_way_of_integrating_a_nlp_model_in_python/,30199,1620594634.0,0,,False,,,,,,499
1206,,LanguageTechnology,,t2_5fsp2x6v,False,,0,False,Pooled Contextualised Embeddings for NER | Research Papers Summary 017,[],r/LanguageTechnology,False,6,,0,,False,t3_n8jz4r,False,dark,0.92,,public,9,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/HJtapl3zWC0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Pooled Contextualised Embeddings for NER | Research Papers Summary 017', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/HJtapl3zWC0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/HJtapl3zWC0/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/HJtapl3zWC0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/n8jz4r', 'height': 200}",,False,9,,False,False,,False,,[],{},,False,,1620612979.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n8jz4r,True,,RyanAI100,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n8jz4r/pooled_contextualised_embeddings_for_ner_research/,all_ads,False,https://youtu.be/HJtapl3zWC0,30199,1620584179.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Pooled Contextualised Embeddings for NER | Research Papers Summary 017', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/HJtapl3zWC0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/HJtapl3zWC0/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,https://youtu.be/HJtapl3zWC0,,,,,0
1207,,LanguageTechnology,,t2_iv8ylbg,False,,0,False,Is there a python library or API that is able to check the grammar of a sentence?,[],r/LanguageTechnology,False,6,,0,,False,t3_n8qpvb,False,dark,0.63,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620632645.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n8qpvb,True,,rodgerdodger17,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n8qpvb/is_there_a_python_library_or_api_that_is_able_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n8qpvb/is_there_a_python_library_or_api_that_is_able_to/,30199,1620603845.0,0,,False,,,,,,0
1208,,LanguageTechnology,,t2_hkv9s,False,,0,False,Text Generation using GPT-Neo,[],r/LanguageTechnology,False,6,,0,,False,t3_n8ao75,False,dark,0.77,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,False,,1620581073.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n8ao75,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n8ao75/text_generation_using_gptneo/,all_ads,False,https://link.medium.com/84WG7KXa7fb,30199,1620552273.0,0,,False,https://link.medium.com/84WG7KXa7fb,,,,,0
1209,,LanguageTechnology,"I am trying to solve a multi-class emotion classification problem using BERT (NLP). It is my first ever BERT model and my model ended up overfitting.

    {""train"": {""eval_examples_count"": 10481, ""metrics"": {""f1_weighted"": 0.8869, ""f1_macro"": 0.8401, ""accuracy"": 0.8884, ""roc_auc"": 0.9686}, ""time_spent"": ""1:27:10""}} 
    {""valid"": {""eval_examples_count"": 3493, ""metrics"": {""f1_weighted"": 0.6112, ""f1_macro"": 0.5257, ""accuracy"": 0.6184, ""roc_auc"": 0.8177}, ""time_spent"": ""0:28:37""}} 
    {""test"": {""eval_examples_count"": 3494, ""metrics"": {""f1_weighted"": 0.6191, ""f1_macro"": 0.5282, ""accuracy"": 0.6259, ""roc_auc"": 0.8271}, ""time_spent"": ""0:28:26”}} 

Classes were pretty unbalanced, that's probably why it is overfitting i thought, and then I used the undersampling technique to come over this issue.

    Train:
    sad          756
    angry        756
    surprised    756
    smile        756
    kind         756
    
    Test:
    sad          235
    surprised    235
    angry        235
    kind         235
    smile        235
    
    Val:
    sad          235
    surprised    235
    angry        235
    kind         235
    smile        235

I know the dataset became too small, but it is just for training my model faster and quickly experiment with different approaches. In fact, using my entire dataset and this testing dataset both have been giving pretty similar results.

After balancing my dataset result looks like this:

    {""train"": {""eval_examples_count"": 3780, ""metrics"": {""f1_weighted"": 0.8719, ""f1_macro"": 0.8719, ""accuracy"": 0.8725, ""roc_auc"": 0.9744}, ""time_spent"": ""0:38:33""}} 
    {""valid"": {""eval_examples_count"": 1215, ""metrics"": {""f1_weighted"": 0.5165, ""f1_macro"": 0.5165, ""accuracy"": 0.5185, ""roc_auc"": 0.7993}, ""time_spent"": ""0:12:16""}} 
    {""test"": {""eval_examples_count"": 1215, ""metrics"": {""f1_weighted"": 0.5264, ""f1_macro"": 0.5264, ""accuracy"": 0.5276, ""roc_auc"": 0.8051}, ""time_spent"": ""0:12:28""}} 

So, dropping instances and balancing the dataset DIT NOT really solve the overfitting problem.

Then I decided to use the Drop Out technique and made it equal to 0.5

            ""keep_prob"": 0.5,
            ""learning_rate"": 1e-05,
            ""learning_rate_drop_patience"": 2,
            ""learning_rate_drop_div"": 2.0,
            ""load_before_drop"": True, 
            ""attention_probs_keep_prob"": 0.5,
            ""hidden_keep_prob"": 0.5,

that really solved the overfitting issue, however, badly affected on the performance of the training/model:

    {""train"": {""eval_examples_count"": 32, ""metrics"": {""f1_weighted"": 0.4099, ""f1_macro"": 0.3828, ""accuracy"": 0.4688, ""roc_auc"": 0.6969}, ""time_spent"": ""3:57:36"", ""epochs_done"": 3, ""batches_seen"": 357, ""train_examples_seen"": 11340, ""learning_rate"": 1e-05, ""loss"": 1.4732106072562081} 
    {""valid"": {""eval_examples_count"": 1215, ""metrics"": {""f1_weighted"": 0.3798, ""f1_macro"": 0.3798, ""accuracy"": 0.4272, ""roc_auc"": 0.7127}, ""time_spent"": ""4:06:33"", ""epochs_done"": 3, ""batches_seen"": 357, ""train_examples_seen"": 11340, ""impatience"": 0, ""patience_limit"": 2}} 

Now, I think I solved the overfitting problem, however, How can I improve the overall performance/accuracy? Maybe increasing the learning rate? I can really try all possible approaches but it is really taking too much time of mine, each training is roughly about 4-5 hours (On the small dataset) on the big one it goes to 15-17 hours. It is really hit or miss right now. I am really new in this field but i need to solve this problem. Any idea how to increase the performance would be appreciated. Thank you",t2_6c0lef9b,False,,0,False,How to increase the performance?,[],r/LanguageTechnology,False,6,,0,,False,t3_n85h06,False,dark,1.0,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,True,,1620559984.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to solve a multi-class emotion classification problem using BERT (NLP). It is my first ever BERT model and my model ended up overfitting.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;train&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 10481, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.8869, &amp;quot;f1_macro&amp;quot;: 0.8401, &amp;quot;accuracy&amp;quot;: 0.8884, &amp;quot;roc_auc&amp;quot;: 0.9686}, &amp;quot;time_spent&amp;quot;: &amp;quot;1:27:10&amp;quot;}} 
{&amp;quot;valid&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 3493, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.6112, &amp;quot;f1_macro&amp;quot;: 0.5257, &amp;quot;accuracy&amp;quot;: 0.6184, &amp;quot;roc_auc&amp;quot;: 0.8177}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:28:37&amp;quot;}} 
{&amp;quot;test&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 3494, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.6191, &amp;quot;f1_macro&amp;quot;: 0.5282, &amp;quot;accuracy&amp;quot;: 0.6259, &amp;quot;roc_auc&amp;quot;: 0.8271}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:28:26”}} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Classes were pretty unbalanced, that&amp;#39;s probably why it is overfitting i thought, and then I used the undersampling technique to come over this issue.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Train:
sad          756
angry        756
surprised    756
smile        756
kind         756

Test:
sad          235
surprised    235
angry        235
kind         235
smile        235

Val:
sad          235
surprised    235
angry        235
kind         235
smile        235
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I know the dataset became too small, but it is just for training my model faster and quickly experiment with different approaches. In fact, using my entire dataset and this testing dataset both have been giving pretty similar results.&lt;/p&gt;

&lt;p&gt;After balancing my dataset result looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;train&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 3780, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.8719, &amp;quot;f1_macro&amp;quot;: 0.8719, &amp;quot;accuracy&amp;quot;: 0.8725, &amp;quot;roc_auc&amp;quot;: 0.9744}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:38:33&amp;quot;}} 
{&amp;quot;valid&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 1215, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.5165, &amp;quot;f1_macro&amp;quot;: 0.5165, &amp;quot;accuracy&amp;quot;: 0.5185, &amp;quot;roc_auc&amp;quot;: 0.7993}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:12:16&amp;quot;}} 
{&amp;quot;test&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 1215, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.5264, &amp;quot;f1_macro&amp;quot;: 0.5264, &amp;quot;accuracy&amp;quot;: 0.5276, &amp;quot;roc_auc&amp;quot;: 0.8051}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:12:28&amp;quot;}} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, dropping instances and balancing the dataset DIT NOT really solve the overfitting problem.&lt;/p&gt;

&lt;p&gt;Then I decided to use the Drop Out technique and made it equal to 0.5&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        &amp;quot;keep_prob&amp;quot;: 0.5,
        &amp;quot;learning_rate&amp;quot;: 1e-05,
        &amp;quot;learning_rate_drop_patience&amp;quot;: 2,
        &amp;quot;learning_rate_drop_div&amp;quot;: 2.0,
        &amp;quot;load_before_drop&amp;quot;: True, 
        &amp;quot;attention_probs_keep_prob&amp;quot;: 0.5,
        &amp;quot;hidden_keep_prob&amp;quot;: 0.5,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;that really solved the overfitting issue, however, badly affected on the performance of the training/model:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;train&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 32, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.4099, &amp;quot;f1_macro&amp;quot;: 0.3828, &amp;quot;accuracy&amp;quot;: 0.4688, &amp;quot;roc_auc&amp;quot;: 0.6969}, &amp;quot;time_spent&amp;quot;: &amp;quot;3:57:36&amp;quot;, &amp;quot;epochs_done&amp;quot;: 3, &amp;quot;batches_seen&amp;quot;: 357, &amp;quot;train_examples_seen&amp;quot;: 11340, &amp;quot;learning_rate&amp;quot;: 1e-05, &amp;quot;loss&amp;quot;: 1.4732106072562081} 
{&amp;quot;valid&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 1215, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.3798, &amp;quot;f1_macro&amp;quot;: 0.3798, &amp;quot;accuracy&amp;quot;: 0.4272, &amp;quot;roc_auc&amp;quot;: 0.7127}, &amp;quot;time_spent&amp;quot;: &amp;quot;4:06:33&amp;quot;, &amp;quot;epochs_done&amp;quot;: 3, &amp;quot;batches_seen&amp;quot;: 357, &amp;quot;train_examples_seen&amp;quot;: 11340, &amp;quot;impatience&amp;quot;: 0, &amp;quot;patience_limit&amp;quot;: 2}} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, I think I solved the overfitting problem, however, How can I improve the overall performance/accuracy? Maybe increasing the learning rate? I can really try all possible approaches but it is really taking too much time of mine, each training is roughly about 4-5 hours (On the small dataset) on the big one it goes to 15-17 hours. It is really hit or miss right now. I am really new in this field but i need to solve this problem. Any idea how to increase the performance would be appreciated. Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n85h06,True,,strangeguy111,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n85h06/how_to_increase_the_performance/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n85h06/how_to_increase_the_performance/,30199,1620531184.0,0,,False,,,,,,3562
1210,,LanguageTechnology,"After doing data cleaning and preprocesses for sentences like converting words to their lemma or lowercase, some sentences are equal to 0 and not greater than the minimal length of sentences of the model.

So I think the best way to deal with these sentences is to delete those sentences whose length is 0. 

But I am wondering if there is a more suitable way in this situation?

Thanks in advance.",t2_9juu3c3s,False,,0,False,How to deal with the processed sentences whose length are equal to 0 and not greater than the minimal length of sentences of model,[],r/LanguageTechnology,False,6,,0,,False,t3_n8bimb,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1620585021.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;After doing data cleaning and preprocesses for sentences like converting words to their lemma or lowercase, some sentences are equal to 0 and not greater than the minimal length of sentences of the model.&lt;/p&gt;

&lt;p&gt;So I think the best way to deal with these sentences is to delete those sentences whose length is 0. &lt;/p&gt;

&lt;p&gt;But I am wondering if there is a more suitable way in this situation?&lt;/p&gt;

&lt;p&gt;Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n8bimb,True,,FreAkReddit0303,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n8bimb/how_to_deal_with_the_processed_sentences_whose/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n8bimb/how_to_deal_with_the_processed_sentences_whose/,30199,1620556221.0,0,,False,,,,,,398
1211,,LanguageTechnology,,t2_128h0c,False,,0,False,"[Q] Why is interpretability important in natural language processing? This is easier to answer for models that make high stakes decisions (e.g., surgical risk assessment; self-driving car slamming brakes; etc.,), but I would like to understand why we care about interpretability in NLP.",[],r/LanguageTechnology,False,6,,0,,False,t3_n7udhn,False,dark,0.74,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1620524536.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n7udhn,True,,synysterbates,,17,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n7udhn/q_why_is_interpretability_important_in_natural/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n7udhn/q_why_is_interpretability_important_in_natural/,30199,1620495736.0,0,,False,,,,,,0
1212,,LanguageTechnology,,t2_hkv9s,False,,0,False,Explanability for Transformers with Transformers-Interpret — A Model Explainability Tool,[],r/LanguageTechnology,False,6,,0,,False,t3_n7o3jb,False,dark,0.93,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,False,,False,,[],{},,False,,1620505303.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n7o3jb,True,,prakhar21,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n7o3jb/explanability_for_transformers_with/,all_ads,False,https://link.medium.com/Xgi4JLHI5fb,30199,1620476503.0,0,,False,https://link.medium.com/Xgi4JLHI5fb,,,,,0
1213,,LanguageTechnology,It seems like a low hanging fruit that the architecture that usually have the top results be trained by the pre-training regimen that usually have the top results.,t2_10efjmjx,False,,0,False,How come we haven't seen the Albert architecture trained by the Electra pretraining method?,[],r/LanguageTechnology,False,6,,0,,False,t3_n7lmg8,False,dark,0.85,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,1620505616.0,,[],{},,True,,1620495320.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;It seems like a low hanging fruit that the architecture that usually have the top results be trained by the pre-training regimen that usually have the top results.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n7lmg8,True,,BatmantoshReturns,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n7lmg8/how_come_we_havent_seen_the_albert_architecture/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n7lmg8/how_come_we_havent_seen_the_albert_architecture/,30199,1620466520.0,0,,False,,,,,,163
1214,,LanguageTechnology,"Hi,

I'm currently a rising senior majoring in CS at a top 10 university in the US. I'm debating between graduating just with bachelor's or pursuing a master's in NLP/Human Language Technology (would only take me extra two semesters). I'm mainly interested in NLP, Text Mining and recommendation systems (haven't taken speech processing yet). My school is huge (top 3-5) in the NLP realm, and this is important not because of the rankings but because of the support and opportunities I'll have access to during my masters. If I graduate just with a bachelor's, then I'm considering an SWE role at big techs. If I end up doing a master's, then an applied ML position (MLE, NLP engineer, etc) at big techs.

I have taken some courses related to NLP/DL; I enjoyed them, but at the moment I'm not sure if I liked it enough to do a master's in it and potentially commit my career path to it. Job prospects and competitiveness of getting such positions at big techs would factor a lot in my decision.

I'm wondering how competitive it is to get an MLE/NLP engineer positions at big tech firms like FAANg, Linkedin, etc. What would the expectations/requirements be for MLE/NLP positions (ML- and NLP-related knowledge, research/internship experience, personal projects, publication, Leetcode, etc)? Also, what would an engineer at such positions work on on a regular day? In your opinion, what are the pros and cons of each role (MLE/NLP engineer vs. general SWE)? What would be the kinds of advantages an MLE/NLP engineer would have over general SWEs?

Thank you!",t2_1684oril,False,,0,False,MLE/NLP Engineer Positions at Big Tech,[],r/LanguageTechnology,False,6,,0,,False,t3_n7s7j8,False,dark,0.44,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1620518353.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m currently a rising senior majoring in CS at a top 10 university in the US. I&amp;#39;m debating between graduating just with bachelor&amp;#39;s or pursuing a master&amp;#39;s in NLP/Human Language Technology (would only take me extra two semesters). I&amp;#39;m mainly interested in NLP, Text Mining and recommendation systems (haven&amp;#39;t taken speech processing yet). My school is huge (top 3-5) in the NLP realm, and this is important not because of the rankings but because of the support and opportunities I&amp;#39;ll have access to during my masters. If I graduate just with a bachelor&amp;#39;s, then I&amp;#39;m considering an SWE role at big techs. If I end up doing a master&amp;#39;s, then an applied ML position (MLE, NLP engineer, etc) at big techs.&lt;/p&gt;

&lt;p&gt;I have taken some courses related to NLP/DL; I enjoyed them, but at the moment I&amp;#39;m not sure if I liked it enough to do a master&amp;#39;s in it and potentially commit my career path to it. Job prospects and competitiveness of getting such positions at big techs would factor a lot in my decision.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m wondering how competitive it is to get an MLE/NLP engineer positions at big tech firms like FAANg, Linkedin, etc. What would the expectations/requirements be for MLE/NLP positions (ML- and NLP-related knowledge, research/internship experience, personal projects, publication, Leetcode, etc)? Also, what would an engineer at such positions work on on a regular day? In your opinion, what are the pros and cons of each role (MLE/NLP engineer vs. general SWE)? What would be the kinds of advantages an MLE/NLP engineer would have over general SWEs?&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n7s7j8,True,,pjwoo3,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n7s7j8/mlenlp_engineer_positions_at_big_tech/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n7s7j8/mlenlp_engineer_positions_at_big_tech/,30199,1620489553.0,0,,False,,,,,,1557
1215,,LanguageTechnology,"This tutorial covers how to implement 5 different question-answering models with Hugging Face, along with the theory behind each model and the different datasets used to pre-train them. We'll also look at the varying baselines for each of the models in terms of F1 and EM scores.  

Topics covered include:

* The Transformer Architecture
* Popular Datasets and Evaluation Metrics
* BERT (Bidirectional Encoder Representations from Transformers)
* ALBERT: A Lite BERT
* ELECTRA
* BART
* Issues with Long Document Question-Answering Using Standard Models
* LONGFORMER: the Long-Document Transformer

Tutorial link: [https://blog.paperspace.com/question-answering-models-a-comparison/](https://blog.paperspace.com/question-answering-models-a-comparison/)

Run the full code on a free GPU: [https://ml-showcase.paperspace.com/projects/question-answering-models](https://ml-showcase.paperspace.com/projects/question-answering-models)

Questions and comments encouraged!",t2_15en0l,False,,0,False,[Tutorial] Implementing different question-answering models with Hugging Face,[],r/LanguageTechnology,False,6,,0,,False,t3_n72vuk,False,dark,0.89,,public,15,2,{},,False,[],,False,False,,{},,False,15,,False,False,,False,,[],{},,True,,1620434571.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This tutorial covers how to implement 5 different question-answering models with Hugging Face, along with the theory behind each model and the different datasets used to pre-train them. We&amp;#39;ll also look at the varying baselines for each of the models in terms of F1 and EM scores.  &lt;/p&gt;

&lt;p&gt;Topics covered include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Transformer Architecture&lt;/li&gt;
&lt;li&gt;Popular Datasets and Evaluation Metrics&lt;/li&gt;
&lt;li&gt;BERT (Bidirectional Encoder Representations from Transformers)&lt;/li&gt;
&lt;li&gt;ALBERT: A Lite BERT&lt;/li&gt;
&lt;li&gt;ELECTRA&lt;/li&gt;
&lt;li&gt;BART&lt;/li&gt;
&lt;li&gt;Issues with Long Document Question-Answering Using Standard Models&lt;/li&gt;
&lt;li&gt;LONGFORMER: the Long-Document Transformer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tutorial link: &lt;a href=""https://blog.paperspace.com/question-answering-models-a-comparison/""&gt;https://blog.paperspace.com/question-answering-models-a-comparison/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Run the full code on a free GPU: &lt;a href=""https://ml-showcase.paperspace.com/projects/question-answering-models""&gt;https://ml-showcase.paperspace.com/projects/question-answering-models&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Questions and comments encouraged!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 1000, 'id': 'award_35c78e6e-507b-4f1d-b3d8-ed43840909a8', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 800, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_128.png', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'The treasure at the end of the rainbow. Gives the author 800 Coins to do with as they please.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': ""Pot o' Coins"", 'resized_static_icons': [{'url': 'https://external-preview.redd.it?width=16&amp;height=16&amp;auto=webp&amp;s=d88c9a453f8ac38850b7a8241cfe5804b7b4905d', 'width': 16, 'height': 16}, {'url': 'https://external-preview.redd.it?width=32&amp;height=32&amp;auto=webp&amp;s=96a25019eb75878bdec4f6c012540f3baffbb1b2', 'width': 32, 'height': 32}, {'url': 'https://external-preview.redd.it?width=48&amp;height=48&amp;auto=webp&amp;s=1a51d27d75afde3fbde8bba84f9338f511211461', 'width': 48, 'height': 48}, {'url': 'https://external-preview.redd.it?width=64&amp;height=64&amp;auto=webp&amp;s=96af5ec460b05669ed60224cb0619bb8884abe27', 'width': 64, 'height': 64}, {'url': 'https://external-preview.redd.it?width=128&amp;height=128&amp;auto=webp&amp;s=2d3e648ed2302e6258673051ca5291f57beb29d4', 'width': 128, 'height': 128}], 'icon_format': 'APNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_512.png'}, {'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n72vuk,True,,hellopaperspace,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n72vuk/tutorial_implementing_different_questionanswering/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n72vuk/tutorial_implementing_different_questionanswering/,30199,1620405771.0,0,,False,,,,,,965
1216,,LanguageTechnology,,t2_45nhbqwu,False,,0,False,Computer-Aided Design as Language,[],r/LanguageTechnology,False,6,,0,,False,t3_n7ct8t,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,False,,1620461391.0,text,6,,,text,arxiv.org,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n7ct8t,True,,usrnme878,,0,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/n7ct8t/computeraided_design_as_language/,all_ads,False,https://arxiv.org/abs/2105.02769,30199,1620432591.0,0,,False,https://arxiv.org/abs/2105.02769,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': '', 'author_fullname': 't2_hwj8c', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[R] Computer-Aided Design as Language', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'three', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_n7bjmo', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 14, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Research', 'can_mod_post': False, 'score': 14, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1620457451.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'arxiv.org', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://arxiv.org/abs/2105.02769', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'bb90e510-4e82-11e6-8635-0ee522e2349b', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'n7bjmo', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'hardmaru', 'discussion_type': None, 'num_comments': 2, 'send_replies': False, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/n7bjmo/r_computeraided_design_as_language/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://arxiv.org/abs/2105.02769', 'subreddit_subscribers': 1930729, 'created_utc': 1620428651.0, 'num_crossposts': 3, 'media': None, 'is_video': False}]",t3_n7bjmo,,,0
1217,,LanguageTechnology,"Some of you might already know the [NLPCloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=23caa648-af1d-11eb-8529-0242ac130003) project I recently launched. The idea was to help developers and data scientists deploy spaCy models to production in a minute. It came from the fact that, as a developer, I spent much more time than I wanted on this DevOps part in my NLP projects. I was also seeing quite a lot of ML projects failing because teams didn't have the skills to deploy their new models to production... 

I had a lot of user requests asking me to support Hugging Face transformer-based models too, in addition to spaCy models. So I'm happy to announce that, after weeks of hard work, **it is now possible to deploy your own transformer-based models** to [https://nlpcloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=23caa648-af1d-11eb-8529-0242ac130003) , whether they are running on PyTorch or TensorFlow!

It can be useful in 2 situations:

* You developed your own models from scratch but you are having a hard time using them in production (because it takes an API, because resource consumption is very high, because you need high-availability, because server costs are too high, because you don't have advanced DevOps skills, etc.)
* You already use one of the Hugging Face pre-trained models on [NLPCloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=23caa648-af1d-11eb-8529-0242ac130003). It's not working so bad but you want to fine-tune them in order to adapt them to your own needs.

You can choose to either have your models run on CPU or GPU, depending on your requirements, and you can upload as many models as you want. Each new model has its own API endpoint, so you can use some of them in production while others are for testing only. It also makes it easy to urgently rollback to a previous model if needed.

Internally, everything is based on FastAPI and tons of Docker containers (if you are curious about our infra please don't hesitate to ask, I will be glad to comment).

For more details here is the API documentation: [https://docs.nlpcloud.io/#upload-your-transformers-based-model](https://docs.nlpcloud.io/#upload-your-transformers-based-model)

I really hope you will like it and find it useful!

I would love to have your opinion on this new feature. Please don't hesitate to answer this post!",t2_4z4m2qcs,False,,0,False,NLPCloud.io for transformer-based models in production (PyTorch and TensorFlow),[],r/LanguageTechnology,False,6,,0,,False,t3_n6vki5,False,dark,0.91,,public,19,0,{},,False,[],,False,False,,{},,False,19,,False,False,,False,,[],{},,True,,1620412874.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Some of you might already know the &lt;a href=""https://nlpcloud.io/?utm_source=reddit&amp;amp;utm_campaign=23caa648-af1d-11eb-8529-0242ac130003""&gt;NLPCloud.io&lt;/a&gt; project I recently launched. The idea was to help developers and data scientists deploy spaCy models to production in a minute. It came from the fact that, as a developer, I spent much more time than I wanted on this DevOps part in my NLP projects. I was also seeing quite a lot of ML projects failing because teams didn&amp;#39;t have the skills to deploy their new models to production... &lt;/p&gt;

&lt;p&gt;I had a lot of user requests asking me to support Hugging Face transformer-based models too, in addition to spaCy models. So I&amp;#39;m happy to announce that, after weeks of hard work, &lt;strong&gt;it is now possible to deploy your own transformer-based models&lt;/strong&gt; to &lt;a href=""https://nlpcloud.io/?utm_source=reddit&amp;amp;utm_campaign=23caa648-af1d-11eb-8529-0242ac130003""&gt;https://nlpcloud.io&lt;/a&gt; , whether they are running on PyTorch or TensorFlow!&lt;/p&gt;

&lt;p&gt;It can be useful in 2 situations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You developed your own models from scratch but you are having a hard time using them in production (because it takes an API, because resource consumption is very high, because you need high-availability, because server costs are too high, because you don&amp;#39;t have advanced DevOps skills, etc.)&lt;/li&gt;
&lt;li&gt;You already use one of the Hugging Face pre-trained models on &lt;a href=""https://nlpcloud.io/?utm_source=reddit&amp;amp;utm_campaign=23caa648-af1d-11eb-8529-0242ac130003""&gt;NLPCloud.io&lt;/a&gt;. It&amp;#39;s not working so bad but you want to fine-tune them in order to adapt them to your own needs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can choose to either have your models run on CPU or GPU, depending on your requirements, and you can upload as many models as you want. Each new model has its own API endpoint, so you can use some of them in production while others are for testing only. It also makes it easy to urgently rollback to a previous model if needed.&lt;/p&gt;

&lt;p&gt;Internally, everything is based on FastAPI and tons of Docker containers (if you are curious about our infra please don&amp;#39;t hesitate to ask, I will be glad to comment).&lt;/p&gt;

&lt;p&gt;For more details here is the API documentation: &lt;a href=""https://docs.nlpcloud.io/#upload-your-transformers-based-model""&gt;https://docs.nlpcloud.io/#upload-your-transformers-based-model&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I really hope you will like it and find it useful!&lt;/p&gt;

&lt;p&gt;I would love to have your opinion on this new feature. Please don&amp;#39;t hesitate to answer this post!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n6vki5,True,,juliensalinas,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n6vki5/nlpcloudio_for_transformerbased_models_in/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n6vki5/nlpcloudio_for_transformerbased_models_in/,30199,1620384074.0,0,,False,,,,,,2375
1218,,LanguageTechnology,"Hello everyone,

Quick question: I am working on a low resource language that even large multilingual models such as [mBERT](https://huggingface.co/bert-base-multilingual-cased) fail to represent properly. So, can I fine-tune these models on MLM just like they were originally trained and then fine-tune it again on a specific task? In other words:

1. Fine-tune mBERT on the masked language modeling task (using a domain-specific corpus)
2. Fine-tune the resulting model on a different task (say semantic analysis)
3. Test the model

Does this make sense? Is this equivalent to training a BERT model from scratch using the same multilingual corpus in mBERT, with my corpus added to it, or is it different? If so, how's it different?

Thank you for your time. I really appreciate any knowledge on the matter.",t2_429oy0lo,False,,0,False,Does a sequential fine-tuning process for BERT make sense?,[],r/LanguageTechnology,False,6,,0,,False,t3_n709v8,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1620427819.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;

&lt;p&gt;Quick question: I am working on a low resource language that even large multilingual models such as &lt;a href=""https://huggingface.co/bert-base-multilingual-cased""&gt;mBERT&lt;/a&gt; fail to represent properly. So, can I fine-tune these models on MLM just like they were originally trained and then fine-tune it again on a specific task? In other words:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Fine-tune mBERT on the masked language modeling task (using a domain-specific corpus)&lt;/li&gt;
&lt;li&gt;Fine-tune the resulting model on a different task (say semantic analysis)&lt;/li&gt;
&lt;li&gt;Test the model&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Does this make sense? Is this equivalent to training a BERT model from scratch using the same multilingual corpus in mBERT, with my corpus added to it, or is it different? If so, how&amp;#39;s it different?&lt;/p&gt;

&lt;p&gt;Thank you for your time. I really appreciate any knowledge on the matter.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n709v8,True,,le-zakkaz,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n709v8/does_a_sequential_finetuning_process_for_bert/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n709v8/does_a_sequential_finetuning_process_for_bert/,30199,1620399019.0,0,,False,,,,,,808
1219,,LanguageTechnology,"Is it okay to use Matthews Correlation Coefficient (phi coefficient) to compare the predictions of two different models?

&amp;#x200B;

Is this code correct:?

&amp;#x200B;

from sklearn.metrics import matthews\_corrcoef  

&amp;#x200B;

model4 = MultinomialNB() 

model4.fit(X\_train, y\_train) 

y\_pred4 = model4.predict(X\_test)  

&amp;#x200B;

model5 = BernoulliNB() 

model5.fit(X\_train, y\_train) 

y\_pred5 = model5.predict(X\_test)  

&amp;#x200B;

matthews\_corrcoef(y\_pred4, y\_pred5)",t2_581q2gxy,False,,0,False,Comparing individual predictions of two models,[],r/LanguageTechnology,False,6,,0,,False,t3_n6zncc,False,dark,1.0,,public,2,1,{},,False,[],,False,False,,{},,False,2,,False,False,,1620397831.0,,[],{},,True,,1620426119.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is it okay to use Matthews Correlation Coefficient (phi coefficient) to compare the predictions of two different models?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Is this code correct:?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;from sklearn.metrics import matthews_corrcoef  &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;model4 = MultinomialNB() &lt;/p&gt;

&lt;p&gt;model4.fit(X_train, y_train) &lt;/p&gt;

&lt;p&gt;y_pred4 = model4.predict(X_test)  &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;model5 = BernoulliNB() &lt;/p&gt;

&lt;p&gt;model5.fit(X_train, y_train) &lt;/p&gt;

&lt;p&gt;y_pred5 = model5.predict(X_test)  &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;matthews_corrcoef(y_pred4, y_pred5)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n6zncc,True,,jgj0707,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n6zncc/comparing_individual_predictions_of_two_models/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n6zncc/comparing_individual_predictions_of_two_models/,30199,1620397319.0,0,,False,,,,,,498
1220,,LanguageTechnology,"Hey guys, I need some help. I need to find an open-source API for doing aspect-oriented sentiment analysis within this week. My brother is doing his first software project right now and I'd like to help him with his research. I was wondering if there are any sentiment analysis APIs or tools that can scan for the sentiment value of a specific topic word. Or maybe a sentiment analysis tool or trick that can be combined with a TF-IDF algorithm in order to do the same job.

Thanks in advance.",t2_5xfvbvw9,False,,0,False,Are there any open-source aspect-oriented sentiment analysis APIs out there?,[],r/LanguageTechnology,False,6,,0,,False,t3_n6xx98,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620421200.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys, I need some help. I need to find an open-source API for doing aspect-oriented sentiment analysis within this week. My brother is doing his first software project right now and I&amp;#39;d like to help him with his research. I was wondering if there are any sentiment analysis APIs or tools that can scan for the sentiment value of a specific topic word. Or maybe a sentiment analysis tool or trick that can be combined with a TF-IDF algorithm in order to do the same job.&lt;/p&gt;

&lt;p&gt;Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n6xx98,True,,ProperWait1,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n6xx98/are_there_any_opensource_aspectoriented_sentiment/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n6xx98/are_there_any_opensource_aspectoriented_sentiment/,30199,1620392400.0,0,,False,,,,,,493
1221,,LanguageTechnology,"Normally, a recursive neural network is trained by back-propagating through every level of a tree (from the top to the bottom).

Did anybody encounter an example where a recursive neural network is trained by back-propagating on each level separately, calculating loss and updating weights on each level of the tree?

I am trying to do that on a ""pyramid"" binary tree, where the whole pyramid has the same consistent structure. I am looking for other similar examples so I can see how to improve my implementation.",t2_5ymrm,False,,0,False,Recursive neural networks,[],r/LanguageTechnology,False,6,,0,,False,t3_n6wsn0,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620417498.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Normally, a recursive neural network is trained by back-propagating through every level of a tree (from the top to the bottom).&lt;/p&gt;

&lt;p&gt;Did anybody encounter an example where a recursive neural network is trained by back-propagating on each level separately, calculating loss and updating weights on each level of the tree?&lt;/p&gt;

&lt;p&gt;I am trying to do that on a &amp;quot;pyramid&amp;quot; binary tree, where the whole pyramid has the same consistent structure. I am looking for other similar examples so I can see how to improve my implementation.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n6wsn0,True,,nox94,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n6wsn0/recursive_neural_networks/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n6wsn0/recursive_neural_networks/,30199,1620388698.0,0,,False,,,,,,514
1222,,LanguageTechnology,"Created a custom NER model, which takes a blob of text as input. 

I’m trying to get the f1-score, recall, etc by using the Scorer.score method. This method requires a list of Example objects. 

Does anyone have or know of any code examples of how to accomplish this task? I looked for hours on the internet today without any luck, could definitely just be a silly mistake on my part as I’m still relatively new to the field !",t2_44md8gjv,False,,0,False,SpaCy NER Scorer/Example Question,[],r/LanguageTechnology,False,6,,0,,False,t3_n6i00e,False,dark,0.92,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1620364745.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Created a custom NER model, which takes a blob of text as input. &lt;/p&gt;

&lt;p&gt;I’m trying to get the f1-score, recall, etc by using the Scorer.score method. This method requires a list of Example objects. &lt;/p&gt;

&lt;p&gt;Does anyone have or know of any code examples of how to accomplish this task? I looked for hours on the internet today without any luck, could definitely just be a silly mistake on my part as I’m still relatively new to the field !&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n6i00e,True,,washknoxnash2255,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n6i00e/spacy_ner_scorerexample_question/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n6i00e/spacy_ner_scorerexample_question/,30199,1620335945.0,0,,False,,,,,,426
1223,,LanguageTechnology,"Hello everyone, I need some help, or can I say, a second opinion on what I'm doing.

In the last few months I've been using spacy and nltk. Currently I'm working on a project in which I have to link multiple entities across multiple sentences. The main goal is to develop a knowledge graph. Now I'm using  mainly the dependency parser and other vanilla nlp techniques to filter out the relations between entities.

Now I'm seeking a second opinion,  after searching a few papers I'm finding a lot of work using machine learning, but I'm a total noob in that area. I'm feeling that I'm using the wrong approach.

Can someone help me out in the sense of, what I'm doing is actually not that bad or should I follow other approach, but again, I'm a total noob in the ML world.",t2_w9khn,False,,0,False,Help on Relation Extraction,[],r/LanguageTechnology,False,6,,0,,False,t3_n6ljt0,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1620374690.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone, I need some help, or can I say, a second opinion on what I&amp;#39;m doing.&lt;/p&gt;

&lt;p&gt;In the last few months I&amp;#39;ve been using spacy and nltk. Currently I&amp;#39;m working on a project in which I have to link multiple entities across multiple sentences. The main goal is to develop a knowledge graph. Now I&amp;#39;m using  mainly the dependency parser and other vanilla nlp techniques to filter out the relations between entities.&lt;/p&gt;

&lt;p&gt;Now I&amp;#39;m seeking a second opinion,  after searching a few papers I&amp;#39;m finding a lot of work using machine learning, but I&amp;#39;m a total noob in that area. I&amp;#39;m feeling that I&amp;#39;m using the wrong approach.&lt;/p&gt;

&lt;p&gt;Can someone help me out in the sense of, what I&amp;#39;m doing is actually not that bad or should I follow other approach, but again, I&amp;#39;m a total noob in the ML world.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n6ljt0,True,,costeirao,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n6ljt0/help_on_relation_extraction/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n6ljt0/help_on_relation_extraction/,30199,1620345890.0,0,,False,,,,,,772
1224,,LanguageTechnology,[https://deepset.ai/germanquad](https://deepset.ai/germanquad),t2_b7hwxbvs,False,,0,False,German Question Answering and Dense Passage Retrieval Datasets,[],r/LanguageTechnology,False,6,,0,,False,t3_n6a94i,False,dark,0.67,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620344646.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://deepset.ai/germanquad""&gt;https://deepset.ai/germanquad&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n6a94i,True,,bcdeepset,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n6a94i/german_question_answering_and_dense_passage/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n6a94i/german_question_answering_and_dense_passage/,30199,1620315846.0,0,,False,,,,,,62
1225,,LanguageTechnology,"Hi All.

I am doing a project on multi-class text classification and could do with some advice.

I have a dataset of reviews which are classified into 7 product categories.

Firstly, I create a term document matrix using TF-IDF (tfidfvectorizer from sklearn). This generates a matrix of n x m where n in the number of reviews in my dataset and m is the number of features.

Then after splitting term document matrix into 80:20 train:test, I pass it through the K-Nearest Neighbours (KNN) algorithm and achieve an accuracy of 53%.

&amp;#x200B;

In another experiment, I used the Google News Word2Vec pretrained embedding (300 dimensional) and averaged all the word vectors for each review. So, each review consists of x words and each of the words has a 300 dimensional vector. Each of the vectors are averaged to produce one 300 dimensional vector per review.

&amp;#x200B;

Then I pass this matrix through KNN. I get an accuracy of 72%.

&amp;#x200B;

As for other classifiers that I tested on the same dataset, all of them performed better on the TF-IDF method of vectorization. However, KNN performed better on word2vec.

Can anyone help me understand why there is a jump in accuracy for KNN in using the word2vec method as compared to when using the tfidf method?",t2_581q2gxy,False,,0,False,KNN performs better on Word2Vec pretrained embedding than on TF-IDF vector representation,[],r/LanguageTechnology,False,6,,0,,False,t3_n65ktg,False,dark,0.82,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1620331086.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All.&lt;/p&gt;

&lt;p&gt;I am doing a project on multi-class text classification and could do with some advice.&lt;/p&gt;

&lt;p&gt;I have a dataset of reviews which are classified into 7 product categories.&lt;/p&gt;

&lt;p&gt;Firstly, I create a term document matrix using TF-IDF (tfidfvectorizer from sklearn). This generates a matrix of n x m where n in the number of reviews in my dataset and m is the number of features.&lt;/p&gt;

&lt;p&gt;Then after splitting term document matrix into 80:20 train:test, I pass it through the K-Nearest Neighbours (KNN) algorithm and achieve an accuracy of 53%.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;In another experiment, I used the Google News Word2Vec pretrained embedding (300 dimensional) and averaged all the word vectors for each review. So, each review consists of x words and each of the words has a 300 dimensional vector. Each of the vectors are averaged to produce one 300 dimensional vector per review.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Then I pass this matrix through KNN. I get an accuracy of 72%.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;As for other classifiers that I tested on the same dataset, all of them performed better on the TF-IDF method of vectorization. However, KNN performed better on word2vec.&lt;/p&gt;

&lt;p&gt;Can anyone help me understand why there is a jump in accuracy for KNN in using the word2vec method as compared to when using the tfidf method?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n65ktg,True,,jgj0707,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n65ktg/knn_performs_better_on_word2vec_pretrained/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n65ktg/knn_performs_better_on_word2vec_pretrained/,30199,1620302286.0,0,,False,,,,,,1268
1226,,LanguageTechnology,"Hi. 

A trend I noticed in many papers that employ Autoencoders in text is that they use RNNs for encoding/decoding.

This might have something to do with data insufficiencies in fields that call for text autoencoders. (Since transformers are known to require a lot of training data). I am not sure.

I’d appreciate any insight into this!

Edit:

Some examples:
- [A Probabilistic Formulation of Unsupervised Text Style Transfer](https://arxiv.org/pdf/2002.03912.pdf)
- [Plug and Play Autoencoders for Conditional Text Generation](https://www.aclweb.org/anthology/2020.emnlp-main.491.pdf)
- [DialogWAE](https://arxiv.org/pdf/1805.12352.pdf)",t2_mwy6r93,False,,0,False,"Is there a reason text autoencoders mostly employ RNNs for encoder and decoder, instead of Transformers?",[],r/LanguageTechnology,False,6,,0,,False,t3_n62d5p,False,dark,0.93,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,False,,1620321672.0,,[],{},,True,,1620317657.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi. &lt;/p&gt;

&lt;p&gt;A trend I noticed in many papers that employ Autoencoders in text is that they use RNNs for encoding/decoding.&lt;/p&gt;

&lt;p&gt;This might have something to do with data insufficiencies in fields that call for text autoencoders. (Since transformers are known to require a lot of training data). I am not sure.&lt;/p&gt;

&lt;p&gt;I’d appreciate any insight into this!&lt;/p&gt;

&lt;p&gt;Edit:&lt;/p&gt;

&lt;p&gt;Some examples:
- &lt;a href=""https://arxiv.org/pdf/2002.03912.pdf""&gt;A Probabilistic Formulation of Unsupervised Text Style Transfer&lt;/a&gt;
- &lt;a href=""https://www.aclweb.org/anthology/2020.emnlp-main.491.pdf""&gt;Plug and Play Autoencoders for Conditional Text Generation&lt;/a&gt;
- &lt;a href=""https://arxiv.org/pdf/1805.12352.pdf""&gt;DialogWAE&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n62d5p,True,,boodleboodle,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n62d5p/is_there_a_reason_text_autoencoders_mostly_employ/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n62d5p/is_there_a_reason_text_autoencoders_mostly_employ/,30199,1620288857.0,0,,False,,,,,,640
1227,,LanguageTechnology,"Hi everyone,

I am a second year Master's student and want to write my thesis about an application of NLP model, however, I am struggling to find an original idea. It would be great if you can give me some ideas/advices, because I am really nervous at this point.",t2_47i6g389,False,,0,False,I need help to find a NLP topic for my thesis,[],r/LanguageTechnology,False,6,,0,,False,t3_n65oq8,False,dark,0.8,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1620331416.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;I am a second year Master&amp;#39;s student and want to write my thesis about an application of NLP model, however, I am struggling to find an original idea. It would be great if you can give me some ideas/advices, because I am really nervous at this point.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n65oq8,True,,kutayoncuyilmaz,,12,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n65oq8/i_need_help_to_find_a_nlp_topic_for_my_thesis/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n65oq8/i_need_help_to_find_a_nlp_topic_for_my_thesis/,30199,1620302616.0,0,,False,,,,,,263
1228,,LanguageTechnology,"This may be a silly question that is an existing topic already heavily explored by others more involved in NLP than me, but are there tools for taking a paragraph of text and transforming it into a machine-generated paragraph written in another person's style, but having similar content?  Essentially, I am looking for the NLP equivalent of what has already been done to death with GANs on image data (e.g. StyleGAN2).  Thanks in advance for any help!",t2_13wzhg,False,,0,False,Something like style transfer for language processing?,[],r/LanguageTechnology,False,6,,0,,False,t3_n5y6an,False,dark,1.0,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1620300607.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This may be a silly question that is an existing topic already heavily explored by others more involved in NLP than me, but are there tools for taking a paragraph of text and transforming it into a machine-generated paragraph written in another person&amp;#39;s style, but having similar content?  Essentially, I am looking for the NLP equivalent of what has already been done to death with GANs on image data (e.g. StyleGAN2).  Thanks in advance for any help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n5y6an,True,,newbie_lurker,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n5y6an/something_like_style_transfer_for_language/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n5y6an/something_like_style_transfer_for_language/,30199,1620271807.0,0,,False,,,,,,452
1229,,LanguageTechnology,"Hey everyone!

I'm a master's student currently doing my research in the field of NLP and I've recently become heavily invested in the idea of implementing concepts such as emotion and empathy in dialogue systems. I was wondering if there was anybody else who was also interested in this topic and wanted to share our resources together? I've made a simple [list of papers](https://github.com/Sahandfer/Empathetic-COAI-Papers) I've read so far (not complete yet). Feel free to let me know if you're interested or if you have any opinions or suggestions. Thanks and wish you have a great day.",t2_aoiqtg2t,False,,0,False,Emotion and Empathy in NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_n5dbnc,False,dark,0.97,,public,36,0,{},,False,[],,False,False,,{},,False,36,,False,False,,False,,[],{},,True,,1620241554.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m a master&amp;#39;s student currently doing my research in the field of NLP and I&amp;#39;ve recently become heavily invested in the idea of implementing concepts such as emotion and empathy in dialogue systems. I was wondering if there was anybody else who was also interested in this topic and wanted to share our resources together? I&amp;#39;ve made a simple &lt;a href=""https://github.com/Sahandfer/Empathetic-COAI-Papers""&gt;list of papers&lt;/a&gt; I&amp;#39;ve read so far (not complete yet). Feel free to let me know if you&amp;#39;re interested or if you have any opinions or suggestions. Thanks and wish you have a great day.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n5dbnc,True,,Sahand_sab,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n5dbnc/emotion_and_empathy_in_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n5dbnc/emotion_and_empathy_in_nlp/,30199,1620212754.0,0,,False,,,,,,591
1230,,LanguageTechnology,"
## 200+ State of the Art Medical Models for NER, Entity Resolution, Relation Extraction, Assertion, Spark 3 and Python 3.8 support in  NLU 3.0 Release and much more
We are incredibly excited to announce the release of `NLU 3.0.0` which makes most of John Snow Labs medical healthcare model available in just 1 line of code in NLU.
These models are the most accurate in their domains and highly scalable in Spark clusters.  
In addition, `Spark 3.0.X`  and `Spark 3.1.X ` is now supported, together with Python3.8

This is enabled by the amazing [Spark NLP3.0.1](https://nlp.johnsnowlabs.com/docs/en/release_notes#300) and [Spark NLP for Healthcare 3.0.1](https://nlp.johnsnowlabs.com/docs/en/licensed_release_notes#301) releases.

# New Features
- Over 200 new models for the `healthcare` domain
- 6 new classes of models, Assertion, Sentence/Chunk Resolvers, Relation Extractors, Medical NER models, De-Identificator Models
- Spark 3.0.X and 3.1.X support
- Python 3.8 Support
- New Output level `relation`
- 1 Line to install NLU  just run `!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash`
- [Various new EMR and Databricks versions supported](https://github.com/JohnSnowLabs/spark-nlp/releases/tag/3.0.0)
- GPU Mode, more then 600% speedup by enabling GPU mode.
- Authorized mode for licensed features

## New Documentation
- [NLU for Healthcare Examples](https://nlu.johnsnowlabs.com/docs/en/examples_hc#usage-examples-of-nluload)
- [Instrunctions to authorize your environment to use Licensed features](https://nlu.johnsnowlabs.com/docs/en/examples_hc#authorize-access-to-licensed-features-and-install-healthcare-dependencies)


## New Notebooks
- [Medical Named Entity Extraction (NER) notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/medical_named_entity_recognition/overview_medical_entity_recognizers.ipynb)
- [Relation extraction notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/relation_extraction/overview_relation.ipynb)
- [Entity Resolution overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/entity_resolution/entity_resolvers_overview.ipynb)
- [Assertion overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/assertion/assertion_overview.ipynb)
- [De-Identification overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/de_identification/DeIdentification_model_overview.ipynb)
- [Graph NLU tutorial](https://github.com/JohnSnowLabs/nlu/blob/3.0rc1/examples/webinars_conferences_etc/graph_ai_summit/Healthcare_Graph_NLU_COVID_Tigergraph.ipynb)


## AssertionDLModels

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [assert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html) | [assertion_dl](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html)                   |
| English  | [assert.biobert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html) | [assertion_dl_biobert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html)                   |
| English  | [assert.healthcare](https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html) | [assertion_dl_healthcare](https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html)                   |
| English  | [assert.large](https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html) | [assertion_dl_large](https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html)                   |

##  New Word Embeddings

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [embed.glove.clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html) | [embeddings_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html)                   |
| English  | [embed.glove.biovec](https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html) | [embeddings_biovec](https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html)                   |
| English  | [embed.glove.healthcare](https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html) | [embeddings_healthcare](https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html)                   |
| English  | [embed.glove.healthcare_100d](https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html) | [embeddings_healthcare_100d](https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html)                   |
| English  | en.embed.glove.icdoem | embeddings_icdoem          |
| English  | en.embed.glove.icdoem_2ng | embeddings_icdoem_2ng          |

## Sentence Entity resolvers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | embed_sentence.biobert.mli | sbiobert_base_cased_mli          |
| English  | resolve | sbiobertresolve_cpt          |
| English  | resolve.cpt | sbiobertresolve_cpt          |
| English  | resolve.cpt.augmented | sbiobertresolve_cpt_augmented          |
| English  | resolve.cpt.procedures_augmented | sbiobertresolve_cpt_procedures_augmented          |
| English  | resolve.hcc.augmented | sbiobertresolve_hcc_augmented          |
| English  | [resolve.icd10cm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html) | [sbiobertresolve_icd10cm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html)                   |
| English  | [resolve.icd10cm.augmented](https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html) | [sbiobertresolve_icd10cm_augmented](https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html)                   |
| English  | [resolve.icd10cm.augmented_billable](https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html) | [sbiobertresolve_icd10cm_augmented_billable_hcc](https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html)                   |
| English  | [resolve.icd10pcs](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html) | [sbiobertresolve_icd10pcs](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html)                   |
| English  | [resolve.icdo](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html) | [sbiobertresolve_icdo](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html)                   |
| English  | [resolve.rxcui](https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html) | [sbiobertresolve_rxcui](https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html)                   |
| English  | [resolve.rxnorm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html) | [sbiobertresolve_rxnorm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html)                   |
| English  | [resolve.snomed](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html) | [sbiobertresolve_snomed_auxConcepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html)                   |
| English  | [resolve.snomed.aux_concepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html) | [sbiobertresolve_snomed_auxConcepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html)                   |
| English  | [resolve.snomed.aux_concepts_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html) | [sbiobertresolve_snomed_auxConcepts_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html)                   |
| English  | [resolve.snomed.findings](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html) | [sbiobertresolve_snomed_findings](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html)                   |
| English  | [resolve.snomed.findings_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html) | [sbiobertresolve_snomed_findings_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html)                   |

## RelationExtractionModel

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | relation.posology | posology_re          |
| English  | [relation](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html) | [redl_bodypart_direction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html)                   |
| English  | [relation.bodypart.direction](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html) | [redl_bodypart_direction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html)                   |
| English  | [relation.bodypart.problem](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html) | [redl_bodypart_problem_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html)                   |
| English  | [relation.bodypart.procedure](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html) | [redl_bodypart_procedure_test_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html)                   |
| English  | [relation.chemprot](https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html) | [redl_chemprot_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html)                   |
| English  | [relation.clinical](https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html) | [redl_clinical_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html)                   |
| English  | [relation.date](https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls) | [redl_date_clinical_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls)                   |
| English  | [relation.drug_drug_interaction](https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html) | [redl_drug_drug_interaction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html)                   |
| English  | [relation.humen_phenotype_gene](https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html) | [redl_human_phenotype_gene_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html)                   |
| English  | [relation.temporal_events](https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html) | [redl_temporal_events_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html)                   |



## NERDLModels

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
|English  | [med_ner.ade.clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html) | [ner_ade_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html)                   |
| English  | [med_ner.ade.clinical_bert](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html) | [ner_ade_clinicalbert](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html)                   |
| English  | [med_ner.ade.ade_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html) | [ner_ade_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html)                   |
| English  | [med_ner.anatomy](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html) | [ner_anatomy](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html)                   |
| English  | [med_ner.anatomy.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html) | [ner_anatomy_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html)                   |
| English  | [med_ner.anatomy.coarse](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html) | [ner_anatomy_coarse](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html)                   |
| English  | [med_ner.anatomy.coarse_biobert](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html) | [ner_anatomy_coarse_biobert](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html)                   |
| English  | [med_ner.aspect_sentiment](https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html) | [ner_aspect_based_sentiment](https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html)                   |
| English  | [med_ner.bacterial_species](https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html) | [ner_bacterial_species](https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html)                   |
| English  | [med_ner.bionlp](https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html) | [ner_bionlp](https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html)                   |
| English  | [med_ner.bionlp.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html) | [ner_bionlp_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html)                   |
| English  | [med_ner.cancer](https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html) | [ner_cancer_genetics](https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html)                   |
| Englishs | [med_ner.cellular](https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html) | [ner_cellular](https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html)                   |
| English  | [med_ner.cellular.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html) | [ner_cellular_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html)                   |
| English  | [med_ner.chemicals](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html) | [ner_chemicals](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html)                   |
| English  | [med_ner.chemprot](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html) | [ner_chemprot_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html)           |
| English  | [med_ner.chemprot.clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html) | [ner_chemprot_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html)           |
| English  | [med_ner.clinical](https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html) | [ner_clinical](https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html)           |
| English  | [med_ner.clinical.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html) | [ner_clinical_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html)           |
| English  | med_ner.clinical.noncontrib | ner_clinical_noncontrib          |
| English  | [med_ner.diseases](https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html) | [ner_diseases](https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html)           |
| English  | [med_ner.diseases.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html) | [ner_diseases_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html)           |
| English  | [med_ner.diseases.large](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html) | [ner_diseases_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html)           |
| English  | [med_ner.drugs](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html) | [ner_drugs](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html)           |
| English  | [med_ner.drugsgreedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html) | [ner_drugs_greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html)           |
| English  | [med_ner.drugs.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html) | [ner_drugs_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html)           |
| English  | [med_ner.events_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html) | [ner_events_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html)           |
| English  | [med_ner.events_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html) | [ner_events_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html)           |
| English  | [med_ner.events_healthcre](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html) | [ner_events_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html)           |
| English  | [med_ner.financial_contract](https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html) | [ner_financial_contract](https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html)           |
| English  | [med_ner.healthcare](https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html) | [ner_healthcare](https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html)           |
| English  | [med_ner.human_phenotype.gene_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html) | [ner_human_phenotype_gene_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html)           |
| English  | [med_ner.human_phenotype.gene_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html) | [ner_human_phenotype_gene_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html)           |
| English  | [med_ner.human_phenotype.go_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html) | [ner_human_phenotype_go_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html)           |
| English  | [med_ner.human_phenotype.go_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html) | [ner_human_phenotype_go_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html)           |
| English  | [med_ner.jsl](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html) | [ner_jsl](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html)           |
| English  | [med_ner.jsl.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html) | [ner_jsl_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html)           |
| English  | [med_ner.jsl.enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html) | [ner_jsl_enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html)           |
| English  | [med_ner.jsl.enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html) | [ner_jsl_enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html)           |
| English  | [med_ner.measurements](https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html) | [ner_measurements_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html)           |
| English  | [med_ner.medmentions](https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html) | [ner_medmentions_coarse](https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html)           |
| English  | [med_ner.posology](https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html) | [ner_posology](https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html)           |
| English  | [med_ner.posology.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html) | [ner_posology_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html)           |
| English  | [med_ner.posology.greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html) | [ner_posology_greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html)           |
| English  | [med_ner.posology.healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html) | [ner_posology_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html)           |
| English  | [med_ner.posology.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html) | [ner_posology_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html)           |
| English  | [med_ner.posology.large_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html) | [ner_posology_large_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html)           |
| English  | [med_ner.posology.small](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html) | [ner_posology_small](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html)           |
| English  | [med_ner.radiology](https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html) | [ner_radiology](https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html)           |
| English  | [med_ner.radiology.wip_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html) | [ner_radiology_wip_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html)           |
| English  | [med_ner.risk_factors](https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html) | [ner_risk_factors](https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html)           |
| English  | [med_ner.risk_factors.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html) | [ner_risk_factors_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html)           |
| English  | med_ner.i2b2 | nerdl_i2b2          |
| English  | [med_ner.tumour](https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html) | [nerdl_tumour_demo](https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html)           |
| English  | med_ner.jsl.wip.clinical | jsl_ner_wip_clinical          |
| English  | [med_ner.jsl.wip.clinical.greedy](https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html) | [jsl_ner_wip_greedy_clinical](https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html)           |
| English  | [med_ner.jsl.wip.clinical.modifier](https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html) | [jsl_ner_wip_modifier_clinical](https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html)           |
| English  | [med_ner.jsl.wip.clinical.rd](https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html) | [jsl_rd_ner_wip_greedy_clinical](https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html)           |


## De-Identification Models

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [med_ner.deid.augmented](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html) | [ner_deid_augmented](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html)           |
| English  | [med_ner.deid.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html) | [ner_deid_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html)           |
| English  | [med_ner.deid.enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html) | [ner_deid_enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html)           |
| English  | [med_ner.deid.enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html) | [ner_deid_enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html)           |
| English  | [med_ner.deid.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html) | [ner_deid_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html)           |
| English  | [med_ner.deid.sd](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html) | [ner_deid_sd](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html)           |
| English  | [med_ner.deid.sd_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html) | [ner_deid_sd_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html)           |
| English  | med_ner.deid | nerdl_deid          |
| English  | med_ner.deid.synthetic | ner_deid_synthetic          |
| English  | [med_ner.deid.dl](https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html) | [ner_deidentify_dl](https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html)           |
| English  | [en.de_identify](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html) | [deidentify_rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html)           |
| English  | de_identify.rules | deid_rules          |
| English  | [de_identify.clinical](https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html) | [deidentify_enriched_clinical](https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html)           |
| English  | [de_identify.large](https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html) | [deidentify_large](https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html)           |
| English  | [de_identify.rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html) | [deidentify_rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html)           |
| English  | de_identify.rb_no_regex | deidentify_rb_no_regex          |



# Chunk resolvers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [resolve_chunk.athena_conditions](https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html) | [chunkresolve_athena_conditions_healthcare](https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html)           |
| English  | [resolve_chunk.cpt_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html) | [chunkresolve_cpt_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html) | [chunkresolve_icd10cm_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.diseases_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html) | [chunkresolve_icd10cm_diseases_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html)           |
| English  | resolve_chunk.icd10cm.hcc_clinical | chunkresolve_icd10cm_hcc_clinical          |
| English  | resolve_chunk.icd10cm.hcc_healthcare | chunkresolve_icd10cm_hcc_healthcare          |
| English  | [resolve_chunk.icd10cm.injuries](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html) | [chunkresolve_icd10cm_injuries_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.musculoskeletal](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html) | [chunkresolve_icd10cm_musculoskeletal_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.neoplasms](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html) | [chunkresolve_icd10cm_neoplasms_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.poison](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html) | [chunkresolve_icd10cm_poison_ext_clinical](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.puerile](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html) | [chunkresolve_icd10cm_puerile_clinical](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html)           |
| English  | resolve_chunk.icd10pcs.clinical | chunkresolve_icd10pcs_clinical          |
| English  | [resolve_chunk.icdo.clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html) | [chunkresolve_icdo_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html)           |
| English  | [resolve_chunk.loinc](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html) | [chunkresolve_loinc_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html)           |
| English  | [resolve_chunk.rxnorm.cd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html) | [chunkresolve_rxnorm_cd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html)           |
| English  | resolve_chunk.rxnorm.in | chunkresolve_rxnorm_in_clinical          |
| English  | resolve_chunk.rxnorm.in_healthcare | chunkresolve_rxnorm_in_healthcare          |
| English  | [resolve_chunk.rxnorm.sbd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html) | [chunkresolve_rxnorm_sbd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html)           |
| English  | [resolve_chunk.rxnorm.scd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html) | [chunkresolve_rxnorm_scd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html)           |
| English  | resolve_chunk.rxnorm.scdc | chunkresolve_rxnorm_scdc_clinical          |
| English  | resolve_chunk.rxnorm.scdc_healthcare | chunkresolve_rxnorm_scdc_healthcare          |
| English  | [resolve_chunk.rxnorm.xsmall.clinical](https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html) | [chunkresolve_rxnorm_xsmall_clinical](https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html)           |
| English  | [resolve_chunk.snomed.findings](https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html) | [chunkresolve_snomed_findings_clinical](https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html)           |


# New Classifiers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | classify.icd10.clinical | classifier_icd10cm_hcc_clinical          |
| English  | classify.icd10.healthcare | classifier_icd10cm_hcc_healthcare          |
| English  | [classify.ade.biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html) | [classifierdl_ade_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html)           |
| English  | [classify.ade.clinical](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html) | [classifierdl_ade_clinicalbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html)           |
| English  | [classify.ade.conversational](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html) | [classifierdl_ade_conversational_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html)           |
| English  | [classify.gender.biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html) | [classifierdl_gender_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html)           |
| English  | [classify.gender.sbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html) | [classifierdl_gender_sbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html)           |
| English  | classify.pico | classifierdl_pico_biobert          |


# German Medical models

| nlu.load() reference                                         | Spark NLP Model reference          |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [embed]    | w2v_cc_300d|
| [embed.w2v]    | w2v_cc_300d|
| [resolve_chunk]    | chunkresolve_ICD10GM|
| [resolve_chunk.icd10gm]    | chunkresolve_ICD10GM|
| resolve_chunk.icd10gm.2021    | chunkresolve_ICD10GM_2021|
| med_ner.legal   | ner_legal|
| med_ner    | ner_healthcare|
| med_ner.healthcare    | ner_healthcare|
| med_ner.healthcare_slim    | ner_healthcare_slim|
| med_ner.traffic    | ner_traffic|

# Spanish Medical models
| nlu.load() reference                                         | Spark NLP Model reference          |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [embed.scielo.150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html) | [embeddings_scielo_150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html)| 
| [embed.scielo.300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html)   | [embeddings_scielo_300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html)| 
| [embed.scielo.50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html)  | [embeddings_scielo_50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html)| 
| [embed.scielowiki.150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html)   | [embeddings_scielowiki_150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html)| 
| [embed.scielowiki.300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html)   | [embeddings_scielowiki_300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html)| 
| [embed.scielowiki.50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html)   | [embeddings_scielowiki_50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html)| 
| [embed.sciwiki.150d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html)   | [embeddings_sciwiki_150d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html)| 
| [embed.sciwiki.300d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html)   | [embeddings_sciwiki_300d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html)| 
| [embed.sciwiki.50d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html)   | [embeddings_sciwiki_50d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html)| 
| [med_ner](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)   |  [ner_diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)| 
| [med_ner.neoplasm](https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html)  | [ner_neoplasms](https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html)| 
| [med_ner.diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)  | [ner_diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)| 

# GPU Mode
You can now enable NLU GPU mode by setting `gpu=true` while loading a model. I.e. `nlu.load('train.sentiment' gpu=True)` . If must resart you kernel, if you already loaded a nlu pipeline withouth GPU mode.

# Output Level Relation
This new output level is used for relation extractors and will give you 1 row per relation extracted.


# Bug fixes
- Fixed a bug that caused loading NLU models in offline mode not to work in some occasions


# 1 line Install NLU
```!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash```

# Install via PIP 
```! pip install nlu pyspark==3.0.1```


## Additional NLU ressources

- [NLU Website](https://nlu.johnsnowlabs.com/)
- [All NLU Tutorial Notebooks](https://nlu.johnsnowlabs.com/docs/en/notebooks)
- [NLU Videos and Blogposts on NLU](https://nlp.johnsnowlabs.com/learn#pythons-nlu-library)
- [NLU on Github](https://github.com/JohnSnowLabs/nlu)
- [Suggestions or Questions? Contact us in Slack!](https://join.slack.com/t/spark-nlp/shared_invite/zt-lutct9gm-kuUazcyFKhuGY3_0AMkxqA)",t2_53n73cus,False,,0,False,"200+ State of the Art Medical Models for NER, Entity Resolution, Relation Extraction, Assertion, Spark 3 and Python 3.8 support - John Snow Labs NLU 3.0.0",[],r/LanguageTechnology,False,6,,0,,False,t3_n5gvel,False,dark,0.88,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,False,,False,,[],{},,True,,1620252798.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;h2&gt;200+ State of the Art Medical Models for NER, Entity Resolution, Relation Extraction, Assertion, Spark 3 and Python 3.8 support in  NLU 3.0 Release and much more&lt;/h2&gt;

&lt;p&gt;We are incredibly excited to announce the release of &lt;code&gt;NLU 3.0.0&lt;/code&gt; which makes most of John Snow Labs medical healthcare model available in just 1 line of code in NLU.
These models are the most accurate in their domains and highly scalable in Spark clusters.&lt;br/&gt;
In addition, &lt;code&gt;Spark 3.0.X&lt;/code&gt;  and &lt;code&gt;Spark 3.1.X&lt;/code&gt; is now supported, together with Python3.8&lt;/p&gt;

&lt;p&gt;This is enabled by the amazing &lt;a href=""https://nlp.johnsnowlabs.com/docs/en/release_notes#300""&gt;Spark NLP3.0.1&lt;/a&gt; and &lt;a href=""https://nlp.johnsnowlabs.com/docs/en/licensed_release_notes#301""&gt;Spark NLP for Healthcare 3.0.1&lt;/a&gt; releases.&lt;/p&gt;

&lt;h1&gt;New Features&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Over 200 new models for the &lt;code&gt;healthcare&lt;/code&gt; domain&lt;/li&gt;
&lt;li&gt;6 new classes of models, Assertion, Sentence/Chunk Resolvers, Relation Extractors, Medical NER models, De-Identificator Models&lt;/li&gt;
&lt;li&gt;Spark 3.0.X and 3.1.X support&lt;/li&gt;
&lt;li&gt;Python 3.8 Support&lt;/li&gt;
&lt;li&gt;New Output level &lt;code&gt;relation&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;1 Line to install NLU  just run &lt;code&gt;!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/spark-nlp/releases/tag/3.0.0""&gt;Various new EMR and Databricks versions supported&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GPU Mode, more then 600% speedup by enabling GPU mode.&lt;/li&gt;
&lt;li&gt;Authorized mode for licensed features&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;New Documentation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/docs/en/examples_hc#usage-examples-of-nluload""&gt;NLU for Healthcare Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/docs/en/examples_hc#authorize-access-to-licensed-features-and-install-healthcare-dependencies""&gt;Instrunctions to authorize your environment to use Licensed features&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;New Notebooks&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/medical_named_entity_recognition/overview_medical_entity_recognizers.ipynb""&gt;Medical Named Entity Extraction (NER) notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/relation_extraction/overview_relation.ipynb""&gt;Relation extraction notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/entity_resolution/entity_resolvers_overview.ipynb""&gt;Entity Resolution overview notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/assertion/assertion_overview.ipynb""&gt;Assertion overview notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/de_identification/DeIdentification_model_overview.ipynb""&gt;De-Identification overview notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/3.0rc1/examples/webinars_conferences_etc/graph_ai_summit/Healthcare_Graph_NLU_COVID_Tigergraph.ipynb""&gt;Graph NLU tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;AssertionDLModels&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html""&gt;assert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html""&gt;assertion_dl&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html""&gt;assert.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html""&gt;assertion_dl_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html""&gt;assert.healthcare&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html""&gt;assertion_dl_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html""&gt;assert.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html""&gt;assertion_dl_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;New Word Embeddings&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html""&gt;embed.glove.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html""&gt;embeddings_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html""&gt;embed.glove.biovec&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html""&gt;embeddings_biovec&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html""&gt;embed.glove.healthcare&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html""&gt;embeddings_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html""&gt;embed.glove.healthcare_100d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html""&gt;embeddings_healthcare_100d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;en.embed.glove.icdoem&lt;/td&gt;
&lt;td&gt;embeddings_icdoem&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;en.embed.glove.icdoem_2ng&lt;/td&gt;
&lt;td&gt;embeddings_icdoem_2ng&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;Sentence Entity resolvers&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;embed_sentence.biobert.mli&lt;/td&gt;
&lt;td&gt;sbiobert_base_cased_mli&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve&lt;/td&gt;
&lt;td&gt;sbiobertresolve_cpt&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve.cpt&lt;/td&gt;
&lt;td&gt;sbiobertresolve_cpt&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve.cpt.augmented&lt;/td&gt;
&lt;td&gt;sbiobertresolve_cpt_augmented&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve.cpt.procedures_augmented&lt;/td&gt;
&lt;td&gt;sbiobertresolve_cpt_procedures_augmented&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve.hcc.augmented&lt;/td&gt;
&lt;td&gt;sbiobertresolve_hcc_augmented&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html""&gt;resolve.icd10cm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html""&gt;sbiobertresolve_icd10cm&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html""&gt;resolve.icd10cm.augmented&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html""&gt;sbiobertresolve_icd10cm_augmented&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html""&gt;resolve.icd10cm.augmented_billable&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html""&gt;sbiobertresolve_icd10cm_augmented_billable_hcc&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html""&gt;resolve.icd10pcs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html""&gt;sbiobertresolve_icd10pcs&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html""&gt;resolve.icdo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html""&gt;sbiobertresolve_icdo&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html""&gt;resolve.rxcui&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html""&gt;sbiobertresolve_rxcui&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html""&gt;resolve.rxnorm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html""&gt;sbiobertresolve_rxnorm&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html""&gt;resolve.snomed&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html""&gt;sbiobertresolve_snomed_auxConcepts&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html""&gt;resolve.snomed.aux_concepts&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html""&gt;sbiobertresolve_snomed_auxConcepts&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html""&gt;resolve.snomed.aux_concepts_int&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html""&gt;sbiobertresolve_snomed_auxConcepts_int&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html""&gt;resolve.snomed.findings&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html""&gt;sbiobertresolve_snomed_findings&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html""&gt;resolve.snomed.findings_int&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html""&gt;sbiobertresolve_snomed_findings_int&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;RelationExtractionModel&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;relation.posology&lt;/td&gt;
&lt;td&gt;posology_re&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html""&gt;relation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html""&gt;redl_bodypart_direction_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html""&gt;relation.bodypart.direction&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html""&gt;redl_bodypart_direction_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html""&gt;relation.bodypart.problem&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html""&gt;redl_bodypart_problem_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html""&gt;relation.bodypart.procedure&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html""&gt;redl_bodypart_procedure_test_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html""&gt;relation.chemprot&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html""&gt;redl_chemprot_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html""&gt;relation.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html""&gt;redl_clinical_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls""&gt;relation.date&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls""&gt;redl_date_clinical_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html""&gt;relation.drug_drug_interaction&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html""&gt;redl_drug_drug_interaction_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html""&gt;relation.humen_phenotype_gene&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html""&gt;redl_human_phenotype_gene_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html""&gt;relation.temporal_events&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html""&gt;redl_temporal_events_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;NERDLModels&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html""&gt;med_ner.ade.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html""&gt;ner_ade_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html""&gt;med_ner.ade.clinical_bert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html""&gt;ner_ade_clinicalbert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html""&gt;med_ner.ade.ade_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html""&gt;ner_ade_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html""&gt;med_ner.anatomy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html""&gt;ner_anatomy&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html""&gt;med_ner.anatomy.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html""&gt;ner_anatomy_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html""&gt;med_ner.anatomy.coarse&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html""&gt;ner_anatomy_coarse&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html""&gt;med_ner.anatomy.coarse_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html""&gt;ner_anatomy_coarse_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html""&gt;med_ner.aspect_sentiment&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html""&gt;ner_aspect_based_sentiment&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html""&gt;med_ner.bacterial_species&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html""&gt;ner_bacterial_species&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html""&gt;med_ner.bionlp&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html""&gt;ner_bionlp&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html""&gt;med_ner.bionlp.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html""&gt;ner_bionlp_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html""&gt;med_ner.cancer&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html""&gt;ner_cancer_genetics&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Englishs&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html""&gt;med_ner.cellular&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html""&gt;ner_cellular&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html""&gt;med_ner.cellular.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html""&gt;ner_cellular_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html""&gt;med_ner.chemicals&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html""&gt;ner_chemicals&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html""&gt;med_ner.chemprot&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html""&gt;ner_chemprot_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html""&gt;med_ner.chemprot.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html""&gt;ner_chemprot_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html""&gt;med_ner.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html""&gt;ner_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html""&gt;med_ner.clinical.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html""&gt;ner_clinical_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;med_ner.clinical.noncontrib&lt;/td&gt;
&lt;td&gt;ner_clinical_noncontrib&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html""&gt;med_ner.diseases&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html""&gt;ner_diseases&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html""&gt;med_ner.diseases.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html""&gt;ner_diseases_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html""&gt;med_ner.diseases.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html""&gt;ner_diseases_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html""&gt;med_ner.drugs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html""&gt;ner_drugs&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html""&gt;med_ner.drugsgreedy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html""&gt;ner_drugs_greedy&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html""&gt;med_ner.drugs.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html""&gt;ner_drugs_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html""&gt;med_ner.events_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html""&gt;ner_events_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html""&gt;med_ner.events_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html""&gt;ner_events_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html""&gt;med_ner.events_healthcre&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html""&gt;ner_events_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html""&gt;med_ner.financial_contract&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html""&gt;ner_financial_contract&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html""&gt;med_ner.healthcare&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html""&gt;ner_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html""&gt;med_ner.human_phenotype.gene_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html""&gt;ner_human_phenotype_gene_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html""&gt;med_ner.human_phenotype.gene_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html""&gt;ner_human_phenotype_gene_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html""&gt;med_ner.human_phenotype.go_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html""&gt;ner_human_phenotype_go_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html""&gt;med_ner.human_phenotype.go_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html""&gt;ner_human_phenotype_go_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html""&gt;med_ner.jsl&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html""&gt;ner_jsl&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html""&gt;med_ner.jsl.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html""&gt;ner_jsl_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html""&gt;med_ner.jsl.enriched&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html""&gt;ner_jsl_enriched&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html""&gt;med_ner.jsl.enriched_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html""&gt;ner_jsl_enriched_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html""&gt;med_ner.measurements&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html""&gt;ner_measurements_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html""&gt;med_ner.medmentions&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html""&gt;ner_medmentions_coarse&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html""&gt;med_ner.posology&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html""&gt;ner_posology&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html""&gt;med_ner.posology.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html""&gt;ner_posology_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html""&gt;med_ner.posology.greedy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html""&gt;ner_posology_greedy&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html""&gt;med_ner.posology.healthcare&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html""&gt;ner_posology_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html""&gt;med_ner.posology.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html""&gt;ner_posology_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html""&gt;med_ner.posology.large_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html""&gt;ner_posology_large_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html""&gt;med_ner.posology.small&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html""&gt;ner_posology_small&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html""&gt;med_ner.radiology&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html""&gt;ner_radiology&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html""&gt;med_ner.radiology.wip_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html""&gt;ner_radiology_wip_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html""&gt;med_ner.risk_factors&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html""&gt;ner_risk_factors&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html""&gt;med_ner.risk_factors.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html""&gt;ner_risk_factors_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;med_ner.i2b2&lt;/td&gt;
&lt;td&gt;nerdl_i2b2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html""&gt;med_ner.tumour&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html""&gt;nerdl_tumour_demo&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;med_ner.jsl.wip.clinical&lt;/td&gt;
&lt;td&gt;jsl_ner_wip_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html""&gt;med_ner.jsl.wip.clinical.greedy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html""&gt;jsl_ner_wip_greedy_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html""&gt;med_ner.jsl.wip.clinical.modifier&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html""&gt;jsl_ner_wip_modifier_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html""&gt;med_ner.jsl.wip.clinical.rd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html""&gt;jsl_rd_ner_wip_greedy_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;De-Identification Models&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html""&gt;med_ner.deid.augmented&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html""&gt;ner_deid_augmented&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html""&gt;med_ner.deid.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html""&gt;ner_deid_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html""&gt;med_ner.deid.enriched&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html""&gt;ner_deid_enriched&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html""&gt;med_ner.deid.enriched_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html""&gt;ner_deid_enriched_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html""&gt;med_ner.deid.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html""&gt;ner_deid_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html""&gt;med_ner.deid.sd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html""&gt;ner_deid_sd&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html""&gt;med_ner.deid.sd_large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html""&gt;ner_deid_sd_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;med_ner.deid&lt;/td&gt;
&lt;td&gt;nerdl_deid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;med_ner.deid.synthetic&lt;/td&gt;
&lt;td&gt;ner_deid_synthetic&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html""&gt;med_ner.deid.dl&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html""&gt;ner_deidentify_dl&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html""&gt;en.de_identify&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html""&gt;deidentify_rb&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;de_identify.rules&lt;/td&gt;
&lt;td&gt;deid_rules&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html""&gt;de_identify.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html""&gt;deidentify_enriched_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html""&gt;de_identify.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html""&gt;deidentify_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html""&gt;de_identify.rb&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html""&gt;deidentify_rb&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;de_identify.rb_no_regex&lt;/td&gt;
&lt;td&gt;deidentify_rb_no_regex&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;Chunk resolvers&lt;/h1&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html""&gt;resolve_chunk.athena_conditions&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html""&gt;chunkresolve_athena_conditions_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html""&gt;resolve_chunk.cpt_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html""&gt;chunkresolve_cpt_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html""&gt;resolve_chunk.icd10cm.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html""&gt;chunkresolve_icd10cm_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html""&gt;resolve_chunk.icd10cm.diseases_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html""&gt;chunkresolve_icd10cm_diseases_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.icd10cm.hcc_clinical&lt;/td&gt;
&lt;td&gt;chunkresolve_icd10cm_hcc_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.icd10cm.hcc_healthcare&lt;/td&gt;
&lt;td&gt;chunkresolve_icd10cm_hcc_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html""&gt;resolve_chunk.icd10cm.injuries&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html""&gt;chunkresolve_icd10cm_injuries_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html""&gt;resolve_chunk.icd10cm.musculoskeletal&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html""&gt;chunkresolve_icd10cm_musculoskeletal_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html""&gt;resolve_chunk.icd10cm.neoplasms&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html""&gt;chunkresolve_icd10cm_neoplasms_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html""&gt;resolve_chunk.icd10cm.poison&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html""&gt;chunkresolve_icd10cm_poison_ext_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html""&gt;resolve_chunk.icd10cm.puerile&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html""&gt;chunkresolve_icd10cm_puerile_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.icd10pcs.clinical&lt;/td&gt;
&lt;td&gt;chunkresolve_icd10pcs_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html""&gt;resolve_chunk.icdo.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html""&gt;chunkresolve_icdo_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html""&gt;resolve_chunk.loinc&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html""&gt;chunkresolve_loinc_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html""&gt;resolve_chunk.rxnorm.cd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html""&gt;chunkresolve_rxnorm_cd_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.rxnorm.in&lt;/td&gt;
&lt;td&gt;chunkresolve_rxnorm_in_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.rxnorm.in_healthcare&lt;/td&gt;
&lt;td&gt;chunkresolve_rxnorm_in_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html""&gt;resolve_chunk.rxnorm.sbd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html""&gt;chunkresolve_rxnorm_sbd_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html""&gt;resolve_chunk.rxnorm.scd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html""&gt;chunkresolve_rxnorm_scd_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.rxnorm.scdc&lt;/td&gt;
&lt;td&gt;chunkresolve_rxnorm_scdc_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.rxnorm.scdc_healthcare&lt;/td&gt;
&lt;td&gt;chunkresolve_rxnorm_scdc_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html""&gt;resolve_chunk.rxnorm.xsmall.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html""&gt;chunkresolve_rxnorm_xsmall_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html""&gt;resolve_chunk.snomed.findings&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html""&gt;chunkresolve_snomed_findings_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;New Classifiers&lt;/h1&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;classify.icd10.clinical&lt;/td&gt;
&lt;td&gt;classifier_icd10cm_hcc_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;classify.icd10.healthcare&lt;/td&gt;
&lt;td&gt;classifier_icd10cm_hcc_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html""&gt;classify.ade.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html""&gt;classifierdl_ade_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html""&gt;classify.ade.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html""&gt;classifierdl_ade_clinicalbert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html""&gt;classify.ade.conversational&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html""&gt;classifierdl_ade_conversational_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html""&gt;classify.gender.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html""&gt;classifierdl_gender_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html""&gt;classify.gender.sbert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html""&gt;classifierdl_gender_sbert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;classify.pico&lt;/td&gt;
&lt;td&gt;classifierdl_pico_biobert&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;German Medical models&lt;/h1&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;[embed]&lt;/td&gt;
&lt;td&gt;w2v_cc_300d&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[embed.w2v]&lt;/td&gt;
&lt;td&gt;w2v_cc_300d&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[resolve_chunk]&lt;/td&gt;
&lt;td&gt;chunkresolve_ICD10GM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[resolve_chunk.icd10gm]&lt;/td&gt;
&lt;td&gt;chunkresolve_ICD10GM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;resolve_chunk.icd10gm.2021&lt;/td&gt;
&lt;td&gt;chunkresolve_ICD10GM_2021&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;med_ner.legal&lt;/td&gt;
&lt;td&gt;ner_legal&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;med_ner&lt;/td&gt;
&lt;td&gt;ner_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;med_ner.healthcare&lt;/td&gt;
&lt;td&gt;ner_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;med_ner.healthcare_slim&lt;/td&gt;
&lt;td&gt;ner_healthcare_slim&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;med_ner.traffic&lt;/td&gt;
&lt;td&gt;ner_traffic&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;Spanish Medical models&lt;/h1&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html""&gt;embed.scielo.150d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html""&gt;embeddings_scielo_150d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html""&gt;embed.scielo.300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html""&gt;embeddings_scielo_300d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html""&gt;embed.scielo.50d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html""&gt;embeddings_scielo_50d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html""&gt;embed.scielowiki.150d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html""&gt;embeddings_scielowiki_150d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html""&gt;embed.scielowiki.300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html""&gt;embeddings_scielowiki_300d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html""&gt;embed.scielowiki.50d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html""&gt;embeddings_scielowiki_50d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html""&gt;embed.sciwiki.150d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html""&gt;embeddings_sciwiki_150d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html""&gt;embed.sciwiki.300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html""&gt;embeddings_sciwiki_300d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html""&gt;embed.sciwiki.50d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html""&gt;embeddings_sciwiki_50d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html""&gt;med_ner&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html""&gt;ner_diag_proc&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html""&gt;med_ner.neoplasm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html""&gt;ner_neoplasms&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html""&gt;med_ner.diag_proc&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html""&gt;ner_diag_proc&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;GPU Mode&lt;/h1&gt;

&lt;p&gt;You can now enable NLU GPU mode by setting &lt;code&gt;gpu=true&lt;/code&gt; while loading a model. I.e. &lt;code&gt;nlu.load(&amp;#39;train.sentiment&amp;#39; gpu=True)&lt;/code&gt; . If must resart you kernel, if you already loaded a nlu pipeline withouth GPU mode.&lt;/p&gt;

&lt;h1&gt;Output Level Relation&lt;/h1&gt;

&lt;p&gt;This new output level is used for relation extractors and will give you 1 row per relation extracted.&lt;/p&gt;

&lt;h1&gt;Bug fixes&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Fixed a bug that caused loading NLU models in offline mode not to work in some occasions&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;1 line Install NLU&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash&lt;/code&gt;&lt;/p&gt;

&lt;h1&gt;Install via PIP&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;! pip install nlu pyspark==3.0.1&lt;/code&gt;&lt;/p&gt;

&lt;h2&gt;Additional NLU ressources&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/""&gt;NLU Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/docs/en/notebooks""&gt;All NLU Tutorial Notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlp.johnsnowlabs.com/learn#pythons-nlu-library""&gt;NLU Videos and Blogposts on NLU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu""&gt;NLU on Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://join.slack.com/t/spark-nlp/shared_invite/zt-lutct9gm-kuUazcyFKhuGY3_0AMkxqA""&gt;Suggestions or Questions? Contact us in Slack!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n5gvel,True,,CKL-IT,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n5gvel/200_state_of_the_art_medical_models_for_ner/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n5gvel/200_state_of_the_art_medical_models_for_ner/,30199,1620223998.0,0,,False,,,,,,38256
1231,,LanguageTechnology,"Hi, everyone!

I posted this earlier to r/MachineLearning, but I just found out about this subreddit and thought people here might be more interested. Sorry if it's not allowed!

I'm a researcher from the Text Mining Unit at the Barcelona Supercomputing  Center, and I wanted to share with you some information about MEDDOPROF,  a Shared Task that we are currently organizing focused on the detection  and normalization of professions and employment status in clinical  texts in Spanish.

Even if these types of entities might seem really niche, every day we learn more and more about their importance. Just think about how someone's occupation  can have a radical impact in their physical and mental health, habits,  lifestyle choices, ... There is even an entire medical specialty,  occupational medicine, that is centered around this topic. In the context of the current pandemic, many people with specific occupations  have been specially affected (for instance, health professionals and  other essential workers). The detection of these terms will help  researchers to better characterize health risks of specific occupations.

Outside  medicine, we foresee that the systems resulting from MEDDOPROF may be  used in fields such as social care, human resources, legal NLP and even gender studies. Personally, I think one of the main contributions of the  task is the inclusion of employment status in a broad sense. We have  annotated unemployed and retired people, family caregivers, people who  are homeless, people who depend on government subsidies, etc, which strengthens the social side of this project.

Additionally,  each mention in the corpus (which includes 2000 documents from over 20  different medical specialties) has been normalized to either the  European Skills, Competences, Qualifications and Occupations  classification (ESCO) or SNOMED-CT. These are multilingual vocabularies,  which we hope might inspire similar tasks in other languages (to the  best of our knowledge, there haven't been any similar tasks yet).

We  released the training set some weeks ago, and on June 1st we will  release the test set. If you are interested in the task, want to see  some annotated examples, the data or the annotation guidelines, ...  please check out the task's website:[ https://temu.bsc.es/meddoprof/](https://temu.bsc.es/meddoprof/)

Thank you if you have read up to here, I hope to see at least some of you at the task!",t2_bffb2m2a,False,,0,False,Call for Participation in a Shared Task about occupations detection in clinical texts,[],r/LanguageTechnology,False,6,,0,,False,t3_n5kw8w,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1620263033.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, everyone!&lt;/p&gt;

&lt;p&gt;I posted this earlier to &lt;a href=""/r/MachineLearning""&gt;r/MachineLearning&lt;/a&gt;, but I just found out about this subreddit and thought people here might be more interested. Sorry if it&amp;#39;s not allowed!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m a researcher from the Text Mining Unit at the Barcelona Supercomputing  Center, and I wanted to share with you some information about MEDDOPROF,  a Shared Task that we are currently organizing focused on the detection  and normalization of professions and employment status in clinical  texts in Spanish.&lt;/p&gt;

&lt;p&gt;Even if these types of entities might seem really niche, every day we learn more and more about their importance. Just think about how someone&amp;#39;s occupation  can have a radical impact in their physical and mental health, habits,  lifestyle choices, ... There is even an entire medical specialty,  occupational medicine, that is centered around this topic. In the context of the current pandemic, many people with specific occupations  have been specially affected (for instance, health professionals and  other essential workers). The detection of these terms will help  researchers to better characterize health risks of specific occupations.&lt;/p&gt;

&lt;p&gt;Outside  medicine, we foresee that the systems resulting from MEDDOPROF may be  used in fields such as social care, human resources, legal NLP and even gender studies. Personally, I think one of the main contributions of the  task is the inclusion of employment status in a broad sense. We have  annotated unemployed and retired people, family caregivers, people who  are homeless, people who depend on government subsidies, etc, which strengthens the social side of this project.&lt;/p&gt;

&lt;p&gt;Additionally,  each mention in the corpus (which includes 2000 documents from over 20  different medical specialties) has been normalized to either the  European Skills, Competences, Qualifications and Occupations  classification (ESCO) or SNOMED-CT. These are multilingual vocabularies,  which we hope might inspire similar tasks in other languages (to the  best of our knowledge, there haven&amp;#39;t been any similar tasks yet).&lt;/p&gt;

&lt;p&gt;We  released the training set some weeks ago, and on June 1st we will  release the test set. If you are interested in the task, want to see  some annotated examples, the data or the annotation guidelines, ...  please check out the task&amp;#39;s website:&lt;a href=""https://temu.bsc.es/meddoprof/""&gt; https://temu.bsc.es/meddoprof/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thank you if you have read up to here, I hope to see at least some of you at the task!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n5kw8w,True,,s-lilo,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n5kw8w/call_for_participation_in_a_shared_task_about/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n5kw8w/call_for_participation_in_a_shared_task_about/,30199,1620234233.0,0,,False,,,,,,2440
1232,,LanguageTechnology,"Hello everyone,

I'm currently working on a topic modelling paper and I in order to evaluate it, I need your help. Topic models are unsupervised clustering methods used to group related words together and extract topics. To evaluate such topic I will in part use the word intrusion task ([source](https://papers.nips.cc/paper/2009/hash/f92586a25bb3145facd64ab20fd554ff-Abstract.html)). The task consist in polluting a topic (group of words) with an intruder. The assumption is that better the topic the easier it is to find the intruder. If you want to help, follow the link to a small 2min survey. No personal information is needed.

Link to the survey : [https://forms.gle/svWu4XmR2PAMeiCD9](https://forms.gle/svWu4XmR2PAMeiCD9)

Thank you to all who will find the time to help :)

&amp;#x200B;

PS: don't hesitate if you have any questions",t2_bybd0cyd,False,,0,False,I am working on a topic modelling paper and I need your help,[],r/LanguageTechnology,False,6,,0,,False,t3_n5h677,False,dark,0.8,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,1620225120.0,,[],{},,True,,1620253589.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m currently working on a topic modelling paper and I in order to evaluate it, I need your help. Topic models are unsupervised clustering methods used to group related words together and extract topics. To evaluate such topic I will in part use the word intrusion task (&lt;a href=""https://papers.nips.cc/paper/2009/hash/f92586a25bb3145facd64ab20fd554ff-Abstract.html""&gt;source&lt;/a&gt;). The task consist in polluting a topic (group of words) with an intruder. The assumption is that better the topic the easier it is to find the intruder. If you want to help, follow the link to a small 2min survey. No personal information is needed.&lt;/p&gt;

&lt;p&gt;Link to the survey : &lt;a href=""https://forms.gle/svWu4XmR2PAMeiCD9""&gt;https://forms.gle/svWu4XmR2PAMeiCD9&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thank you to all who will find the time to help :)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;PS: don&amp;#39;t hesitate if you have any questions&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n5h677,True,,Addicted_to_math,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n5h677/i_am_working_on_a_topic_modelling_paper_and_i/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n5h677/i_am_working_on_a_topic_modelling_paper_and_i/,30199,1620224789.0,0,,False,,,,,,842
1233,,LanguageTechnology,"Is there a curated list of Advanced Natural Language Processing (NLP) Resources (Model Zoo, GitHub Repositories, Datasets, etc.)  
I think Advanced NLP is progress in the last 3 years since BERT and GPT which are SOTA versatile models and also great for Transfer Learning (Zero-Shot and Few-Shot Learning) like Hugging Face models. Essentially, they've changed the way we approach NLP problems today.

I could not find it on the internet (including on GitHub, Kaggle, Medium, or Reddit.) And, I know about [NLP Progress](http://nlpprogress.com/) and [The Super Duper NLP Repo](https://notebooks.quantumstat.com/).

I need it for a project, and any help is greatly appreciated.",t2_32tnn9,False,,0,False,[Request] Curated Advanced NLP Resources,[],r/LanguageTechnology,False,6,,0,,False,t3_n5h4hy,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620253464.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there a curated list of Advanced Natural Language Processing (NLP) Resources (Model Zoo, GitHub Repositories, Datasets, etc.)&lt;br/&gt;
I think Advanced NLP is progress in the last 3 years since BERT and GPT which are SOTA versatile models and also great for Transfer Learning (Zero-Shot and Few-Shot Learning) like Hugging Face models. Essentially, they&amp;#39;ve changed the way we approach NLP problems today.&lt;/p&gt;

&lt;p&gt;I could not find it on the internet (including on GitHub, Kaggle, Medium, or Reddit.) And, I know about &lt;a href=""http://nlpprogress.com/""&gt;NLP Progress&lt;/a&gt; and &lt;a href=""https://notebooks.quantumstat.com/""&gt;The Super Duper NLP Repo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I need it for a project, and any help is greatly appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n5h4hy,True,,TheGupta,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n5h4hy/request_curated_advanced_nlp_resources/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n5h4hy/request_curated_advanced_nlp_resources/,30199,1620224664.0,0,,False,,,,,,676
1234,,LanguageTechnology,,t2_icvu8,False,,0,False,Tagalog: a text labeling platform for teams,[],r/LanguageTechnology,False,6,,0,,False,t3_n5h2jx,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,False,,1620253327.0,text,6,,,text,tagalog.ai,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n5h2jx,True,,yvespeirsman,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n5h2jx/tagalog_a_text_labeling_platform_for_teams/,all_ads,False,https://www.tagalog.ai/tagalog/,30199,1620224527.0,0,,False,https://www.tagalog.ai/tagalog/,,,,,0
1235,,LanguageTechnology,,t2_hkv9s,False,,0,False,EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks,[],r/LanguageTechnology,False,6,,0,,False,t3_n59z8e,False,dark,0.82,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,False,,1620227697.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n59z8e,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n59z8e/eda_easy_data_augmentation_techniques_for/,all_ads,False,https://link.medium.com/YkCBPe5n0fb,30199,1620198897.0,0,,False,https://link.medium.com/YkCBPe5n0fb,,,,,0
1236,,LanguageTechnology,"
## 200+ State of the Art Medical Models for NER, Entity Resolution, Relation Extraction, Assertion, Spark 3 and Python 3.8 support in  NLU 3.0 Release and much more
We are incredibly excited to announce the release of `NLU 3.0.0` which makes most of John Snow Labs medical healthcare model available in just 1 line of code in NLU.
These models are the most accurate in their domains and highly scalable in Spark clusters.  
In addition, `Spark 3.0.X`  and `Spark 3.1.X ` is now supported, together with Python3.8

This is enabled by the amazing [Spark NLP3.0.1](https://nlp.johnsnowlabs.com/docs/en/release_notes#300) and [Spark NLP for Healthcare 3.0.1](https://nlp.johnsnowlabs.com/docs/en/licensed_release_notes#301) releases.

# New Features
- Over 200 new models for the `healthcare` domain
- 6 new classes of models, Assertion, Sentence/Chunk Resolvers, Relation Extractors, Medical NER models, De-Identificator Models
- Spark 3.0.X and 3.1.X support
- Python 3.8 Support
- New Output level `relation`
- 1 Line to install NLU  just run `!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash`
- [Various new EMR and Databricks versions supported](https://github.com/JohnSnowLabs/spark-nlp/releases/tag/3.0.0)
- GPU Mode, more then 600% speedup by enabling GPU mode.
- Authorized mode for licensed features

## New Documentation
- [NLU for Healthcare Examples](https://nlu.johnsnowlabs.com/docs/en/examples_hc#usage-examples-of-nluload)
- [Instrunctions to authorize your environment to use Licensed features](https://nlu.johnsnowlabs.com/docs/en/examples_hc#authorize-access-to-licensed-features-and-install-healthcare-dependencies)


## New Notebooks
- [Medical Named Entity Extraction (NER) notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/medical_named_entity_recognition/overview_medical_entity_recognizers.ipynb)
- [Relation extraction notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/relation_extraction/overview_relation.ipynb)
- [Entity Resolution overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/entity_resolution/entity_resolvers_overview.ipynb)
- [Assertion overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/assertion/assertion_overview.ipynb)
- [De-Identification overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/de_identification/DeIdentification_model_overview.ipynb)
- [Graph NLU tutorial](https://github.com/JohnSnowLabs/nlu/blob/3.0rc1/examples/webinars_conferences_etc/graph_ai_summit/Healthcare_Graph_NLU_COVID_Tigergraph.ipynb)


## AssertionDLModels

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [assert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html) | [assertion_dl](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html)                   |
| English  | [assert.biobert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html) | [assertion_dl_biobert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html)                   |
| English  | [assert.healthcare](https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html) | [assertion_dl_healthcare](https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html)                   |
| English  | [assert.large](https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html) | [assertion_dl_large](https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html)                   |

##  New Word Embeddings

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [embed.glove.clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html) | [embeddings_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html)                   |
| English  | [embed.glove.biovec](https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html) | [embeddings_biovec](https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html)                   |
| English  | [embed.glove.healthcare](https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html) | [embeddings_healthcare](https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html)                   |
| English  | [embed.glove.healthcare_100d](https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html) | [embeddings_healthcare_100d](https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html)                   |
| English  | en.embed.glove.icdoem | embeddings_icdoem          |
| English  | en.embed.glove.icdoem_2ng | embeddings_icdoem_2ng          |

## Sentence Entity resolvers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | embed_sentence.biobert.mli | sbiobert_base_cased_mli          |
| English  | resolve | sbiobertresolve_cpt          |
| English  | resolve.cpt | sbiobertresolve_cpt          |
| English  | resolve.cpt.augmented | sbiobertresolve_cpt_augmented          |
| English  | resolve.cpt.procedures_augmented | sbiobertresolve_cpt_procedures_augmented          |
| English  | resolve.hcc.augmented | sbiobertresolve_hcc_augmented          |
| English  | [resolve.icd10cm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html) | [sbiobertresolve_icd10cm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html)                   |
| English  | [resolve.icd10cm.augmented](https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html) | [sbiobertresolve_icd10cm_augmented](https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html)                   |
| English  | [resolve.icd10cm.augmented_billable](https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html) | [sbiobertresolve_icd10cm_augmented_billable_hcc](https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html)                   |
| English  | [resolve.icd10pcs](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html) | [sbiobertresolve_icd10pcs](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html)                   |
| English  | [resolve.icdo](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html) | [sbiobertresolve_icdo](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html)                   |
| English  | [resolve.rxcui](https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html) | [sbiobertresolve_rxcui](https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html)                   |
| English  | [resolve.rxnorm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html) | [sbiobertresolve_rxnorm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html)                   |
| English  | [resolve.snomed](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html) | [sbiobertresolve_snomed_auxConcepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html)                   |
| English  | [resolve.snomed.aux_concepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html) | [sbiobertresolve_snomed_auxConcepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html)                   |
| English  | [resolve.snomed.aux_concepts_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html) | [sbiobertresolve_snomed_auxConcepts_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html)                   |
| English  | [resolve.snomed.findings](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html) | [sbiobertresolve_snomed_findings](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html)                   |
| English  | [resolve.snomed.findings_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html) | [sbiobertresolve_snomed_findings_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html)                   |

## RelationExtractionModel

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | relation.posology | posology_re          |
| English  | [relation](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html) | [redl_bodypart_direction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html)                   |
| English  | [relation.bodypart.direction](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html) | [redl_bodypart_direction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html)                   |
| English  | [relation.bodypart.problem](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html) | [redl_bodypart_problem_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html)                   |
| English  | [relation.bodypart.procedure](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html) | [redl_bodypart_procedure_test_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html)                   |
| English  | [relation.chemprot](https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html) | [redl_chemprot_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html)                   |
| English  | [relation.clinical](https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html) | [redl_clinical_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html)                   |
| English  | [relation.date](https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls) | [redl_date_clinical_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls)                   |
| English  | [relation.drug_drug_interaction](https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html) | [redl_drug_drug_interaction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html)                   |
| English  | [relation.humen_phenotype_gene](https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html) | [redl_human_phenotype_gene_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html)                   |
| English  | [relation.temporal_events](https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html) | [redl_temporal_events_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html)                   |



## NERDLModels

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
|English  | [med_ner.ade.clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html) | [ner_ade_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html)                   |
| English  | [med_ner.ade.clinical_bert](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html) | [ner_ade_clinicalbert](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html)                   |
| English  | [med_ner.ade.ade_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html) | [ner_ade_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html)                   |
| English  | [med_ner.anatomy](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html) | [ner_anatomy](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html)                   |
| English  | [med_ner.anatomy.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html) | [ner_anatomy_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html)                   |
| English  | [med_ner.anatomy.coarse](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html) | [ner_anatomy_coarse](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html)                   |
| English  | [med_ner.anatomy.coarse_biobert](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html) | [ner_anatomy_coarse_biobert](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html)                   |
| English  | [med_ner.aspect_sentiment](https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html) | [ner_aspect_based_sentiment](https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html)                   |
| English  | [med_ner.bacterial_species](https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html) | [ner_bacterial_species](https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html)                   |
| English  | [med_ner.bionlp](https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html) | [ner_bionlp](https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html)                   |
| English  | [med_ner.bionlp.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html) | [ner_bionlp_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html)                   |
| English  | [med_ner.cancer](https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html) | [ner_cancer_genetics](https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html)                   |
| Englishs | [med_ner.cellular](https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html) | [ner_cellular](https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html)                   |
| English  | [med_ner.cellular.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html) | [ner_cellular_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html)                   |
| English  | [med_ner.chemicals](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html) | [ner_chemicals](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html)                   |
| English  | [med_ner.chemprot](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html) | [ner_chemprot_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html)           |
| English  | [med_ner.chemprot.clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html) | [ner_chemprot_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html)           |
| English  | [med_ner.clinical](https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html) | [ner_clinical](https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html)           |
| English  | [med_ner.clinical.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html) | [ner_clinical_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html)           |
| English  | med_ner.clinical.noncontrib | ner_clinical_noncontrib          |
| English  | [med_ner.diseases](https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html) | [ner_diseases](https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html)           |
| English  | [med_ner.diseases.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html) | [ner_diseases_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html)           |
| English  | [med_ner.diseases.large](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html) | [ner_diseases_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html)           |
| English  | [med_ner.drugs](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html) | [ner_drugs](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html)           |
| English  | [med_ner.drugsgreedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html) | [ner_drugs_greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html)           |
| English  | [med_ner.drugs.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html) | [ner_drugs_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html)           |
| English  | [med_ner.events_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html) | [ner_events_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html)           |
| English  | [med_ner.events_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html) | [ner_events_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html)           |
| English  | [med_ner.events_healthcre](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html) | [ner_events_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html)           |
| English  | [med_ner.financial_contract](https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html) | [ner_financial_contract](https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html)           |
| English  | [med_ner.healthcare](https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html) | [ner_healthcare](https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html)           |
| English  | [med_ner.human_phenotype.gene_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html) | [ner_human_phenotype_gene_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html)           |
| English  | [med_ner.human_phenotype.gene_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html) | [ner_human_phenotype_gene_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html)           |
| English  | [med_ner.human_phenotype.go_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html) | [ner_human_phenotype_go_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html)           |
| English  | [med_ner.human_phenotype.go_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html) | [ner_human_phenotype_go_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html)           |
| English  | [med_ner.jsl](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html) | [ner_jsl](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html)           |
| English  | [med_ner.jsl.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html) | [ner_jsl_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html)           |
| English  | [med_ner.jsl.enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html) | [ner_jsl_enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html)           |
| English  | [med_ner.jsl.enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html) | [ner_jsl_enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html)           |
| English  | [med_ner.measurements](https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html) | [ner_measurements_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html)           |
| English  | [med_ner.medmentions](https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html) | [ner_medmentions_coarse](https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html)           |
| English  | [med_ner.posology](https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html) | [ner_posology](https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html)           |
| English  | [med_ner.posology.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html) | [ner_posology_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html)           |
| English  | [med_ner.posology.greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html) | [ner_posology_greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html)           |
| English  | [med_ner.posology.healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html) | [ner_posology_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html)           |
| English  | [med_ner.posology.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html) | [ner_posology_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html)           |
| English  | [med_ner.posology.large_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html) | [ner_posology_large_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html)           |
| English  | [med_ner.posology.small](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html) | [ner_posology_small](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html)           |
| English  | [med_ner.radiology](https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html) | [ner_radiology](https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html)           |
| English  | [med_ner.radiology.wip_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html) | [ner_radiology_wip_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html)           |
| English  | [med_ner.risk_factors](https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html) | [ner_risk_factors](https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html)           |
| English  | [med_ner.risk_factors.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html) | [ner_risk_factors_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html)           |
| English  | med_ner.i2b2 | nerdl_i2b2          |
| English  | [med_ner.tumour](https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html) | [nerdl_tumour_demo](https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html)           |
| English  | med_ner.jsl.wip.clinical | jsl_ner_wip_clinical          |
| English  | [med_ner.jsl.wip.clinical.greedy](https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html) | [jsl_ner_wip_greedy_clinical](https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html)           |
| English  | [med_ner.jsl.wip.clinical.modifier](https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html) | [jsl_ner_wip_modifier_clinical](https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html)           |
| English  | [med_ner.jsl.wip.clinical.rd](https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html) | [jsl_rd_ner_wip_greedy_clinical](https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html)           |


## De-Identification Models

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [med_ner.deid.augmented](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html) | [ner_deid_augmented](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html)           |
| English  | [med_ner.deid.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html) | [ner_deid_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html)           |
| English  | [med_ner.deid.enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html) | [ner_deid_enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html)           |
| English  | [med_ner.deid.enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html) | [ner_deid_enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html)           |
| English  | [med_ner.deid.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html) | [ner_deid_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html)           |
| English  | [med_ner.deid.sd](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html) | [ner_deid_sd](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html)           |
| English  | [med_ner.deid.sd_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html) | [ner_deid_sd_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html)           |
| English  | med_ner.deid | nerdl_deid          |
| English  | med_ner.deid.synthetic | ner_deid_synthetic          |
| English  | [med_ner.deid.dl](https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html) | [ner_deidentify_dl](https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html)           |
| English  | [en.de_identify](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html) | [deidentify_rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html)           |
| English  | de_identify.rules | deid_rules          |
| English  | [de_identify.clinical](https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html) | [deidentify_enriched_clinical](https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html)           |
| English  | [de_identify.large](https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html) | [deidentify_large](https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html)           |
| English  | [de_identify.rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html) | [deidentify_rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html)           |
| English  | de_identify.rb_no_regex | deidentify_rb_no_regex          |



# Chunk resolvers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [resolve_chunk.athena_conditions](https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html) | [chunkresolve_athena_conditions_healthcare](https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html)           |
| English  | [resolve_chunk.cpt_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html) | [chunkresolve_cpt_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html) | [chunkresolve_icd10cm_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.diseases_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html) | [chunkresolve_icd10cm_diseases_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html)           |
| English  | resolve_chunk.icd10cm.hcc_clinical | chunkresolve_icd10cm_hcc_clinical          |
| English  | resolve_chunk.icd10cm.hcc_healthcare | chunkresolve_icd10cm_hcc_healthcare          |
| English  | [resolve_chunk.icd10cm.injuries](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html) | [chunkresolve_icd10cm_injuries_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.musculoskeletal](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html) | [chunkresolve_icd10cm_musculoskeletal_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.neoplasms](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html) | [chunkresolve_icd10cm_neoplasms_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.poison](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html) | [chunkresolve_icd10cm_poison_ext_clinical](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.puerile](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html) | [chunkresolve_icd10cm_puerile_clinical](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html)           |
| English  | resolve_chunk.icd10pcs.clinical | chunkresolve_icd10pcs_clinical          |
| English  | [resolve_chunk.icdo.clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html) | [chunkresolve_icdo_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html)           |
| English  | [resolve_chunk.loinc](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html) | [chunkresolve_loinc_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html)           |
| English  | [resolve_chunk.rxnorm.cd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html) | [chunkresolve_rxnorm_cd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html)           |
| English  | resolve_chunk.rxnorm.in | chunkresolve_rxnorm_in_clinical          |
| English  | resolve_chunk.rxnorm.in_healthcare | chunkresolve_rxnorm_in_healthcare          |
| English  | [resolve_chunk.rxnorm.sbd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html) | [chunkresolve_rxnorm_sbd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html)           |
| English  | [resolve_chunk.rxnorm.scd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html) | [chunkresolve_rxnorm_scd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html)           |
| English  | resolve_chunk.rxnorm.scdc | chunkresolve_rxnorm_scdc_clinical          |
| English  | resolve_chunk.rxnorm.scdc_healthcare | chunkresolve_rxnorm_scdc_healthcare          |
| English  | [resolve_chunk.rxnorm.xsmall.clinical](https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html) | [chunkresolve_rxnorm_xsmall_clinical](https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html)           |
| English  | [resolve_chunk.snomed.findings](https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html) | [chunkresolve_snomed_findings_clinical](https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html)           |


# New Classifiers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | classify.icd10.clinical | classifier_icd10cm_hcc_clinical          |
| English  | classify.icd10.healthcare | classifier_icd10cm_hcc_healthcare          |
| English  | [classify.ade.biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html) | [classifierdl_ade_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html)           |
| English  | [classify.ade.clinical](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html) | [classifierdl_ade_clinicalbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html)           |
| English  | [classify.ade.conversational](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html) | [classifierdl_ade_conversational_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html)           |
| English  | [classify.gender.biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html) | [classifierdl_gender_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html)           |
| English  | [classify.gender.sbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html) | [classifierdl_gender_sbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html)           |
| English  | classify.pico | classifierdl_pico_biobert          |


# German Medical models

| nlu.load() reference                                         | Spark NLP Model reference          |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [embed]    | w2v_cc_300d|
| [embed.w2v]    | w2v_cc_300d|
| [resolve_chunk]    | chunkresolve_ICD10GM|
| [resolve_chunk.icd10gm]    | chunkresolve_ICD10GM|
| resolve_chunk.icd10gm.2021    | chunkresolve_ICD10GM_2021|
| med_ner.legal   | ner_legal|
| med_ner    | ner_healthcare|
| med_ner.healthcare    | ner_healthcare|
| med_ner.healthcare_slim    | ner_healthcare_slim|
| med_ner.traffic    | ner_traffic|

# Spanish Medical models
| nlu.load() reference                                         | Spark NLP Model reference          |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [embed.scielo.150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html) | [embeddings_scielo_150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html)| 
| [embed.scielo.300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html)   | [embeddings_scielo_300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html)| 
| [embed.scielo.50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html)  | [embeddings_scielo_50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html)| 
| [embed.scielowiki.150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html)   | [embeddings_scielowiki_150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html)| 
| [embed.scielowiki.300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html)   | [embeddings_scielowiki_300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html)| 
| [embed.scielowiki.50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html)   | [embeddings_scielowiki_50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html)| 
| [embed.sciwiki.150d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html)   | [embeddings_sciwiki_150d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html)| 
| [embed.sciwiki.300d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html)   | [embeddings_sciwiki_300d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html)| 
| [embed.sciwiki.50d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html)   | [embeddings_sciwiki_50d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html)| 
| [med_ner](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)   |  [ner_diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)| 
| [med_ner.neoplasm](https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html)  | [ner_neoplasms](https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html)| 
| [med_ner.diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)  | [ner_diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)| 

# GPU Mode
You can now enable NLU GPU mode by setting `gpu=true` while loading a model. I.e. `nlu.load('train.sentiment' gpu=True)` . If must resart you kernel, if you already loaded a nlu pipeline withouth GPU mode.

# Output Level Relation
This new output level is used for relation extractors and will give you 1 row per relation extracted.


# Bug fixes
- Fixed a bug that caused loading NLU models in offline mode not to work in some occasions


# 1 line Install NLU
```!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash```

# Install via PIP 
```! pip install nlu pyspark==3.0.1```


## Additional NLU ressources

- [NLU Website](https://nlu.johnsnowlabs.com/)
- [All NLU Tutorial Notebooks](https://nlu.johnsnowlabs.com/docs/en/notebooks)
- [NLU Videos and Blogposts on NLU](https://nlp.johnsnowlabs.com/learn#pythons-nlu-library)
- [NLU on Github](https://github.com/JohnSnowLabs/nlu)
- [Suggestions or Questions? Contact us in Slack!](https://join.slack.com/t/spark-nlp/shared_invite/zt-lutct9gm-kuUazcyFKhuGY3_0AMkxqA)",t2_53n73cus,False,,0,False,"200+ State of the Art Medical Models for NER, Entity Resolution, Relation Extraction, Assertion, Spark 3 and Python 3.8 support - John Snow Labs NLU 3.0.0",[],r/LanguageTechnology,False,6,,0,,False,t3_n5guxs,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1620252763.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;h2&gt;200+ State of the Art Medical Models for NER, Entity Resolution, Relation Extraction, Assertion, Spark 3 and Python 3.8 support in  NLU 3.0 Release and much more&lt;/h2&gt;

&lt;p&gt;We are incredibly excited to announce the release of &lt;code&gt;NLU 3.0.0&lt;/code&gt; which makes most of John Snow Labs medical healthcare model available in just 1 line of code in NLU.
These models are the most accurate in their domains and highly scalable in Spark clusters.&lt;br/&gt;
In addition, &lt;code&gt;Spark 3.0.X&lt;/code&gt;  and &lt;code&gt;Spark 3.1.X&lt;/code&gt; is now supported, together with Python3.8&lt;/p&gt;

&lt;p&gt;This is enabled by the amazing &lt;a href=""https://nlp.johnsnowlabs.com/docs/en/release_notes#300""&gt;Spark NLP3.0.1&lt;/a&gt; and &lt;a href=""https://nlp.johnsnowlabs.com/docs/en/licensed_release_notes#301""&gt;Spark NLP for Healthcare 3.0.1&lt;/a&gt; releases.&lt;/p&gt;

&lt;h1&gt;New Features&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Over 200 new models for the &lt;code&gt;healthcare&lt;/code&gt; domain&lt;/li&gt;
&lt;li&gt;6 new classes of models, Assertion, Sentence/Chunk Resolvers, Relation Extractors, Medical NER models, De-Identificator Models&lt;/li&gt;
&lt;li&gt;Spark 3.0.X and 3.1.X support&lt;/li&gt;
&lt;li&gt;Python 3.8 Support&lt;/li&gt;
&lt;li&gt;New Output level &lt;code&gt;relation&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;1 Line to install NLU  just run &lt;code&gt;!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/spark-nlp/releases/tag/3.0.0""&gt;Various new EMR and Databricks versions supported&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GPU Mode, more then 600% speedup by enabling GPU mode.&lt;/li&gt;
&lt;li&gt;Authorized mode for licensed features&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;New Documentation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/docs/en/examples_hc#usage-examples-of-nluload""&gt;NLU for Healthcare Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/docs/en/examples_hc#authorize-access-to-licensed-features-and-install-healthcare-dependencies""&gt;Instrunctions to authorize your environment to use Licensed features&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;New Notebooks&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/medical_named_entity_recognition/overview_medical_entity_recognizers.ipynb""&gt;Medical Named Entity Extraction (NER) notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/relation_extraction/overview_relation.ipynb""&gt;Relation extraction notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/entity_resolution/entity_resolvers_overview.ipynb""&gt;Entity Resolution overview notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/assertion/assertion_overview.ipynb""&gt;Assertion overview notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/de_identification/DeIdentification_model_overview.ipynb""&gt;De-Identification overview notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu/blob/3.0rc1/examples/webinars_conferences_etc/graph_ai_summit/Healthcare_Graph_NLU_COVID_Tigergraph.ipynb""&gt;Graph NLU tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;AssertionDLModels&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html""&gt;assert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html""&gt;assertion_dl&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html""&gt;assert.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html""&gt;assertion_dl_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html""&gt;assert.healthcare&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html""&gt;assertion_dl_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html""&gt;assert.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html""&gt;assertion_dl_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;New Word Embeddings&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html""&gt;embed.glove.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html""&gt;embeddings_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html""&gt;embed.glove.biovec&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html""&gt;embeddings_biovec&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html""&gt;embed.glove.healthcare&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html""&gt;embeddings_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html""&gt;embed.glove.healthcare_100d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html""&gt;embeddings_healthcare_100d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;en.embed.glove.icdoem&lt;/td&gt;
&lt;td&gt;embeddings_icdoem&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;en.embed.glove.icdoem_2ng&lt;/td&gt;
&lt;td&gt;embeddings_icdoem_2ng&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;Sentence Entity resolvers&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;embed_sentence.biobert.mli&lt;/td&gt;
&lt;td&gt;sbiobert_base_cased_mli&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve&lt;/td&gt;
&lt;td&gt;sbiobertresolve_cpt&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve.cpt&lt;/td&gt;
&lt;td&gt;sbiobertresolve_cpt&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve.cpt.augmented&lt;/td&gt;
&lt;td&gt;sbiobertresolve_cpt_augmented&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve.cpt.procedures_augmented&lt;/td&gt;
&lt;td&gt;sbiobertresolve_cpt_procedures_augmented&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve.hcc.augmented&lt;/td&gt;
&lt;td&gt;sbiobertresolve_hcc_augmented&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html""&gt;resolve.icd10cm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html""&gt;sbiobertresolve_icd10cm&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html""&gt;resolve.icd10cm.augmented&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html""&gt;sbiobertresolve_icd10cm_augmented&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html""&gt;resolve.icd10cm.augmented_billable&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html""&gt;sbiobertresolve_icd10cm_augmented_billable_hcc&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html""&gt;resolve.icd10pcs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html""&gt;sbiobertresolve_icd10pcs&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html""&gt;resolve.icdo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html""&gt;sbiobertresolve_icdo&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html""&gt;resolve.rxcui&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html""&gt;sbiobertresolve_rxcui&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html""&gt;resolve.rxnorm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html""&gt;sbiobertresolve_rxnorm&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html""&gt;resolve.snomed&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html""&gt;sbiobertresolve_snomed_auxConcepts&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html""&gt;resolve.snomed.aux_concepts&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html""&gt;sbiobertresolve_snomed_auxConcepts&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html""&gt;resolve.snomed.aux_concepts_int&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html""&gt;sbiobertresolve_snomed_auxConcepts_int&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html""&gt;resolve.snomed.findings&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html""&gt;sbiobertresolve_snomed_findings&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html""&gt;resolve.snomed.findings_int&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html""&gt;sbiobertresolve_snomed_findings_int&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;RelationExtractionModel&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;relation.posology&lt;/td&gt;
&lt;td&gt;posology_re&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html""&gt;relation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html""&gt;redl_bodypart_direction_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html""&gt;relation.bodypart.direction&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html""&gt;redl_bodypart_direction_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html""&gt;relation.bodypart.problem&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html""&gt;redl_bodypart_problem_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html""&gt;relation.bodypart.procedure&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html""&gt;redl_bodypart_procedure_test_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html""&gt;relation.chemprot&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html""&gt;redl_chemprot_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html""&gt;relation.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html""&gt;redl_clinical_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls""&gt;relation.date&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls""&gt;redl_date_clinical_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html""&gt;relation.drug_drug_interaction&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html""&gt;redl_drug_drug_interaction_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html""&gt;relation.humen_phenotype_gene&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html""&gt;redl_human_phenotype_gene_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html""&gt;relation.temporal_events&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html""&gt;redl_temporal_events_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;NERDLModels&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html""&gt;med_ner.ade.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html""&gt;ner_ade_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html""&gt;med_ner.ade.clinical_bert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html""&gt;ner_ade_clinicalbert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html""&gt;med_ner.ade.ade_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html""&gt;ner_ade_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html""&gt;med_ner.anatomy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html""&gt;ner_anatomy&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html""&gt;med_ner.anatomy.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html""&gt;ner_anatomy_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html""&gt;med_ner.anatomy.coarse&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html""&gt;ner_anatomy_coarse&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html""&gt;med_ner.anatomy.coarse_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html""&gt;ner_anatomy_coarse_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html""&gt;med_ner.aspect_sentiment&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html""&gt;ner_aspect_based_sentiment&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html""&gt;med_ner.bacterial_species&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html""&gt;ner_bacterial_species&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html""&gt;med_ner.bionlp&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html""&gt;ner_bionlp&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html""&gt;med_ner.bionlp.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html""&gt;ner_bionlp_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html""&gt;med_ner.cancer&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html""&gt;ner_cancer_genetics&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Englishs&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html""&gt;med_ner.cellular&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html""&gt;ner_cellular&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html""&gt;med_ner.cellular.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html""&gt;ner_cellular_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html""&gt;med_ner.chemicals&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html""&gt;ner_chemicals&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html""&gt;med_ner.chemprot&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html""&gt;ner_chemprot_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html""&gt;med_ner.chemprot.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html""&gt;ner_chemprot_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html""&gt;med_ner.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html""&gt;ner_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html""&gt;med_ner.clinical.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html""&gt;ner_clinical_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;med_ner.clinical.noncontrib&lt;/td&gt;
&lt;td&gt;ner_clinical_noncontrib&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html""&gt;med_ner.diseases&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html""&gt;ner_diseases&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html""&gt;med_ner.diseases.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html""&gt;ner_diseases_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html""&gt;med_ner.diseases.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html""&gt;ner_diseases_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html""&gt;med_ner.drugs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html""&gt;ner_drugs&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html""&gt;med_ner.drugsgreedy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html""&gt;ner_drugs_greedy&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html""&gt;med_ner.drugs.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html""&gt;ner_drugs_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html""&gt;med_ner.events_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html""&gt;ner_events_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html""&gt;med_ner.events_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html""&gt;ner_events_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html""&gt;med_ner.events_healthcre&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html""&gt;ner_events_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html""&gt;med_ner.financial_contract&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html""&gt;ner_financial_contract&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html""&gt;med_ner.healthcare&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html""&gt;ner_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html""&gt;med_ner.human_phenotype.gene_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html""&gt;ner_human_phenotype_gene_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html""&gt;med_ner.human_phenotype.gene_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html""&gt;ner_human_phenotype_gene_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html""&gt;med_ner.human_phenotype.go_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html""&gt;ner_human_phenotype_go_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html""&gt;med_ner.human_phenotype.go_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html""&gt;ner_human_phenotype_go_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html""&gt;med_ner.jsl&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html""&gt;ner_jsl&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html""&gt;med_ner.jsl.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html""&gt;ner_jsl_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html""&gt;med_ner.jsl.enriched&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html""&gt;ner_jsl_enriched&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html""&gt;med_ner.jsl.enriched_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html""&gt;ner_jsl_enriched_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html""&gt;med_ner.measurements&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html""&gt;ner_measurements_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html""&gt;med_ner.medmentions&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html""&gt;ner_medmentions_coarse&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html""&gt;med_ner.posology&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html""&gt;ner_posology&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html""&gt;med_ner.posology.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html""&gt;ner_posology_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html""&gt;med_ner.posology.greedy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html""&gt;ner_posology_greedy&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html""&gt;med_ner.posology.healthcare&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html""&gt;ner_posology_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html""&gt;med_ner.posology.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html""&gt;ner_posology_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html""&gt;med_ner.posology.large_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html""&gt;ner_posology_large_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html""&gt;med_ner.posology.small&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html""&gt;ner_posology_small&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html""&gt;med_ner.radiology&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html""&gt;ner_radiology&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html""&gt;med_ner.radiology.wip_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html""&gt;ner_radiology_wip_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html""&gt;med_ner.risk_factors&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html""&gt;ner_risk_factors&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html""&gt;med_ner.risk_factors.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html""&gt;ner_risk_factors_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;med_ner.i2b2&lt;/td&gt;
&lt;td&gt;nerdl_i2b2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html""&gt;med_ner.tumour&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html""&gt;nerdl_tumour_demo&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;med_ner.jsl.wip.clinical&lt;/td&gt;
&lt;td&gt;jsl_ner_wip_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html""&gt;med_ner.jsl.wip.clinical.greedy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html""&gt;jsl_ner_wip_greedy_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html""&gt;med_ner.jsl.wip.clinical.modifier&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html""&gt;jsl_ner_wip_modifier_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html""&gt;med_ner.jsl.wip.clinical.rd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html""&gt;jsl_rd_ner_wip_greedy_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2&gt;De-Identification Models&lt;/h2&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html""&gt;med_ner.deid.augmented&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html""&gt;ner_deid_augmented&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html""&gt;med_ner.deid.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html""&gt;ner_deid_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html""&gt;med_ner.deid.enriched&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html""&gt;ner_deid_enriched&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html""&gt;med_ner.deid.enriched_biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html""&gt;ner_deid_enriched_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html""&gt;med_ner.deid.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html""&gt;ner_deid_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html""&gt;med_ner.deid.sd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html""&gt;ner_deid_sd&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html""&gt;med_ner.deid.sd_large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html""&gt;ner_deid_sd_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;med_ner.deid&lt;/td&gt;
&lt;td&gt;nerdl_deid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;med_ner.deid.synthetic&lt;/td&gt;
&lt;td&gt;ner_deid_synthetic&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html""&gt;med_ner.deid.dl&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html""&gt;ner_deidentify_dl&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html""&gt;en.de_identify&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html""&gt;deidentify_rb&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;de_identify.rules&lt;/td&gt;
&lt;td&gt;deid_rules&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html""&gt;de_identify.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html""&gt;deidentify_enriched_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html""&gt;de_identify.large&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html""&gt;deidentify_large&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html""&gt;de_identify.rb&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html""&gt;deidentify_rb&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;de_identify.rb_no_regex&lt;/td&gt;
&lt;td&gt;deidentify_rb_no_regex&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;Chunk resolvers&lt;/h1&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html""&gt;resolve_chunk.athena_conditions&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html""&gt;chunkresolve_athena_conditions_healthcare&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html""&gt;resolve_chunk.cpt_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html""&gt;chunkresolve_cpt_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html""&gt;resolve_chunk.icd10cm.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html""&gt;chunkresolve_icd10cm_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html""&gt;resolve_chunk.icd10cm.diseases_clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html""&gt;chunkresolve_icd10cm_diseases_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.icd10cm.hcc_clinical&lt;/td&gt;
&lt;td&gt;chunkresolve_icd10cm_hcc_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.icd10cm.hcc_healthcare&lt;/td&gt;
&lt;td&gt;chunkresolve_icd10cm_hcc_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html""&gt;resolve_chunk.icd10cm.injuries&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html""&gt;chunkresolve_icd10cm_injuries_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html""&gt;resolve_chunk.icd10cm.musculoskeletal&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html""&gt;chunkresolve_icd10cm_musculoskeletal_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html""&gt;resolve_chunk.icd10cm.neoplasms&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html""&gt;chunkresolve_icd10cm_neoplasms_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html""&gt;resolve_chunk.icd10cm.poison&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html""&gt;chunkresolve_icd10cm_poison_ext_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html""&gt;resolve_chunk.icd10cm.puerile&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html""&gt;chunkresolve_icd10cm_puerile_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.icd10pcs.clinical&lt;/td&gt;
&lt;td&gt;chunkresolve_icd10pcs_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html""&gt;resolve_chunk.icdo.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html""&gt;chunkresolve_icdo_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html""&gt;resolve_chunk.loinc&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html""&gt;chunkresolve_loinc_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html""&gt;resolve_chunk.rxnorm.cd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html""&gt;chunkresolve_rxnorm_cd_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.rxnorm.in&lt;/td&gt;
&lt;td&gt;chunkresolve_rxnorm_in_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.rxnorm.in_healthcare&lt;/td&gt;
&lt;td&gt;chunkresolve_rxnorm_in_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html""&gt;resolve_chunk.rxnorm.sbd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html""&gt;chunkresolve_rxnorm_sbd_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html""&gt;resolve_chunk.rxnorm.scd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html""&gt;chunkresolve_rxnorm_scd_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.rxnorm.scdc&lt;/td&gt;
&lt;td&gt;chunkresolve_rxnorm_scdc_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;resolve_chunk.rxnorm.scdc_healthcare&lt;/td&gt;
&lt;td&gt;chunkresolve_rxnorm_scdc_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html""&gt;resolve_chunk.rxnorm.xsmall.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html""&gt;chunkresolve_rxnorm_xsmall_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html""&gt;resolve_chunk.snomed.findings&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html""&gt;chunkresolve_snomed_findings_clinical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;New Classifiers&lt;/h1&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;classify.icd10.clinical&lt;/td&gt;
&lt;td&gt;classifier_icd10cm_hcc_clinical&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;classify.icd10.healthcare&lt;/td&gt;
&lt;td&gt;classifier_icd10cm_hcc_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html""&gt;classify.ade.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html""&gt;classifierdl_ade_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html""&gt;classify.ade.clinical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html""&gt;classifierdl_ade_clinicalbert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html""&gt;classify.ade.conversational&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html""&gt;classifierdl_ade_conversational_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html""&gt;classify.gender.biobert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html""&gt;classifierdl_gender_biobert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html""&gt;classify.gender.sbert&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html""&gt;classifierdl_gender_sbert&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;classify.pico&lt;/td&gt;
&lt;td&gt;classifierdl_pico_biobert&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;German Medical models&lt;/h1&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;[embed]&lt;/td&gt;
&lt;td&gt;w2v_cc_300d&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[embed.w2v]&lt;/td&gt;
&lt;td&gt;w2v_cc_300d&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[resolve_chunk]&lt;/td&gt;
&lt;td&gt;chunkresolve_ICD10GM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[resolve_chunk.icd10gm]&lt;/td&gt;
&lt;td&gt;chunkresolve_ICD10GM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;resolve_chunk.icd10gm.2021&lt;/td&gt;
&lt;td&gt;chunkresolve_ICD10GM_2021&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;med_ner.legal&lt;/td&gt;
&lt;td&gt;ner_legal&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;med_ner&lt;/td&gt;
&lt;td&gt;ner_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;med_ner.healthcare&lt;/td&gt;
&lt;td&gt;ner_healthcare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;med_ner.healthcare_slim&lt;/td&gt;
&lt;td&gt;ner_healthcare_slim&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;med_ner.traffic&lt;/td&gt;
&lt;td&gt;ner_traffic&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;Spanish Medical models&lt;/h1&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;nlu.load() reference&lt;/th&gt;
&lt;th&gt;Spark NLP Model reference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html""&gt;embed.scielo.150d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html""&gt;embeddings_scielo_150d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html""&gt;embed.scielo.300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html""&gt;embeddings_scielo_300d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html""&gt;embed.scielo.50d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html""&gt;embeddings_scielo_50d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html""&gt;embed.scielowiki.150d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html""&gt;embeddings_scielowiki_150d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html""&gt;embed.scielowiki.300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html""&gt;embeddings_scielowiki_300d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html""&gt;embed.scielowiki.50d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html""&gt;embeddings_scielowiki_50d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html""&gt;embed.sciwiki.150d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html""&gt;embeddings_sciwiki_150d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html""&gt;embed.sciwiki.300d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html""&gt;embeddings_sciwiki_300d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html""&gt;embed.sciwiki.50d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html""&gt;embeddings_sciwiki_50d&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html""&gt;med_ner&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html""&gt;ner_diag_proc&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html""&gt;med_ner.neoplasm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html""&gt;ner_neoplasms&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html""&gt;med_ner.diag_proc&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=""https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html""&gt;ner_diag_proc&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h1&gt;GPU Mode&lt;/h1&gt;

&lt;p&gt;You can now enable NLU GPU mode by setting &lt;code&gt;gpu=true&lt;/code&gt; while loading a model. I.e. &lt;code&gt;nlu.load(&amp;#39;train.sentiment&amp;#39; gpu=True)&lt;/code&gt; . If must resart you kernel, if you already loaded a nlu pipeline withouth GPU mode.&lt;/p&gt;

&lt;h1&gt;Output Level Relation&lt;/h1&gt;

&lt;p&gt;This new output level is used for relation extractors and will give you 1 row per relation extracted.&lt;/p&gt;

&lt;h1&gt;Bug fixes&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Fixed a bug that caused loading NLU models in offline mode not to work in some occasions&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;1 line Install NLU&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash&lt;/code&gt;&lt;/p&gt;

&lt;h1&gt;Install via PIP&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;! pip install nlu pyspark==3.0.1&lt;/code&gt;&lt;/p&gt;

&lt;h2&gt;Additional NLU ressources&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/""&gt;NLU Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlu.johnsnowlabs.com/docs/en/notebooks""&gt;All NLU Tutorial Notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://nlp.johnsnowlabs.com/learn#pythons-nlu-library""&gt;NLU Videos and Blogposts on NLU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://github.com/JohnSnowLabs/nlu""&gt;NLU on Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://join.slack.com/t/spark-nlp/shared_invite/zt-lutct9gm-kuUazcyFKhuGY3_0AMkxqA""&gt;Suggestions or Questions? Contact us in Slack!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n5guxs,True,,CKL-IT,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n5guxs/200_state_of_the_art_medical_models_for_ner/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n5guxs/200_state_of_the_art_medical_models_for_ner/,30199,1620223963.0,0,,False,,,,,,38256
1237,,LanguageTechnology,"Hello there,

im currently doing my research for my bachelor thesis.  
What is a nice technique to extract keywords from one or multiple documents. Currently i've tried TF-IDF, RAKE and YAKE. The hard part somehow is to find the Least Common Multiple of the documents.

Could someone help me out?

&amp;#x200B;

appreciate it",t2_8it6p6no,False,,0,False,Q: Multi-Document Keyword Extraction,[],r/LanguageTechnology,False,6,,0,,False,t3_n5e67a,False,dark,0.99,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1620244623.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello there,&lt;/p&gt;

&lt;p&gt;im currently doing my research for my bachelor thesis.&lt;br/&gt;
What is a nice technique to extract keywords from one or multiple documents. Currently i&amp;#39;ve tried TF-IDF, RAKE and YAKE. The hard part somehow is to find the Least Common Multiple of the documents.&lt;/p&gt;

&lt;p&gt;Could someone help me out?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;appreciate it&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n5e67a,True,,Logical-Necessary524,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n5e67a/q_multidocument_keyword_extraction/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n5e67a/q_multidocument_keyword_extraction/,30199,1620215823.0,0,,False,,,,,,325
1238,,LanguageTechnology,"Hello NLP redditors.

I'm currently working on a project where I need to find the similarity between sentences. At the moment I'm using cosine similarity with [fastText word embeddings](https://fasttext.cc/docs/en/pretrained-vectors.html) and TF-IDF weights. On single languages results are pretty good so far, my boss is happy.

The next step is to solve the same problem with multiple languages. My idea is to use the same technology but with [aligned fastText word embeddings](https://fasttext.cc/docs/en/aligned-vectors.html). In theory, using vocabularies with aligned word vectors should mean that words with similar meanings (in different languages) have a similar vector. Thus, comparing ""the car runs fast"" and ""l'auto corre veloce"" (Italian) should bring a high cosine similarity.

In practice, results are disappointing. Even a comparison between simple particles such as ""yes"" and ""sì"", or even ""no"" and ""no"", bring a cosine similarity well below any acceptable threshold.

Is there anyone here that had some experience with aligned word vectors and can tell me if it's a lost war or there is something I can do to improve the results?

We currently train our own vocabularies on Wikipedia and other sources, and we align the vocabularies using [MUSE](https://github.com/facebookresearch/MUSE) with default settings (0-5000 dictionary for training, 5000-6500 dictionary for evaluation and 5 refinements).

Any help will be appreciated. Thanks in advance.

**Edit: Thanks all for the answers! Will take a look at each suggestion.**",t2_5pyizy4o,False,,0,False,Help with aligned word embeddings,[],r/LanguageTechnology,False,6,,0,,False,t3_n4wx6v,False,dark,0.94,,public,13,0,{},,False,[],,False,False,,{},,False,13,,False,False,,1620203830.0,,[],{},,True,,1620187863.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello NLP redditors.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m currently working on a project where I need to find the similarity between sentences. At the moment I&amp;#39;m using cosine similarity with &lt;a href=""https://fasttext.cc/docs/en/pretrained-vectors.html""&gt;fastText word embeddings&lt;/a&gt; and TF-IDF weights. On single languages results are pretty good so far, my boss is happy.&lt;/p&gt;

&lt;p&gt;The next step is to solve the same problem with multiple languages. My idea is to use the same technology but with &lt;a href=""https://fasttext.cc/docs/en/aligned-vectors.html""&gt;aligned fastText word embeddings&lt;/a&gt;. In theory, using vocabularies with aligned word vectors should mean that words with similar meanings (in different languages) have a similar vector. Thus, comparing &amp;quot;the car runs fast&amp;quot; and &amp;quot;l&amp;#39;auto corre veloce&amp;quot; (Italian) should bring a high cosine similarity.&lt;/p&gt;

&lt;p&gt;In practice, results are disappointing. Even a comparison between simple particles such as &amp;quot;yes&amp;quot; and &amp;quot;sì&amp;quot;, or even &amp;quot;no&amp;quot; and &amp;quot;no&amp;quot;, bring a cosine similarity well below any acceptable threshold.&lt;/p&gt;

&lt;p&gt;Is there anyone here that had some experience with aligned word vectors and can tell me if it&amp;#39;s a lost war or there is something I can do to improve the results?&lt;/p&gt;

&lt;p&gt;We currently train our own vocabularies on Wikipedia and other sources, and we align the vocabularies using &lt;a href=""https://github.com/facebookresearch/MUSE""&gt;MUSE&lt;/a&gt; with default settings (0-5000 dictionary for training, 5000-6500 dictionary for evaluation and 5 refinements).&lt;/p&gt;

&lt;p&gt;Any help will be appreciated. Thanks in advance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Edit: Thanks all for the answers! Will take a look at each suggestion.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n4wx6v,True,,Next-Initiative,,12,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n4wx6v/help_with_aligned_word_embeddings/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n4wx6v/help_with_aligned_word_embeddings/,30199,1620159063.0,0,,False,,,,,,1542
1239,,LanguageTechnology,"Hi everyone! I have started here a blog for discussing various papers in the field of NLP/CompLing: https://selfassuredpaperreads.medium.com/
Do give it a read and let me know what you think!",t2_67r0a3tc,False,,0,False,Blog for papers in NLP/CompLing,[],r/LanguageTechnology,False,6,,0,,False,t3_n4nt8n,False,dark,0.9,,public,17,1,{},,False,[],,False,False,,{},,False,17,,False,False,,1620190043.0,,[],{},,True,,1620162169.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone! I have started here a blog for discussing various papers in the field of NLP/CompLing: &lt;a href=""https://selfassuredpaperreads.medium.com/""&gt;https://selfassuredpaperreads.medium.com/&lt;/a&gt;
Do give it a read and let me know what you think!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 1000, 'id': 'award_35c78e6e-507b-4f1d-b3d8-ed43840909a8', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 800, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_128.png', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'The treasure at the end of the rainbow. Gives the author 800 Coins to do with as they please.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': ""Pot o' Coins"", 'resized_static_icons': [{'url': 'https://external-preview.redd.it?width=16&amp;height=16&amp;auto=webp&amp;s=d88c9a453f8ac38850b7a8241cfe5804b7b4905d', 'width': 16, 'height': 16}, {'url': 'https://external-preview.redd.it?width=32&amp;height=32&amp;auto=webp&amp;s=96a25019eb75878bdec4f6c012540f3baffbb1b2', 'width': 32, 'height': 32}, {'url': 'https://external-preview.redd.it?width=48&amp;height=48&amp;auto=webp&amp;s=1a51d27d75afde3fbde8bba84f9338f511211461', 'width': 48, 'height': 48}, {'url': 'https://external-preview.redd.it?width=64&amp;height=64&amp;auto=webp&amp;s=96af5ec460b05669ed60224cb0619bb8884abe27', 'width': 64, 'height': 64}, {'url': 'https://external-preview.redd.it?width=128&amp;height=128&amp;auto=webp&amp;s=2d3e648ed2302e6258673051ca5291f57beb29d4', 'width': 128, 'height': 128}], 'icon_format': 'APNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/pot_o_coins_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n4nt8n,True,,patrickmelroseisi,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n4nt8n/blog_for_papers_in_nlpcompling/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n4nt8n/blog_for_papers_in_nlpcompling/,30199,1620133369.0,0,,False,,,,,,191
1240,,LanguageTechnology,,t2_hkv9s,False,,0,False,EmbedRank: Simple Unsupervised Keyphrase Extraction using Sentence Embeddings,[],r/LanguageTechnology,False,6,,0,,False,t3_n4sbzu,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,False,,1620173859.0,text,6,,,text,link.medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n4sbzu,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n4sbzu/embedrank_simple_unsupervised_keyphrase/,all_ads,False,https://link.medium.com/YGm9rNXlZfb,30199,1620145059.0,0,,False,https://link.medium.com/YGm9rNXlZfb,,,,,0
1241,,LanguageTechnology,In most standard applications of text classification - using algorithms like word2vec/glove/bert ... is text data automatically converted into vectors of numbers? Can you then use regression models right away once the text has been converted into numbers?,t2_3f0i9m72,False,,0,False,Text classification: words to numbers,[],r/LanguageTechnology,False,6,,0,,False,t3_n4v4bu,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1620183586.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In most standard applications of text classification - using algorithms like word2vec/glove/bert ... is text data automatically converted into vectors of numbers? Can you then use regression models right away once the text has been converted into numbers?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n4v4bu,True,,SQL_beginner,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n4v4bu/text_classification_words_to_numbers/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n4v4bu/text_classification_words_to_numbers/,30199,1620154786.0,0,,False,,,,,,255
1242,,LanguageTechnology,"Hi everyone!  


I need to create a Bot that uses NLP with R!  


has anyone ever done this?  


Thank you so much!",t2_7fjpeklw,False,,0,False,Chatbot with R,[],r/LanguageTechnology,False,6,,0,,False,t3_n4p8ur,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1620166225.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone!  &lt;/p&gt;

&lt;p&gt;I need to create a Bot that uses NLP with R!  &lt;/p&gt;

&lt;p&gt;has anyone ever done this?  &lt;/p&gt;

&lt;p&gt;Thank you so much!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n4p8ur,True,,DanielaSMPereira,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n4p8ur/chatbot_with_r/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n4p8ur/chatbot_with_r/,30199,1620137425.0,0,,False,,,,,,115
1243,,LanguageTechnology,"1. How is it possible to extract a single word (may be in textgrid) from the textgrid file of a sentence through a code in praat?

&amp;#x200B;

Suppose, from a long file I have put boundary and created one textgrid containing 4 tokens sentences containing the same key word life. The sentence is ""He loves life and laughter."" The tokens are:

&amp;#x200B;

S1\_life1-life, S1\_life2-life, S1\_life3-life, S1\_life4-life. I need to write a praat code that will separate the ""life"" from the sentence text grid.

&amp;#x200B;

2. Will the same code be applicable for the same function in a different sentence where the positioning of the keyword is different. For instance, ""Life has a different meaning in the mountains."" In here, they keyword is at the beginning of the sentence.",t2_8frpvpx5,False,,0,False,How is it possible to extract a single word (may be in textgrid) from the textgrid file of a sentence through a code in praat?,[],r/LanguageTechnology,False,6,,0,,False,t3_n4og06,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1620164044.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;ol&gt;
&lt;li&gt;How is it possible to extract a single word (may be in textgrid) from the textgrid file of a sentence through a code in praat?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Suppose, from a long file I have put boundary and created one textgrid containing 4 tokens sentences containing the same key word life. The sentence is &amp;quot;He loves life and laughter.&amp;quot; The tokens are:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;S1_life1-life, S1_life2-life, S1_life3-life, S1_life4-life. I need to write a praat code that will separate the &amp;quot;life&amp;quot; from the sentence text grid.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Will the same code be applicable for the same function in a different sentence where the positioning of the keyword is different. For instance, &amp;quot;Life has a different meaning in the mountains.&amp;quot; In here, they keyword is at the beginning of the sentence.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n4og06,True,,phonomonal,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n4og06/how_is_it_possible_to_extract_a_single_word_may/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n4og06/how_is_it_possible_to_extract_a_single_word_may/,30199,1620135244.0,0,,False,,,,,,779
1244,,LanguageTechnology,"I have feeling that not many people are willing to admit - but ultimately, is a significant part of many data mining projects (e.g. checking data quality, parsing through data, etc.) still done manually? 

For example here is an example I just made up relating to Supervised NLP (Natural Language Processing) Classification : Suppose I have 1000 medical reports of patients, containing unstructured text made by a doctor during a hospital visit. For a given patient, each report contains all the text notes that the doctor made for that patient, for visits between 2010 and 2020. These reports make mention of the patients bio data (e.g. age, gender, medical history, etc.) and the details of the symptoms that the patient is experiencing over a long period of time (e.g. let's say that these reports are 2000 words on average). The problem is, different doctors have different styles of writing - each of these 1000 reports is different from another. If a human were to read the report, the human could figure out what happened to the patient - did the patient have a serious condition (let's call this ""class 1"") or a non-serious condition (let's call this ""class 0""). This is what we are interested in predicting for future patients based on the limited medical notes made by doctors for these future patients. 

The problem is - there is no clear and fast way (not that I know of) to take the 1000 medical reports that are available, and label each report as ""class 1"" or ""class 0"". For example, for ""class 0"" : one of the doctors could clearly write at the end of a report ""all medical tests were conducted and the results and were all negative"", and another doctor could end the report by saying ""the patient should seriously consider changing their lifestyle and eat healthier food. benign."" . 

In this example, how would someone assign labels to all these 1000 cases, without manually reading them and deciding if the information in the report corresponds to a ""serious condition"" or a ""non-serious condition""? I was thinking of using something like ""sentiment analysis"" to capture the ""mood"" of these reports, and use sentiment analysis a method to informally gauge if the tone of the report is ""dark"" (serious condition) or ""light"" (non serious condition). But I am not sure if this is the best way to approach this problem. Is there a way to do this without reading all the reports and manually deciding labels?

In the end - this is what I am interested in doing : suppose a new patient comes in and on the first visit, the doctor makes some quick notes (e.g. patient is male, 30 years old, 180 cm, 100 kg, non-smoker, frequently complains of chest pains, no high blood pressure, works a construction worker and takes daily medicine for acid reflex). Just based on these quick notes and the 1000 reports available (NOTE: I am trying to illustrate a point here, that the medical notes for the new patient and the 1000 reports DO NOT have the same format), can a researcher predict (supervised classification, e.g. decision tree) if this patient will have a ""serious"" or a ""non-serious"" condition?

PS: suppose the doctors have a very detailed medical encyclopedia on their computers - can this medical encyclopedia be used alongside the 1000 medical reports to improve the prediction results?",t2_xtuyc,False,,0,False,Inevitable Manual Work involved in NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_n4i0le,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1620140092.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have feeling that not many people are willing to admit - but ultimately, is a significant part of many data mining projects (e.g. checking data quality, parsing through data, etc.) still done manually? &lt;/p&gt;

&lt;p&gt;For example here is an example I just made up relating to Supervised NLP (Natural Language Processing) Classification : Suppose I have 1000 medical reports of patients, containing unstructured text made by a doctor during a hospital visit. For a given patient, each report contains all the text notes that the doctor made for that patient, for visits between 2010 and 2020. These reports make mention of the patients bio data (e.g. age, gender, medical history, etc.) and the details of the symptoms that the patient is experiencing over a long period of time (e.g. let&amp;#39;s say that these reports are 2000 words on average). The problem is, different doctors have different styles of writing - each of these 1000 reports is different from another. If a human were to read the report, the human could figure out what happened to the patient - did the patient have a serious condition (let&amp;#39;s call this &amp;quot;class 1&amp;quot;) or a non-serious condition (let&amp;#39;s call this &amp;quot;class 0&amp;quot;). This is what we are interested in predicting for future patients based on the limited medical notes made by doctors for these future patients. &lt;/p&gt;

&lt;p&gt;The problem is - there is no clear and fast way (not that I know of) to take the 1000 medical reports that are available, and label each report as &amp;quot;class 1&amp;quot; or &amp;quot;class 0&amp;quot;. For example, for &amp;quot;class 0&amp;quot; : one of the doctors could clearly write at the end of a report &amp;quot;all medical tests were conducted and the results and were all negative&amp;quot;, and another doctor could end the report by saying &amp;quot;the patient should seriously consider changing their lifestyle and eat healthier food. benign.&amp;quot; . &lt;/p&gt;

&lt;p&gt;In this example, how would someone assign labels to all these 1000 cases, without manually reading them and deciding if the information in the report corresponds to a &amp;quot;serious condition&amp;quot; or a &amp;quot;non-serious condition&amp;quot;? I was thinking of using something like &amp;quot;sentiment analysis&amp;quot; to capture the &amp;quot;mood&amp;quot; of these reports, and use sentiment analysis a method to informally gauge if the tone of the report is &amp;quot;dark&amp;quot; (serious condition) or &amp;quot;light&amp;quot; (non serious condition). But I am not sure if this is the best way to approach this problem. Is there a way to do this without reading all the reports and manually deciding labels?&lt;/p&gt;

&lt;p&gt;In the end - this is what I am interested in doing : suppose a new patient comes in and on the first visit, the doctor makes some quick notes (e.g. patient is male, 30 years old, 180 cm, 100 kg, non-smoker, frequently complains of chest pains, no high blood pressure, works a construction worker and takes daily medicine for acid reflex). Just based on these quick notes and the 1000 reports available (NOTE: I am trying to illustrate a point here, that the medical notes for the new patient and the 1000 reports DO NOT have the same format), can a researcher predict (supervised classification, e.g. decision tree) if this patient will have a &amp;quot;serious&amp;quot; or a &amp;quot;non-serious&amp;quot; condition?&lt;/p&gt;

&lt;p&gt;PS: suppose the doctors have a very detailed medical encyclopedia on their computers - can this medical encyclopedia be used alongside the 1000 medical reports to improve the prediction results?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n4i0le,True,,ottawalanguages,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n4i0le/inevitable_manual_work_involved_in_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n4i0le/inevitable_manual_work_involved_in_nlp/,30199,1620111292.0,0,,False,,,,,,3304
1245,,LanguageTechnology,"I am trying to solve multi-class classification problem using BERT.

My training accuracy is way higher than the validation and test sets so I assume that is clearly overfitting.

&amp;#x200B;

This is the configuration for my BERT Classifier

    ""class_name"": ""bert_classifier"",
            ""n_classes"": 5,
            ""return_probas"": True,
            ""one_hot_labels"": True,
            ""bert_config_file"": ""/content/ru_conversational_cased_L-12_H-768_A-12/bert_config.json"",
            ""pretrained_bert"": ""/content/ru_conversational_cased_L-12_H-768_A-12/bert_model.ckpt"",
            ""save_path"": ""sst_bert_model/model"",
            ""load_path"": ""sst_bert_model/model"",
            ""keep_prob"": 0.5,
            ""learning_rate"": 1e-05,
            ""weight_decay_rate"": 0.001, #this line added after the overfitting
            ""learning_rate_drop_patience"": 5,
            ""learning_rate_drop_div"": 2.0,
            ""in"": [
              ""bert_features""
                ],
            ""in_y"": [
              ""y_onehot""
            ],
            ""out"": [
              ""y_pred_probas""
            ]

Here is my plan to overcome the issue of overfitting.

1. I added a little `more data` to the current one, (just a bit, at least something that I could find)
2. I added `""weight_decay_rate"": 0.001`, to basically regularize the weight of the model
3. I increased the `batch size to 32`. It was 16 before ( I don't know if that helps)
4. I set a `""learning_rate_drop_patience"": 2,` so it stops training after 2 epochs if there no is an improvement.

&amp;#x200B;

The last thing I wanna do is to increase my dropout

    DropOut = 0.5

&amp;#x200B;

**QUESTIONS:** 

1. However, I can't figure out which parameter is actually ""dropout"" as there is no parameter called directly dropout.  Mainly because the language I am targeting is Russian and I need to use Deep Pavlov [http://docs.deeppavlov.ai/en/master/apiref/models/bert.html](http://docs.deeppavlov.ai/en/master/apiref/models/bert.html) not the hugging face directly so Deep Pavlov Bert Classifier has only these 3 things that are kinda close to what I am looking for (`dropout function`)

&amp;#8203;

    keep_prob             -     dropout keep_prob for non-Bert layers
    attention_probs_keep_prob – keep_prob for Bert self-attention layers
    hidden_keep_prob –           keep_prob for Bert hidden layers

which one is the `Dropout`? which function I need to make 0.5 out of these three?

   2.    I have 3 `epochs` in total, is it correct that I am making `patience rate` (it stops training after 2 epochs if there is no improvement) ==  2, instead of 1? 

   3.    I am thinking to minimize the `learning rate` as well. Now it is currently `1e-05.`  Should I make it maybe 1e-3? 

   4. And overall, what else can you suggest to me to overcome the issue of overfitting? is the plan good enough to tackle this problem?",t2_6c0lef9b,False,,0,False,Overfitting,[],r/LanguageTechnology,False,6,,0,,False,t3_n4ei50,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,1620106976.0,,[],{},,True,,1620126051.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to solve multi-class classification problem using BERT.&lt;/p&gt;

&lt;p&gt;My training accuracy is way higher than the validation and test sets so I assume that is clearly overfitting.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;This is the configuration for my BERT Classifier&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;class_name&amp;quot;: &amp;quot;bert_classifier&amp;quot;,
        &amp;quot;n_classes&amp;quot;: 5,
        &amp;quot;return_probas&amp;quot;: True,
        &amp;quot;one_hot_labels&amp;quot;: True,
        &amp;quot;bert_config_file&amp;quot;: &amp;quot;/content/ru_conversational_cased_L-12_H-768_A-12/bert_config.json&amp;quot;,
        &amp;quot;pretrained_bert&amp;quot;: &amp;quot;/content/ru_conversational_cased_L-12_H-768_A-12/bert_model.ckpt&amp;quot;,
        &amp;quot;save_path&amp;quot;: &amp;quot;sst_bert_model/model&amp;quot;,
        &amp;quot;load_path&amp;quot;: &amp;quot;sst_bert_model/model&amp;quot;,
        &amp;quot;keep_prob&amp;quot;: 0.5,
        &amp;quot;learning_rate&amp;quot;: 1e-05,
        &amp;quot;weight_decay_rate&amp;quot;: 0.001, #this line added after the overfitting
        &amp;quot;learning_rate_drop_patience&amp;quot;: 5,
        &amp;quot;learning_rate_drop_div&amp;quot;: 2.0,
        &amp;quot;in&amp;quot;: [
          &amp;quot;bert_features&amp;quot;
            ],
        &amp;quot;in_y&amp;quot;: [
          &amp;quot;y_onehot&amp;quot;
        ],
        &amp;quot;out&amp;quot;: [
          &amp;quot;y_pred_probas&amp;quot;
        ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is my plan to overcome the issue of overfitting.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I added a little &lt;code&gt;more data&lt;/code&gt; to the current one, (just a bit, at least something that I could find)&lt;/li&gt;
&lt;li&gt;I added &lt;code&gt;&amp;quot;weight_decay_rate&amp;quot;: 0.001&lt;/code&gt;, to basically regularize the weight of the model&lt;/li&gt;
&lt;li&gt;I increased the &lt;code&gt;batch size to 32&lt;/code&gt;. It was 16 before ( I don&amp;#39;t know if that helps)&lt;/li&gt;
&lt;li&gt;I set a &lt;code&gt;&amp;quot;learning_rate_drop_patience&amp;quot;: 2,&lt;/code&gt; so it stops training after 2 epochs if there no is an improvement.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The last thing I wanna do is to increase my dropout&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DropOut = 0.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;QUESTIONS:&lt;/strong&gt; &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;However, I can&amp;#39;t figure out which parameter is actually &amp;quot;dropout&amp;quot; as there is no parameter called directly dropout.  Mainly because the language I am targeting is Russian and I need to use Deep Pavlov &lt;a href=""http://docs.deeppavlov.ai/en/master/apiref/models/bert.html""&gt;http://docs.deeppavlov.ai/en/master/apiref/models/bert.html&lt;/a&gt; not the hugging face directly so Deep Pavlov Bert Classifier has only these 3 things that are kinda close to what I am looking for (&lt;code&gt;dropout function&lt;/code&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;#8203;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;keep_prob             -     dropout keep_prob for non-Bert layers
attention_probs_keep_prob – keep_prob for Bert self-attention layers
hidden_keep_prob –           keep_prob for Bert hidden layers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which one is the &lt;code&gt;Dropout&lt;/code&gt;? which function I need to make 0.5 out of these three?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;I have 3 &lt;code&gt;epochs&lt;/code&gt; in total, is it correct that I am making &lt;code&gt;patience rate&lt;/code&gt; (it stops training after 2 epochs if there is no improvement) ==  2, instead of 1? &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I am thinking to minimize the &lt;code&gt;learning rate&lt;/code&gt; as well. Now it is currently &lt;code&gt;1e-05.&lt;/code&gt;  Should I make it maybe 1e-3? &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;And overall, what else can you suggest to me to overcome the issue of overfitting? is the plan good enough to tackle this problem?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n4ei50,True,,strangeguy111,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n4ei50/overfitting/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n4ei50/overfitting/,30199,1620097251.0,0,,False,,,,,,2890
1246,,LanguageTechnology,"In my last post [https://www.reddit.com/r/learnmachinelearning/comments/n4ejdp/text\_classification\_problem\_overfitting/](https://www.reddit.com/r/learnmachinelearning/comments/n4ejdp/text_classification_problem_overfitting/)  I already said that I am trying to solve the multiclass text classification problem using BERT.

&amp;#x200B;

After the first training which took 13 hours, I noticed that my model is overfitting. I did some changes to my model, and now I wanna test it out. I can't afford training the whole model everytime because each time it takes huge amount of time, so how can i quickly test it to see how my changes affected my model? Maybe I can test it on a smaller dataset? I heard this is a way of doing so, however, i am not sure if I should use my test set to do so? if yes, should I break my test into 3 pieces (train,test,val ?) again? My test set is not that big (around 3000 rows of data) and I am wondering if such a small data can do the job? 

&amp;#x200B;

I am new in this field, and this is the first me doing this, so it would be highly appreciated if anyone could guide me through it. What are the best practices that allow me to quickly test my model?",t2_6c0lef9b,False,,0,False,How to quickly test my model?,[],r/LanguageTechnology,False,6,,0,,False,t3_n4eocz,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1620126662.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In my last post &lt;a href=""https://www.reddit.com/r/learnmachinelearning/comments/n4ejdp/text_classification_problem_overfitting/""&gt;https://www.reddit.com/r/learnmachinelearning/comments/n4ejdp/text_classification_problem_overfitting/&lt;/a&gt;  I already said that I am trying to solve the multiclass text classification problem using BERT.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;After the first training which took 13 hours, I noticed that my model is overfitting. I did some changes to my model, and now I wanna test it out. I can&amp;#39;t afford training the whole model everytime because each time it takes huge amount of time, so how can i quickly test it to see how my changes affected my model? Maybe I can test it on a smaller dataset? I heard this is a way of doing so, however, i am not sure if I should use my test set to do so? if yes, should I break my test into 3 pieces (train,test,val ?) again? My test set is not that big (around 3000 rows of data) and I am wondering if such a small data can do the job? &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I am new in this field, and this is the first me doing this, so it would be highly appreciated if anyone could guide me through it. What are the best practices that allow me to quickly test my model?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n4eocz,True,,strangeguy111,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n4eocz/how_to_quickly_test_my_model/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n4eocz/how_to_quickly_test_my_model/,30199,1620097862.0,0,,False,,,,,,1190
1247,,LanguageTechnology,"Hello,

I am a undergraduate student writing my final thesis right now. I am exploring links between different social media platforms in regards to sentiment. How does Sentiment „flow“ from thematically and temporally near posts. 

One of the platforms that I explore is 4chan and I am having a lot of problems with the special vocabulary there. Most resources I can find solve twitter specific problems where a character limit is involved. The lack of such a limit presents me a plethora of variations of the same token. Elongated words and written out laughter with typos really messes up my work. 

Additionally I can‘t correct typos easily because there are many special words that aren‘t written in any dictionary. Some of these I could identify through urban dictionary, but many others are falsely corrected by a spellchecker. 

With the FastText-Algorithm I could identify a couple hundred tokens and normalize them, but I suspect there is a bunch more. Right now I am reading Dr. Farrell‘s work on jargon detection: http://oro.open.ac.uk/70529/

I wonder if you guys could give me some pointers. Thank you very much!",t2_6oph4mve,False,,0,False,Jargon Detection and Normalization,[],r/LanguageTechnology,False,6,,0,,False,t3_n3u6uh,False,dark,0.82,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1620071959.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am a undergraduate student writing my final thesis right now. I am exploring links between different social media platforms in regards to sentiment. How does Sentiment „flow“ from thematically and temporally near posts. &lt;/p&gt;

&lt;p&gt;One of the platforms that I explore is 4chan and I am having a lot of problems with the special vocabulary there. Most resources I can find solve twitter specific problems where a character limit is involved. The lack of such a limit presents me a plethora of variations of the same token. Elongated words and written out laughter with typos really messes up my work. &lt;/p&gt;

&lt;p&gt;Additionally I can‘t correct typos easily because there are many special words that aren‘t written in any dictionary. Some of these I could identify through urban dictionary, but many others are falsely corrected by a spellchecker. &lt;/p&gt;

&lt;p&gt;With the FastText-Algorithm I could identify a couple hundred tokens and normalize them, but I suspect there is a bunch more. Right now I am reading Dr. Farrell‘s work on jargon detection: &lt;a href=""http://oro.open.ac.uk/70529/""&gt;http://oro.open.ac.uk/70529/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I wonder if you guys could give me some pointers. Thank you very much!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n3u6uh,True,,gamboty,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n3u6uh/jargon_detection_and_normalization/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n3u6uh/jargon_detection_and_normalization/,30199,1620043159.0,0,,False,,,,,,1125
1248,,LanguageTechnology,"I  am trying to find a software that could tell   
\-if the letter ""y"" in a word is a vowel or a consonant.  
\-Or if ""ti"" should be read as ""sh""

I found multiple tool that return a list of phoneme but none that tell me which letter in the original word match each phoneme (an alignment).  
I assume this is doable because this is essentially what speech-to-text tool are doing.  


But I would like a tool that give me a list of matching pair (grapheme/phoneme) so I display the annotation on the the correct range of letter in the original word.",t2_38k7g,False,,0,False,Any software that can annotate (grapheme/phonogram) in a word with the matching phoneme?,[],r/LanguageTechnology,False,6,,0,,False,t3_n458pt,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1620099541.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I  am trying to find a software that could tell&lt;br/&gt;
-if the letter &amp;quot;y&amp;quot; in a word is a vowel or a consonant.&lt;br/&gt;
-Or if &amp;quot;ti&amp;quot; should be read as &amp;quot;sh&amp;quot;&lt;/p&gt;

&lt;p&gt;I found multiple tool that return a list of phoneme but none that tell me which letter in the original word match each phoneme (an alignment).&lt;br/&gt;
I assume this is doable because this is essentially what speech-to-text tool are doing.  &lt;/p&gt;

&lt;p&gt;But I would like a tool that give me a list of matching pair (grapheme/phoneme) so I display the annotation on the the correct range of letter in the original word.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n458pt,True,,skyde,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n458pt/any_software_that_can_annotate_graphemephonogram/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n458pt/any_software_that_can_annotate_graphemephonogram/,30199,1620070741.0,0,,False,,,,,,548
1249,,LanguageTechnology,"I am learning about NLP and trying to understand how to tokenize text and remove stop words.

I tried the following line of code (from ""quanteada) and got the following error:

first_method &lt;- tokens(tdm) %&gt;% tokens_remove(stopwords(""en""), pad = TRUE)

Error: tokens() only works on character, corpus, list, tokens objects.

Has anyone ever gotten this error before?

I posted the full details to my question over here: https://stackoverflow.com/questions/67376045/r-error-only-works-with-character-objects

Thanks",t2_o4xj9,False,,0,False,Stop Word and Tokenization (with R),[],r/LanguageTechnology,False,6,,0,,False,t3_n494bh,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1620109386.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am learning about NLP and trying to understand how to tokenize text and remove stop words.&lt;/p&gt;

&lt;p&gt;I tried the following line of code (from &amp;quot;quanteada) and got the following error:&lt;/p&gt;

&lt;p&gt;first_method &amp;lt;- tokens(tdm) %&amp;gt;% tokens_remove(stopwords(&amp;quot;en&amp;quot;), pad = TRUE)&lt;/p&gt;

&lt;p&gt;Error: tokens() only works on character, corpus, list, tokens objects.&lt;/p&gt;

&lt;p&gt;Has anyone ever gotten this error before?&lt;/p&gt;

&lt;p&gt;I posted the full details to my question over here: &lt;a href=""https://stackoverflow.com/questions/67376045/r-error-only-works-with-character-objects""&gt;https://stackoverflow.com/questions/67376045/r-error-only-works-with-character-objects&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n494bh,True,,blueest,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n494bh/stop_word_and_tokenization_with_r/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n494bh/stop_word_and_tokenization_with_r/,30199,1620080586.0,0,,False,,,,,True,520
1250,,LanguageTechnology,,t2_97mkm0fp,False,,0,False,The giant leaps in language technology -- and who's left behind,[],r/LanguageTechnology,False,6,,0,,False,t3_n3e5en,False,dark,0.8,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,True,,False,,[],{},,False,,1620013241.0,text,6,,,text,ted.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n3e5en,True,,Madame_President_,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n3e5en/the_giant_leaps_in_language_technology_and_whos/,all_ads,False,https://www.ted.com/talks/kalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind?utm_campaign=tedspread&amp;utm_medium=referral&amp;utm_source=tedcomshare,30199,1619984441.0,0,,False,https://www.ted.com/talks/kalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind?utm_campaign=tedspread&amp;utm_medium=referral&amp;utm_source=tedcomshare,"[{'approved_at_utc': None, 'subreddit': 'AskWomenOfColorOver30', 'selftext': '', 'author_fullname': 't2_97mkm0fp', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ""The giant leaps in language technology -- and who's left behind"", 'link_flair_richtext': [{'e': 'text', 't': 'ARTS &amp; CULTURE'}], 'subreddit_name_prefixed': 'r/AskWomenOfColorOver30', 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_n2wqof', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.83, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {'content': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fembed.ted.com%2Ftalks%2Fkalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind&amp;display_name=TED&amp;url=https%3A%2F%2Fwww.ted.com%2Ftalks%2Fkalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind&amp;image=https%3A%2F%2Fpi.tedcdn.com%2Fr%2Ftalkstar-photos.s3.amazonaws.com%2Fuploads%2Ff546d184-50e3-4226-bea6-ae8c09647d0b%2FKalikaBali_2020X-embed.jpg%3Fh%3D316%26w%3D560&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=ted"" width=""560"" height=""316"" scrolling=""no"" title=""TED embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'width': 560, 'scrolling': False, 'height': 316}, 'author_flair_template_id': '00bbbe72-36a9-11eb-8a5e-0e573b3e90cb', 'is_original_content': False, 'user_reports': [], 'secure_media': {'type': 'ted.com', 'oembed': {'provider_url': 'https://www.ted.com', 'description': 'Thousands of languages thrive across the globe, yet modern speech technology -- with all of its benefits -- supports just over a hundred. Computational linguist Kalika Bali dreams of a day when technology acts as a bridge instead of a barrier, working passionately to build new and inclusive systems for the millions who speak low-resource languages.', 'title': ""Kalika Bali: The giant leaps in language technology -- and who's left behind"", 'url': 'https://www.ted.com/talks/kalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind', 'type': 'video', 'author_name': 'Kalika Bali', 'height': 316, 'width': 560, 'html': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fembed.ted.com%2Ftalks%2Fkalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind&amp;display_name=TED&amp;url=https%3A%2F%2Fwww.ted.com%2Ftalks%2Fkalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind&amp;image=https%3A%2F%2Fpi.tedcdn.com%2Fr%2Ftalkstar-photos.s3.amazonaws.com%2Fuploads%2Ff546d184-50e3-4226-bea6-ae8c09647d0b%2FKalikaBali_2020X-embed.jpg%3Fh%3D316%26w%3D560&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=ted"" width=""560"" height=""316"" scrolling=""no"" title=""TED embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'thumbnail_width': 560, 'version': '1.0', 'provider_name': 'TED', 'cache_age': 300, 'thumbnail_url': 'https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/f546d184-50e3-4226-bea6-ae8c09647d0b/KalikaBali_2020X-embed.jpg?h=316&amp;w=560', 'thumbnail_height': 315, 'author_url': 'https://www.ted.com/speakers/kalika_bali'}}, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {'content': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fembed.ted.com%2Ftalks%2Fkalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind&amp;display_name=TED&amp;url=https%3A%2F%2Fwww.ted.com%2Ftalks%2Fkalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind&amp;image=https%3A%2F%2Fpi.tedcdn.com%2Fr%2Ftalkstar-photos.s3.amazonaws.com%2Fuploads%2Ff546d184-50e3-4226-bea6-ae8c09647d0b%2FKalikaBali_2020X-embed.jpg%3Fh%3D316%26w%3D560&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=ted"" width=""560"" height=""316"" scrolling=""no"" title=""TED embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'width': 560, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/n2wqof', 'height': 316}, 'link_flair_text': 'ARTS &amp; CULTURE', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': True, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [{'e': 'text', 't': 'Identifies as a WOC over 30'}], 'gildings': {}, 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1619949621.0, 'link_flair_type': 'richtext', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'richtext', 'domain': 'ted.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': 'new', 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.ted.com/talks/kalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind?utm_campaign=tedspread&amp;utm_medium=referral&amp;utm_source=tedcomshare', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'e6c4278a-3b39-11eb-8025-0e048a754485', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Identifies as a WOC over 30', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_3ihx1z', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0266b3', 'id': 'n2wqof', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Madame_President_', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/AskWomenOfColorOver30/comments/n2wqof/the_giant_leaps_in_language_technology_and_whos/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.ted.com/talks/kalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind?utm_campaign=tedspread&amp;utm_medium=referral&amp;utm_source=tedcomshare', 'subreddit_subscribers': 702, 'created_utc': 1619920821.0, 'num_crossposts': 1, 'media': {'type': 'ted.com', 'oembed': {'provider_url': 'https://www.ted.com', 'description': 'Thousands of languages thrive across the globe, yet modern speech technology -- with all of its benefits -- supports just over a hundred. Computational linguist Kalika Bali dreams of a day when technology acts as a bridge instead of a barrier, working passionately to build new and inclusive systems for the millions who speak low-resource languages.', 'title': ""Kalika Bali: The giant leaps in language technology -- and who's left behind"", 'url': 'https://www.ted.com/talks/kalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind', 'type': 'video', 'author_name': 'Kalika Bali', 'height': 316, 'width': 560, 'html': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fembed.ted.com%2Ftalks%2Fkalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind&amp;display_name=TED&amp;url=https%3A%2F%2Fwww.ted.com%2Ftalks%2Fkalika_bali_the_giant_leaps_in_language_technology_and_who_s_left_behind&amp;image=https%3A%2F%2Fpi.tedcdn.com%2Fr%2Ftalkstar-photos.s3.amazonaws.com%2Fuploads%2Ff546d184-50e3-4226-bea6-ae8c09647d0b%2FKalikaBali_2020X-embed.jpg%3Fh%3D316%26w%3D560&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=ted"" width=""560"" height=""316"" scrolling=""no"" title=""TED embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'thumbnail_width': 560, 'version': '1.0', 'provider_name': 'TED', 'cache_age': 300, 'thumbnail_url': 'https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/f546d184-50e3-4226-bea6-ae8c09647d0b/KalikaBali_2020X-embed.jpg?h=316&amp;w=560', 'thumbnail_height': 315, 'author_url': 'https://www.ted.com/speakers/kalika_bali'}}, 'is_video': False}]",t3_n2wqof,,,0
1251,,LanguageTechnology,,t2_hkv9s,False,,0,False,LINE: Large-scale Information Network Embedding (Machine Learning with Graphs),[],r/LanguageTechnology,False,6,,0,,False,t3_n31jh9,False,dark,0.89,,public,21,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/VuqvD3qp76M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'LINE: Large-scale Information Network Embedding (Machine Learning with Graphs)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/VuqvD3qp76M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/VuqvD3qp76M/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/VuqvD3qp76M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/n31jh9', 'height': 200}",,False,21,,False,False,,False,,[],{},,False,,1619970053.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n31jh9,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n31jh9/line_largescale_information_network_embedding/,all_ads,False,https://youtu.be/VuqvD3qp76M,30199,1619941253.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'LINE: Large-scale Information Network Embedding (Machine Learning with Graphs)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/VuqvD3qp76M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/VuqvD3qp76M/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/VuqvD3qp76M,,,,,0
1252,,LanguageTechnology,,t2_5fsp2x6v,False,,0,False,CrossWeigh: Training NER with Imperfect Annotations | Research Papers Summary 016,[],r/LanguageTechnology,False,6,,0,,False,t3_n3d0kn,False,dark,1.0,,public,1,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/IXrwYWgnijQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'CrossWeigh: Training NER with Imperfect Annotations | Research Papers Summary 016', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/IXrwYWgnijQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/IXrwYWgnijQ/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/IXrwYWgnijQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/n3d0kn', 'height': 200}",,False,1,,False,False,,False,,[],{},,False,,1620010184.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n3d0kn,True,,RyanAI100,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n3d0kn/crossweigh_training_ner_with_imperfect/,all_ads,False,https://youtu.be/IXrwYWgnijQ,30199,1619981384.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'CrossWeigh: Training NER with Imperfect Annotations | Research Papers Summary 016', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/IXrwYWgnijQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/IXrwYWgnijQ/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,https://youtu.be/IXrwYWgnijQ,,,,,0
1253,,LanguageTechnology,"Hello,
I'm new to this field.
What model would you recommend for creating single sentences? I'm looking for something a bit more advanced than a markov chain. I would like to have it choose a random word using a word probability distribution, and then give probability distributions for generating the next and previous words, repeating this until the start and end of a sentence.",t2_4irxns4h,False,,0,False,What model to create a sentence generator?,[],r/LanguageTechnology,False,6,,0,,False,t3_n2quai,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1619930124.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,
I&amp;#39;m new to this field.
What model would you recommend for creating single sentences? I&amp;#39;m looking for something a bit more advanced than a markov chain. I would like to have it choose a random word using a word probability distribution, and then give probability distributions for generating the next and previous words, repeating this until the start and end of a sentence.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n2quai,True,,qlpxumni,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n2quai/what_model_to_create_a_sentence_generator/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n2quai/what_model_to_create_a_sentence_generator/,30199,1619901324.0,0,,False,,,,,,380
1254,,LanguageTechnology,,t2_apv7o2y8,False,,0,False,"Clinical Natural Language Processing – Challenges, Tasks and Datasets #NLProc",[],r/LanguageTechnology,False,6,,0,,False,t3_n2dl4b,False,dark,0.8,,public,12,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/yrN8VqkCWWc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Clinical Natural Language Processing – Challenges, Tasks and Datasets #NLProc', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/yrN8VqkCWWc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/yrN8VqkCWWc/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCPAfv1GeraR1WM0608He6ow'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/yrN8VqkCWWc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/n2dl4b', 'height': 200}",,False,12,,False,False,,False,,[],{},,False,,1619883256.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n2dl4b,True,,rsree123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n2dl4b/clinical_natural_language_processing_challenges/,all_ads,False,https://youtu.be/yrN8VqkCWWc,30199,1619854456.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Clinical Natural Language Processing – Challenges, Tasks and Datasets #NLProc', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/yrN8VqkCWWc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/yrN8VqkCWWc/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCPAfv1GeraR1WM0608He6ow'}}",False,https://youtu.be/yrN8VqkCWWc,,,,,0
1255,,LanguageTechnology,"I have a collection of order forms from different manufacturers. Each manufacturer will generally use the same standardized order form but there is some variance where for whatever reason, a manufacturer might have 5-6 different order forms that they use.

Since each document contains a ton of legal language, I need to create a specific pipeline for each order form type, but I don't know how many different types there are in my corpus to begin with.

Can someone recommend a clustering algorithm to that I can use to figure out all the different document ""types"" that are in the corpus?

Thanks!",t2_bcdq12gz,False,,0,False,Advice on a clustering algorithm for a corpus of order forms that will sort documents into 'document' types without knowing how many different kinds there are.,[],r/LanguageTechnology,False,6,,0,,False,t3_n26mty,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1619855780.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a collection of order forms from different manufacturers. Each manufacturer will generally use the same standardized order form but there is some variance where for whatever reason, a manufacturer might have 5-6 different order forms that they use.&lt;/p&gt;

&lt;p&gt;Since each document contains a ton of legal language, I need to create a specific pipeline for each order form type, but I don&amp;#39;t know how many different types there are in my corpus to begin with.&lt;/p&gt;

&lt;p&gt;Can someone recommend a clustering algorithm to that I can use to figure out all the different document &amp;quot;types&amp;quot; that are in the corpus?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n26mty,True,,fastgoatboy,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n26mty/advice_on_a_clustering_algorithm_for_a_corpus_of/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n26mty/advice_on_a_clustering_algorithm_for_a_corpus_of/,30199,1619826980.0,0,,False,,,,,,599
1256,,LanguageTechnology,"Hello everyone,

As part of my dissertation, I want to examine the general question of ""What do people talk about on stock market-related subreddits?"" To this end, I have identified around 200 subreddits that I would like to examine, and I plan to examine both the posts and comments to these subreddits. I am trying to decide the best and most efficient way of extracting the content of thesis subreddit discussions.

Some features of the data that I think are relevant to my decision:

1. Some subreddits are naturally focused around a certain topic or set of topics (e.g., r/technicalanalysis or r/thcinvesting) while others are more general (e.g., r/StockMarket).
2. The size of the subreddits (i.e., # of posts and comments) varies dramatically (e.g., r/wallstreetbets with millions of members vs r/stockanalysis with a couple of hundred members).
3. The size of my dataset is going to be extremely large (unsure of exact size yet, but just r/wallstreetbets is a large amount of data).

&amp;#x200B;

A couple of options that I am considering include LDA, author-topic LDA (with ""author"" being the subreddit) ([https://radimrehurek.com/gensim/models/atmodel.html](https://radimrehurek.com/gensim/models/atmodel.html)), and BERTopic ([https://github.com/MaartenGr/BERTopic](https://github.com/MaartenGr/BERTopic)).

I am also wondering whether I should manually group subreddits by category (e.g., technical analysis subreddits, fundamental analysis subreddits, penny stock subreddits, general subreddits, etc.) first and then run topic analysis separately for each group?

Any and all thoughts are greatly appreciated, and I am definitely open to hearing about alternative approaches or concerns that I haven't discussed. I am trying to weigh the pros and cons of each approach to make sure that my methodology is not obviously sub-optimal to some alternative.

Thanks in advance!",t2_1ch4r0dd,False,,0,False,topic modeling over many subreddits,[],r/LanguageTechnology,False,6,,0,,False,t3_n1x8vo,False,dark,0.85,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1619828058.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;

&lt;p&gt;As part of my dissertation, I want to examine the general question of &amp;quot;What do people talk about on stock market-related subreddits?&amp;quot; To this end, I have identified around 200 subreddits that I would like to examine, and I plan to examine both the posts and comments to these subreddits. I am trying to decide the best and most efficient way of extracting the content of thesis subreddit discussions.&lt;/p&gt;

&lt;p&gt;Some features of the data that I think are relevant to my decision:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Some subreddits are naturally focused around a certain topic or set of topics (e.g., &lt;a href=""/r/technicalanalysis""&gt;r/technicalanalysis&lt;/a&gt; or &lt;a href=""/r/thcinvesting""&gt;r/thcinvesting&lt;/a&gt;) while others are more general (e.g., &lt;a href=""/r/StockMarket""&gt;r/StockMarket&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;The size of the subreddits (i.e., # of posts and comments) varies dramatically (e.g., &lt;a href=""/r/wallstreetbets""&gt;r/wallstreetbets&lt;/a&gt; with millions of members vs &lt;a href=""/r/stockanalysis""&gt;r/stockanalysis&lt;/a&gt; with a couple of hundred members).&lt;/li&gt;
&lt;li&gt;The size of my dataset is going to be extremely large (unsure of exact size yet, but just &lt;a href=""/r/wallstreetbets""&gt;r/wallstreetbets&lt;/a&gt; is a large amount of data).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;A couple of options that I am considering include LDA, author-topic LDA (with &amp;quot;author&amp;quot; being the subreddit) (&lt;a href=""https://radimrehurek.com/gensim/models/atmodel.html""&gt;https://radimrehurek.com/gensim/models/atmodel.html&lt;/a&gt;), and BERTopic (&lt;a href=""https://github.com/MaartenGr/BERTopic""&gt;https://github.com/MaartenGr/BERTopic&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;I am also wondering whether I should manually group subreddits by category (e.g., technical analysis subreddits, fundamental analysis subreddits, penny stock subreddits, general subreddits, etc.) first and then run topic analysis separately for each group?&lt;/p&gt;

&lt;p&gt;Any and all thoughts are greatly appreciated, and I am definitely open to hearing about alternative approaches or concerns that I haven&amp;#39;t discussed. I am trying to weigh the pros and cons of each approach to make sure that my methodology is not obviously sub-optimal to some alternative.&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n1x8vo,True,,acctphd,,14,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n1x8vo/topic_modeling_over_many_subreddits/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n1x8vo/topic_modeling_over_many_subreddits/,30199,1619799258.0,0,,False,,,,,,1885
1257,,LanguageTechnology,"Hi Everybody, 

Wondering if anyone has experience with Stanford's NLP project Chirpy Cardinal. Looking to find a freelance consultant that has experience with Chirpy architecture. Any suggestions on where I might find some good leads?

Thank you.",t2_buw4jm5s,False,,0,False,Stanford's Chirpy Cardinal,[],r/LanguageTechnology,False,6,,0,,False,t3_n1yt21,False,dark,0.76,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619832380.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi Everybody, &lt;/p&gt;

&lt;p&gt;Wondering if anyone has experience with Stanford&amp;#39;s NLP project Chirpy Cardinal. Looking to find a freelance consultant that has experience with Chirpy architecture. Any suggestions on where I might find some good leads?&lt;/p&gt;

&lt;p&gt;Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n1yt21,True,,Pro_Mgmt,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n1yt21/stanfords_chirpy_cardinal/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n1yt21/stanfords_chirpy_cardinal/,30199,1619803580.0,0,,False,,,,,,247
1258,,LanguageTechnology,"I asked you guys about the opinion of choosing which domain to 

choose between NLP and CV few weeks ago.

&amp;#x200B;

I finally decided to study NLP area more deeply.

&amp;#x200B;

But I have concerns. It is really difficult to get accepted in 

NLP Labs and artificial intelligence graduate school.

&amp;#x200B;

So I made two plans if I failed to have master degree in graduate school.

&amp;#x200B;

Plan A

Studying NLP in data science graduate school

This plan have risk; there are no lab in this graduate school 

so I have to study NLP by myself.

&amp;#x200B;

Plan B

Studying CL(Computational Linguistics) in language graduate school

Also this plan have risk; the same reason with Plan A

&amp;#x200B;

There are not that many NLP labs in Korea, so I wonder if I go to

plan A and B, I can work in NLP field.

&amp;#x200B;

The best way is to read articles published by data and language graduate school, 

and NLP lab in AI graduate school and compare it, but the sad thing is

I don't have enough time left to contact labs and graduate schools above.

&amp;#x200B;

I guess that there are some people who graduated those graduate school 

that I mentioned, or at least worked with the people who went to Plan A and Plan B.

&amp;#x200B;

I am looking forward to have any advice from you guys.

I appreciate to the people who gave me advice last time when I asked about

the prospective domain to choose.

&amp;#x200B;

Thank you for reading my post and wish you guys all have good days today !",t2_bpsj0h63,False,,0,False,Concerns about studying NLP,[],r/LanguageTechnology,False,6,,0,,False,t3_n1x2c6,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619827549.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I asked you guys about the opinion of choosing which domain to &lt;/p&gt;

&lt;p&gt;choose between NLP and CV few weeks ago.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I finally decided to study NLP area more deeply.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;But I have concerns. It is really difficult to get accepted in &lt;/p&gt;

&lt;p&gt;NLP Labs and artificial intelligence graduate school.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So I made two plans if I failed to have master degree in graduate school.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Plan A&lt;/p&gt;

&lt;p&gt;Studying NLP in data science graduate school&lt;/p&gt;

&lt;p&gt;This plan have risk; there are no lab in this graduate school &lt;/p&gt;

&lt;p&gt;so I have to study NLP by myself.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Plan B&lt;/p&gt;

&lt;p&gt;Studying CL(Computational Linguistics) in language graduate school&lt;/p&gt;

&lt;p&gt;Also this plan have risk; the same reason with Plan A&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;There are not that many NLP labs in Korea, so I wonder if I go to&lt;/p&gt;

&lt;p&gt;plan A and B, I can work in NLP field.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The best way is to read articles published by data and language graduate school, &lt;/p&gt;

&lt;p&gt;and NLP lab in AI graduate school and compare it, but the sad thing is&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t have enough time left to contact labs and graduate schools above.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I guess that there are some people who graduated those graduate school &lt;/p&gt;

&lt;p&gt;that I mentioned, or at least worked with the people who went to Plan A and Plan B.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I am looking forward to have any advice from you guys.&lt;/p&gt;

&lt;p&gt;I appreciate to the people who gave me advice last time when I asked about&lt;/p&gt;

&lt;p&gt;the prospective domain to choose.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thank you for reading my post and wish you guys all have good days today !&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n1x2c6,True,,Korean_Arabic_AI,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n1x2c6/concerns_about_studying_nlp/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n1x2c6/concerns_about_studying_nlp/,30199,1619798749.0,0,,False,,,,,,1512
1259,,LanguageTechnology,"Hey NLPeople,

&amp;#x200B;

at the end of the year, I’m planning on writing my Master’s Thesis. My Master’s program is a combination of Computational Linguistics and Cognitive Science. I already know that I want to focus on the broad area of computational phonology.

Because we live in a productivity-driven dystopia apparently I’m already thinking about what would look good on my CV which I will embroider with the topic I will have spent a lot of time working on.

&amp;#x200B;

Do you guys have any general ideas for open research topics in phonologically driven speech recognition for example? Or would you nudge me towards literature in that field that comes to mind?

&amp;#x200B;

I’m really only looking for general inspiration right now.

&amp;#x200B;

Thanks a lot!",t2_12vsbb,False,,0,False,Master’s Thesis in Computational Phonology,[],r/LanguageTechnology,False,6,,0,,False,t3_n1yw5t,False,dark,0.6,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1619832615.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey NLPeople,&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;at the end of the year, I’m planning on writing my Master’s Thesis. My Master’s program is a combination of Computational Linguistics and Cognitive Science. I already know that I want to focus on the broad area of computational phonology.&lt;/p&gt;

&lt;p&gt;Because we live in a productivity-driven dystopia apparently I’m already thinking about what would look good on my CV which I will embroider with the topic I will have spent a lot of time working on.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Do you guys have any general ideas for open research topics in phonologically driven speech recognition for example? Or would you nudge me towards literature in that field that comes to mind?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I’m really only looking for general inspiration right now.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks a lot!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n1yw5t,True,,bronchialbalsam,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n1yw5t/masters_thesis_in_computational_phonology/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n1yw5t/masters_thesis_in_computational_phonology/,30199,1619803815.0,0,,False,,,,,,778
1260,,LanguageTechnology,"Say I have a research paper (or papers) and I want to see how they  define a common term. For example, the papers might be on the topic of ""engagement"" and have definitions for the term they use in the paper. I was wondering if it would be possible to extract those definitions given the tools of NLP.

From some light research this seems to be a ""Terminology extraction"" problem. There are some papers on the topic, but my problem seems easier than that. I already know what to look for (e.g., the term ""engagement""), I just want to capture a definition of that from a text.

An idea I had was to search the text for the term I'm looking for (e.g., engagement) and then capture the text around it for further processing; For example, a sentence or two near the ""engagement"" term. But it's the additional processing I'm not sure about.

Any ideas? Thanks.",t2_h1axq,False,,0,False,Extracting term definitions from research papers,[],r/LanguageTechnology,False,6,,0,,False,t3_n1h5px,False,dark,1.0,,public,21,0,{},,False,[],,False,False,,{},,False,21,,False,False,,False,,[],{},,True,,1619767565.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Say I have a research paper (or papers) and I want to see how they  define a common term. For example, the papers might be on the topic of &amp;quot;engagement&amp;quot; and have definitions for the term they use in the paper. I was wondering if it would be possible to extract those definitions given the tools of NLP.&lt;/p&gt;

&lt;p&gt;From some light research this seems to be a &amp;quot;Terminology extraction&amp;quot; problem. There are some papers on the topic, but my problem seems easier than that. I already know what to look for (e.g., the term &amp;quot;engagement&amp;quot;), I just want to capture a definition of that from a text.&lt;/p&gt;

&lt;p&gt;An idea I had was to search the text for the term I&amp;#39;m looking for (e.g., engagement) and then capture the text around it for further processing; For example, a sentence or two near the &amp;quot;engagement&amp;quot; term. But it&amp;#39;s the additional processing I&amp;#39;m not sure about.&lt;/p&gt;

&lt;p&gt;Any ideas? Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n1h5px,True,,GetsTrimAPlenty,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n1h5px/extracting_term_definitions_from_research_papers/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n1h5px/extracting_term_definitions_from_research_papers/,30199,1619738765.0,0,,False,,,,,,855
1261,,LanguageTechnology,"How do you deal with large text when using sentiment analysis? Even if SA is often used to analyze small pieces of text like tweets and reviews, it's often complex to encode a sentence because negative and positive phrases modify each other and end up to modify the overall sentiment of the review. This is even more true when dealing with texts made out of multiple sentences. A reasonable way to handle that would be to analyze the sentiment of each sentence and then get the average score for that text. Are there better ways to improve the accuracy of our analysis with large texts?
I found an article of Shocher and colleagues where they classify sentiment phrase-by-phrase  rather than on the sentence level. 

This is the paper
https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf",t2_b31us5mn,False,,0,False,Sentiment analysis of long text,[],r/LanguageTechnology,False,6,,0,,False,t3_n1gkh4,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619765802.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How do you deal with large text when using sentiment analysis? Even if SA is often used to analyze small pieces of text like tweets and reviews, it&amp;#39;s often complex to encode a sentence because negative and positive phrases modify each other and end up to modify the overall sentiment of the review. This is even more true when dealing with texts made out of multiple sentences. A reasonable way to handle that would be to analyze the sentiment of each sentence and then get the average score for that text. Are there better ways to improve the accuracy of our analysis with large texts?
I found an article of Shocher and colleagues where they classify sentiment phrase-by-phrase  rather than on the sentence level. &lt;/p&gt;

&lt;p&gt;This is the paper
&lt;a href=""https://nlp.stanford.edu/%7Esocherr/EMNLP2013_RNTN.pdf""&gt;https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n1gkh4,True,,Dr_Funkmachine,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n1gkh4/sentiment_analysis_of_long_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n1gkh4/sentiment_analysis_of_long_text/,30199,1619737002.0,0,,False,,,,,,787
1262,,LanguageTechnology,"While LDA in Gensim supports both multiprocessing and distributed computation, the author-topic model implementation does not at the moment. Much of the infrastructure that allows both multiprocessing and distributed computation should however already be in place (class models.ldamulticore), as the model inherits it from LDA. Therefore, I was asking myself if these functionalities could be enabled also for atmodel without major issues",t2_9p25lyud,False,,0,False,Parallelization for Author-topic models (atmodel) in Gensim,[],r/LanguageTechnology,False,6,,0,,False,t3_n17myt,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1619741069.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;While LDA in Gensim supports both multiprocessing and distributed computation, the author-topic model implementation does not at the moment. Much of the infrastructure that allows both multiprocessing and distributed computation should however already be in place (class models.ldamulticore), as the model inherits it from LDA. Therefore, I was asking myself if these functionalities could be enabled also for atmodel without major issues&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n17myt,True,,Senior_Time_2928,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n17myt/parallelization_for_authortopic_models_atmodel_in/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n17myt/parallelization_for_authortopic_models_atmodel_in/,30199,1619712269.0,0,,False,,,,,,438
1263,,LanguageTechnology,"My dataset is a little unbalanced:

    smile        4852 
    kind         2027 
    angry        1926 
    surprised     979 
    sad           698 

pretty same for the validation and test sets. My goal was to predict the emotion of user tweets, which I already did, however now I am wondering what are the best evaluation metrics for this type of problem?

My initial metrics score is as following

    {""train"": {""eval_examples_count"": 10481, ""metrics"": {""f1_weighted"": 0.8869, ""f1_macro"": 0.8401, ""accuracy"": 0.8884, ""roc_auc"": 0.9686}, ""time_spent"": ""1:27:10""}} {""valid"": {""eval_examples_count"": 3493, ""metrics"": {""f1_weighted"": 0.6112, ""f1_macro"": 0.5257, ""accuracy"": 0.6184, ""roc_auc"": 0.8177}, ""time_spent"": ""0:28:37""}} {""test"": {""eval_examples_count"": 3494, ""metrics"": {""f1_weighted"": 0.6191, ""f1_macro"": 0.5282, ""accuracy"": 0.6259, ""roc_auc"": 0.8271}, ""time_spent"": ""0:28:26""}} 

I am pretty new to ML and NLP in general so I am kinda confused. I have several questions here:

1. Why train f1\_macro is so high but valid and test sets are not that high? How to interpret it?
2. How to interpret the above result in general?
3. I've seen someone using Matthews Correlation Coefficient for the kinda similar task, but I heard that is only for the binary class problem, How can i use it for the multiclass problem?
4. How valuable roc\_auc information in a multi-class classification problem?

In general, even though metrics above are kinda giving me not really result, but i ve been testing the model manually and it just work pretty good.",t2_6c0lef9b,False,,0,False,Best evaluation metrics for the BERT model,[],r/LanguageTechnology,False,6,,0,,False,t3_n0yido,False,dark,1.0,,public,15,0,{},,False,[],,False,False,,{},,False,15,,False,False,,False,,[],{},,True,,1619706977.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My dataset is a little unbalanced:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;smile        4852 
kind         2027 
angry        1926 
surprised     979 
sad           698 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pretty same for the validation and test sets. My goal was to predict the emotion of user tweets, which I already did, however now I am wondering what are the best evaluation metrics for this type of problem?&lt;/p&gt;

&lt;p&gt;My initial metrics score is as following&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;train&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 10481, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.8869, &amp;quot;f1_macro&amp;quot;: 0.8401, &amp;quot;accuracy&amp;quot;: 0.8884, &amp;quot;roc_auc&amp;quot;: 0.9686}, &amp;quot;time_spent&amp;quot;: &amp;quot;1:27:10&amp;quot;}} {&amp;quot;valid&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 3493, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.6112, &amp;quot;f1_macro&amp;quot;: 0.5257, &amp;quot;accuracy&amp;quot;: 0.6184, &amp;quot;roc_auc&amp;quot;: 0.8177}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:28:37&amp;quot;}} {&amp;quot;test&amp;quot;: {&amp;quot;eval_examples_count&amp;quot;: 3494, &amp;quot;metrics&amp;quot;: {&amp;quot;f1_weighted&amp;quot;: 0.6191, &amp;quot;f1_macro&amp;quot;: 0.5282, &amp;quot;accuracy&amp;quot;: 0.6259, &amp;quot;roc_auc&amp;quot;: 0.8271}, &amp;quot;time_spent&amp;quot;: &amp;quot;0:28:26&amp;quot;}} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I am pretty new to ML and NLP in general so I am kinda confused. I have several questions here:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Why train f1_macro is so high but valid and test sets are not that high? How to interpret it?&lt;/li&gt;
&lt;li&gt;How to interpret the above result in general?&lt;/li&gt;
&lt;li&gt;I&amp;#39;ve seen someone using Matthews Correlation Coefficient for the kinda similar task, but I heard that is only for the binary class problem, How can i use it for the multiclass problem?&lt;/li&gt;
&lt;li&gt;How valuable roc_auc information in a multi-class classification problem?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In general, even though metrics above are kinda giving me not really result, but i ve been testing the model manually and it just work pretty good.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n0yido,True,,strangeguy111,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n0yido/best_evaluation_metrics_for_the_bert_model/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n0yido/best_evaluation_metrics_for_the_bert_model/,30199,1619678177.0,0,,False,,,,,,1550
1264,,LanguageTechnology,I am supposed to be doing a research paper(my first time) and thought nlp related topics would be good. Can you suggest some easy to understand research papers that I can refer to understand how this research paper thing actually works?,t2_6k07isk4,False,,0,False,What are some easy research topics for beginners related to nlp?,[],r/LanguageTechnology,False,6,,0,,False,t3_n1103z,False,dark,0.82,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1619718809.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am supposed to be doing a research paper(my first time) and thought nlp related topics would be good. Can you suggest some easy to understand research papers that I can refer to understand how this research paper thing actually works?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n1103z,True,,artsymin,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n1103z/what_are_some_easy_research_topics_for_beginners/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n1103z/what_are_some_easy_research_topics_for_beginners/,30199,1619690009.0,0,,False,,,,,,236
1265,,LanguageTechnology,"I am working on the **CUAD**(Contract Understanding Atticus Dataset) which is a Q&amp;A based dataset. But training 80% of the dataset in one go is impossible due to resource constraints. I am using the boilerplate code provided by HuggingFace Transformer docs for Q&amp;A task [here](https://huggingface.co/transformers/examples.html). My hands are tied with Google Colab Pro. So, it's not possible for me to use multiple GPU's in training the dataset. Inspite of using the hyperparameters below, I'm unable to avoid errors due to memory constraints like ""CUDA out of Memory"" etc.

```
args = TrainingArguments(
    'cuad-roberta',
    evaluation_strategy = ""epoch"",
    learning_rate=3e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=2,
    weight_decay=0.01,
    save_steps=5000,
    logging_steps=5000,
    save_total_limit=100,
    gradient_accumulation_steps = 12,
    eval_accumulation_steps = 4,
)
```
Under these circumstances, I have divided my training set(80%) into 4 parts with each part holding 25% data. So, using any Q&amp;A supported pretrained model from Transformers, I've trained the first 25% of the training data and then saved the model in a directory of my drive. Then, I have loaded that tokenizer and model from the saved directory and trained the next 25% of my training data on the same model as shown below. 

```
tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/models/cuad-25%-roberta-base')
model = AutoModelForQuestionAnswering.from_pretrained('/content/drive/MyDrive/models/cuad-25%-roberta-base')
```

I repeated the step two more times to complete training the model on the entire training data.

Now, my question is that, **Is this approach correct in terms of training a model when I have resource constraints? If it is correct, will this approach hurt the performance of my model?** I'm relatively new to ML and NLP so please kindly consider any silly mistakes.

Also, any sources for understanding, visualising or implementing the Q&amp;A task through HuggingFace Transformers would be really helpful.",t2_3f58jdna,False,,0,False,Training a model on an entire dataset by dividing the dataset into chunks &amp; loading the model back again untill all chunks of the dataset are trained,[],r/LanguageTechnology,False,6,,0,,False,t3_n0xasz,False,dark,0.92,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1619701531.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am working on the &lt;strong&gt;CUAD&lt;/strong&gt;(Contract Understanding Atticus Dataset) which is a Q&amp;amp;A based dataset. But training 80% of the dataset in one go is impossible due to resource constraints. I am using the boilerplate code provided by HuggingFace Transformer docs for Q&amp;amp;A task &lt;a href=""https://huggingface.co/transformers/examples.html""&gt;here&lt;/a&gt;. My hands are tied with Google Colab Pro. So, it&amp;#39;s not possible for me to use multiple GPU&amp;#39;s in training the dataset. Inspite of using the hyperparameters below, I&amp;#39;m unable to avoid errors due to memory constraints like &amp;quot;CUDA out of Memory&amp;quot; etc.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
args = TrainingArguments(
    &amp;#39;cuad-roberta&amp;#39;,
    evaluation_strategy = &amp;quot;epoch&amp;quot;,
    learning_rate=3e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=2,
    weight_decay=0.01,
    save_steps=5000,
    logging_steps=5000,
    save_total_limit=100,
    gradient_accumulation_steps = 12,
    eval_accumulation_steps = 4,
)
&lt;/code&gt;
Under these circumstances, I have divided my training set(80%) into 4 parts with each part holding 25% data. So, using any Q&amp;amp;A supported pretrained model from Transformers, I&amp;#39;ve trained the first 25% of the training data and then saved the model in a directory of my drive. Then, I have loaded that tokenizer and model from the saved directory and trained the next 25% of my training data on the same model as shown below. &lt;/p&gt;

&lt;p&gt;&lt;code&gt;
tokenizer = AutoTokenizer.from_pretrained(&amp;#39;/content/drive/MyDrive/models/cuad-25%-roberta-base&amp;#39;)
model = AutoModelForQuestionAnswering.from_pretrained(&amp;#39;/content/drive/MyDrive/models/cuad-25%-roberta-base&amp;#39;)
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I repeated the step two more times to complete training the model on the entire training data.&lt;/p&gt;

&lt;p&gt;Now, my question is that, &lt;strong&gt;Is this approach correct in terms of training a model when I have resource constraints? If it is correct, will this approach hurt the performance of my model?&lt;/strong&gt; I&amp;#39;m relatively new to ML and NLP so please kindly consider any silly mistakes.&lt;/p&gt;

&lt;p&gt;Also, any sources for understanding, visualising or implementing the Q&amp;amp;A task through HuggingFace Transformers would be really helpful.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n0xasz,True,,MohammedRakib,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n0xasz/training_a_model_on_an_entire_dataset_by_dividing/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n0xasz/training_a_model_on_an_entire_dataset_by_dividing/,30199,1619672731.0,0,,False,,,,,,2104
1266,,LanguageTechnology,"Dear all, I'm relatively new to NLP and ML. Currently I'm working on projects in which I have to compare and score two audio clips. Original clip (single sentence) will be from movie character (animated or real) and in second clip human will try to mimic it. I have to come up with my model to determine similarly and score human clip out of 5.
Factors which I have considered to be used are:
1. Getting sentence from speech and comparing with original
2. Evaluating similarly from audio spectrogram (spectral centroid and zero crossing rate)
3. Identification of emotion from speech and using emotions embedding (assuming it's available on internet) to measure similarly (will probably use cosine similarly)
I couldn't come up with more factors. Can you please help me come up with new comparison factors or suggest how can I approach this problem in better way?
Thanking you in anticipation...",t2_7n480jd4,False,,0,False,A model to evaluate audio clips similarly,[],r/LanguageTechnology,False,6,,0,,False,t3_n0x12t,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,1619709772.0,,[],{},,True,,1619700365.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Dear all, I&amp;#39;m relatively new to NLP and ML. Currently I&amp;#39;m working on projects in which I have to compare and score two audio clips. Original clip (single sentence) will be from movie character (animated or real) and in second clip human will try to mimic it. I have to come up with my model to determine similarly and score human clip out of 5.
Factors which I have considered to be used are:
1. Getting sentence from speech and comparing with original
2. Evaluating similarly from audio spectrogram (spectral centroid and zero crossing rate)
3. Identification of emotion from speech and using emotions embedding (assuming it&amp;#39;s available on internet) to measure similarly (will probably use cosine similarly)
I couldn&amp;#39;t come up with more factors. Can you please help me come up with new comparison factors or suggest how can I approach this problem in better way?
Thanking you in anticipation...&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n0x12t,True,,Drakshh,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n0x12t/a_model_to_evaluate_audio_clips_similarly/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n0x12t/a_model_to_evaluate_audio_clips_similarly/,30199,1619671565.0,0,,False,,,,,,895
1267,,LanguageTechnology,"Hi everyone, 

I am in the hunt for open-domain datasets on Meronyms and Hypernyms. There are a few available online, but so far I've found only sets of entity pairs and **my research requires full sentences.** 

Any suggestion on where to search for datasets like this will be very much appreciated. Also if you happen to have one, please Dm me!

Thanks",t2_88cohh7w,False,,0,False,Looking for Datasets on Meronyms and Hypernyms - Full sentences,[],r/LanguageTechnology,False,6,,0,,False,t3_n0ntyi,False,dark,0.88,,public,6,1,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1619670018.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;

&lt;p&gt;I am in the hunt for open-domain datasets on Meronyms and Hypernyms. There are a few available online, but so far I&amp;#39;ve found only sets of entity pairs and &lt;strong&gt;my research requires full sentences.&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;Any suggestion on where to search for datasets like this will be very much appreciated. Also if you happen to have one, please Dm me!&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n0ntyi,True,,legendary_child,,11,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n0ntyi/looking_for_datasets_on_meronyms_and_hypernyms/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n0ntyi/looking_for_datasets_on_meronyms_and_hypernyms/,30199,1619641218.0,0,,False,,,,,,354
1268,,LanguageTechnology,"Hi NLP lovers,

I found this exciting Teachable NLP Challenge! Is there anyone who wants to participate with me?

Teachable NLP Challenge is free and open to everyone interested in training their own AI without coding! All you need to be prepared for is good ideas and datasets.

* When: 05/05/2021 - 05/18/2021 11:59 EDT
* How: You just need to submit your AI model link and explanations on your AI (Good example: [https://forum.ainetwork.ai/c/ai-showcase/11](https://forum.ainetwork.ai/c/ai-showcase/11))
* Prizes: Apple Store gift cards, Winners' interviews will be broadcasted through AI Network Youtube Channel(1.48K subscribers)

To participate, submit your info via [https://forms.gle/XfUuNSS2heAn7JtH7](https://forms.gle/XfUuNSS2heAn7JtH7). You will receive an invitation email!

Check how Teachable NLP works: [https://forum.ainetwork.ai/t/teachable-nlp-how-to-use-teachable-nlp/65](https://forum.ainetwork.ai/t/teachable-nlp-how-to-use-teachable-nlp/65)Or watch a 1-minute tutorial video: [https://youtu.be/hzujZOT1qz8](https://youtu.be/hzujZOT1qz8)",t2_bsm6d0e4,False,,0,False,Call for Teachable NLP Challenge,[],r/LanguageTechnology,False,6,,0,,False,t3_n0fzcb,False,dark,0.86,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1619648734.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi NLP lovers,&lt;/p&gt;

&lt;p&gt;I found this exciting Teachable NLP Challenge! Is there anyone who wants to participate with me?&lt;/p&gt;

&lt;p&gt;Teachable NLP Challenge is free and open to everyone interested in training their own AI without coding! All you need to be prepared for is good ideas and datasets.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;When: 05/05/2021 - 05/18/2021 11:59 EDT&lt;/li&gt;
&lt;li&gt;How: You just need to submit your AI model link and explanations on your AI (Good example: &lt;a href=""https://forum.ainetwork.ai/c/ai-showcase/11""&gt;https://forum.ainetwork.ai/c/ai-showcase/11&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Prizes: Apple Store gift cards, Winners&amp;#39; interviews will be broadcasted through AI Network Youtube Channel(1.48K subscribers)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To participate, submit your info via &lt;a href=""https://forms.gle/XfUuNSS2heAn7JtH7""&gt;https://forms.gle/XfUuNSS2heAn7JtH7&lt;/a&gt;. You will receive an invitation email!&lt;/p&gt;

&lt;p&gt;Check how Teachable NLP works: &lt;a href=""https://forum.ainetwork.ai/t/teachable-nlp-how-to-use-teachable-nlp/65""&gt;https://forum.ainetwork.ai/t/teachable-nlp-how-to-use-teachable-nlp/65&lt;/a&gt;Or watch a 1-minute tutorial video: &lt;a href=""https://youtu.be/hzujZOT1qz8""&gt;https://youtu.be/hzujZOT1qz8&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n0fzcb,True,,Candid-Wishbone-692,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n0fzcb/call_for_teachable_nlp_challenge/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n0fzcb/call_for_teachable_nlp_challenge/,30199,1619619934.0,0,,False,,,,,,1059
1269,,LanguageTechnology,"I’m building a classifier to predict whether or not a mechanical part is or is not automotive based on its description using a training dataset of approx. 13k labeled data points. After preprocessing, vectorizing, and TF-IDF the array is (12,918, 16,230). 

After training, I tried predicting on a new dataset of 173 descriptions. After preprocessing, vectorizing, and TF-IDF the new data array is (173, 492) and I’m getting a “dimension mismatch” error when passing it through my models’ predict function. 

Can anyone help with how to get the new unlabeled test data shaped to fit the training data?",t2_cb6se,False,,0,False,Classification dimension mismatch,[],r/LanguageTechnology,False,6,,0,,False,t3_n0lrrl,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1619664360.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m building a classifier to predict whether or not a mechanical part is or is not automotive based on its description using a training dataset of approx. 13k labeled data points. After preprocessing, vectorizing, and TF-IDF the array is (12,918, 16,230). &lt;/p&gt;

&lt;p&gt;After training, I tried predicting on a new dataset of 173 descriptions. After preprocessing, vectorizing, and TF-IDF the new data array is (173, 492) and I’m getting a “dimension mismatch” error when passing it through my models’ predict function. &lt;/p&gt;

&lt;p&gt;Can anyone help with how to get the new unlabeled test data shaped to fit the training data?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n0lrrl,True,,mikess314,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n0lrrl/classification_dimension_mismatch/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n0lrrl/classification_dimension_mismatch/,30199,1619635560.0,0,,False,,,,,,601
1270,,LanguageTechnology,,t2_5sx1ux80,False,,0,False,3 Things I Learned About SPACs Using Knowledge Graphs,[],r/LanguageTechnology,False,6,,0,,False,t3_n0fx7y,False,dark,0.69,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,False,,1619648570.0,text,6,,,text,medium.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n0fx7y,True,,Glittering_Show_3414,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n0fx7y/3_things_i_learned_about_spacs_using_knowledge/,all_ads,False,https://medium.com/kgbase-blog/3-things-i-learned-about-spacs-using-knowledge-graphs-2ad0a7c12cf9,30199,1619619770.0,0,,False,https://medium.com/kgbase-blog/3-things-i-learned-about-spacs-using-knowledge-graphs-2ad0a7c12cf9,,,,,0
1271,,LanguageTechnology,"Hi!

I have 2 separately trained word2vec models. I want to be able to compare the same word in both models and see how they relate to each other and their surroundings through a metric. Now of course, I can't use the cosine similarity as the word vectors are completely different. I also want to avoid using the `wv.most_similar()` method as I would like to effectively capture the similarity of the same word in both models through some kind of metric if it were possible (as if i were using the cosine similarity between 2 words in the same model), but i am not sure of any that may exist! 

Does anybody know of any such metrics or ways of comparing two word vectors like this?

thank you!",t2_2yirhe6m,False,,0,False,[D] metrics for measuring similarity of word vectors between different models,[],r/LanguageTechnology,False,6,,0,,False,t3_n0h695,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1619651996.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;I have 2 separately trained word2vec models. I want to be able to compare the same word in both models and see how they relate to each other and their surroundings through a metric. Now of course, I can&amp;#39;t use the cosine similarity as the word vectors are completely different. I also want to avoid using the &lt;code&gt;wv.most_similar()&lt;/code&gt; method as I would like to effectively capture the similarity of the same word in both models through some kind of metric if it were possible (as if i were using the cosine similarity between 2 words in the same model), but i am not sure of any that may exist! &lt;/p&gt;

&lt;p&gt;Does anybody know of any such metrics or ways of comparing two word vectors like this?&lt;/p&gt;

&lt;p&gt;thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n0h695,True,,amjass12,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n0h695/d_metrics_for_measuring_similarity_of_word/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n0h695/d_metrics_for_measuring_similarity_of_word/,30199,1619623196.0,0,,False,,,,,,693
1272,,LanguageTechnology,"Wanted to introduce [chai](https://chai.ml) to y'all - it's a free, open-source platform for developers to build and deploy their own chat AIs. Currently over 100 AIs are hosted on the platform, which has over 12,000 conversations 😱

Following the tutorials at [https://chai.ml/docs](https://chai.ml/docs) you can deploy your own in under 10 minutes. I think this is such a cool way to learn NLP and it's really fun seeing your score go up on the [developer platform](https://chai.ml/dev).",t2_4o04wt,False,,0,False,Chai - Build your own chat AI in 10 minutes,[],r/LanguageTechnology,False,6,,0,,False,t3_n08m10,False,dark,0.8,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,False,,False,,[],{},,True,,1619620026.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Wanted to introduce &lt;a href=""https://chai.ml""&gt;chai&lt;/a&gt; to y&amp;#39;all - it&amp;#39;s a free, open-source platform for developers to build and deploy their own chat AIs. Currently over 100 AIs are hosted on the platform, which has over 12,000 conversations 😱&lt;/p&gt;

&lt;p&gt;Following the tutorials at &lt;a href=""https://chai.ml/docs""&gt;https://chai.ml/docs&lt;/a&gt; you can deploy your own in under 10 minutes. I think this is such a cool way to learn NLP and it&amp;#39;s really fun seeing your score go up on the &lt;a href=""https://chai.ml/dev""&gt;developer platform&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n08m10,True,,zecharias99,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n08m10/chai_build_your_own_chat_ai_in_10_minutes/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n08m10/chai_build_your_own_chat_ai_in_10_minutes/,30199,1619591226.0,0,,False,,,,,,489
1273,,LanguageTechnology,"Hi guys, 

I’m working on a student assignment that essentially involves determining whether the sentiment is good/bad for certain keywords.

For example if a review is: 

‘I love the facilities and staff here, however the bathrooms were dirty.’

I need to be able to determine that the facilities and staff were positive and the cleanliness was negative. 

Does anyone have any suggestions for how to go about this? 

Relatively new to NLP!!!",t2_437mget6,False,,0,False,Does anyone have any suggestions for determining sentiment associated with key words in online reviews? Pls help,[],r/LanguageTechnology,False,6,,0,,False,t3_n0e2wq,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619643019.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys, &lt;/p&gt;

&lt;p&gt;I’m working on a student assignment that essentially involves determining whether the sentiment is good/bad for certain keywords.&lt;/p&gt;

&lt;p&gt;For example if a review is: &lt;/p&gt;

&lt;p&gt;‘I love the facilities and staff here, however the bathrooms were dirty.’&lt;/p&gt;

&lt;p&gt;I need to be able to determine that the facilities and staff were positive and the cleanliness was negative. &lt;/p&gt;

&lt;p&gt;Does anyone have any suggestions for how to go about this? &lt;/p&gt;

&lt;p&gt;Relatively new to NLP!!!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,n0e2wq,True,,Fancy-Shelter7149,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/n0e2wq/does_anyone_have_any_suggestions_for_determining/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/n0e2wq/does_anyone_have_any_suggestions_for_determining/,30199,1619614219.0,0,,False,,,,,,443
1274,,LanguageTechnology,"Want to introduce “The NLP Index”, a new asset in NLP code discovery. It's free and open to the public.

It houses over 3,000 code repositories that one can search including a side bar with some of the most important topics in NLP today. The engine is search as you type and typo tolerant (it’s crazy fast). The index includes the arxiv research paper PDF, ConnectedPapers link, and its GitHub repo.

[https://index.quantumstat.com/](https://index.quantumstat.com/)",t2_27omm5ij,False,,0,False,"The NLP Index: 3,000+ code repos for hackers and researchers. [self-promotion]",[],r/LanguageTechnology,False,6,,0,,False,t3_mzosly,False,dark,0.99,,public,78,1,{},,False,[],,False,False,,{},,False,78,,False,False,,False,,[],{'gid_1': 1},,True,,1619559938.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Want to introduce “The NLP Index”, a new asset in NLP code discovery. It&amp;#39;s free and open to the public.&lt;/p&gt;

&lt;p&gt;It houses over 3,000 code repositories that one can search including a side bar with some of the most important topics in NLP today. The engine is search as you type and typo tolerant (it’s crazy fast). The index includes the arxiv research paper PDF, ConnectedPapers link, and its GitHub repo.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://index.quantumstat.com/""&gt;https://index.quantumstat.com/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mzosly,True,,Quantum_Stat,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mzosly/the_nlp_index_3000_code_repos_for_hackers_and/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mzosly/the_nlp_index_3000_code_repos_for_hackers_and/,30199,1619531138.0,0,,False,,,,,,465
1275,,LanguageTechnology,,t2_5zb7u,False,,0,False,Facebook's Internal Report About Its Role In The Capitol Insurrection -- Interested in Thoughts about the analysts' approach to text analytics as an adjunct to their social network and time analytics,[],r/LanguageTechnology,False,6,,0,,False,t3_mzn432,False,dark,1.0,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,False,,1619554517.0,text,6,,,text,buzzfeednews.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mzn432,True,,androbot,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mzn432/facebooks_internal_report_about_its_role_in_the/,all_ads,False,https://www.buzzfeednews.com/article/ryanmac/full-facebook-stop-the-steal-internal-report,30199,1619525717.0,0,,False,https://www.buzzfeednews.com/article/ryanmac/full-facebook-stop-the-steal-internal-report,,,,,0
1276,,LanguageTechnology,As the title suggests I'm looking for some reading material (or videos) on Topic Modelling (esp. Hierarchical Topic Modelling). My aim is to understand the concept.,t2_sx582e5,False,,0,False,Reading material for Topic Modelling,[],r/LanguageTechnology,False,6,,0,,False,t3_mzk8y4,False,dark,0.94,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,False,,False,,[],{},,True,,1619543082.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;As the title suggests I&amp;#39;m looking for some reading material (or videos) on Topic Modelling (esp. Hierarchical Topic Modelling). My aim is to understand the concept.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mzk8y4,True,,mayanknagda,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mzk8y4/reading_material_for_topic_modelling/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mzk8y4/reading_material_for_topic_modelling/,30199,1619514282.0,0,,False,,,,,,164
1277,,LanguageTechnology,"Hello!

Many users were asking me to add transformer-based translation models to the [NLPCloud.io](https://nlpcloud.io/?utm_source=reddit&amp;utm_campaign=f80a8332-aaaf-11eb-bcbc-0242ac130002)'s API. So I added the 12 following NLP models based on Hugging Face transformers and Helsinki NLP's Opus MT:

* English to French
* French to English
* English to Spanish
* Spanish to English
* English to German
* German to English
* English to Dutch
* Dutch to English
* English to Chinese
* Chinese to English
* English to Russian
* Russian to English

I personally find the accuracy of these translation models very good, and the latency is pretty good too. Here's the link to the docs: [https://docs.nlpcloud.io/#translation](https://docs.nlpcloud.io/#translation)

**I would LOVE to have your opinion on this guys!**  Do you think that quality is good enough for  production use? Are there other languages you would like to see?

If you want to have a try, the API is free for up to 3 requests per minute, but if it's not enough please don't hesitate to ping me so I can grant you more requests.

Thanks!",t2_4z4m2qcs,False,,0,False,"I added translation models to the NLPCloud.io API, based on Helsinki NLP's Opus MT",[],r/LanguageTechnology,False,6,,0,,False,t3_mzqdgi,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,1619895876.0,,[],{},,True,,1619564409.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;Many users were asking me to add transformer-based translation models to the &lt;a href=""https://nlpcloud.io/?utm_source=reddit&amp;amp;utm_campaign=f80a8332-aaaf-11eb-bcbc-0242ac130002""&gt;NLPCloud.io&lt;/a&gt;&amp;#39;s API. So I added the 12 following NLP models based on Hugging Face transformers and Helsinki NLP&amp;#39;s Opus MT:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;English to French&lt;/li&gt;
&lt;li&gt;French to English&lt;/li&gt;
&lt;li&gt;English to Spanish&lt;/li&gt;
&lt;li&gt;Spanish to English&lt;/li&gt;
&lt;li&gt;English to German&lt;/li&gt;
&lt;li&gt;German to English&lt;/li&gt;
&lt;li&gt;English to Dutch&lt;/li&gt;
&lt;li&gt;Dutch to English&lt;/li&gt;
&lt;li&gt;English to Chinese&lt;/li&gt;
&lt;li&gt;Chinese to English&lt;/li&gt;
&lt;li&gt;English to Russian&lt;/li&gt;
&lt;li&gt;Russian to English&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I personally find the accuracy of these translation models very good, and the latency is pretty good too. Here&amp;#39;s the link to the docs: &lt;a href=""https://docs.nlpcloud.io/#translation""&gt;https://docs.nlpcloud.io/#translation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;I would LOVE to have your opinion on this guys!&lt;/strong&gt;  Do you think that quality is good enough for  production use? Are there other languages you would like to see?&lt;/p&gt;

&lt;p&gt;If you want to have a try, the API is free for up to 3 requests per minute, but if it&amp;#39;s not enough please don&amp;#39;t hesitate to ping me so I can grant you more requests.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mzqdgi,True,,juliensalinas,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mzqdgi/i_added_translation_models_to_the_nlpcloudio_api/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mzqdgi/i_added_translation_models_to_the_nlpcloudio_api/,30199,1619535609.0,0,,False,,,,,,1102
1278,,LanguageTechnology,"I imagine in voice recognition training data there are simple nouns which you can hear clearly, and with no similar words?",t2_dfvmc,False,,0,False,"I often help companies do branding, and I'm wanting to find words that are simple to hear, are there words that are unique and easy to hear?",[],r/LanguageTechnology,False,6,,0,,False,t3_mzybpa,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1619585537.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I imagine in voice recognition training data there are simple nouns which you can hear clearly, and with no similar words?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mzybpa,True,,jaybestnz,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mzybpa/i_often_help_companies_do_branding_and_im_wanting/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mzybpa/i_often_help_companies_do_branding_and_im_wanting/,30199,1619556737.0,0,,False,,,,,,122
1279,,LanguageTechnology,"Hi everyone, I'm really interested in building chatbots. I'm wondering what kinds of chatbot people need. 

If you can build your own AI chatbots, what kinds of chatbots do you want to build? (e.g., BTS chatbot, the chatbot that brings your loved one back from the dead, and business chatbot, etc.) Why you need those chatbots?",t2_bsm6d0e4,False,,0,False,"If you can build AI chatbots, what kinds of chatbots do you want to build?",[],r/LanguageTechnology,False,6,,0,,False,t3_mzoos1,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619559619.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, I&amp;#39;m really interested in building chatbots. I&amp;#39;m wondering what kinds of chatbot people need. &lt;/p&gt;

&lt;p&gt;If you can build your own AI chatbots, what kinds of chatbots do you want to build? (e.g., BTS chatbot, the chatbot that brings your loved one back from the dead, and business chatbot, etc.) Why you need those chatbots?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mzoos1,True,,Candid-Wishbone-692,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mzoos1/if_you_can_build_ai_chatbots_what_kinds_of/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mzoos1/if_you_can_build_ai_chatbots_what_kinds_of/,30199,1619530819.0,0,,False,,,,,,327
1280,,LanguageTechnology,Need suggestions to fine tune Gan-Bert model on mechanical design dataset. Any leads would be appreciated. I can share the detailed problem if you can suggest some insights.,t2_5cjqpkl9,False,,0,False,Gan-Bert for Topic modelling,[],r/LanguageTechnology,False,6,,0,,False,t3_mzm7kp,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1619551263.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Need suggestions to fine tune Gan-Bert model on mechanical design dataset. Any leads would be appreciated. I can share the detailed problem if you can suggest some insights.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mzm7kp,True,,thecrimsonhead,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mzm7kp/ganbert_for_topic_modelling/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mzm7kp/ganbert_for_topic_modelling/,30199,1619522463.0,0,,False,,,,,,173
1281,,LanguageTechnology,"I have an upcoming project that has a speech recognition component, but I'm pretty unfamiliar with the basics of this branch of NLP.

I suspect that I will need to create my own training dataset because the speech will involve a lot of niche terminology. From [what little I've seen on the tooling side](https://prodi.gy/docs/audio-video#transcribe), it looks like people segment audio into rough sentence equivalents, and then just type in a transcript to an input field. 

Is this best practice for creating speech recognition models? I would think you would need to provide word-level alignments to the audio, and that the models would be word level sequence models. But the training data tools seem mostly to facilitate capture of whole sentence transcripts.

Any help is appreciated",t2_9ozjj,False,,0,False,Speech Recognition Training Data Tools?,[],r/LanguageTechnology,False,6,,0,,False,t3_mzf2ee,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1619521455.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have an upcoming project that has a speech recognition component, but I&amp;#39;m pretty unfamiliar with the basics of this branch of NLP.&lt;/p&gt;

&lt;p&gt;I suspect that I will need to create my own training dataset because the speech will involve a lot of niche terminology. From &lt;a href=""https://prodi.gy/docs/audio-video#transcribe""&gt;what little I&amp;#39;ve seen on the tooling side&lt;/a&gt;, it looks like people segment audio into rough sentence equivalents, and then just type in a transcript to an input field. &lt;/p&gt;

&lt;p&gt;Is this best practice for creating speech recognition models? I would think you would need to provide word-level alignments to the audio, and that the models would be word level sequence models. But the training data tools seem mostly to facilitate capture of whole sentence transcripts.&lt;/p&gt;

&lt;p&gt;Any help is appreciated&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mzf2ee,True,,SurplusPopulation,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mzf2ee/speech_recognition_training_data_tools/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mzf2ee/speech_recognition_training_data_tools/,30199,1619492655.0,0,,False,,,,,,787
1282,,LanguageTechnology,,t2_5smymm7p,False,,0,False,Into NLP - Text Normalization,[],r/LanguageTechnology,False,6,,0,,False,t3_myv82r,False,dark,0.93,,public,23,0,{},,False,[],,False,False,,{},,False,23,,False,False,,False,,[],{},,False,,1619464233.0,text,6,,,text,qualicen.de,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,myv82r,True,,QualicenDS,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/myv82r/into_nlp_text_normalization/,all_ads,False,https://www.qualicen.de/into-nlp-4-normal-language-perfection-text-normalization/,30199,1619435433.0,0,,False,https://www.qualicen.de/into-nlp-4-normal-language-perfection-text-normalization/,,,,,0
1283,,LanguageTechnology,"As part of an internship, I have to build a biomedical knowledge graph from textual data, to do this I have to go through the tasks of named entity extraction and relation extraction as well as coreference resolution using BERT variant models. My problem is the availability of fine tune data for the three tasks.

Is there any open access datasets that I can use to fine tune my models in the three previous tasks in the biomedical domain?",t2_55lweil3,False,,0,False,Biomedical datasets suggestions for fine tuning Bert variant models on three tasks,[],r/LanguageTechnology,False,6,,0,,False,t3_mywo9c,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619469427.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;As part of an internship, I have to build a biomedical knowledge graph from textual data, to do this I have to go through the tasks of named entity extraction and relation extraction as well as coreference resolution using BERT variant models. My problem is the availability of fine tune data for the three tasks.&lt;/p&gt;

&lt;p&gt;Is there any open access datasets that I can use to fine tune my models in the three previous tasks in the biomedical domain?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mywo9c,True,,theweirdinstruction,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mywo9c/biomedical_datasets_suggestions_for_fine_tuning/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mywo9c/biomedical_datasets_suggestions_for_fine_tuning/,30199,1619440627.0,0,,False,,,,,,440
1284,,LanguageTechnology,"Please state specific math subjects in the format used below (these are simply examples).

* Calculus 1

* Calculus 2

* Calculus 3

* Calculus 4

* Discrete Math

* Introduction to Probability

* Introduction to Statistics

* Statistical Inference

* Linear Algebra

Thank You",t2_botme4lw,False,,0,False,What specific math subjects do I need to fully understand every NLP/Computational Linguistics journal article (espeically the most important ones)?,[],r/LanguageTechnology,False,6,,0,,False,t3_myqlea,False,dark,0.73,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1619444355.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Please state specific math subjects in the format used below (these are simply examples).&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Calculus 1&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Calculus 2&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Calculus 3&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Calculus 4&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Discrete Math&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Introduction to Probability&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Introduction to Statistics&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Statistical Inference&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Linear Algebra&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank You&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,myqlea,True,,RockHardAndVeryHorny,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/myqlea/what_specific_math_subjects_do_i_need_to_fully/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/myqlea/what_specific_math_subjects_do_i_need_to_fully/,30199,1619415555.0,0,,False,,,,,,277
1285,,LanguageTechnology,"There's a dataset which has conversational data with customer &amp; sales persons &amp; I have to incorporate that data into a sales forecasting model. 

The conversational data is mainly in sentiment - related topics (For reference)

What NLP technique can be applied here to maybe get a feature out that can act as a dependent variable when I am predicting my final sales output?

PS. I have other variables that can help to predict sales, but I need to use the text data as well. How am  I supposed to move forward with this?",t2_8j8sr5mz,False,,0,False,Incorporating Text Data for a Sales Forecasting Model,[],r/LanguageTechnology,False,6,,0,,False,t3_myqvhj,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619445566.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;There&amp;#39;s a dataset which has conversational data with customer &amp;amp; sales persons &amp;amp; I have to incorporate that data into a sales forecasting model. &lt;/p&gt;

&lt;p&gt;The conversational data is mainly in sentiment - related topics (For reference)&lt;/p&gt;

&lt;p&gt;What NLP technique can be applied here to maybe get a feature out that can act as a dependent variable when I am predicting my final sales output?&lt;/p&gt;

&lt;p&gt;PS. I have other variables that can help to predict sales, but I need to use the text data as well. How am  I supposed to move forward with this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,myqvhj,True,,Educational-Bid-5263,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/myqvhj/incorporating_text_data_for_a_sales_forecasting/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/myqvhj/incorporating_text_data_for_a_sales_forecasting/,30199,1619416766.0,0,,False,,,,,,528
1286,,LanguageTechnology,I'm pretty new to NLP so I hope you can help me out. I have a file with \~700 paragraphs on a similar topic and I want to make a semantic search engine so that the user can input a query and it will return the paragraphs that match the closest. Thanks!,t2_2scuv8zh,False,,0,False,Any good tutorials for creating a semantic search engine?,[],r/LanguageTechnology,False,6,,0,,False,t3_myk81n,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1619421444.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m pretty new to NLP so I hope you can help me out. I have a file with ~700 paragraphs on a similar topic and I want to make a semantic search engine so that the user can input a query and it will return the paragraphs that match the closest. Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,myk81n,True,,beardedjoy,,10,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/myk81n/any_good_tutorials_for_creating_a_semantic_search/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/myk81n/any_good_tutorials_for_creating_a_semantic_search/,30199,1619392644.0,0,,False,,,,,,252
1287,,LanguageTechnology,,t2_5fsp2x6v,False,,0,False,BERTweet (SOTA) for Named Entity Recognition in Social Media | Research Papers Summary 015,[],r/LanguageTechnology,False,6,,0,,False,t3_myf69x,False,dark,0.8,,public,6,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/npxjcPhFLKE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'BERTweet (SOTA) for Named Entity Recognition in Social Media | Research Papers Summary 015', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/npxjcPhFLKE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/npxjcPhFLKE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/npxjcPhFLKE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/myf69x', 'height': 200}",,False,6,,False,False,,False,,[],{},,False,,1619406598.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,myf69x,True,,RyanAI100,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/myf69x/bertweet_sota_for_named_entity_recognition_in/,all_ads,False,https://youtu.be/npxjcPhFLKE,30199,1619377798.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'BERTweet (SOTA) for Named Entity Recognition in Social Media | Research Papers Summary 015', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/npxjcPhFLKE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/npxjcPhFLKE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,https://youtu.be/npxjcPhFLKE,,,,,0
1288,,LanguageTechnology,"I was wondering whether there are models that can handle text input with two or more languages.

For instance, a multilingualBert can probably understand both English and Spanish:

`(en) This is a test for language model`

`(es)Esta es una prueba para el modelo de lenguaje`

But it fails in understanding the following :

`(en-es) This is a prueba for language modelo`

`(en-jp) This is a テスト for language モデル`

A text that has both English and Spanish or Japanese(where even the script is different than Latin).",t2_nwzpkmq,False,,0,False,Handling Multi-lingual text,[],r/LanguageTechnology,False,6,,0,,False,t3_my6w2t,False,dark,1.0,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,False,,False,,[],{},,True,,1619380828.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was wondering whether there are models that can handle text input with two or more languages.&lt;/p&gt;

&lt;p&gt;For instance, a multilingualBert can probably understand both English and Spanish:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;(en) This is a test for language model&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;(es)Esta es una prueba para el modelo de lenguaje&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;But it fails in understanding the following :&lt;/p&gt;

&lt;p&gt;&lt;code&gt;(en-es) This is a prueba for language modelo&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;(en-jp) This is a テスト for language モデル&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;A text that has both English and Spanish or Japanese(where even the script is different than Latin).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,my6w2t,True,,10zin_,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/my6w2t/handling_multilingual_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/my6w2t/handling_multilingual_text/,30199,1619352028.0,0,,False,,,,,,513
1289,,LanguageTechnology,,t2_hkv9s,False,,0,False,Aspect-based Document Similarity for Research Papers (Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_my6c2k,False,dark,0.83,,public,11,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ZO6QWG7-Ye0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Aspect-based Document Similarity for Research Papers (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ZO6QWG7-Ye0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/ZO6QWG7-Ye0/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ZO6QWG7-Ye0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/my6c2k', 'height': 200}",,False,11,,False,False,,False,,[],{},,False,,1619378514.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,my6c2k,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/my6c2k/aspectbased_document_similarity_for_research/,all_ads,False,https://youtu.be/ZO6QWG7-Ye0,30199,1619349714.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Aspect-based Document Similarity for Research Papers (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ZO6QWG7-Ye0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/ZO6QWG7-Ye0/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/ZO6QWG7-Ye0,,,,,0
1290,,LanguageTechnology,"I'm building a open source government complaint redressal system. For the purpose of complaint classification (for routing the complaints to a department), I need a existing government complaint dataset. If anyone has come across any such datasets, please help by commenting.

It basically needs to have a complaint column and a column with department name to which the complaints belong",t2_8n5d56qx,False,,0,False,Government complaint dataset,[],r/LanguageTechnology,False,6,,0,,False,t3_myc2ee,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1619397755.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m building a open source government complaint redressal system. For the purpose of complaint classification (for routing the complaints to a department), I need a existing government complaint dataset. If anyone has come across any such datasets, please help by commenting.&lt;/p&gt;

&lt;p&gt;It basically needs to have a complaint column and a column with department name to which the complaints belong&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,myc2ee,True,,ChandlerBingggggggg,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/myc2ee/government_complaint_dataset/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/myc2ee/government_complaint_dataset/,30199,1619368955.0,0,,False,,,,,,387
1291,,LanguageTechnology," Hi all,

My  name is Kushel Ramanayake, and I am a current 4th undergraduate at  Informatics Institute of Technology affiliated with University of  Westminster UK. As part of my Final Year Thesis, I am required to gather some feedback on my project, from experts within the domain. The project I am working on is similar to an automatic question generation system, and main technology is NLP.

Ideally we'll get on a 1 on 1 zoom call (\~15 minutes), where I'd go over my project and you'd give me feedback on;

* The Scope
* The Architecture of the Solution
* The Implementation of the Solution

The data collected will only be used for my thesis and will be discarded with afterwards. No personal information other than maybe name and occupation will be collected. 

Thank you in advance,

Kushel Ramanayake",t2_ic2y7,False,,0,False,Feedback for NLP project,[],r/LanguageTechnology,False,6,,0,,False,t3_my41vc,False,dark,0.89,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1619368300.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;My  name is Kushel Ramanayake, and I am a current 4th undergraduate at  Informatics Institute of Technology affiliated with University of  Westminster UK. As part of my Final Year Thesis, I am required to gather some feedback on my project, from experts within the domain. The project I am working on is similar to an automatic question generation system, and main technology is NLP.&lt;/p&gt;

&lt;p&gt;Ideally we&amp;#39;ll get on a 1 on 1 zoom call (~15 minutes), where I&amp;#39;d go over my project and you&amp;#39;d give me feedback on;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Scope&lt;/li&gt;
&lt;li&gt;The Architecture of the Solution&lt;/li&gt;
&lt;li&gt;The Implementation of the Solution&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The data collected will only be used for my thesis and will be discarded with afterwards. No personal information other than maybe name and occupation will be collected. &lt;/p&gt;

&lt;p&gt;Thank you in advance,&lt;/p&gt;

&lt;p&gt;Kushel Ramanayake&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,my41vc,True,,kushel,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/my41vc/feedback_for_nlp_project/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/my41vc/feedback_for_nlp_project/,30199,1619339500.0,0,,False,,,,,,809
1292,,LanguageTechnology,"[https://preview.redd.it/t60n4t6z08v61.gif?format=mp4&amp;s=b3e2748ba96ec723266ee5c8e14bd49081cb6c57](https://preview.redd.it/t60n4t6z08v61.gif?format=mp4&amp;s=b3e2748ba96ec723266ee5c8e14bd49081cb6c57)

Information:  
For my dissertation project I fine-tuned a pre-trained language model on a self-mined dataset of ""left"" and ""right"" leaning subreddits to classify comments and subreddit's.

I mined the data over a few months using praw, I used a list of around 20-25 different subreddits taking between 10-20,000 comments from each from within the past year, so the model is quite American election biased but the model was fine tuned a few weeks ago so the comments you are seeing the gif it has not seen before.

I used DistilBert to fine-tune the model on pre-processed text, I spent a few months fine-tuning different models on different versions of the data set until I minimised overfitting and got a decent validation to training trade-off.

I also made a fun venn diagram tool to help find similar subreddits, I used this tool with a much larger sample size to help find similar leaning subreddits to help remove my personal bias although I am certain the left-wing subreddits tend to the far left more than the right which is why you may see a fair bit of negative biden commentary leading more left than right.

Disclaimer:  
The venn diagram tool and the subreddit classifier tool utilise praw which has a decent rate limit so may take 10-20 seconds before it returns a result, I have moved to psaw although loading times have not improved much.

View this project: [https://reddit-political-analysis.com/](https://reddit-political-analysis.com/)",t2_6l9u9,False,,0,False,I fine-tuned a language model on left and right leaning political commentary on Reddit,[],r/LanguageTechnology,False,6,,0,,False,t3_mxxwlc,False,dark,0.9,,public,16,0,{},,False,[],,False,False,,{},,False,16,,False,False,,False,,[],{},,True,,1619343228.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://preview.redd.it/t60n4t6z08v61.gif?format=mp4&amp;amp;s=b3e2748ba96ec723266ee5c8e14bd49081cb6c57""&gt;https://preview.redd.it/t60n4t6z08v61.gif?format=mp4&amp;amp;s=b3e2748ba96ec723266ee5c8e14bd49081cb6c57&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Information:&lt;br/&gt;
For my dissertation project I fine-tuned a pre-trained language model on a self-mined dataset of &amp;quot;left&amp;quot; and &amp;quot;right&amp;quot; leaning subreddits to classify comments and subreddit&amp;#39;s.&lt;/p&gt;

&lt;p&gt;I mined the data over a few months using praw, I used a list of around 20-25 different subreddits taking between 10-20,000 comments from each from within the past year, so the model is quite American election biased but the model was fine tuned a few weeks ago so the comments you are seeing the gif it has not seen before.&lt;/p&gt;

&lt;p&gt;I used DistilBert to fine-tune the model on pre-processed text, I spent a few months fine-tuning different models on different versions of the data set until I minimised overfitting and got a decent validation to training trade-off.&lt;/p&gt;

&lt;p&gt;I also made a fun venn diagram tool to help find similar subreddits, I used this tool with a much larger sample size to help find similar leaning subreddits to help remove my personal bias although I am certain the left-wing subreddits tend to the far left more than the right which is why you may see a fair bit of negative biden commentary leading more left than right.&lt;/p&gt;

&lt;p&gt;Disclaimer:&lt;br/&gt;
The venn diagram tool and the subreddit classifier tool utilise praw which has a decent rate limit so may take 10-20 seconds before it returns a result, I have moved to psaw although loading times have not improved much.&lt;/p&gt;

&lt;p&gt;View this project: &lt;a href=""https://reddit-political-analysis.com/""&gt;https://reddit-political-analysis.com/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mxxwlc,True,,rockwilly,,12,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mxxwlc/i_finetuned_a_language_model_on_left_and_right/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mxxwlc/i_finetuned_a_language_model_on_left_and_right/,30199,1619314428.0,0,,False,,,,,,1660
1293,,LanguageTechnology,"I'm researching language mixing and English borrowings and I need to compare two txt corpora (one in English, one in Polish) to find words that overlap between them, excluding stop words, and preferably the frequency of overlap as well. I'm using Python. One difficulty is that Polish is an inflectional language, so e.g. ""to ghost"" is ""ghostOWAĆ"" in Polish. How can I account for that in my code so the program to find the instances of these inflected words as well?

Thanks!",t2_60py1dqs,False,,0,False,Corpus overlap and inflection,[],r/LanguageTechnology,False,6,,0,,False,t3_my4vju,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619372094.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m researching language mixing and English borrowings and I need to compare two txt corpora (one in English, one in Polish) to find words that overlap between them, excluding stop words, and preferably the frequency of overlap as well. I&amp;#39;m using Python. One difficulty is that Polish is an inflectional language, so e.g. &amp;quot;to ghost&amp;quot; is &amp;quot;ghostOWAĆ&amp;quot; in Polish. How can I account for that in my code so the program to find the instances of these inflected words as well?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,my4vju,True,,Complex-Intention256,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/my4vju/corpus_overlap_and_inflection/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/my4vju/corpus_overlap_and_inflection/,30199,1619343294.0,0,,False,,,,,,476
1294,,LanguageTechnology,,t2_10efjmjx,False,,0,False,Anyone know of any papers about training with a traditional pretraining task (MLM) simultaneously with a finetuning task; as opposed to first doing pretraining then finetuning ?,[],r/LanguageTechnology,False,6,,0,,False,t3_my1ruz,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619357749.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,my1ruz,True,,BatmantoshReturns,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/my1ruz/anyone_know_of_any_papers_about_training_with_a/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/my1ruz/anyone_know_of_any_papers_about_training_with_a/,30199,1619328949.0,0,,False,,,,,,0
1295,,LanguageTechnology,"I have data which is basically a group of paragraphs that I know could be related.

Inside a group of paragraphs (let’s say 100 paragraphs). There could be a sentence of two that are almost/exactly the same text across 60% of the paragraphs. Another completely different similar text across 20% of the paragraphs while the other 20% might be unrelated. 

I have billions of these groups of 100 paragraphs and have no idea where to start! Thanks for any pointers",t2_8dmgu,False,,0,False,How to approach finding Similar wording in paragraph,[],r/LanguageTechnology,False,6,,0,,False,t3_mxm06o,False,dark,0.94,,public,13,1,{},,False,[],,False,False,,{},,False,13,,False,False,,False,,[],{},,True,,1619306339.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have data which is basically a group of paragraphs that I know could be related.&lt;/p&gt;

&lt;p&gt;Inside a group of paragraphs (let’s say 100 paragraphs). There could be a sentence of two that are almost/exactly the same text across 60% of the paragraphs. Another completely different similar text across 20% of the paragraphs while the other 20% might be unrelated. &lt;/p&gt;

&lt;p&gt;I have billions of these groups of 100 paragraphs and have no idea where to start! Thanks for any pointers&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mxm06o,True,,johne898,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mxm06o/how_to_approach_finding_similar_wording_in/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mxm06o/how_to_approach_finding_similar_wording_in/,30199,1619277539.0,0,,False,,,,,,461
1296,,LanguageTechnology,"What would be a good method to find, for example, the best method to compare two sentences for having the same or a similar meaning? I mean something like a overview or a search engine, based on a problem/solution pattern. Finding the best ways to solve a problem, without asking around a lot or looking into abstracts for papers.",t2_6jxf653x,False,,0,False,Best way to find the (best) solution to any task?,[],r/LanguageTechnology,False,6,,0,,False,t3_mxodes,False,dark,0.81,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1619313468.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What would be a good method to find, for example, the best method to compare two sentences for having the same or a similar meaning? I mean something like a overview or a search engine, based on a problem/solution pattern. Finding the best ways to solve a problem, without asking around a lot or looking into abstracts for papers.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mxodes,True,,botfiddler,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mxodes/best_way_to_find_the_best_solution_to_any_task/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mxodes/best_way_to_find_the_best_solution_to_any_task/,30199,1619284668.0,0,,False,,,,,,330
1297,,LanguageTechnology,"Any expert in the domain here?

I had learnt about LDA/ LSA, and it seems that BERT artchitecture is not a fit if there's no pretagged labels on the topics. What are some of the latst trends? Are there any good teams/ companies working on this sector?",t2_11dikh,False,,0,False,Latest trends in topic modelling?,[],r/LanguageTechnology,False,6,,0,,False,t3_mxcvxk,False,dark,1.0,,public,21,0,{},,False,[],,False,False,,{},,False,21,,False,False,,False,,[],{},,True,,1619268532.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Any expert in the domain here?&lt;/p&gt;

&lt;p&gt;I had learnt about LDA/ LSA, and it seems that BERT artchitecture is not a fit if there&amp;#39;s no pretagged labels on the topics. What are some of the latst trends? Are there any good teams/ companies working on this sector?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mxcvxk,True,,reddithurc,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mxcvxk/latest_trends_in_topic_modelling/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mxcvxk/latest_trends_in_topic_modelling/,30199,1619239732.0,0,,False,,,,,,251
1298,,LanguageTechnology,How do I fine tune another language? Who will tell you?,t2_82cn9rut,False,,0,False,Transformers Pegasus - how do I fine tune another language?,[],r/LanguageTechnology,False,6,,0,,False,t3_mxj2j1,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1619296396.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How do I fine tune another language? Who will tell you?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mxj2j1,True,,Careless-Shift-4100,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mxj2j1/transformers_pegasus_how_do_i_fine_tune_another/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mxj2j1/transformers_pegasus_how_do_i_fine_tune_another/,30199,1619267596.0,0,,False,,,,,,55
1299,,LanguageTechnology,"Hello,

I'm currently working on a NER that extracts products and capabilities of company based on texts from their website. At the moment I don't have any training data that I could use to train my NLP Model. I need German training data.

I found this farmework: [https://github.com/NorskRegnesentral/skweak](https://github.com/NorskRegnesentral/skweak) and it looks great to automatically label data, but I would still need some kind of structured data in form of gazetters or another ML model to automatically annotate words.

One thing I tried is using Masked Language Model to predict missing words. I found this approach here: [https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a](https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a)  
If one of the predicted words is in my target dictionary then I tag this word with the corresponding entity. This approach works in some cases, but it is not very accurate and I'm not sure how to deal with products that consist of multiple words.

Do any of you have an idea where I could find training data in German with labeled products or how i could generate the training data myself?

Thanks",t2_6eol9zo6,False,,0,False,How to get Training data for NER?,[],r/LanguageTechnology,False,6,,0,,False,t3_mxg8pj,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1619283923.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m currently working on a NER that extracts products and capabilities of company based on texts from their website. At the moment I don&amp;#39;t have any training data that I could use to train my NLP Model. I need German training data.&lt;/p&gt;

&lt;p&gt;I found this farmework: &lt;a href=""https://github.com/NorskRegnesentral/skweak""&gt;https://github.com/NorskRegnesentral/skweak&lt;/a&gt; and it looks great to automatically label data, but I would still need some kind of structured data in form of gazetters or another ML model to automatically annotate words.&lt;/p&gt;

&lt;p&gt;One thing I tried is using Masked Language Model to predict missing words. I found this approach here: &lt;a href=""https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a""&gt;https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a&lt;/a&gt;&lt;br/&gt;
If one of the predicted words is in my target dictionary then I tag this word with the corresponding entity. This approach works in some cases, but it is not very accurate and I&amp;#39;m not sure how to deal with products that consist of multiple words.&lt;/p&gt;

&lt;p&gt;Do any of you have an idea where I could find training data in German with labeled products or how i could generate the training data myself?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mxg8pj,True,,SWEQuestion123,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mxg8pj/how_to_get_training_data_for_ner/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mxg8pj/how_to_get_training_data_for_ner/,30199,1619255123.0,0,,False,,,,,True,1179
1300,,LanguageTechnology,"I'm coming from the world of computer vision and was wondering whether there's the YOLOv5 model equivalent for NLP? I'm looking to train a classification model to categorize short strings (ex: classifying area-related words - Acreage, Land Area, acres, estimated size, lot size, deeded acres, and land area - all as ""area""). I'm thinking of using transfer learning on a custom dataset.

Are there industry-standard models that you'd recommend looking into? Or, put differently, models that offer a good tradeoff between performance and ease-of-use?

Edit: Looking for deep learning approaches to NLP. Perhaps with a [fast.ai](https://fast.ai) flavour",t2_8qjmpb1m,False,,0,False,YOLOv5 equivalent for NLP classification?,[],r/LanguageTechnology,False,6,,0,,False,t3_mxae9h,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,1619229900.0,,[],{},,True,,1619258437.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m coming from the world of computer vision and was wondering whether there&amp;#39;s the YOLOv5 model equivalent for NLP? I&amp;#39;m looking to train a classification model to categorize short strings (ex: classifying area-related words - Acreage, Land Area, acres, estimated size, lot size, deeded acres, and land area - all as &amp;quot;area&amp;quot;). I&amp;#39;m thinking of using transfer learning on a custom dataset.&lt;/p&gt;

&lt;p&gt;Are there industry-standard models that you&amp;#39;d recommend looking into? Or, put differently, models that offer a good tradeoff between performance and ease-of-use?&lt;/p&gt;

&lt;p&gt;Edit: Looking for deep learning approaches to NLP. Perhaps with a &lt;a href=""https://fast.ai""&gt;fast.ai&lt;/a&gt; flavour&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mxae9h,True,,kaia_1527,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mxae9h/yolov5_equivalent_for_nlp_classification/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mxae9h/yolov5_equivalent_for_nlp_classification/,30199,1619229637.0,0,,False,,,,,,650
1301,,LanguageTechnology,,t2_apv7o2y8,False,,0,False,Text Classification using Convolutional Neural Network with TensorFlow 2.1 in Python | #NLProc,[],r/LanguageTechnology,False,6,,0,,False,t3_mxdj43,False,dark,0.67,,public,1,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/MsL79ZIqWpg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Text Classification using Convolutional Neural Network with TensorFlow 2.1  in Python |  #NLProc', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/MsL79ZIqWpg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/MsL79ZIqWpg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/MsL79ZIqWpg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mxdj43', 'height': 200}",,False,1,,False,False,,False,,[],{},,False,,1619271379.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mxdj43,True,,rsree123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mxdj43/text_classification_using_convolutional_neural/,all_ads,False,https://youtu.be/MsL79ZIqWpg,30199,1619242579.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Text Classification using Convolutional Neural Network with TensorFlow 2.1  in Python |  #NLProc', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/MsL79ZIqWpg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/MsL79ZIqWpg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,https://youtu.be/MsL79ZIqWpg,,,,,0
1302,,LanguageTechnology,"Hi everybody!

&amp;#x200B;

A couple of years ago, I made a [post to announce the release of mlconjug](https://www.reddit.com/r/Python/comments/bb8400/mlconjug_a_python_library_to_conjugate_verbs_in/),  a Python package/library to conjugate verbs (even made-up verbs, or verbs coming from slang and not covered in traditional conjugation tables) in French, English, Spanish, Italian, Portuguese and Romanian using Machine Learning techniques.

Since then, mlconjug has had a lot of success with thousands of students of foreign languages using it as a standalone application to improve their conjugation skills, but it has also been incorporated as a library dependency in more than a dozen different python software projects ranging from traditional NLP tasks using Machine Learning, to twitter bots, Voice Assistants, and even games.

It has also been used in several Academic publications, for example: [""Generative Grading: Near Human-level Accuracy for Automated Feedback on Richly Structured Problems""](https://arxiv.org/abs/1905.09916) where it is used for automatically grading students essays, and  United States citizenship exam questions.

I released a new and improved version of the software, called [mlconjug3](https://github.com/SekouDiaoNlp/mlconjug3) as it is only compatible with Python 3.x, and had many enhancements and bug fixes. The accuracy of the conjugations models has improved a lot and I am in the process of implementing regional European languages (in beta version for now), like Catalan,  Valencian , Basque language, etc... as well as slavic languages (Czech and Polish for now).

Those new languages should be available in the beginning of summer and I am looking for native speakers of the languages that are in beta status to test the software and check that the conjugated forms are correct.

You can install mlconjug3 from [PyPi](https://pypi.python.org/pypi/mlconjug3) or [Anaconda](https://anaconda.org/conda-forge/mlconjug3).

Some of the features of mlconjug3 are the following:

* Easy to use API.
* Includes pre-trained language models with 99% + accuracy in predicting conjugation class of unknown verbs.
* Easily train new models or add new languages.
* Easily integrate MLConjug in your own projects.
* Can be used as a command line tool.

I invite everyone to try it out and if you are a native speaker of Catalan,  Valencian , Basque, Czech or Polish and are willing to beta-test the software, please pm me, you are would be greatly appreciated, and it will make mlconjug3 more versatile and therefore more useful.

Thanks Everyone,

Peace,

SekouDiaoNlp",t2_46ggkdhi,False,,0,False,"Introducing mlconjug3. A Python library to conjugate verbs in French, English, Spanish, Italian, Portuguese and Romanian (with more in beta version) using Machine Learning techniques.",[],r/LanguageTechnology,False,6,,0,,False,t3_mwsymt,False,dark,0.96,,public,29,0,{},,False,[],,False,False,,{},,False,29,,False,False,,1619292255.0,,[],{},,True,,1619205397.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everybody!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;A couple of years ago, I made a &lt;a href=""https://www.reddit.com/r/Python/comments/bb8400/mlconjug_a_python_library_to_conjugate_verbs_in/""&gt;post to announce the release of mlconjug&lt;/a&gt;,  a Python package/library to conjugate verbs (even made-up verbs, or verbs coming from slang and not covered in traditional conjugation tables) in French, English, Spanish, Italian, Portuguese and Romanian using Machine Learning techniques.&lt;/p&gt;

&lt;p&gt;Since then, mlconjug has had a lot of success with thousands of students of foreign languages using it as a standalone application to improve their conjugation skills, but it has also been incorporated as a library dependency in more than a dozen different python software projects ranging from traditional NLP tasks using Machine Learning, to twitter bots, Voice Assistants, and even games.&lt;/p&gt;

&lt;p&gt;It has also been used in several Academic publications, for example: &lt;a href=""https://arxiv.org/abs/1905.09916""&gt;&amp;quot;Generative Grading: Near Human-level Accuracy for Automated Feedback on Richly Structured Problems&amp;quot;&lt;/a&gt; where it is used for automatically grading students essays, and  United States citizenship exam questions.&lt;/p&gt;

&lt;p&gt;I released a new and improved version of the software, called &lt;a href=""https://github.com/SekouDiaoNlp/mlconjug3""&gt;mlconjug3&lt;/a&gt; as it is only compatible with Python 3.x, and had many enhancements and bug fixes. The accuracy of the conjugations models has improved a lot and I am in the process of implementing regional European languages (in beta version for now), like Catalan,  Valencian , Basque language, etc... as well as slavic languages (Czech and Polish for now).&lt;/p&gt;

&lt;p&gt;Those new languages should be available in the beginning of summer and I am looking for native speakers of the languages that are in beta status to test the software and check that the conjugated forms are correct.&lt;/p&gt;

&lt;p&gt;You can install mlconjug3 from &lt;a href=""https://pypi.python.org/pypi/mlconjug3""&gt;PyPi&lt;/a&gt; or &lt;a href=""https://anaconda.org/conda-forge/mlconjug3""&gt;Anaconda&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Some of the features of mlconjug3 are the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Easy to use API.&lt;/li&gt;
&lt;li&gt;Includes pre-trained language models with 99% + accuracy in predicting conjugation class of unknown verbs.&lt;/li&gt;
&lt;li&gt;Easily train new models or add new languages.&lt;/li&gt;
&lt;li&gt;Easily integrate MLConjug in your own projects.&lt;/li&gt;
&lt;li&gt;Can be used as a command line tool.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I invite everyone to try it out and if you are a native speaker of Catalan,  Valencian , Basque, Czech or Polish and are willing to beta-test the software, please pm me, you are would be greatly appreciated, and it will make mlconjug3 more versatile and therefore more useful.&lt;/p&gt;

&lt;p&gt;Thanks Everyone,&lt;/p&gt;

&lt;p&gt;Peace,&lt;/p&gt;

&lt;p&gt;SekouDiaoNlp&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mwsymt,True,,BlackPythonGuru,,19,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mwsymt/introducing_mlconjug3_a_python_library_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mwsymt/introducing_mlconjug3_a_python_library_to/,30199,1619176597.0,2,,False,,,,,,2603
1303,,LanguageTechnology,"Hello guys, I am living in korea and majored arabic in HUFS(Hankook university of foreign studies).

I got interest on singularity concept since 2013, and I got into this field

from learning machine learning and big data in the academy funded by government.

&amp;#x200B;

Now I am preparing to go artificial intelligence in graduate school, which is master degree

but there are not that many information in korea that which part; computer vision, natural language processing, have more good prospect.

&amp;#x200B;

I was actually thinking about studying nlp, because I majored language in university; english and arabic, and korean speaker, so I applied to the program that only teach nlp for 6 months.

&amp;#x200B;

But I saw one opinion in the forum that nlp have no future after gpt3, and if gpt4 comes out,

there will be no future for nlp.

&amp;#x200B;

So I felt that I have to consider which part will survive for long.

&amp;#x200B;

One of the member in A.I group recommended me that Medical image analysis is hot trend now,

so it is better to study computer vision for the future.

&amp;#x200B;

From now, I have to decide which part I should study more deeply in order to appeal graduate school that I had interest on specific field.

&amp;#x200B;

Briefly, the question is two.

&amp;#x200B;

1.Which part of A.I will survive long? CV or NLP? And which part is more prospective regarding

job opportunity and studying ph.D?

&amp;#x200B;

2.I applied to two kind of government program, first one is teaching general artificial intelligence

including brief lesson of CV and NLP. Second one is the program that only teaches NLP. I am 

wondering which program is better for me entering A.I field.

My second question will be depend on the first question, because if the future of NLP is dark,

I will apply to general A.I program and then study medical image analysis and contact the graduate school in CV lab.

&amp;#x200B;

thank you for reading all of my stories and questions.",t2_bpsj0h63,False,,0,False,Which domain in master degree to choose,[],r/LanguageTechnology,False,6,,0,,False,t3_mwwkk0,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1619217103.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello guys, I am living in korea and majored arabic in HUFS(Hankook university of foreign studies).&lt;/p&gt;

&lt;p&gt;I got interest on singularity concept since 2013, and I got into this field&lt;/p&gt;

&lt;p&gt;from learning machine learning and big data in the academy funded by government.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Now I am preparing to go artificial intelligence in graduate school, which is master degree&lt;/p&gt;

&lt;p&gt;but there are not that many information in korea that which part; computer vision, natural language processing, have more good prospect.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I was actually thinking about studying nlp, because I majored language in university; english and arabic, and korean speaker, so I applied to the program that only teach nlp for 6 months.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;But I saw one opinion in the forum that nlp have no future after gpt3, and if gpt4 comes out,&lt;/p&gt;

&lt;p&gt;there will be no future for nlp.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So I felt that I have to consider which part will survive for long.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;One of the member in A.I group recommended me that Medical image analysis is hot trend now,&lt;/p&gt;

&lt;p&gt;so it is better to study computer vision for the future.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;From now, I have to decide which part I should study more deeply in order to appeal graduate school that I had interest on specific field.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Briefly, the question is two.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;1.Which part of A.I will survive long? CV or NLP? And which part is more prospective regarding&lt;/p&gt;

&lt;p&gt;job opportunity and studying ph.D?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;2.I applied to two kind of government program, first one is teaching general artificial intelligence&lt;/p&gt;

&lt;p&gt;including brief lesson of CV and NLP. Second one is the program that only teaches NLP. I am &lt;/p&gt;

&lt;p&gt;wondering which program is better for me entering A.I field.&lt;/p&gt;

&lt;p&gt;My second question will be depend on the first question, because if the future of NLP is dark,&lt;/p&gt;

&lt;p&gt;I will apply to general A.I program and then study medical image analysis and contact the graduate school in CV lab.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;thank you for reading all of my stories and questions.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mwwkk0,True,,Korean_Arabic_AI,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mwwkk0/which_domain_in_master_degree_to_choose/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mwwkk0/which_domain_in_master_degree_to_choose/,30199,1619188303.0,0,,False,,,,,,1998
1304,,LanguageTechnology,"Hi everyone, been a lurker here for a while. I'd like some advice on an NLP task I'm working on.
 I have a list of paragraphs, and a list of possible key words/phrases to choose from. I would like to assign the most probable keyword from the list for each paragraph, based on semantic meaning. 
Currently I am just computing an embedding for the keyword and the paragraph and comparing the two, but the results aren't great. What would be the best way to do this? Is there some preprocessing that would improve results? Thanks in advance",t2_4b0mttq8,False,,0,False,Best way to choose topic keyword for text?,[],r/LanguageTechnology,False,6,,0,,False,t3_mx1um3,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619231446.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, been a lurker here for a while. I&amp;#39;d like some advice on an NLP task I&amp;#39;m working on.
 I have a list of paragraphs, and a list of possible key words/phrases to choose from. I would like to assign the most probable keyword from the list for each paragraph, based on semantic meaning. 
Currently I am just computing an embedding for the keyword and the paragraph and comparing the two, but the results aren&amp;#39;t great. What would be the best way to do this? Is there some preprocessing that would improve results? Thanks in advance&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mx1um3,True,,anihm136,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mx1um3/best_way_to_choose_topic_keyword_for_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mx1um3/best_way_to_choose_topic_keyword_for_text/,30199,1619202646.0,0,,False,,,,,,537
1305,,LanguageTechnology,"**\*\*\* CFP2  MESINESP2 track: Medical Semantic Indexing (BioASQ – CLEF 2021) \*\*\***

[https://temu.bsc.es/mesinesp2/](https://temu.bsc.es/mesinesp2/) 

**MESINESP2 Awards by BSC-Plan TL \[2,700€\]**

**Test sets and additional data are now available**

There is a pressing need for advanced multilingual semantic search strategies for health related content like literature, patents and clinical trials (cross-genre). The use of semantic search techniques in combination with structured vocabularies is critical for sophisticated searches or content analysis as needed by healthcare professionals, researchers, the pharmaceutical industry, patient groups and private citizens.

Following the impact of past BioASQ tracks for benchmarking studies (e.g. BioBERT) and organization of other initiatives like BioCreative or IberLEF, we propose three semantic labelling subtracks using the widely used DeCS vocabulary (similar to MeSH terms):

**MESINESP-L – Scientific Literature**: for automatic labelling of medical literature abstracts in Spanish (including recent COVID-19 literature).

**MESINESP-T – Clinical trials**: for automatic labelling of clinical trials summaries.

**MESINESP-P – Patents:** for automatic labelling of health-related patents in Spanish to improve patent intelligence.

**Key information**

**Web**:[ https://temu.bsc.es/mesinesp2](https://temu.bsc.es/mesinesp2) 

**Registration**:[ http://clef2021-labs-registration.dei.unipd.it/](http://clef2021-labs-registration.dei.unipd.it/) (BioASQ **Task 3 - MESINESP**)

**Data**: [https://doi.org/10.5281/zenodo.4707104](https://doi.org/10.5281/zenodo.4707104)

MESINESP2 is organized in close collaboration with widely used multilingual medical literature databases (BIREME/WHO, ISCIII/Spain), which expressed a direct need for advanced technologies to accelerate manual indexing efforts for the contents in Spanish (spoken globally by over 572 million people). They do face a challenge to keep up with the increasing number of published medical papers when using purely manual indexing.

A large manually indexed collection of training documents will be provided. These documents have already been automatically annotated (&gt; 1.5 million entity mentions) with  medical entities such as diseases, medical procedures, drugs and symptoms to facilitate the use of complementary strategies like *multi-label classification*, *multilingual transformers*, *graph matching*, *text similarity,* *advanced term matching* or *named entity recognition components*. 

Participating systems will be directly useful for ongoing medical literature indexing efforts, and thus improve competitive intelligence/prior art searches, enable complex search queries needed for evidence-based medicine, clinical decision making, or elaboration of clinical practice guidelines and serve as base for future tasks on semantic indexing of medical records or content in other languages. 

**Important dates**

* April 19: Updated Train, Validation and Test sets release
* April 19: Additional datasets release (Medical entities present in documents)
* April , 30: BioASQ9 Lab u/CLEF 2021 Registration Deadline
* May, 7: Start of the evaluation period
* May, 17: End of the evaluation period
* May,28 :Submission of Participant Papers at CLEF2021
* July, 2: Camera ready paper submission.
* Sep 21-24: CLEF 2021 Conference

**Publications and BioASQ/CLEF2021 workshop**

Teams participating in MESINESP2 will be invited to contribute a systems description paper for the BioASQ (CLEF 2021) Working Notes proceedings, and a short presentation of their approach at the BioASQ 2021 workshop.

**Main Track organizers**

* **Martin Krallinger**, Barcelona Supercomputing Center (BSC), Spain.
* **Luis Gascó**, Barcelona Supercomputing Center (BSC), Spain.
* **Anastasios Nentidis**, National Center for Scientific Research Demokritos, Greece.
* **Elena Primo-Peña,** Biblioteca Nacional de Ciencias de Salud. Instituto de Salud Carlos III, Spain.
* **Cristina Bojo Canales,** Biblioteca Nacional de Ciencias de la Salud. Instituto de Salud Carlos III, Spain.
* **George Paliouras**, National Center for Scientific Research Demokritos, Greece.
* **Anastasia Krithara**, National Center for Scientific Research Demokritos, Greece.
* **Renato Murasaki,** BIREME – Organización Panamericana de la Salud (WHO), Brasil.

**Scientific Committee**

* **Tristan Naumann,** Microsoft Research (USA)
* **Prof. Xavier Tannier,** Sorbonne Université and LIMICS (France)
* **Lucy Lu Wang,** Allen Institute for AI (AI2) (USA)
* **Prof. David Camacho,** Applied Intelligence and Data Analysis Research Group, Universidad Politécnica de Madrid (Spain)
* **Prof. Oscar Corcho,** Ontology Engineering Group, Universidad Politécnica de Madrid (Spain)
* **Parminder Batia,** Amazon Health AI (USA)
* **Prof. Irena Spasic,** School of Computer Science &amp; Informatics, co-Director of the Data Innovation Research Institute, Cardiff University (UK)
* **Jose Luis Redondo García,** Amazon Alexa, Amazon (UK)
* **Carlos Badenes-Olmedo,** Ontology Engineering Group, Universidad Politécnica de Madrid (Spain)
* **Prof. Allan Hanbury,**  E-Commerce Research Unit in the Faculty of Informatics, TU Wien (Austria)
* **Prof. Alfonso Valencia,** Barcelona Supercomputing Center (Spain)
* **Prof. Stefan J. Darmoni,** Department of Biomedical Informatics, Rouen University Hospital (France) and LIMICS (France)
* **Rezarta Islamaj,** National Center for Biotechnology Information (USA)
* **Prof. Rafael Berlanga Llavori,** Universidad Jaume I (Spain)
* **Prof. Henning Müller,** University of Applied Sciences Western Switzerland – Valais (Switzerland)
* **Prof. Gareth J.F. Jones,** School of Computing at Dublin City University (Ireland)
* **Georg Rehm,** Deutsches Forschungszentrum für Künstliche Intelligenz (Germany)
* **Petr Knoth,** Research Studios Austria Forschungsgesellschaft mbH (Austria)
* **Natalia Manola,** CEO at OpenAIRE AMKE (Greece)
* **Prof. Jesús Tramullas,** Departamento de Ciencias de la Documentación e Historia de la Ciencia, Universidad de Zaragoza (Spain)",t2_y7ny0,False,,0,False,"[Call For Participants] MESINESP2 (BioASQ / CLEF2021 shared task) on semantic indexing of heterogenous health content: literature, clinical trials and patents",[],r/LanguageTechnology,False,6,,0,,False,t3_mwqxru,False,dark,0.8,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1619196565.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;strong&gt;*** CFP2  MESINESP2 track: Medical Semantic Indexing (BioASQ – CLEF 2021) **\&lt;/strong&gt;*&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://temu.bsc.es/mesinesp2/""&gt;https://temu.bsc.es/mesinesp2/&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MESINESP2 Awards by BSC-Plan TL [2,700€]&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Test sets and additional data are now available&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There is a pressing need for advanced multilingual semantic search strategies for health related content like literature, patents and clinical trials (cross-genre). The use of semantic search techniques in combination with structured vocabularies is critical for sophisticated searches or content analysis as needed by healthcare professionals, researchers, the pharmaceutical industry, patient groups and private citizens.&lt;/p&gt;

&lt;p&gt;Following the impact of past BioASQ tracks for benchmarking studies (e.g. BioBERT) and organization of other initiatives like BioCreative or IberLEF, we propose three semantic labelling subtracks using the widely used DeCS vocabulary (similar to MeSH terms):&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MESINESP-L – Scientific Literature&lt;/strong&gt;: for automatic labelling of medical literature abstracts in Spanish (including recent COVID-19 literature).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MESINESP-T – Clinical trials&lt;/strong&gt;: for automatic labelling of clinical trials summaries.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MESINESP-P – Patents:&lt;/strong&gt; for automatic labelling of health-related patents in Spanish to improve patent intelligence.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Key information&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Web&lt;/strong&gt;:&lt;a href=""https://temu.bsc.es/mesinesp2""&gt; https://temu.bsc.es/mesinesp2&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Registration&lt;/strong&gt;:&lt;a href=""http://clef2021-labs-registration.dei.unipd.it/""&gt; http://clef2021-labs-registration.dei.unipd.it/&lt;/a&gt; (BioASQ &lt;strong&gt;Task 3 - MESINESP&lt;/strong&gt;)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;: &lt;a href=""https://doi.org/10.5281/zenodo.4707104""&gt;https://doi.org/10.5281/zenodo.4707104&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;MESINESP2 is organized in close collaboration with widely used multilingual medical literature databases (BIREME/WHO, ISCIII/Spain), which expressed a direct need for advanced technologies to accelerate manual indexing efforts for the contents in Spanish (spoken globally by over 572 million people). They do face a challenge to keep up with the increasing number of published medical papers when using purely manual indexing.&lt;/p&gt;

&lt;p&gt;A large manually indexed collection of training documents will be provided. These documents have already been automatically annotated (&amp;gt; 1.5 million entity mentions) with  medical entities such as diseases, medical procedures, drugs and symptoms to facilitate the use of complementary strategies like &lt;em&gt;multi-label classification&lt;/em&gt;, &lt;em&gt;multilingual transformers&lt;/em&gt;, &lt;em&gt;graph matching&lt;/em&gt;, &lt;em&gt;text similarity,&lt;/em&gt; &lt;em&gt;advanced term matching&lt;/em&gt; or &lt;em&gt;named entity recognition components&lt;/em&gt;. &lt;/p&gt;

&lt;p&gt;Participating systems will be directly useful for ongoing medical literature indexing efforts, and thus improve competitive intelligence/prior art searches, enable complex search queries needed for evidence-based medicine, clinical decision making, or elaboration of clinical practice guidelines and serve as base for future tasks on semantic indexing of medical records or content in other languages. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Important dates&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;April 19: Updated Train, Validation and Test sets release&lt;/li&gt;
&lt;li&gt;April 19: Additional datasets release (Medical entities present in documents)&lt;/li&gt;
&lt;li&gt;April , 30: BioASQ9 Lab &lt;a href=""/u/CLEF""&gt;u/CLEF&lt;/a&gt; 2021 Registration Deadline&lt;/li&gt;
&lt;li&gt;May, 7: Start of the evaluation period&lt;/li&gt;
&lt;li&gt;May, 17: End of the evaluation period&lt;/li&gt;
&lt;li&gt;May,28 :Submission of Participant Papers at CLEF2021&lt;/li&gt;
&lt;li&gt;July, 2: Camera ready paper submission.&lt;/li&gt;
&lt;li&gt;Sep 21-24: CLEF 2021 Conference&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Publications and BioASQ/CLEF2021 workshop&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Teams participating in MESINESP2 will be invited to contribute a systems description paper for the BioASQ (CLEF 2021) Working Notes proceedings, and a short presentation of their approach at the BioASQ 2021 workshop.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Main Track organizers&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Martin Krallinger&lt;/strong&gt;, Barcelona Supercomputing Center (BSC), Spain.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Luis Gascó&lt;/strong&gt;, Barcelona Supercomputing Center (BSC), Spain.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Anastasios Nentidis&lt;/strong&gt;, National Center for Scientific Research Demokritos, Greece.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Elena Primo-Peña,&lt;/strong&gt; Biblioteca Nacional de Ciencias de Salud. Instituto de Salud Carlos III, Spain.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cristina Bojo Canales,&lt;/strong&gt; Biblioteca Nacional de Ciencias de la Salud. Instituto de Salud Carlos III, Spain.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;George Paliouras&lt;/strong&gt;, National Center for Scientific Research Demokritos, Greece.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Anastasia Krithara&lt;/strong&gt;, National Center for Scientific Research Demokritos, Greece.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Renato Murasaki,&lt;/strong&gt; BIREME – Organización Panamericana de la Salud (WHO), Brasil.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Scientific Committee&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tristan Naumann,&lt;/strong&gt; Microsoft Research (USA)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Xavier Tannier,&lt;/strong&gt; Sorbonne Université and LIMICS (France)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lucy Lu Wang,&lt;/strong&gt; Allen Institute for AI (AI2) (USA)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. David Camacho,&lt;/strong&gt; Applied Intelligence and Data Analysis Research Group, Universidad Politécnica de Madrid (Spain)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Oscar Corcho,&lt;/strong&gt; Ontology Engineering Group, Universidad Politécnica de Madrid (Spain)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parminder Batia,&lt;/strong&gt; Amazon Health AI (USA)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Irena Spasic,&lt;/strong&gt; School of Computer Science &amp;amp; Informatics, co-Director of the Data Innovation Research Institute, Cardiff University (UK)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jose Luis Redondo García,&lt;/strong&gt; Amazon Alexa, Amazon (UK)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Carlos Badenes-Olmedo,&lt;/strong&gt; Ontology Engineering Group, Universidad Politécnica de Madrid (Spain)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Allan Hanbury,&lt;/strong&gt;  E-Commerce Research Unit in the Faculty of Informatics, TU Wien (Austria)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Alfonso Valencia,&lt;/strong&gt; Barcelona Supercomputing Center (Spain)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Stefan J. Darmoni,&lt;/strong&gt; Department of Biomedical Informatics, Rouen University Hospital (France) and LIMICS (France)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rezarta Islamaj,&lt;/strong&gt; National Center for Biotechnology Information (USA)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Rafael Berlanga Llavori,&lt;/strong&gt; Universidad Jaume I (Spain)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Henning Müller,&lt;/strong&gt; University of Applied Sciences Western Switzerland – Valais (Switzerland)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Gareth J.F. Jones,&lt;/strong&gt; School of Computing at Dublin City University (Ireland)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Georg Rehm,&lt;/strong&gt; Deutsches Forschungszentrum für Künstliche Intelligenz (Germany)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Petr Knoth,&lt;/strong&gt; Research Studios Austria Forschungsgesellschaft mbH (Austria)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Natalia Manola,&lt;/strong&gt; CEO at OpenAIRE AMKE (Greece)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Jesús Tramullas,&lt;/strong&gt; Departamento de Ciencias de la Documentación e Historia de la Ciencia, Universidad de Zaragoza (Spain)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mwqxru,True,,luisgasco,,0,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/mwqxru/call_for_participants_mesinesp2_bioasq_clef2021/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mwqxru/call_for_participants_mesinesp2_bioasq_clef2021/,30199,1619167765.0,0,,False,,,,,,6098
1306,,LanguageTechnology,"I didn't build it. But I found it interesting. What could be other alternative approach to solve this problem?

[Research Paper](https://publications.waset.org/10011676/ai-tutor-a-computer-science-domain-knowledge-graph-based-qa-system-on-jade-platform)",t2_auwgbh53,False,,0,False,AI Tutor: A Computer Science Domain Knowledge Graph-Based QA System,[],r/LanguageTechnology,False,6,,0,,False,t3_mwakja,False,dark,0.87,,public,11,0,{},,False,[],,False,False,,{},,False,11,,False,True,,False,,[],{},,True,,1619142933.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I didn&amp;#39;t build it. But I found it interesting. What could be other alternative approach to solve this problem?&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://publications.waset.org/10011676/ai-tutor-a-computer-science-domain-knowledge-graph-based-qa-system-on-jade-platform""&gt;Research Paper&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mwakja,True,,opensourcecolumbus,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mwakja/ai_tutor_a_computer_science_domain_knowledge/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mwakja/ai_tutor_a_computer_science_domain_knowledge/,30199,1619114133.0,0,,False,,,,,,253
1307,,LanguageTechnology,"I was reading reviews for a food outlet and saw tags like ""wrap"" ""hummus"" ""chicken"" ""lady"" ""platter"" ""sauce""",t2_5r4mo7fh,False,,0,False,Any idea what technology google map uses to tag reviews into categories,[],r/LanguageTechnology,False,6,,0,,False,t3_mw41p3,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1619124744.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was reading reviews for a food outlet and saw tags like &amp;quot;wrap&amp;quot; &amp;quot;hummus&amp;quot; &amp;quot;chicken&amp;quot; &amp;quot;lady&amp;quot; &amp;quot;platter&amp;quot; &amp;quot;sauce&amp;quot;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mw41p3,True,,Epiphany925,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mw41p3/any_idea_what_technology_google_map_uses_to_tag/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mw41p3/any_idea_what_technology_google_map_uses_to_tag/,30199,1619095944.0,0,,False,,,,,,108
1308,,LanguageTechnology,"I am not sure if this question fit here, but I am working with a regression task on a songs dataset,  one of the variables is the name, I am trying to use it to get a better prediction.

What I did:
I use the python library langdetec to get the language of the name, transform this categorical variable to numerical, and it work good, but I want to improve it.

- Langdetec is not too good, I saw Spanish songs catalogued as italian, but others better libraries takes a lot of time to evaluate all the dataset (this dataset has 130000 rows).
- My transformation from categorical to numerical is basic (number=1, other=2, af=3, ..., en=13, etc).
- I think using a word embedded would be too much for this kind of problem, there is other 17 variables in the data set and the results are good, I just want to take advantage of the name too.

Does anybody recommend me something?
It would be great if anybody recommend me a paper to base on because this task is for an assignment.

Also, I am thinking in a future research about best way to have language categorical variable or language prediction based on frequency.

Thanks in advance for any comments.",t2_662s6zz6,False,,0,False,Recommendation about regression + language,[],r/LanguageTechnology,False,6,,0,,False,t3_mwdzeq,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1619152117.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am not sure if this question fit here, but I am working with a regression task on a songs dataset,  one of the variables is the name, I am trying to use it to get a better prediction.&lt;/p&gt;

&lt;p&gt;What I did:
I use the python library langdetec to get the language of the name, transform this categorical variable to numerical, and it work good, but I want to improve it.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Langdetec is not too good, I saw Spanish songs catalogued as italian, but others better libraries takes a lot of time to evaluate all the dataset (this dataset has 130000 rows).&lt;/li&gt;
&lt;li&gt;My transformation from categorical to numerical is basic (number=1, other=2, af=3, ..., en=13, etc).&lt;/li&gt;
&lt;li&gt;I think using a word embedded would be too much for this kind of problem, there is other 17 variables in the data set and the results are good, I just want to take advantage of the name too.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Does anybody recommend me something?
It would be great if anybody recommend me a paper to base on because this task is for an assignment.&lt;/p&gt;

&lt;p&gt;Also, I am thinking in a future research about best way to have language categorical variable or language prediction based on frequency.&lt;/p&gt;

&lt;p&gt;Thanks in advance for any comments.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mwdzeq,True,,alandragonrojo,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mwdzeq/recommendation_about_regression_language/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mwdzeq/recommendation_about_regression_language/,30199,1619123317.0,0,,False,,,,,,1151
1309,,LanguageTechnology,"If anyone is teaching NLP this summer, feel free to use any of these free assignments from OpenClass: [https://openclass.ai/catalog/nlp](https://openclass.ai/catalog/nlp)

These assignments leverage learning principles proven to optimize knowledge retention, and identify &amp; bridge knowledge gaps to personalize the experience for learners. The idea is to provide a low-stakes environment for learners to master fundamental concepts at their own pace.

Our mission with this project is to improve &amp; democratize education. We're looking to grow our community of NLP educators, so if you're interested in contributing, feel free to join on. (Note: you can also keep your assignments private.)",t2_2uzfmau,False,,0,False,Free NLP assignments,[],r/LanguageTechnology,False,6,,0,,False,t3_mvjzhp,False,dark,0.99,,public,48,0,{},,False,[],,False,False,,{},,False,48,,False,False,,False,,[],{},,True,,1619052836.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If anyone is teaching NLP this summer, feel free to use any of these free assignments from OpenClass: &lt;a href=""https://openclass.ai/catalog/nlp""&gt;https://openclass.ai/catalog/nlp&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;These assignments leverage learning principles proven to optimize knowledge retention, and identify &amp;amp; bridge knowledge gaps to personalize the experience for learners. The idea is to provide a low-stakes environment for learners to master fundamental concepts at their own pace.&lt;/p&gt;

&lt;p&gt;Our mission with this project is to improve &amp;amp; democratize education. We&amp;#39;re looking to grow our community of NLP educators, so if you&amp;#39;re interested in contributing, feel free to join on. (Note: you can also keep your assignments private.)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mvjzhp,True,,galalalal,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mvjzhp/free_nlp_assignments/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mvjzhp/free_nlp_assignments/,30199,1619024036.0,0,,False,,,,,,697
1310,,LanguageTechnology,"I am trying to do multi classification for sentences.

What are the best ways to make sure my clusters are homogenous?

(I am having a coverage of aprox 80%))

Edit; More context as requested;

I am working on clustering methods for textual data (sentences). Implemented an unsupervised clustering method . When I go through the output, it makes sense. I went through literatures to see what metrics would to tell us ""how good the clusters are"" and got confused. This will help me compare my methods to other methods out there and maybe tweak my method to perform better. I would like to know from the fellow researchers if there are methods which worked best for you which :  
 1. gives a score on homogeneity of clusters  
 2. gives a score on what's the optimal inter-cluster distance.  
 3. gives significance of a cluster  
 4. gives a number on ""optimal number of clusters""",t2_b7kbdmn4,False,,0,False,Measuring accuracy of my clusters,[],r/LanguageTechnology,False,6,,0,,False,t3_mw4a6p,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,1619150550.0,,[],{},,True,,1619125494.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to do multi classification for sentences.&lt;/p&gt;

&lt;p&gt;What are the best ways to make sure my clusters are homogenous?&lt;/p&gt;

&lt;p&gt;(I am having a coverage of aprox 80%))&lt;/p&gt;

&lt;p&gt;Edit; More context as requested;&lt;/p&gt;

&lt;p&gt;I am working on clustering methods for textual data (sentences). Implemented an unsupervised clustering method . When I go through the output, it makes sense. I went through literatures to see what metrics would to tell us &amp;quot;how good the clusters are&amp;quot; and got confused. This will help me compare my methods to other methods out there and maybe tweak my method to perform better. I would like to know from the fellow researchers if there are methods which worked best for you which :&lt;br/&gt;
 1. gives a score on homogeneity of clusters&lt;br/&gt;
 2. gives a score on what&amp;#39;s the optimal inter-cluster distance.&lt;br/&gt;
 3. gives significance of a cluster&lt;br/&gt;
 4. gives a number on &amp;quot;optimal number of clusters&amp;quot;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mw4a6p,True,,TadpoleAware1774,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mw4a6p/measuring_accuracy_of_my_clusters/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mw4a6p/measuring_accuracy_of_my_clusters/,30199,1619096694.0,0,,False,,,,,,879
1311,,LanguageTechnology,,t2_b3tr954r,False,,0,False,How to make a multilingual translator like Google Translate,[],r/LanguageTechnology,False,6,,0,,False,t3_mvlh37,False,dark,1.0,,public,16,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/HuZq5KkLx8Q?list=PL_49VD9KwQ_ObGMW5g9hMOLnDY01NHv91"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': ""Let's Recreate Google Translate! | Neural Machine Translation"", 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/HuZq5KkLx8Q?list=PL_49VD9KwQ_ObGMW5g9hMOLnDY01NHv91"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Slick Blue', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/HuZq5KkLx8Q/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC66Ggxy8MHX9DCDohdRYDTA'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/HuZq5KkLx8Q?list=PL_49VD9KwQ_ObGMW5g9hMOLnDY01NHv91"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mvlh37', 'height': 200}",,False,16,,False,False,,False,,[],{},,False,,1619056919.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mvlh37,True,,SlickBlueML,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mvlh37/how_to_make_a_multilingual_translator_like_google/,all_ads,False,https://www.youtube.com/watch?v=HuZq5KkLx8Q&amp;list=PL_49VD9KwQ_ObGMW5g9hMOLnDY01NHv91&amp;index=2&amp;t=1s,30199,1619028119.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': ""Let's Recreate Google Translate! | Neural Machine Translation"", 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/HuZq5KkLx8Q?list=PL_49VD9KwQ_ObGMW5g9hMOLnDY01NHv91"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Slick Blue', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/HuZq5KkLx8Q/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC66Ggxy8MHX9DCDohdRYDTA'}}",False,https://www.youtube.com/watch?v=HuZq5KkLx8Q&amp;list=PL_49VD9KwQ_ObGMW5g9hMOLnDY01NHv91&amp;index=2&amp;t=1s,,,,,0
1312,,LanguageTechnology,"My dataset looks like following:

    smile        7957
    kind         3399
    angry        3288
    surprised    1702
    sad          1124

My goal is to classify an emotion using BERT.

and I decided to use SMOTE to balance it back

    train['numeric'] = train['emotions'].replace(to_replace=['smile', 'kind','angry','surprised','sad'], value=[0,1,2,3,4])
    
    smote = SMOTE()
    tv = TfidfVectorizer(stop_words=None, max_features=100000)
    tfidf = tv.fit_transform(train['content'].values.astype('U'))
    X_SMOTE, y_SMOTE = smote.fit_resample(tfidf, train['numeric'])
    

Should I even worry about balancing this dataset when using the BERT? or it is handled by BERT already?",t2_6c0lef9b,False,,0,False,Do we need to balance a dataset when using BERT,[],r/LanguageTechnology,False,6,,0,,False,t3_mvmujf,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1619060656.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My dataset looks like following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;smile        7957
kind         3399
angry        3288
surprised    1702
sad          1124
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My goal is to classify an emotion using BERT.&lt;/p&gt;

&lt;p&gt;and I decided to use SMOTE to balance it back&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train[&amp;#39;numeric&amp;#39;] = train[&amp;#39;emotions&amp;#39;].replace(to_replace=[&amp;#39;smile&amp;#39;, &amp;#39;kind&amp;#39;,&amp;#39;angry&amp;#39;,&amp;#39;surprised&amp;#39;,&amp;#39;sad&amp;#39;], value=[0,1,2,3,4])

smote = SMOTE()
tv = TfidfVectorizer(stop_words=None, max_features=100000)
tfidf = tv.fit_transform(train[&amp;#39;content&amp;#39;].values.astype(&amp;#39;U&amp;#39;))
X_SMOTE, y_SMOTE = smote.fit_resample(tfidf, train[&amp;#39;numeric&amp;#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Should I even worry about balancing this dataset when using the BERT? or it is handled by BERT already?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mvmujf,True,,strangeguy111,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mvmujf/do_we_need_to_balance_a_dataset_when_using_bert/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mvmujf/do_we_need_to_balance_a_dataset_when_using_bert/,30199,1619031856.0,0,,False,,,,,,693
1313,,LanguageTechnology,,t2_9lo3x1bz,False,,1,False,How do you guys find/ keep up to date with the latest NLP papers?,[],r/LanguageTechnology,False,6,,0,,False,t3_mv8vn3,False,dark,1.0,,public,37,1,{},,False,[],,False,False,,{},,False,37,,False,False,,False,,[],{'gid_2': 1},,True,,1619011104.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 500, 'id': 'gid_2', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 100, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/gold_512.png', 'days_of_premium': 7, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/gold_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Gold', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/gold_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/gold_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mv8vn3,True,,wherll,,14,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mv8vn3/how_do_you_guys_find_keep_up_to_date_with_the/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mv8vn3/how_do_you_guys_find_keep_up_to_date_with_the/,30199,1618982304.0,0,,False,,,,,,0
1314,,LanguageTechnology,,t2_l6ugg,False,,0,False,"The Conversational AI &amp; NLP Summit is hosting speakers from DeepMind, Facebook, Nestle, McDonald's and more next week. See the full list of talks, networking opps and speakers below!",[],r/LanguageTechnology,False,6,,0,,False,t3_mvdqql,False,dark,0.92,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,False,,1619033792.0,text,6,,,text,re-work.co,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mvdqql,True,,nikitaljohnson,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mvdqql/the_conversational_ai_nlp_summit_is_hosting/,all_ads,False,https://www.re-work.co/events/conversational-ai-nlp-summit-2021?utm_source=LK&amp;utm_medium=Promo&amp;utm_campaign=LK_Conv_NLP_LC,30199,1619004992.0,0,,False,https://www.re-work.co/events/conversational-ai-nlp-summit-2021?utm_source=LK&amp;utm_medium=Promo&amp;utm_campaign=LK_Conv_NLP_LC,,,,,0
1315,,LanguageTechnology,"Hello

I am a student and I am working on emotion analysis with deep learning. My supervisor asked me to extract semantic features from the text ( convert raw data into useful semantic features ) before using deep learning. But I am confused when I read research about text classification with deep learning. DL does not need feature extraction only using different types of word embedding to convert data, I need to clarify this. Is it possible to extract features before applying Deep Learning.? Is there any research that can help me with that? Any suggestions tools or techniques to extract semantic features from the text.",t2_tx187ox,False,,0,False,Using Deep Learning Based On Semantic Features for Emotion Classification in Tweets,[],r/LanguageTechnology,False,6,,0,,False,t3_mvlr8c,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619057685.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello&lt;/p&gt;

&lt;p&gt;I am a student and I am working on emotion analysis with deep learning. My supervisor asked me to extract semantic features from the text ( convert raw data into useful semantic features ) before using deep learning. But I am confused when I read research about text classification with deep learning. DL does not need feature extraction only using different types of word embedding to convert data, I need to clarify this. Is it possible to extract features before applying Deep Learning.? Is there any research that can help me with that? Any suggestions tools or techniques to extract semantic features from the text.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mvlr8c,True,,Tahani_cs,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mvlr8c/using_deep_learning_based_on_semantic_features/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mvlr8c/using_deep_learning_based_on_semantic_features/,30199,1619028885.0,0,,False,,,,,,627
1316,,LanguageTechnology,"The [huggingface documentation](https://huggingface.co/transformers/v2.5.0/examples.html) states:

&gt;GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss.

Is this a common distinction you'd find in the NLP literature? Is it a sensible distinction in your opinion? While I totally agree with the first category, I don't understand why you would call BERT &amp; co. ""masked language models"", since causal language models do the actual masking in next token prediction.",t2_113mvs,False,,0,False,Is causal language modeling (CLM) vs masked language modeling (MLM) a common distinction in NLP research?,[],r/LanguageTechnology,False,6,,0,,False,t3_mvepoi,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1619037275.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The &lt;a href=""https://huggingface.co/transformers/v2.5.0/examples.html""&gt;huggingface documentation&lt;/a&gt; states:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Is this a common distinction you&amp;#39;d find in the NLP literature? Is it a sensible distinction in your opinion? While I totally agree with the first category, I don&amp;#39;t understand why you would call BERT &amp;amp; co. &amp;quot;masked language models&amp;quot;, since causal language models do the actual masking in next token prediction.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mvepoi,True,,EntropyGoAway,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mvepoi/is_causal_language_modeling_clm_vs_masked/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mvepoi/is_causal_language_modeling_clm_vs_masked/,30199,1619008475.0,0,,False,,,,,,571
1317,,LanguageTechnology,"Hey everyone! Made a post and started delving into this space a few months ago when i decided to take this on for my Final Year Project. Find my post here: [https://www.reddit.com/r/LanguageTechnology/comments/jvo6ec/what\_is\_the\_technology\_behind\_automated\_insights/?utm\_source=share&amp;utm\_medium=web2x&amp;context=3](https://www.reddit.com/r/LanguageTechnology/comments/jvo6ec/what_is_the_technology_behind_automated_insights/?utm_source=share&amp;utm_medium=web2x&amp;context=3)

&amp;#x200B;

I've been building and simultaneously reading papers and enhancing my designs. Wanted to ask if anyone else is or has been working on this type of projects. Would love to know if there is a community in this space or any open source projects happening because from what i see, people working on this would be mainly also working in the companies that are building these systems :/ Would love to hear what challenges have been faced while building these systems. Coding is hard enough, creating a system that generates text that makes grammatical sense and fluent is SUPER difficult.",t2_1lcmbnjy,False,,0,False,Natural Language Generation Systems - are there any communities or open source projects around here?,[],r/LanguageTechnology,False,6,,0,,False,t3_mv94ps,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1619012264.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone! Made a post and started delving into this space a few months ago when i decided to take this on for my Final Year Project. Find my post here: &lt;a href=""https://www.reddit.com/r/LanguageTechnology/comments/jvo6ec/what_is_the_technology_behind_automated_insights/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3""&gt;https://www.reddit.com/r/LanguageTechnology/comments/jvo6ec/what_is_the_technology_behind_automated_insights/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been building and simultaneously reading papers and enhancing my designs. Wanted to ask if anyone else is or has been working on this type of projects. Would love to know if there is a community in this space or any open source projects happening because from what i see, people working on this would be mainly also working in the companies that are building these systems :/ Would love to hear what challenges have been faced while building these systems. Coding is hard enough, creating a system that generates text that makes grammatical sense and fluent is SUPER difficult.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mv94ps,True,,jffryteo,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mv94ps/natural_language_generation_systems_are_there_any/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mv94ps/natural_language_generation_systems_are_there_any/,30199,1618983464.0,0,,False,,,,,,1088
1318,,LanguageTechnology,,t2_21qaqh1p,False,,0,False,AI-Generated Blog Content with GPT-Neo (GPT-3 Alternative) - Python Web App,[],r/LanguageTechnology,False,6,,0,,False,t3_muzgu1,False,dark,0.78,,public,5,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/d_xRYyy2LFM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'AI-Generated Blog Content with GPT-Neo (GPT-3 Alternative) + Gradio | Python ML Web App', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/d_xRYyy2LFM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/d_xRYyy2LFM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/d_xRYyy2LFM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/muzgu1', 'height': 200}",,False,5,,False,False,,False,,[],{},,False,,1618978969.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,muzgu1,True,,dulldata,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/muzgu1/aigenerated_blog_content_with_gptneo_gpt3/,all_ads,False,https://youtu.be/d_xRYyy2LFM,30199,1618950169.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'AI-Generated Blog Content with GPT-Neo (GPT-3 Alternative) + Gradio | Python ML Web App', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/d_xRYyy2LFM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/d_xRYyy2LFM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,https://youtu.be/d_xRYyy2LFM,,,,,0
1319,,LanguageTechnology,"I am following this tutorial  [dp\_tutorials/Tutorial\_3\_RU\_Fine\_tuning\_BERT\_classifier.ipynb at master · deepmipt/dp\_tutorials · GitHub](https://github.com/deepmipt/dp_tutorials/blob/master/russian_tutorials/Tutorial_3_RU_Fine_tuning_BERT_classifier.ipynb)

However, I am constantly running out of memory so I decided to use Gradient Accumulation to reduce the memory size as it was suggested in my previous post. How can I add it to my model? I can’t find any tutorial that explains it well.

&amp;#x200B;

I already reduced the batch size however it did not helped that much

Thank you.",t2_6c0lef9b,False,,0,False,How to add the Gradient Accumulation to my model?,[],r/LanguageTechnology,False,6,,0,,False,t3_mv2tzf,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1618988604.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am following this tutorial  &lt;a href=""https://github.com/deepmipt/dp_tutorials/blob/master/russian_tutorials/Tutorial_3_RU_Fine_tuning_BERT_classifier.ipynb""&gt;dp_tutorials/Tutorial_3_RU_Fine_tuning_BERT_classifier.ipynb at master · deepmipt/dp_tutorials · GitHub&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;However, I am constantly running out of memory so I decided to use Gradient Accumulation to reduce the memory size as it was suggested in my previous post. How can I add it to my model? I can’t find any tutorial that explains it well.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I already reduced the batch size however it did not helped that much&lt;/p&gt;

&lt;p&gt;Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mv2tzf,True,,strangeguy111,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mv2tzf/how_to_add_the_gradient_accumulation_to_my_model/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mv2tzf/how_to_add_the_gradient_accumulation_to_my_model/,30199,1618959804.0,0,,False,,,,,,595
1320,,LanguageTechnology,"Hi everyone! I'm currently trying to develop a text classification program that detects emotions and social instances from written text of individuals diagnosed with depression. I thought it'd be a good idea to use lists of words for specific categories in order to build a vocabulary, like for social words I used ""family, friends, girlfriend"" and so on. This classification task is still simple though because it doesn't take into account context and negation (""not happy"" = unhappy). For example, the program can detect a word like ""bad"" but that word could be unrelated, like ""bad driver"". That doesn't give me any hint about the emotional state of that person but ""bad"" still gets detected as part of ""bad driver"". The downside of making a vocabulary using a list is that it can't contain complex words such as ""pissed off"" because they're two words and the text is tokenized at the beginning of the process, so they are passed in as two different words, ""piss"" and ""off"".  I'm trying to implement bigrams to add a little bit of context. I'm open to any suggestion and ready to hear from you.",t2_b31us5mn,False,,0,False,Use NLP to detect and classify emotion in text,[],r/LanguageTechnology,False,6,,0,,False,t3_mv2f32,False,dark,0.83,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1618987362.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone! I&amp;#39;m currently trying to develop a text classification program that detects emotions and social instances from written text of individuals diagnosed with depression. I thought it&amp;#39;d be a good idea to use lists of words for specific categories in order to build a vocabulary, like for social words I used &amp;quot;family, friends, girlfriend&amp;quot; and so on. This classification task is still simple though because it doesn&amp;#39;t take into account context and negation (&amp;quot;not happy&amp;quot; = unhappy). For example, the program can detect a word like &amp;quot;bad&amp;quot; but that word could be unrelated, like &amp;quot;bad driver&amp;quot;. That doesn&amp;#39;t give me any hint about the emotional state of that person but &amp;quot;bad&amp;quot; still gets detected as part of &amp;quot;bad driver&amp;quot;. The downside of making a vocabulary using a list is that it can&amp;#39;t contain complex words such as &amp;quot;pissed off&amp;quot; because they&amp;#39;re two words and the text is tokenized at the beginning of the process, so they are passed in as two different words, &amp;quot;piss&amp;quot; and &amp;quot;off&amp;quot;.  I&amp;#39;m trying to implement bigrams to add a little bit of context. I&amp;#39;m open to any suggestion and ready to hear from you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mv2f32,True,,Dr_Funkmachine,,25,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mv2f32/use_nlp_to_detect_and_classify_emotion_in_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mv2f32/use_nlp_to_detect_and_classify_emotion_in_text/,30199,1618958562.0,0,,False,,,,,,1097
1321,,LanguageTechnology,,t2_i12ut,False,,0,False,Bitextor: a tool for generating translation memories from multilingual websites,[],r/LanguageTechnology,False,6,,0,,False,t3_mur9ki,False,dark,1.0,,public,13,0,{},,False,[],,False,False,,{},,False,13,,False,False,,False,,[],{},,False,,1618956883.0,text,6,,,text,github.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mur9ki,True,,leops,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mur9ki/bitextor_a_tool_for_generating_translation/,all_ads,False,https://github.com/bitextor/bitextor,30199,1618928083.0,0,,False,https://github.com/bitextor/bitextor,,,,,0
1322,,LanguageTechnology,"I'm in the process of labeling data that I'll be using for an NLP binary classification project.  Some of my text is longer and some of it is shorter.  

If I have some text (call it two paragraphs), that exhibits traits of both classifications (label A and label not-A), is it better to break the text apart and apply the specific labels to each?  

Or is it better to keep the text together and apply just one of the labels (label A, since it does include some text that is ""A"")?  

If I keep the paragraphs together, I don't want the model to just learn that long text = A.  But if I break it apart, will the model struggle with longer text when it was only trained on single paragraphs?",t2_bkxsovae,False,,0,False,"For binary classification, is it better to train with more focused examples?",[],r/LanguageTechnology,False,6,,0,,False,t3_muzfp7,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1618978881.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m in the process of labeling data that I&amp;#39;ll be using for an NLP binary classification project.  Some of my text is longer and some of it is shorter.  &lt;/p&gt;

&lt;p&gt;If I have some text (call it two paragraphs), that exhibits traits of both classifications (label A and label not-A), is it better to break the text apart and apply the specific labels to each?  &lt;/p&gt;

&lt;p&gt;Or is it better to keep the text together and apply just one of the labels (label A, since it does include some text that is &amp;quot;A&amp;quot;)?  &lt;/p&gt;

&lt;p&gt;If I keep the paragraphs together, I don&amp;#39;t want the model to just learn that long text = A.  But if I break it apart, will the model struggle with longer text when it was only trained on single paragraphs?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,muzfp7,True,,NLMNOP,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/muzfp7/for_binary_classification_is_it_better_to_train/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/muzfp7/for_binary_classification_is_it_better_to_train/,30199,1618950081.0,0,,False,,,,,,690
1323,,LanguageTechnology,"Large-scale transformer-based language models have produced significant gains in natural language processing (NLP) in recent years. However, training such models is difficult because no single GPU has enough memory to accommodate parameter totals that have grown exponentially in recent years. Even if these parameters could be trained on a single GPU, limited computing power would result in longer training times without model parallelism. 

In a [paper](https://arxiv.org/pdf/2104.04473.pdf) by NVIDIA, Stanford University, and Microsoft Research, a research team has proposed a new parallelization schedule that improves throughput by more than 10 percent with a comparable memory footprint. The paper demonstrated that such strategies could be composed to achieve high aggregate throughput when training large models with nearly a trillion parameters. 

Summary: [https://www.marktechpost.com/2021/04/19/researchers-from-nvidia-stanford-university-and-microsoft-research-propose-efficient-trillion-parameter-language-model-training-on-gpu-clusters/](https://www.marktechpost.com/2021/04/19/researchers-from-nvidia-stanford-university-and-microsoft-research-propose-efficient-trillion-parameter-language-model-training-on-gpu-clusters/) 

Paper: https://arxiv.org/pdf/2104.04473.pdf

Github: https://github.com/nvidia/megatron-lm",t2_4wudjgid,False,,0,False,"Researchers From NVIDIA, Stanford University and Microsoft Research Propose Efficient Trillion-Parameter Language Model Training on GPU Clusters (Paper and Github link included)",[],r/LanguageTechnology,False,6,,0,,False,t3_mufmdd,False,dark,0.96,,public,23,0,{},,False,[],,False,False,,{},,False,23,,False,False,,False,,[],{},,True,,1618910929.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Large-scale transformer-based language models have produced significant gains in natural language processing (NLP) in recent years. However, training such models is difficult because no single GPU has enough memory to accommodate parameter totals that have grown exponentially in recent years. Even if these parameters could be trained on a single GPU, limited computing power would result in longer training times without model parallelism. &lt;/p&gt;

&lt;p&gt;In a &lt;a href=""https://arxiv.org/pdf/2104.04473.pdf""&gt;paper&lt;/a&gt; by NVIDIA, Stanford University, and Microsoft Research, a research team has proposed a new parallelization schedule that improves throughput by more than 10 percent with a comparable memory footprint. The paper demonstrated that such strategies could be composed to achieve high aggregate throughput when training large models with nearly a trillion parameters. &lt;/p&gt;

&lt;p&gt;Summary: &lt;a href=""https://www.marktechpost.com/2021/04/19/researchers-from-nvidia-stanford-university-and-microsoft-research-propose-efficient-trillion-parameter-language-model-training-on-gpu-clusters/""&gt;https://www.marktechpost.com/2021/04/19/researchers-from-nvidia-stanford-university-and-microsoft-research-propose-efficient-trillion-parameter-language-model-training-on-gpu-clusters/&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=""https://arxiv.org/pdf/2104.04473.pdf""&gt;https://arxiv.org/pdf/2104.04473.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Github: &lt;a href=""https://github.com/nvidia/megatron-lm""&gt;https://github.com/nvidia/megatron-lm&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mufmdd,True,,techsucker,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mufmdd/researchers_from_nvidia_stanford_university_and/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mufmdd/researchers_from_nvidia_stanford_university_and/,30199,1618882129.0,0,,False,,,,,,1333
1324,,LanguageTechnology,,t2_hkv9s,False,,0,False,Automatic Title Generation for Text with Transformer Language Model (Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_muo3fn,False,dark,1.0,,public,2,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/KhxHo_OQGtI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Automatic Title Generation for Text with Transformer Language Model (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/KhxHo_OQGtI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/KhxHo_OQGtI/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/KhxHo_OQGtI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/muo3fn', 'height': 200}",,False,2,,False,False,,False,,[],{},,False,,1618946304.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,muo3fn,True,,prakhar21,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/muo3fn/automatic_title_generation_for_text_with/,all_ads,False,https://youtu.be/KhxHo_OQGtI,30199,1618917504.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Automatic Title Generation for Text with Transformer Language Model (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/KhxHo_OQGtI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/KhxHo_OQGtI/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/KhxHo_OQGtI,,,,,0
1325,,LanguageTechnology,"Hi everyone, 

I am currently doing a project where I am supposed to be using named entity recognition with models such as biobert to process clinical data and possibly link them to ICD (International Classification of Diseases) codes however I am currently extremely lost. 

I am planning on using spacy to do the pre-processing on the data however, I am unsure as to what format biobert needs the dataset in.

Would anyone be able to point me in the right direction?",t2_4utjfbbm,False,,0,False,nlp project with biobert,[],r/LanguageTechnology,False,6,,0,,False,t3_muc80y,False,dark,0.88,,public,6,1,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{'gid_1': 1},,True,,1618899882.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;

&lt;p&gt;I am currently doing a project where I am supposed to be using named entity recognition with models such as biobert to process clinical data and possibly link them to ICD (International Classification of Diseases) codes however I am currently extremely lost. &lt;/p&gt;

&lt;p&gt;I am planning on using spacy to do the pre-processing on the data however, I am unsure as to what format biobert needs the dataset in.&lt;/p&gt;

&lt;p&gt;Would anyone be able to point me in the right direction?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,muc80y,True,,ninjamurai009,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/muc80y/nlp_project_with_biobert/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/muc80y/nlp_project_with_biobert/,30199,1618871082.0,0,,False,,,,,,468
1326,,LanguageTechnology,"Hey, people! Hope you are all well!

I've been reading some papers on NER and started to notice a development towards a more general and less strict definition of the task. Firstly, at MUC, only named mentions (proper nouns) would be considered. Then, at ACE, both nominal (common nouns) and pronominal phrases were added and also recursive mentions were considered. Further, other datasets kept exploring broader settings, like LitBank accepting personifications and recursive mentions in literary texts, HAREM enforcing ambiguity, etc.

As a part of this extension of the NER definition, do you believe that non-noun phrases can also be considered as mentions? Do you see other constructions being considered as mentions in the future?  

Also, if there are other interesting extensions you might know or believe may occur, please share!",t2_1uz63xco,False,,0,False,Named Entity Recognition and possible extensions such as Non-noun phrases,[],r/LanguageTechnology,False,6,,0,,False,t3_mud88u,False,dark,1.0,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1618902961.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, people! Hope you are all well!&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been reading some papers on NER and started to notice a development towards a more general and less strict definition of the task. Firstly, at MUC, only named mentions (proper nouns) would be considered. Then, at ACE, both nominal (common nouns) and pronominal phrases were added and also recursive mentions were considered. Further, other datasets kept exploring broader settings, like LitBank accepting personifications and recursive mentions in literary texts, HAREM enforcing ambiguity, etc.&lt;/p&gt;

&lt;p&gt;As a part of this extension of the NER definition, do you believe that non-noun phrases can also be considered as mentions? Do you see other constructions being considered as mentions in the future?  &lt;/p&gt;

&lt;p&gt;Also, if there are other interesting extensions you might know or believe may occur, please share!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mud88u,True,,pauloamed,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mud88u/named_entity_recognition_and_possible_extensions/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mud88u/named_entity_recognition_and_possible_extensions/,30199,1618874161.0,0,,False,,,,,,839
1327,,LanguageTechnology,"Hello everyone,

I am currently developing a system (for one of my thesis components) which analyses the mathematical relationships between numbers extracted from a given text and generates alternatives, unfortunately there are requirements such that the numbers can't be negative of a majority of 0.

I am currently making improvements on my current system however I think it would be wise to look at more feasible alternatives as this is for my thesis however I have not found any previous work in this niche. Does anyone have any ideas on this front please?",t2_37a99pnk,False,,0,False,NLP and Mathematical Generation?,[],r/LanguageTechnology,False,6,,0,,False,t3_mu5x5k,False,dark,0.82,,public,7,0,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1618882583.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;

&lt;p&gt;I am currently developing a system (for one of my thesis components) which analyses the mathematical relationships between numbers extracted from a given text and generates alternatives, unfortunately there are requirements such that the numbers can&amp;#39;t be negative of a majority of 0.&lt;/p&gt;

&lt;p&gt;I am currently making improvements on my current system however I think it would be wise to look at more feasible alternatives as this is for my thesis however I have not found any previous work in this niche. Does anyone have any ideas on this front please?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mu5x5k,True,,Leli1024,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mu5x5k/nlp_and_mathematical_generation/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mu5x5k/nlp_and_mathematical_generation/,30199,1618853783.0,0,,False,,,,,,560
1328,,LanguageTechnology,"Hello, 

I have a few hundred txt documents, each containing a few sentences about someone's history with substance abuse. 

Based on 2 types of substances, I am trying to go through each file and collect the frequencies of 4 entities: **status** (e.g., past, current, none), **method** (e.g., inhale, chew), **amount** (e.g., 2 packs, 3-4 glasses), and **frequency** (e.g., a day). 

Example txt file: ""He does not use tobacco. He sometimes drinks wine. He does not use drugs ever."" 

**My dilemma:** again, I need to scrape the frequencies of these entities (essentially attributes of some type of substance abuse), ***but the wording/sentence structure varies a lot*** \--&gt; the **status** entity, for example, could see 'past', 'currently', 'still', 'sometimes', 'on weekends', 'back in 1984', etc... just a whole lot of things. I think I need to employ some NLP technique to classify/annotate this stuff but I am not sure how. **Any ideas?**

Thank you so much for your consideration.",t2_2ich5u4y,False,,0,False,NLP Help | Scraping Question,[],r/LanguageTechnology,False,6,,0,,False,t3_mtvquw,False,dark,1.0,,public,16,0,{},,False,[],,False,False,,{},,False,16,,False,False,,False,,[],{},,True,,1618847724.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, &lt;/p&gt;

&lt;p&gt;I have a few hundred txt documents, each containing a few sentences about someone&amp;#39;s history with substance abuse. &lt;/p&gt;

&lt;p&gt;Based on 2 types of substances, I am trying to go through each file and collect the frequencies of 4 entities: &lt;strong&gt;status&lt;/strong&gt; (e.g., past, current, none), &lt;strong&gt;method&lt;/strong&gt; (e.g., inhale, chew), &lt;strong&gt;amount&lt;/strong&gt; (e.g., 2 packs, 3-4 glasses), and &lt;strong&gt;frequency&lt;/strong&gt; (e.g., a day). &lt;/p&gt;

&lt;p&gt;Example txt file: &amp;quot;He does not use tobacco. He sometimes drinks wine. He does not use drugs ever.&amp;quot; &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;My dilemma:&lt;/strong&gt; again, I need to scrape the frequencies of these entities (essentially attributes of some type of substance abuse), &lt;strong&gt;&lt;em&gt;but the wording/sentence structure varies a lot&lt;/em&gt;&lt;/strong&gt; --&amp;gt; the &lt;strong&gt;status&lt;/strong&gt; entity, for example, could see &amp;#39;past&amp;#39;, &amp;#39;currently&amp;#39;, &amp;#39;still&amp;#39;, &amp;#39;sometimes&amp;#39;, &amp;#39;on weekends&amp;#39;, &amp;#39;back in 1984&amp;#39;, etc... just a whole lot of things. I think I need to employ some NLP technique to classify/annotate this stuff but I am not sure how. &lt;strong&gt;Any ideas?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Thank you so much for your consideration.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mtvquw,True,,Stutoucan12,,10,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mtvquw/nlp_help_scraping_question/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mtvquw/nlp_help_scraping_question/,30199,1618818924.0,0,,False,,,,,,991
1329,,LanguageTechnology,"I would like to start a career in NLP. Before investing my time in NLP, I would like to know if the skills needed in NLP can be transferred to another fields or not. For example, if in the future I want to work in DSP( digital signal processing), or Image processing, can NLP skills applied in these fields? 


Thank you very much! 

Note: I am asking because tomorrow I have an interview for  NLP job in a startup company. My background is CS, networking and a limited knowledge in Genetic algorithms  .

Edit: language mistakes",t2_zj4u90g,False,,0,False,NLP and Skill Transfer,[],r/LanguageTechnology,False,6,,0,,False,t3_mu3v2m,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,1618850666.0,,[],{},,True,,1618877001.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I would like to start a career in NLP. Before investing my time in NLP, I would like to know if the skills needed in NLP can be transferred to another fields or not. For example, if in the future I want to work in DSP( digital signal processing), or Image processing, can NLP skills applied in these fields? &lt;/p&gt;

&lt;p&gt;Thank you very much! &lt;/p&gt;

&lt;p&gt;Note: I am asking because tomorrow I have an interview for  NLP job in a startup company. My background is CS, networking and a limited knowledge in Genetic algorithms  .&lt;/p&gt;

&lt;p&gt;Edit: language mistakes&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mu3v2m,True,,Beginner4ever,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mu3v2m/nlp_and_skill_transfer/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mu3v2m/nlp_and_skill_transfer/,30199,1618848201.0,0,,False,,,,,,529
1330,,LanguageTechnology,"Guys I am trying to develop a BERT model that basically predicts an emotion. (Emotion classifier)

I never got a chance to eventually finish my training because I every time run out of Memory Ram

I tried Google Colab (ran out of memory) then tried Kaggle Kernel (ran out of space as well), both 12 and 16 GB RAM.

I do not know what is wrong with it? how people even train this type of model?

&amp;#x200B;

    ......DATA LOADING AND PREPROCESSING...
    
    !pip install deeppavlov
    from deeppavlov.dataset_readers.basic_classification_reader import BasicClassificationDatasetReader
    data = BasicClassificationDatasetReader().read(
        data_path='./',
        train='../input/pikabucsva/train_pikabu_a.csv',
        valid=""../input/pikabucsva/validation_pikabu_a.csv"", 
        test=""../input/pikabucsva/test_pikabu_a.csv"",
        x = 'content',
        y = 'emotions'
    )
    
    #ITERATOR
    from deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator
    # initializing an iterator
    iterator = BasicClassificationDatasetIterator(data, seed=42, shuffle=True)
    
    #BERT PREPROCESSOR
    !python -m deeppavlov install squad_bert
    from deeppavlov.models.preprocessors.bert_preprocessor import BertPreprocessor
    bert_preprocessor = BertPreprocessor(vocab_file=""../input/bertmodel/vocab.txt"",
                                         do_lower_case=False,
                                         max_seq_length=64)
    
    #SIMPLE VOCABULARY
    from deeppavlov.core.data.simple_vocab import SimpleVocabulary
    vocab = SimpleVocabulary(save_path=""./binary_classes.dict"")
    
    
    #ONEHOTTER
    from deeppavlov.models.preprocessors.one_hotter import OneHotter
    one_hotter = OneHotter(depth=vocab.len, 
                           single_vector=True  # means we want to have one vector per sample
                          )
    #PROB TO LABELS
    from deeppavlov.models.classifiers.proba2labels import Proba2Labels
    prob2labels = Proba2Labels(max_proba=True)
    vocab(prob2labels([[0.6, 0.4], 
                       [0.2, 0.8],
                       [0.1, 0.9]]))
    
    
    #BERT CLASSIFIER
    from deeppavlov.models.bert.bert_classifier import BertClassifierModel
    from deeppavlov.metrics.accuracy import sets_accuracy
    
    bert_classifier = BertClassifierModel(
        n_classes=vocab.len,
        return_probas=True,
        one_hot_labels=True,
        bert_config_file=""../input/bertmodel/bert_config.json"",
        pretrained_bert=""../input/bertmodel/bert_model.ckpt"",
        save_path=""sst_bert_model/model"",
        load_path=""sst_bert_model/model"",
        keep_prob=0.5,
        learning_rate=1e-05,
        learning_rate_drop_patience=5,
        learning_rate_drop_div=2.0
        )
    
    #TRAINING
    # Method `get_instances` returns all the samples of particular data field
    x_valid, y_valid = iterator.get_instances(data_type=""valid"")
    # You need to save model only when validation score is higher than previous one.
    # This variable will contain the highest accuracy score
    best_score = 0.
    patience = 2
    impatience = 0
    
    # let's train for 3 epochs
    for ep in range(3):
      
        nbatches = 0
        for x, y in iterator.gen_batches(batch_size=256, 
                                         data_type=""train"", shuffle=True):
            x_feat = bert_preprocessor(x)
            y_onehot = one_hotter(vocab(y))
            bert_classifier.train_on_batch(x_feat, y_onehot)
            print(""Batch done\n"")
            nbatches += 1
            
            if nbatches % 1 == 0:
                # валидируемся каждые 100 батчей
                y_valid_pred = bert_classifier(bert_preprocessor(x_valid))
                score = sets_accuracy(y_valid, vocab(prob2labels(y_valid_pred)))
                print(""Batches done: {}. Valid Accuracy: {}"".format(nbatches, score))
                
        y_valid_pred = bert_classifier(bert_preprocessor(x_valid))
        score = sets_accuracy(y_valid, vocab(prob2labels(y_valid_pred)))
        print(""Epochs done: {}. Valid Accuracy: {}"".format(ep + 1, score))
        if score &gt; best_score:
            bert_classifier.save()
            print(""New best score. Saving model."")
            best_score = score    
            impatience = 0
        else:
            impatience += 1
            if impatience == patience:
                print(""Out of patience. Stop training."")
                break",t2_6c0lef9b,False,,0,False,WHY DO I KEEP RUNNING OUT OF MEMORY (RAM),[],r/LanguageTechnology,False,6,,0,,False,t3_muf1be,False,dark,0.25,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1618908914.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Guys I am trying to develop a BERT model that basically predicts an emotion. (Emotion classifier)&lt;/p&gt;

&lt;p&gt;I never got a chance to eventually finish my training because I every time run out of Memory Ram&lt;/p&gt;

&lt;p&gt;I tried Google Colab (ran out of memory) then tried Kaggle Kernel (ran out of space as well), both 12 and 16 GB RAM.&lt;/p&gt;

&lt;p&gt;I do not know what is wrong with it? how people even train this type of model?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;......DATA LOADING AND PREPROCESSING...

!pip install deeppavlov
from deeppavlov.dataset_readers.basic_classification_reader import BasicClassificationDatasetReader
data = BasicClassificationDatasetReader().read(
    data_path=&amp;#39;./&amp;#39;,
    train=&amp;#39;../input/pikabucsva/train_pikabu_a.csv&amp;#39;,
    valid=&amp;quot;../input/pikabucsva/validation_pikabu_a.csv&amp;quot;, 
    test=&amp;quot;../input/pikabucsva/test_pikabu_a.csv&amp;quot;,
    x = &amp;#39;content&amp;#39;,
    y = &amp;#39;emotions&amp;#39;
)

#ITERATOR
from deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator
# initializing an iterator
iterator = BasicClassificationDatasetIterator(data, seed=42, shuffle=True)

#BERT PREPROCESSOR
!python -m deeppavlov install squad_bert
from deeppavlov.models.preprocessors.bert_preprocessor import BertPreprocessor
bert_preprocessor = BertPreprocessor(vocab_file=&amp;quot;../input/bertmodel/vocab.txt&amp;quot;,
                                     do_lower_case=False,
                                     max_seq_length=64)

#SIMPLE VOCABULARY
from deeppavlov.core.data.simple_vocab import SimpleVocabulary
vocab = SimpleVocabulary(save_path=&amp;quot;./binary_classes.dict&amp;quot;)


#ONEHOTTER
from deeppavlov.models.preprocessors.one_hotter import OneHotter
one_hotter = OneHotter(depth=vocab.len, 
                       single_vector=True  # means we want to have one vector per sample
                      )
#PROB TO LABELS
from deeppavlov.models.classifiers.proba2labels import Proba2Labels
prob2labels = Proba2Labels(max_proba=True)
vocab(prob2labels([[0.6, 0.4], 
                   [0.2, 0.8],
                   [0.1, 0.9]]))


#BERT CLASSIFIER
from deeppavlov.models.bert.bert_classifier import BertClassifierModel
from deeppavlov.metrics.accuracy import sets_accuracy

bert_classifier = BertClassifierModel(
    n_classes=vocab.len,
    return_probas=True,
    one_hot_labels=True,
    bert_config_file=&amp;quot;../input/bertmodel/bert_config.json&amp;quot;,
    pretrained_bert=&amp;quot;../input/bertmodel/bert_model.ckpt&amp;quot;,
    save_path=&amp;quot;sst_bert_model/model&amp;quot;,
    load_path=&amp;quot;sst_bert_model/model&amp;quot;,
    keep_prob=0.5,
    learning_rate=1e-05,
    learning_rate_drop_patience=5,
    learning_rate_drop_div=2.0
    )

#TRAINING
# Method `get_instances` returns all the samples of particular data field
x_valid, y_valid = iterator.get_instances(data_type=&amp;quot;valid&amp;quot;)
# You need to save model only when validation score is higher than previous one.
# This variable will contain the highest accuracy score
best_score = 0.
patience = 2
impatience = 0

# let&amp;#39;s train for 3 epochs
for ep in range(3):

    nbatches = 0
    for x, y in iterator.gen_batches(batch_size=256, 
                                     data_type=&amp;quot;train&amp;quot;, shuffle=True):
        x_feat = bert_preprocessor(x)
        y_onehot = one_hotter(vocab(y))
        bert_classifier.train_on_batch(x_feat, y_onehot)
        print(&amp;quot;Batch done\n&amp;quot;)
        nbatches += 1

        if nbatches % 1 == 0:
            # валидируемся каждые 100 батчей
            y_valid_pred = bert_classifier(bert_preprocessor(x_valid))
            score = sets_accuracy(y_valid, vocab(prob2labels(y_valid_pred)))
            print(&amp;quot;Batches done: {}. Valid Accuracy: {}&amp;quot;.format(nbatches, score))

    y_valid_pred = bert_classifier(bert_preprocessor(x_valid))
    score = sets_accuracy(y_valid, vocab(prob2labels(y_valid_pred)))
    print(&amp;quot;Epochs done: {}. Valid Accuracy: {}&amp;quot;.format(ep + 1, score))
    if score &amp;gt; best_score:
        bert_classifier.save()
        print(&amp;quot;New best score. Saving model.&amp;quot;)
        best_score = score    
        impatience = 0
    else:
        impatience += 1
        if impatience == patience:
            print(&amp;quot;Out of patience. Stop training.&amp;quot;)
            break
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,muf1be,True,,strangeguy111,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/muf1be/why_do_i_keep_running_out_of_memory_ram/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/muf1be/why_do_i_keep_running_out_of_memory_ram/,30199,1618880114.0,0,,False,,,,,,4503
1331,,LanguageTechnology,"Hi

about 3 years ago Google showed a demo in which a human-sounding ""AI assistant"" made a phone call to a shop (a hairdresser, if I remember well). Is anybody using this technology in production, for example to accept restaurant/takeaway orders? Is DialogFlow production-ready? What's the SOTA in this field?

Thanks!",t2_hdiwog1,False,,0,False,production-ready NLP/NLU for restaurants?,[],r/LanguageTechnology,False,6,,0,,False,t3_mu0rxw,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1618868252.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi&lt;/p&gt;

&lt;p&gt;about 3 years ago Google showed a demo in which a human-sounding &amp;quot;AI assistant&amp;quot; made a phone call to a shop (a hairdresser, if I remember well). Is anybody using this technology in production, for example to accept restaurant/takeaway orders? Is DialogFlow production-ready? What&amp;#39;s the SOTA in this field?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mu0rxw,True,,_aus_,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mu0rxw/productionready_nlpnlu_for_restaurants/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mu0rxw/productionready_nlpnlu_for_restaurants/,30199,1618839452.0,0,,False,,,,,,318
1332,,LanguageTechnology,,t2_5fsp2x6v,False,,0,False,A Rigorous Study on Pretrained Model for NER | Research Papers Summary 014,[],r/LanguageTechnology,False,6,,0,,False,t3_mtixvt,False,dark,0.89,,public,15,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/rL6FvknTdKw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'A Rigorous Study on Pretrained Model for NER | Research Papers Summary 014', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/rL6FvknTdKw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/rL6FvknTdKw/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/rL6FvknTdKw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mtixvt', 'height': 200}",,False,15,,False,False,,False,,[],{},,False,,1618800917.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mtixvt,True,,RyanAI100,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mtixvt/a_rigorous_study_on_pretrained_model_for_ner/,all_ads,False,https://youtu.be/rL6FvknTdKw,30199,1618772117.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'A Rigorous Study on Pretrained Model for NER | Research Papers Summary 014', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/rL6FvknTdKw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/rL6FvknTdKw/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,https://youtu.be/rL6FvknTdKw,,,,,0
1333,,LanguageTechnology,"Hey, I've created a tutorial on how to specify a large formula for a regression model using the R programming language. The tutorial shows different tips and tricks to make your code more efficient: [https://statisticsglobe.com/write-model-formula-with-many-variables-in-r](https://statisticsglobe.com/write-model-formula-with-many-variables-in-r)",t2_77cigax1,False,,0,False,Tutorial on how to specify a large formula for a regression model,[],r/LanguageTechnology,False,6,,0,,False,t3_mtvm0d,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1618846994.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, I&amp;#39;ve created a tutorial on how to specify a large formula for a regression model using the R programming language. The tutorial shows different tips and tricks to make your code more efficient: &lt;a href=""https://statisticsglobe.com/write-model-formula-with-many-variables-in-r""&gt;https://statisticsglobe.com/write-model-formula-with-many-variables-in-r&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mtvm0d,True,,JoachimSchork,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mtvm0d/tutorial_on_how_to_specify_a_large_formula_for_a/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mtvm0d/tutorial_on_how_to_specify_a_large_formula_for_a/,30199,1618818194.0,0,,False,,,,,,347
1334,,LanguageTechnology,"In our most recent paper, we introduce ""Datasets from Instructions"" (DINO 🦕) and show how LMs can create entire datasets from scratch if provided with instructions. These datasets can be used to train much smaller models.

This is an early draft, so I'd be very happy to hear your thoughts 😊

📄 Paper: https://arxiv.org/abs/2104.07540

🖥️ Code: https://github.com/timoschick/dino",t2_383etasr,False,,0,False,Generating Datasets with Pretrained Language Models,[],r/LanguageTechnology,False,6,,0,,False,t3_mtacs2,False,dark,0.95,,public,14,1,{},,False,[],,False,False,,{},,False,14,,False,False,,False,,[],{},,True,,1618770646.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In our most recent paper, we introduce &amp;quot;Datasets from Instructions&amp;quot; (DINO 🦕) and show how LMs can create entire datasets from scratch if provided with instructions. These datasets can be used to train much smaller models.&lt;/p&gt;

&lt;p&gt;This is an early draft, so I&amp;#39;d be very happy to hear your thoughts 😊&lt;/p&gt;

&lt;p&gt;📄 Paper: &lt;a href=""https://arxiv.org/abs/2104.07540""&gt;https://arxiv.org/abs/2104.07540&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;🖥️ Code: &lt;a href=""https://github.com/timoschick/dino""&gt;https://github.com/timoschick/dino&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mtacs2,True,,timoschick,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mtacs2/generating_datasets_with_pretrained_language/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mtacs2/generating_datasets_with_pretrained_language/,30199,1618741846.0,0,,False,,,,,,379
1335,,LanguageTechnology,,t2_hkv9s,False,,0,False,BART: Denoising Sequence-to-Sequence Pre-training for NLG (Research Paper Walkthrough),[],r/LanguageTechnology,False,6,,0,,False,t3_mt7zun,False,dark,0.93,,public,26,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/BGWpNQHIcs4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'BART: Denoising Sequence-to-Sequence Pre-training for NLG (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/BGWpNQHIcs4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/BGWpNQHIcs4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/BGWpNQHIcs4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mt7zun', 'height': 200}",,False,26,,False,False,,False,,[],{},,False,,1618758176.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mt7zun,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mt7zun/bart_denoising_sequencetosequence_pretraining_for/,all_ads,False,https://youtu.be/BGWpNQHIcs4,30199,1618729376.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'BART: Denoising Sequence-to-Sequence Pre-training for NLG (Research Paper Walkthrough)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/BGWpNQHIcs4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'TechViz - The Data Science Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/BGWpNQHIcs4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/TechVizTheDataScienceGuy'}}",False,https://youtu.be/BGWpNQHIcs4,,,,,0
1336,,LanguageTechnology,"I'm doing some research on biomedical language models and would like to use search engines as a baseline. What I have in mind is something like the [Biomedical Entity Search Tool (BEST)](http://best.korea.ac.kr/). Polysearch is also a candidate, but for some reason it's not working anymore.

I'd appreciate it if someone who knows of other search engines like these ones would be able to provide some tips on where I could find more. Thanks!",t2_m8kccne,False,,0,False,Does anyone know of any biomedical entity search engines out there?,[],r/LanguageTechnology,False,6,,0,,False,t3_mta54d,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1618769584.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m doing some research on biomedical language models and would like to use search engines as a baseline. What I have in mind is something like the &lt;a href=""http://best.korea.ac.kr/""&gt;Biomedical Entity Search Tool (BEST)&lt;/a&gt;. Polysearch is also a candidate, but for some reason it&amp;#39;s not working anymore.&lt;/p&gt;

&lt;p&gt;I&amp;#39;d appreciate it if someone who knows of other search engines like these ones would be able to provide some tips on where I could find more. Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mta54d,True,,Seankala,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mta54d/does_anyone_know_of_any_biomedical_entity_search/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mta54d/does_anyone_know_of_any_biomedical_entity_search/,30199,1618740784.0,0,,False,,,,,,442
1337,,LanguageTechnology,"Hello everyone!

Over the last few months I've been knee deep in the process for applying to grad school in compling/language technology, and now that decisions have come back I'm left with the choice between programs at Saarland University and the University of Gothenburg.

My research so far has favored Saarland: it's often mentioned in online forums and lists of the best compling universities, it ranks highly in the CSRankings for NLP in Europe (number 6 is you only consider English language master programs), and appears to be well regarded on this subreddit. Gothenburg, on the other hand, is rarely if ever mentioned, and ranks in the 50s in CSRankings for NLP in Europe. Indeed, I only applied for Gothenburg at the suggestion of a friend who goes there, when I was panicking after getting rejected from Edinburgh (which came as a surprise, given that it was my alma mater).

While this may appear to be an open and shut case, given the above, as far as I can tell Gothenburg does appear to have one major advantage over Saarland: job placement. According to my friend, Gothenburg works closely with Swedish tech firms to place its students, and indeed it actively encourages and facilitates collaborating with local firms on your dissertation. Saarland, on the other hand, does not appear to provide this level of support: according to the Saarland program's study coordinator, Saarland does not offer individual support for placement, and it appears that the university's relationship with local companies is not a strong as Gothenburg's appears to be.

My question is, does anyone here have experience with Saarland in terms of career placement? And is Saarland truly a better overall choice than Gothenburg?

Thank you in advance!",t2_1ybka8j,False,,0,False,Deciding between Saarland and Gothenburg,[],r/LanguageTechnology,False,6,,0,,False,t3_mtestc,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1618788017.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;

&lt;p&gt;Over the last few months I&amp;#39;ve been knee deep in the process for applying to grad school in compling/language technology, and now that decisions have come back I&amp;#39;m left with the choice between programs at Saarland University and the University of Gothenburg.&lt;/p&gt;

&lt;p&gt;My research so far has favored Saarland: it&amp;#39;s often mentioned in online forums and lists of the best compling universities, it ranks highly in the CSRankings for NLP in Europe (number 6 is you only consider English language master programs), and appears to be well regarded on this subreddit. Gothenburg, on the other hand, is rarely if ever mentioned, and ranks in the 50s in CSRankings for NLP in Europe. Indeed, I only applied for Gothenburg at the suggestion of a friend who goes there, when I was panicking after getting rejected from Edinburgh (which came as a surprise, given that it was my alma mater).&lt;/p&gt;

&lt;p&gt;While this may appear to be an open and shut case, given the above, as far as I can tell Gothenburg does appear to have one major advantage over Saarland: job placement. According to my friend, Gothenburg works closely with Swedish tech firms to place its students, and indeed it actively encourages and facilitates collaborating with local firms on your dissertation. Saarland, on the other hand, does not appear to provide this level of support: according to the Saarland program&amp;#39;s study coordinator, Saarland does not offer individual support for placement, and it appears that the university&amp;#39;s relationship with local companies is not a strong as Gothenburg&amp;#39;s appears to be.&lt;/p&gt;

&lt;p&gt;My question is, does anyone here have experience with Saarland in terms of career placement? And is Saarland truly a better overall choice than Gothenburg?&lt;/p&gt;

&lt;p&gt;Thank you in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mtestc,True,,Identifies-Birds,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mtestc/deciding_between_saarland_and_gothenburg/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mtestc/deciding_between_saarland_and_gothenburg/,30199,1618759217.0,0,,False,,,,,,1746
1338,,LanguageTechnology,"Can someone help me understand what is the purpose of word embeddings in neural machine translation models? 

I understand (mostly) he word embeddings in the context of sentiment analysis and other tasks, but what purpose does it have in the context of neural machine translation? Doesn't the neural model learn everything itself without needing any embedding space?",t2_4m6jgrsb,False,,0,False,Word embeddings and neural machine translation,[],r/LanguageTechnology,False,6,,0,,False,t3_mtb0c7,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1618773780.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can someone help me understand what is the purpose of word embeddings in neural machine translation models? &lt;/p&gt;

&lt;p&gt;I understand (mostly) he word embeddings in the context of sentiment analysis and other tasks, but what purpose does it have in the context of neural machine translation? Doesn&amp;#39;t the neural model learn everything itself without needing any embedding space?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mtb0c7,True,,permadressed,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mtb0c7/word_embeddings_and_neural_machine_translation/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mtb0c7/word_embeddings_and_neural_machine_translation/,30199,1618744980.0,0,,False,,,,,,366
1339,,LanguageTechnology,"Many of you were asking for the DUC 2004 dataset. Although it is available through their website, I have also uploaded it to GitHub and Kaggle for a better workflow. You can download it from here. 

&amp;#x200B;

 [UsmanNiazi/DUC-2004-Dataset: This Repo Contains the DUC 2004 Dataset (github.com)](https://github.com/UsmanNiazi/DUC-2004-Dataset)   


 [DUC 2004 Dataset | Kaggle](https://www.kaggle.com/usmanniazi/duc-2004-dataset)",t2_65fqlohg,False,,0,False,DUC 2004 Dataset,[],r/LanguageTechnology,False,6,,0,,False,t3_mt3228,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1618736440.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Many of you were asking for the DUC 2004 dataset. Although it is available through their website, I have also uploaded it to GitHub and Kaggle for a better workflow. You can download it from here. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/UsmanNiazi/DUC-2004-Dataset""&gt;UsmanNiazi/DUC-2004-Dataset: This Repo Contains the DUC 2004 Dataset (github.com)&lt;/a&gt;   &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.kaggle.com/usmanniazi/duc-2004-dataset""&gt;DUC 2004 Dataset | Kaggle&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mt3228,True,,usmannkhan,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mt3228/duc_2004_dataset/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mt3228/duc_2004_dataset/,30199,1618707640.0,0,,False,,,,,,431
1340,,LanguageTechnology,"BERT is made up of Transformer encoders, and therefore uses layer normalization instead of batch normalization. Layer normalization normalizes each datapoint independently from other datapoints in the batch. So why would batch size matter for BERT's performance? Where else does it factor in?",t2_8gn71,False,,0,False,Why is batch size a relevant hyperparameter for BERT?,[],r/LanguageTechnology,False,6,,0,,False,t3_mt9ex7,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1618765846.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;BERT is made up of Transformer encoders, and therefore uses layer normalization instead of batch normalization. Layer normalization normalizes each datapoint independently from other datapoints in the batch. So why would batch size matter for BERT&amp;#39;s performance? Where else does it factor in?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mt9ex7,True,,boxdreper,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mt9ex7/why_is_batch_size_a_relevant_hyperparameter_for/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mt9ex7/why_is_batch_size_a_relevant_hyperparameter_for/,30199,1618737046.0,0,,False,,,,,,292
1341,,LanguageTechnology,"I’m doing a school project which is building a chatbot to help students learn English and give tips for the TOEIC test (I dont come from English native speaking country), so far my ideas are using masked language models to help solving the reading test in TOEIC and I’m kinda stuck there. Can you give me some suggestions of what to implement and maybe some SOTA open source chatbots and how to tune it into specific domain (English and TOEIC) that would be super life saving. Thanks in advance",t2_10b2sjxx,False,,0,False,Please give me some suggestions.,[],r/LanguageTechnology,False,6,,0,,False,t3_mt3xp2,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1618739980.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I’m doing a school project which is building a chatbot to help students learn English and give tips for the TOEIC test (I dont come from English native speaking country), so far my ideas are using masked language models to help solving the reading test in TOEIC and I’m kinda stuck there. Can you give me some suggestions of what to implement and maybe some SOTA open source chatbots and how to tune it into specific domain (English and TOEIC) that would be super life saving. Thanks in advance&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mt3xp2,True,,SEMClovesYOU,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mt3xp2/please_give_me_some_suggestions/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mt3xp2/please_give_me_some_suggestions/,30199,1618711180.0,0,,False,,,,,,494
1342,,LanguageTechnology,"Hi All, 

I am writing here because I wish to get in touch with those brilliant minds that are active in the group and learning from them :)   
In a course about Finance, I saw how I could use TextBlob and Scrapy on ICO whitepapers and how to exploit experts´ posts in [icobench.com](https://icobench.com) to determine a sentiment - based on Bayes Classifier. We have used Python. 

My goal is to demonstrate that I can be a point of reference for NLP and ML in a company (relative terms). I would proceed following: 

* Setup scraper/crawler to obtain data from several websites
* Store the data in google firebase /firestore
* Cleanup the data for ML
* Create a training set/test set
* Build a ML model (classification/prediction)
* Measure effectiveness
* Report on what I did 

A) Do you suggest me to be familiar first to Vue.js? I have no experience with HTML, CSS and java and I assume that crawl data &gt; store &gt; clean &gt; give label would take a relevant amount of time.

B) If I build a text corpus from wikipedia and use it for my data, which are the problems that I may encounter and take into account? Fx the text blob offers a classifier from movie database but if you apply it on another context like finance, then you need specific label for certain words. 

C) Which models can be more interesting to compare? 

I thank you in advance for the attention given to this post. 

  
Have a good weekend",t2_a80a17xm,False,,0,False,Project NLP / ML from scratch - Beginner,[],r/LanguageTechnology,False,6,,0,,False,t3_mssi5w,False,dark,0.84,,public,8,0,{},,False,[],,False,False,,{},,False,8,,False,False,,False,,[],{},,True,,1618701167.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All, &lt;/p&gt;

&lt;p&gt;I am writing here because I wish to get in touch with those brilliant minds that are active in the group and learning from them :)&lt;br/&gt;
In a course about Finance, I saw how I could use TextBlob and Scrapy on ICO whitepapers and how to exploit experts´ posts in &lt;a href=""https://icobench.com""&gt;icobench.com&lt;/a&gt; to determine a sentiment - based on Bayes Classifier. We have used Python. &lt;/p&gt;

&lt;p&gt;My goal is to demonstrate that I can be a point of reference for NLP and ML in a company (relative terms). I would proceed following: &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Setup scraper/crawler to obtain data from several websites&lt;/li&gt;
&lt;li&gt;Store the data in google firebase /firestore&lt;/li&gt;
&lt;li&gt;Cleanup the data for ML&lt;/li&gt;
&lt;li&gt;Create a training set/test set&lt;/li&gt;
&lt;li&gt;Build a ML model (classification/prediction)&lt;/li&gt;
&lt;li&gt;Measure effectiveness&lt;/li&gt;
&lt;li&gt;Report on what I did &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A) Do you suggest me to be familiar first to Vue.js? I have no experience with HTML, CSS and java and I assume that crawl data &amp;gt; store &amp;gt; clean &amp;gt; give label would take a relevant amount of time.&lt;/p&gt;

&lt;p&gt;B) If I build a text corpus from wikipedia and use it for my data, which are the problems that I may encounter and take into account? Fx the text blob offers a classifier from movie database but if you apply it on another context like finance, then you need specific label for certain words. &lt;/p&gt;

&lt;p&gt;C) Which models can be more interesting to compare? &lt;/p&gt;

&lt;p&gt;I thank you in advance for the attention given to this post. &lt;/p&gt;

&lt;p&gt;Have a good weekend&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mssi5w,True,,risar16,,12,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mssi5w/project_nlp_ml_from_scratch_beginner/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mssi5w/project_nlp_ml_from_scratch_beginner/,30199,1618672367.0,0,,False,,,,,,1419
1343,,LanguageTechnology,What are some good document intelligence resources to begin with?,t2_7tb2j,False,,0,False,Di resources,[],r/LanguageTechnology,False,6,,0,,False,t3_mswji6,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1618714151.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What are some good document intelligence resources to begin with?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mswji6,True,,gevezex,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mswji6/di_resources/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mswji6/di_resources/,30199,1618685351.0,0,,False,,,,,,65
1344,,LanguageTechnology,"Hello community!

So basically what we want to achieve is a benchmarking of pretrained models in order to find out which ones are the most useful/efficient for calculating semantinc text similarity and whether one multi-lingual model would do, or separate models are better for this case (it's nothing really serious, this is for a scholar project but not the study per-se, just a way for us to argument a choice)

The context is not bound to a particular field. In practice, we will be comparing sentences from documents from any and all fields (a situation similar to plagiarism detection) so fine-tuning on a field-specific corpus is not an option and we will test the models as-is.

We are seeking recommendation:

* **For the corpora**: We will be benchmarking for languages other than english (but english is also included). Where can we find parallel corpora (for each language) that are suitable for this task? if not, what means can we use to build a small corpus of our own without doing manual paraphrase
* **For the metrics**: Is cosine distance good enough for the comparison? (by taking 1.0 as the perfect similarity score and the averages of all cosine distances accross the corpus, it will be pretty much like an accuracy  score).
   * What metrics/distances are better suited for the comparison, and what would you recommend for textual similarity in general?

oh and one last point that is not related to this benchmarking in particular. Supposing we have two documents (for instance 2 research papers) that we want to compare agaisnt eachother:

* What tokenization strategy would you recommend, considering we will compare the tokens from document 1 to those of document 2, then highlight the matching text in both documents (again, the usecase is very similar to plagiarism detection). The goal is minimal pairs loss.

Tokenization by sentence/punctuation doesn't seem to play nicely  as sentences in one document can be very long and can include many sentences of the second.

&amp;#x200B;",t2_a6fffcza,False,,0,False,Benchmarking of pretrained models for semantic textual similarity,[],r/LanguageTechnology,False,6,,0,,False,t3_msh1sk,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,1618681863.0,,[],{},,True,,1618650949.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello community!&lt;/p&gt;

&lt;p&gt;So basically what we want to achieve is a benchmarking of pretrained models in order to find out which ones are the most useful/efficient for calculating semantinc text similarity and whether one multi-lingual model would do, or separate models are better for this case (it&amp;#39;s nothing really serious, this is for a scholar project but not the study per-se, just a way for us to argument a choice)&lt;/p&gt;

&lt;p&gt;The context is not bound to a particular field. In practice, we will be comparing sentences from documents from any and all fields (a situation similar to plagiarism detection) so fine-tuning on a field-specific corpus is not an option and we will test the models as-is.&lt;/p&gt;

&lt;p&gt;We are seeking recommendation:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;For the corpora&lt;/strong&gt;: We will be benchmarking for languages other than english (but english is also included). Where can we find parallel corpora (for each language) that are suitable for this task? if not, what means can we use to build a small corpus of our own without doing manual paraphrase&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;For the metrics&lt;/strong&gt;: Is cosine distance good enough for the comparison? (by taking 1.0 as the perfect similarity score and the averages of all cosine distances accross the corpus, it will be pretty much like an accuracy  score).

&lt;ul&gt;
&lt;li&gt;What metrics/distances are better suited for the comparison, and what would you recommend for textual similarity in general?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;oh and one last point that is not related to this benchmarking in particular. Supposing we have two documents (for instance 2 research papers) that we want to compare agaisnt eachother:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What tokenization strategy would you recommend, considering we will compare the tokens from document 1 to those of document 2, then highlight the matching text in both documents (again, the usecase is very similar to plagiarism detection). The goal is minimal pairs loss.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tokenization by sentence/punctuation doesn&amp;#39;t seem to play nicely  as sentences in one document can be very long and can include many sentences of the second.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,msh1sk,True,,Ok-Caregiver3587,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/msh1sk/benchmarking_of_pretrained_models_for_semantic/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/msh1sk/benchmarking_of_pretrained_models_for_semantic/,30199,1618622149.0,0,,False,,,,,,2011
1345,,LanguageTechnology,"Can anyone recommend any projects, thesis available online in which nlp methods are applied to data from the legal/law domain? I am trying to learn more about this topic, and so far couldnt find anything other than the ""LEGALBert"" paper.

Does anyone recommend checking out any masters university projects where nlp was applied to data from the legal domain? I already know the basics *about word clouds, lda topic modelling and sentiment analysis - I am looking for a bit more intermediate level stuff about information extraction, transfer learning, summarization, fuzzy match, etc. 

Thanks",t2_3tosvccj,False,,0,False,"Links to nlp applied projects/thesis relating to ""law/legal""",[],r/LanguageTechnology,False,6,,0,,False,t3_msaync,False,dark,0.95,,public,14,0,{},,False,[],,False,False,,{},,False,14,,False,False,,False,,[],{},,True,,1618631340.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can anyone recommend any projects, thesis available online in which nlp methods are applied to data from the legal/law domain? I am trying to learn more about this topic, and so far couldnt find anything other than the &amp;quot;LEGALBert&amp;quot; paper.&lt;/p&gt;

&lt;p&gt;Does anyone recommend checking out any masters university projects where nlp was applied to data from the legal domain? I already know the basics *about word clouds, lda topic modelling and sentiment analysis - I am looking for a bit more intermediate level stuff about information extraction, transfer learning, summarization, fuzzy match, etc. &lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,msaync,True,,jj4646,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/msaync/links_to_nlp_applied_projectsthesis_relating_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/msaync/links_to_nlp_applied_projectsthesis_relating_to/,30199,1618602540.0,0,,False,,,,,,593
1346,,LanguageTechnology,"Hello, fellow Subredditors!

I'm curious to know if any of you are familiar with this program and if you could share your opinion about it.

I have a BA in Linguistics and I've done some work during my degree with corpora and have some experience in programming, specifically in Python. Mostly computer applications for linguistic research and digital humanities, more than anything else. I'm interested in studying a master's degree and Germany (for many reasons) is my place of choice. I've already looked at the programs at Tübingen and Stuttgart and the info I've seen on this subreddit about those two has been very useful. However, I haven't found much about the program at Darmstadt, and I'm particularly interested in this program, given its digital humanities component.

I would really appreciate your input! Thanks a lot!",t2_xxvoz,False,,0,False,MA in Linguistic and Literary Computing (TU Darmstadt): Thoughts?,[],r/LanguageTechnology,False,6,,0,,False,t3_ms63a1,False,dark,0.72,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1618617230.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, fellow Subredditors!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m curious to know if any of you are familiar with this program and if you could share your opinion about it.&lt;/p&gt;

&lt;p&gt;I have a BA in Linguistics and I&amp;#39;ve done some work during my degree with corpora and have some experience in programming, specifically in Python. Mostly computer applications for linguistic research and digital humanities, more than anything else. I&amp;#39;m interested in studying a master&amp;#39;s degree and Germany (for many reasons) is my place of choice. I&amp;#39;ve already looked at the programs at Tübingen and Stuttgart and the info I&amp;#39;ve seen on this subreddit about those two has been very useful. However, I haven&amp;#39;t found much about the program at Darmstadt, and I&amp;#39;m particularly interested in this program, given its digital humanities component.&lt;/p&gt;

&lt;p&gt;I would really appreciate your input! Thanks a lot!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ms63a1,True,,Alchelinguist,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ms63a1/ma_in_linguistic_and_literary_computing_tu/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/ms63a1/ma_in_linguistic_and_literary_computing_tu/,30199,1618588430.0,0,,False,,,,,,832
1347,,LanguageTechnology,"Link to paper: https://arxiv.org/abs/2104.06644

Is BERT just a giant bag-of-words??

Relevant meme: https://imgur.com/a/RGUtOvt",t2_bkc0iaxi,False,,0,False,[D] WTF these results are incredible?!! What do you guys think of this? BERT seems to learn equally well on word-shuffled sentences?,[],r/LanguageTechnology,False,6,,0,,False,t3_mrrcda,False,dark,0.98,,public,52,1,{},,False,[],,False,False,,{},,False,52,,False,False,,False,,[],{},,True,,1618559980.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Link to paper: &lt;a href=""https://arxiv.org/abs/2104.06644""&gt;https://arxiv.org/abs/2104.06644&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Is BERT just a giant bag-of-words??&lt;/p&gt;

&lt;p&gt;Relevant meme: &lt;a href=""https://imgur.com/a/RGUtOvt""&gt;https://imgur.com/a/RGUtOvt&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': 0, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 80, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'penny_donate': 0, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Everything is better with a good hug', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Hugz', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128, 'height': 128}], 'icon_format': 'PNG', 'icon_height': 2048, 'penny_price': 0, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mrrcda,True,,Choice_Willow1890,,10,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/mrrcda/d_wtf_these_results_are_incredible_what_do_you/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mrrcda/d_wtf_these_results_are_incredible_what_do_you/,30199,1618531180.0,0,,False,,,,,,128
1348,,LanguageTechnology,"Hi everyone. I want to ask you if this is possible:

Consider we have a minimal text, or just a couple of keywords, say X. And we have another large text or corpus, Y, that we assume they are about similar topics. Assume that X is a sentence and Y is a book.

Is it possible to generate a text Z that may be to an answer or comment to X that using facts or style from Y?

E.g. consider X is a text or keywords like ""I like to swim in Maldives!"". Y is a news about Maldives. Then Z might be a generated text like ""Maldives has the most clean beaches in the world.""

Thank you all.",t2_3kwtnien,False,,0,False,Text Generation from Another Text,[],r/LanguageTechnology,False,6,,0,,False,t3_mrywon,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1618590633.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone. I want to ask you if this is possible:&lt;/p&gt;

&lt;p&gt;Consider we have a minimal text, or just a couple of keywords, say X. And we have another large text or corpus, Y, that we assume they are about similar topics. Assume that X is a sentence and Y is a book.&lt;/p&gt;

&lt;p&gt;Is it possible to generate a text Z that may be to an answer or comment to X that using facts or style from Y?&lt;/p&gt;

&lt;p&gt;E.g. consider X is a text or keywords like &amp;quot;I like to swim in Maldives!&amp;quot;. Y is a news about Maldives. Then Z might be a generated text like &amp;quot;Maldives has the most clean beaches in the world.&amp;quot;&lt;/p&gt;

&lt;p&gt;Thank you all.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mrywon,True,,emremrah,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mrywon/text_generation_from_another_text/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mrywon/text_generation_from_another_text/,30199,1618561833.0,0,,False,,,,,,579
1349,,LanguageTechnology,,t2_16gftjf,False,,0,False,The first ever Gesture Generation Challenge. More info in comments,[],r/LanguageTechnology,False,6,,0,,False,t3_mrh9zp,False,dark,0.95,,public,15,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ja7IXGFrYGA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': ""[IUI'21] A large, crowdsourced evaluation of gesture generation systems on common data"", 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ja7IXGFrYGA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'GENEA Workshop', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/ja7IXGFrYGA/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCViP_2AQz55MsM3D_1OCbpw'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ja7IXGFrYGA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mrh9zp', 'height': 200}",,False,15,,False,False,,False,,[],{},,False,,1618529950.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mrh9zp,True,,Svito-zar,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mrh9zp/the_first_ever_gesture_generation_challenge_more/,all_ads,False,https://youtu.be/ja7IXGFrYGA,30199,1618501150.0,1,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': ""[IUI'21] A large, crowdsourced evaluation of gesture generation systems on common data"", 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/ja7IXGFrYGA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'GENEA Workshop', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/ja7IXGFrYGA/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCViP_2AQz55MsM3D_1OCbpw'}}",False,https://youtu.be/ja7IXGFrYGA,,,,,0
1350,,LanguageTechnology,,t2_13p32d,False,,0,False,NLP equivalent to speaker diarization?,[],r/LanguageTechnology,False,6,,0,,False,t3_mrmc87,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,False,,1618544331.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mrmc87,True,,tim_gabie,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mrmc87/nlp_equivalent_to_speaker_diarization/,all_ads,False,/r/learnmachinelearning/comments/mrmaym/equivalent_to_speaker_diarization_for_books/,30199,1618515531.0,0,,False,/r/learnmachinelearning/comments/mrmaym/equivalent_to_speaker_diarization_for_books/,"[{'approved_at_utc': None, 'subreddit': 'learnmachinelearning', 'selftext': ""I want to segment a novel based on which character in the novel said what. e.g. listing all text the character in the novel called Peter said.\n\n I think of it as the NLP equivalent for speaker diarization. I couldn't find papers on this yet. Anyone has a hint for me what I have to search for to find more about this topic?"", 'author_fullname': 't2_13p32d', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'equivalent to speaker diarization for books?', 'link_flair_richtext': [{'e': 'text', 't': 'Question'}], 'subreddit_name_prefixed': 'r/learnmachinelearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_mrmaym', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Question', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1618544223.0, 'link_flair_type': 'richtext', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.learnmachinelearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to segment a novel based on which character in the novel said what. e.g. listing all text the character in the novel called Peter said.&lt;/p&gt;\n\n&lt;p&gt;I think of it as the NLP equivalent for speaker diarization. I couldn&amp;#39;t find papers on this yet. Anyone has a hint for me what I have to search for to find more about this topic?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'ec81b8ee-accf-11e9-b8f8-0ebea2df7d78', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_3cqa1', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ffb000', 'id': 'mrmaym', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'tim_gabie', 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/learnmachinelearning/comments/mrmaym/equivalent_to_speaker_diarization_for_books/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/learnmachinelearning/comments/mrmaym/equivalent_to_speaker_diarization_for_books/', 'subreddit_subscribers': 232286, 'created_utc': 1618515423.0, 'num_crossposts': 4, 'media': None, 'is_video': False}]",t3_mrmaym,,,0
1351,,LanguageTechnology,"I am working on a classification problem whose data includes both text and numerical features. My first approach to tacke this problem was to convert the text features into embeddings and append those as new features to original dataset. The problem with approach is that since embeddings are usually of high dimensions, they overwhelm the numerical features.

Second approach I used was append every feature together into a string, convert it into embedding and train the model. This gave me very poor accuracy.

Is there any other way I can handle this problem?",t2_5zyo23hz,False,,0,False,Classification problem with text and numerical features,[],r/LanguageTechnology,False,6,,0,,False,t3_mr8rsi,False,dark,0.92,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1618495482.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am working on a classification problem whose data includes both text and numerical features. My first approach to tacke this problem was to convert the text features into embeddings and append those as new features to original dataset. The problem with approach is that since embeddings are usually of high dimensions, they overwhelm the numerical features.&lt;/p&gt;

&lt;p&gt;Second approach I used was append every feature together into a string, convert it into embedding and train the model. This gave me very poor accuracy.&lt;/p&gt;

&lt;p&gt;Is there any other way I can handle this problem?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mr8rsi,True,,eagleandwolf,,15,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mr8rsi/classification_problem_with_text_and_numerical/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mr8rsi/classification_problem_with_text_and_numerical/,30199,1618466682.0,0,,False,,,,,,563
1352,,LanguageTechnology,Best method for aspect analysis ??,t2_75yo37zi,False,,0,False,Aspect analysis,[],r/LanguageTechnology,False,6,,0,,False,t3_mrecsh,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1618521096.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Best method for aspect analysis ??&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mrecsh,True,,Snoo71755,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mrecsh/aspect_analysis/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mrecsh/aspect_analysis/,30199,1618492296.0,0,,False,,,,,,34
1353,,LanguageTechnology,"I just graduated with a degree in CS and now I'm interested in doing research in NLP. The material available online, ever-changing technology and also the hype makes me overwhelmed. Also the real research happens at the PhD level which is far too above from an undergrad level. How do I approach things? Will reading more papers and learning more about the theory helps? Or should I just dive into the practical parts and start using pretrained models and libraries? 

The way I like to do things is first to have a solid theoretical foundations of a certain topic after which I look around for implementations. I am more of a theory person, but NLP is more practical than theory. 

My goal is to focus on only 1 field for my future studies and I've chosen Nlp. Now I want to get good at it, but it's not clear how?",t2_8s1tooht,False,,0,False,How to do undergrad research the right way?,[],r/LanguageTechnology,False,6,,0,,False,t3_mqwzpv,False,dark,0.97,,public,30,0,{},,False,[],,False,False,,{},,False,30,,False,False,,False,,[],{},,True,,1618454399.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I just graduated with a degree in CS and now I&amp;#39;m interested in doing research in NLP. The material available online, ever-changing technology and also the hype makes me overwhelmed. Also the real research happens at the PhD level which is far too above from an undergrad level. How do I approach things? Will reading more papers and learning more about the theory helps? Or should I just dive into the practical parts and start using pretrained models and libraries? &lt;/p&gt;

&lt;p&gt;The way I like to do things is first to have a solid theoretical foundations of a certain topic after which I look around for implementations. I am more of a theory person, but NLP is more practical than theory. &lt;/p&gt;

&lt;p&gt;My goal is to focus on only 1 field for my future studies and I&amp;#39;ve chosen Nlp. Now I want to get good at it, but it&amp;#39;s not clear how?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mqwzpv,True,,rapchickk,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mqwzpv/how_to_do_undergrad_research_the_right_way/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mqwzpv/how_to_do_undergrad_research_the_right_way/,30199,1618425599.0,0,,False,,,,,,815
1354,,LanguageTechnology,"Sharing our new [interactive research graph for BERT](https://crossminds.ai/graphlist/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-60709500c8663c4cfa875fc4/), which maps important prior contextual representation work and various pre-trained language models derived from BERT.  Hope you find it helpful!

Here are the papers (and video presentations) included in the graph:

* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (core paper)
* Attention Is All You Need
* Semi-supervised Sequence Learning
* Deep contextualized word representations
* Universal Language Model Fine-tuning for Text Classification
* Improving Language Understanding by Generative Pre-Training
* RoBERTa: A Robustly Optimized BERT Pretraining Approach
* Defending Against Neural Fake News
* Language Models as Knowledge Bases?
* ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
* XLNet: Generalized Autoregressive Pretraining for Language Understanding
* Cross-lingual Language Model Pretraining
* 75 Languages, 1 Model: Parsing Universal Dependencies Universally
* Multi-Task Deep Neural Networks for Natural Language Understanding
* MASS: Masked Sequence to Sequence Pre-training for Language Generation
* Unified Language Model Pre-training for Natural Language Understanding and Generation
* SpanBERT: Improving Pre-training by Representing and Predicting Spans
* ERNIE: Enhanced Language Representation with Informative Entities
* ERNIE: Enhanced Representation through Knowledge Integration
* Knowledge Enhanced Contextual Word Representations
* VideoBERT: A Joint Model for Video and Language Representation Learning
* Contrastive Bidirectional Transformer for Temporal Representation Learning
* ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks
* VisualBERT: A Simple and Performant Baseline for Vision and Language
* Fusion of Detected Objects in Text for Visual Question Answering
* Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training
* LXMERT: Learning Cross-Modality Encoder Representations from Transformers
* VL-BERT: Pre-training of Generic Visual-Linguistic Representations
* UNITER: Learning UNiversal Image-TExt Representations
* Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks
* DeBERTa: Decoding-enhanced BERT with Disentangled Attention",t2_779ciqdy,False,,0,False,Research paper mapping for BERT: foundational work &amp; latest advancements,[],r/LanguageTechnology,False,6,,0,,False,t3_mqgqcz,False,dark,1.0,,public,37,1,{},,False,[],,False,False,,{},,False,37,,False,False,,False,,[],{'gid_1': 1},,True,,1618393923.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Sharing our new &lt;a href=""https://crossminds.ai/graphlist/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-60709500c8663c4cfa875fc4/""&gt;interactive research graph for BERT&lt;/a&gt;, which maps important prior contextual representation work and various pre-trained language models derived from BERT.  Hope you find it helpful!&lt;/p&gt;

&lt;p&gt;Here are the papers (and video presentations) included in the graph:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (core paper)&lt;/li&gt;
&lt;li&gt;Attention Is All You Need&lt;/li&gt;
&lt;li&gt;Semi-supervised Sequence Learning&lt;/li&gt;
&lt;li&gt;Deep contextualized word representations&lt;/li&gt;
&lt;li&gt;Universal Language Model Fine-tuning for Text Classification&lt;/li&gt;
&lt;li&gt;Improving Language Understanding by Generative Pre-Training&lt;/li&gt;
&lt;li&gt;RoBERTa: A Robustly Optimized BERT Pretraining Approach&lt;/li&gt;
&lt;li&gt;Defending Against Neural Fake News&lt;/li&gt;
&lt;li&gt;Language Models as Knowledge Bases?&lt;/li&gt;
&lt;li&gt;ALBERT: A Lite BERT for Self-supervised Learning of Language Representations&lt;/li&gt;
&lt;li&gt;XLNet: Generalized Autoregressive Pretraining for Language Understanding&lt;/li&gt;
&lt;li&gt;Cross-lingual Language Model Pretraining&lt;/li&gt;
&lt;li&gt;75 Languages, 1 Model: Parsing Universal Dependencies Universally&lt;/li&gt;
&lt;li&gt;Multi-Task Deep Neural Networks for Natural Language Understanding&lt;/li&gt;
&lt;li&gt;MASS: Masked Sequence to Sequence Pre-training for Language Generation&lt;/li&gt;
&lt;li&gt;Unified Language Model Pre-training for Natural Language Understanding and Generation&lt;/li&gt;
&lt;li&gt;SpanBERT: Improving Pre-training by Representing and Predicting Spans&lt;/li&gt;
&lt;li&gt;ERNIE: Enhanced Language Representation with Informative Entities&lt;/li&gt;
&lt;li&gt;ERNIE: Enhanced Representation through Knowledge Integration&lt;/li&gt;
&lt;li&gt;Knowledge Enhanced Contextual Word Representations&lt;/li&gt;
&lt;li&gt;VideoBERT: A Joint Model for Video and Language Representation Learning&lt;/li&gt;
&lt;li&gt;Contrastive Bidirectional Transformer for Temporal Representation Learning&lt;/li&gt;
&lt;li&gt;ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks&lt;/li&gt;
&lt;li&gt;VisualBERT: A Simple and Performant Baseline for Vision and Language&lt;/li&gt;
&lt;li&gt;Fusion of Detected Objects in Text for Visual Question Answering&lt;/li&gt;
&lt;li&gt;Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training&lt;/li&gt;
&lt;li&gt;LXMERT: Learning Cross-Modality Encoder Representations from Transformers&lt;/li&gt;
&lt;li&gt;VL-BERT: Pre-training of Generic Visual-Linguistic Representations&lt;/li&gt;
&lt;li&gt;UNITER: Learning UNiversal Image-TExt Representations&lt;/li&gt;
&lt;li&gt;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&lt;/li&gt;
&lt;li&gt;DeBERTa: Decoding-enhanced BERT with Disentangled Attention&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'static_icon_width': 512, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 512, 'name': 'Silver', 'resized_static_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 512, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mqgqcz,True,,ccrbltscm,,2,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/mqgqcz/research_paper_mapping_for_bert_foundational_work/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mqgqcz/research_paper_mapping_for_bert_foundational_work/,30199,1618365123.0,0,,False,,,,,,2423
1355,,LanguageTechnology,"Looking for other types or ways to use this data and come up with analysis on it. I feel like my project needs a little more but I’m unsure what else I can do with what I have. It takes in user reviews, like Yelp reviews, and tags them as either positive or negative.",t2_7wo3mdn,False,,0,False,"Finished an NLP based binary text classified for user reviews, what else can I do with this to add to my project?",[],r/LanguageTechnology,False,6,,0,,False,t3_mqtitw,False,dark,0.67,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1618444545.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Looking for other types or ways to use this data and come up with analysis on it. I feel like my project needs a little more but I’m unsure what else I can do with what I have. It takes in user reviews, like Yelp reviews, and tags them as either positive or negative.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mqtitw,True,,edwardsrk,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mqtitw/finished_an_nlp_based_binary_text_classified_for/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mqtitw/finished_an_nlp_based_binary_text_classified_for/,30199,1618415745.0,0,,False,,,,,,267
1356,,LanguageTechnology,"For example, how perplexed is our model assigning the topic that it assigned to a document. The reason I want to know this is because I want to weed out low frequency documents that were wrongly assigned to high frequency topics",t2_5r4mo7fh,False,,0,False,Is there a way to get document wise perplexity in gensim LDA,[],r/LanguageTechnology,False,6,,0,,False,t3_mqpuw5,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1618433337.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For example, how perplexed is our model assigning the topic that it assigned to a document. The reason I want to know this is because I want to weed out low frequency documents that were wrongly assigned to high frequency topics&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mqpuw5,True,,Epiphany925,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mqpuw5/is_there_a_way_to_get_document_wise_perplexity_in/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mqpuw5/is_there_a_way_to_get_document_wise_perplexity_in/,30199,1618404537.0,0,,False,,,,,,228
1357,,LanguageTechnology,"*Long time lurker, first time poster!* 

*otso has just launched our Annotator, and have built with the needs of many in this sub!*

&amp;#x200B;

**A cloud-based Text Annotator built for Machine Learning Engineers and Data Scientists.**

We have been working on otso Annotator for over two years. It began as an internal tool, used to manage annotation and data labelling for our own machine learning projects. As the tool and the interface developed, we began providing it to select enterprise customers. We’ve received so much significant positive feedback from our clients, that we decided to launch the Annotator as a standalone product.

otso Annotator provides three key benefits; the user experience, which prioritises ease-of-use and understanding. The project management features, which let you allocate and manage annotation tasks. Finally, as a cloud-first tool, you no longer need to have annotators use a CLI to get started, which makes it a much easier tool for teams to use.

**Why a focus on the user experience of teams?**

Text annotation is best done in a team environment. Ideally, machine learning engineers and data scientists will set up and run projects, while subject matter experts provide annotations. We have built otso Annotator with these different user types in mind - enabling seamless project setup for project admins and an easy and keyboard enabled annotation experience for annotators.

With this public launch, we are granting users and teams that sign up during April an extended trial period of 30 days.

To check it out, head to [otso.ai/annotator](https://otso.ai/annotator).No credit card required.",t2_2wqhs6ll,False,,0,False,Checkout our team-based cloud-first text annotator.,[],r/LanguageTechnology,False,6,,0,,False,t3_mqcra9,False,dark,0.89,,public,7,1,{},,False,[],,False,False,,{},,False,7,,False,False,,False,,[],{},,True,,1618380371.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;em&gt;Long time lurker, first time poster!&lt;/em&gt; &lt;/p&gt;

&lt;p&gt;&lt;em&gt;otso has just launched our Annotator, and have built with the needs of many in this sub!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A cloud-based Text Annotator built for Machine Learning Engineers and Data Scientists.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We have been working on otso Annotator for over two years. It began as an internal tool, used to manage annotation and data labelling for our own machine learning projects. As the tool and the interface developed, we began providing it to select enterprise customers. We’ve received so much significant positive feedback from our clients, that we decided to launch the Annotator as a standalone product.&lt;/p&gt;

&lt;p&gt;otso Annotator provides three key benefits; the user experience, which prioritises ease-of-use and understanding. The project management features, which let you allocate and manage annotation tasks. Finally, as a cloud-first tool, you no longer need to have annotators use a CLI to get started, which makes it a much easier tool for teams to use.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why a focus on the user experience of teams?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Text annotation is best done in a team environment. Ideally, machine learning engineers and data scientists will set up and run projects, while subject matter experts provide annotations. We have built otso Annotator with these different user types in mind - enabling seamless project setup for project admins and an easy and keyboard enabled annotation experience for annotators.&lt;/p&gt;

&lt;p&gt;With this public launch, we are granting users and teams that sign up during April an extended trial period of 30 days.&lt;/p&gt;

&lt;p&gt;To check it out, head to &lt;a href=""https://otso.ai/annotator""&gt;otso.ai/annotator&lt;/a&gt;.No credit card required.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mqcra9,True,,otso_ai,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mqcra9/checkout_our_teambased_cloudfirst_text_annotator/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mqcra9/checkout_our_teambased_cloudfirst_text_annotator/,30199,1618351571.0,0,,False,,,,,,1641
1358,,LanguageTechnology,"Hello,

I am developing a an extractive text summarizer for french language, I would like to explore the self-attention mechanism to see at which linguistic patterns it looks to by generating a heat map or? Any help?",t2_9e6mmc22,False,,0,False,At which linguistic patterns and features attention heads of BERT look to ?,[],r/LanguageTechnology,False,6,,0,,False,t3_mq3ov5,False,dark,0.84,,public,4,0,{},,False,[],,False,False,,{},,False,4,,False,False,,False,,[],{},,True,,1618353858.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am developing a an extractive text summarizer for french language, I would like to explore the self-attention mechanism to see at which linguistic patterns it looks to by generating a heat map or? Any help?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mq3ov5,True,,Ok_Inspection_5208,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mq3ov5/at_which_linguistic_patterns_and_features/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mq3ov5/at_which_linguistic_patterns_and_features/,30199,1618325058.0,0,,False,,,,,,216
1359,,LanguageTechnology,,t2_21qaqh1p,False,,0,False,Youtube Video Transcript Summarization with Hugging Face,[],r/LanguageTechnology,False,6,,0,,False,t3_mq9rwa,False,dark,0.6,,public,1,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3V-MJhJvRWg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Youtube Video Transcript Summarization with Hugging Face Transformers | Python NLP Projects', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3V-MJhJvRWg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3V-MJhJvRWg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3V-MJhJvRWg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mq9rwa', 'height': 200}",,False,1,,False,False,,False,,[],{},,False,,1618371353.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mq9rwa,True,,dulldata,,0,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/mq9rwa/youtube_video_transcript_summarization_with/,all_ads,False,https://youtu.be/3V-MJhJvRWg,30199,1618342553.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Youtube Video Transcript Summarization with Hugging Face Transformers | Python NLP Projects', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3V-MJhJvRWg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3V-MJhJvRWg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}}",False,https://youtu.be/3V-MJhJvRWg,,,,,0
1360,,LanguageTechnology,,t2_bhyxnyna,False,,0,False,hello I need DUC 2004 dataset for my master project .... if anyone share it with me i'll so thankful,[],r/LanguageTechnology,False,6,,0,,False,t3_mq18qz,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1618345759.0,text,6,,,text,self.LanguageTechnology,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mq18qz,True,,Agile-Cartoonist-912,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mq18qz/hello_i_need_duc_2004_dataset_for_my_master/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mq18qz/hello_i_need_duc_2004_dataset_for_my_master/,30199,1618316959.0,0,,False,,,,,,0
1361,,LanguageTechnology,"Hello,

I'm using BERT language model for predicting mental health issues. Later, I'll be using more language models for comparison. The dataset is big and the whole computation requires GPUs which I don't have. I was wondering if there's any quick way to make a training cluster with multiple CPU based computers to make training faster. I'm no knowledge of distributed computing so idk how to proceed with this. Do you guys have any idea?

Thanks.",t2_8s1tooht,False,,0,False,Deep learning on multiple computers,[],r/LanguageTechnology,False,6,,0,,False,t3_mpgwsl,False,dark,0.93,,public,13,0,{},,False,[],,False,False,,{},,False,13,,False,False,,False,,[],{},,True,,1618272058.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m using BERT language model for predicting mental health issues. Later, I&amp;#39;ll be using more language models for comparison. The dataset is big and the whole computation requires GPUs which I don&amp;#39;t have. I was wondering if there&amp;#39;s any quick way to make a training cluster with multiple CPU based computers to make training faster. I&amp;#39;m no knowledge of distributed computing so idk how to proceed with this. Do you guys have any idea?&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mpgwsl,True,,rapchickk,,21,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mpgwsl/deep_learning_on_multiple_computers/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mpgwsl/deep_learning_on_multiple_computers/,30199,1618243258.0,0,,False,,,,,,449
1362,,LanguageTechnology,"Hello there,
Im currently working on my Bachelorthesis and have only little knowledge on NLP. 
My idea was to finetune a BERT Model on a task that is a mix of fill-mask and text-summarization. 

Basically i would scrape for short sentences about a specific topic and display those. The user would have to crate template sentences with masked entries. And finally the transformer guessing those masked entries.

Does this sound realistic?

Could someone provide any information about how to fine tune something like that? Is it eveb possible to focus the transformer on a specific text?

Thanks !",t2_9vkre7v4,False,,0,False,[Q:] MLM for short phrases,[],r/LanguageTechnology,False,6,,0,,False,t3_mpee92,False,dark,1.0,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1618264658.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello there,
Im currently working on my Bachelorthesis and have only little knowledge on NLP. 
My idea was to finetune a BERT Model on a task that is a mix of fill-mask and text-summarization. &lt;/p&gt;

&lt;p&gt;Basically i would scrape for short sentences about a specific topic and display those. The user would have to crate template sentences with masked entries. And finally the transformer guessing those masked entries.&lt;/p&gt;

&lt;p&gt;Does this sound realistic?&lt;/p&gt;

&lt;p&gt;Could someone provide any information about how to fine tune something like that? Is it eveb possible to focus the transformer on a specific text?&lt;/p&gt;

&lt;p&gt;Thanks !&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mpee92,True,,Ieoure,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mpee92/q_mlm_for_short_phrases/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mpee92/q_mlm_for_short_phrases/,30199,1618235858.0,0,,False,,,,,,595
1363,,LanguageTechnology,,t2_apv7o2y8,False,,0,False,Tweet Scraping using Twint and Sentiment Analysis using Hugging Face Transformers #python #NLProc,[],r/LanguageTechnology,False,6,,0,,False,t3_mp8fos,False,dark,0.85,,public,14,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/z-sFQVN7hgg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Tweet Scraping using Twint and Sentiment Analysis using Hugging Face Transformers #python #NLProc', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/z-sFQVN7hgg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/z-sFQVN7hgg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/z-sFQVN7hgg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mp8fos', 'height': 200}",,False,14,,False,False,,False,,[],{},,False,,1618238776.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mp8fos,True,,rsree123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mp8fos/tweet_scraping_using_twint_and_sentiment_analysis/,all_ads,False,https://youtu.be/z-sFQVN7hgg,30199,1618209976.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Tweet Scraping using Twint and Sentiment Analysis using Hugging Face Transformers #python #NLProc', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/z-sFQVN7hgg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/z-sFQVN7hgg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,https://youtu.be/z-sFQVN7hgg,,,,,0
1364,,LanguageTechnology,"Hello,

I am going to make a simple sentiment analysis of Twitter posts. Basically searching for a word(name) and than retrieving n tweets and later returning the distribution positive, negative, neutral.",t2_9xraodz5,False,,0,False,Plan to make twitter sentiment analysis,[],r/LanguageTechnology,False,6,,0,,False,t3_mpbpel,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,1618520204.0,,[],{},,True,,1618255096.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am going to make a simple sentiment analysis of Twitter posts. Basically searching for a word(name) and than retrieving n tweets and later returning the distribution positive, negative, neutral.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mpbpel,True,,Overall_Flamingo_668,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mpbpel/plan_to_make_twitter_sentiment_analysis/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mpbpel/plan_to_make_twitter_sentiment_analysis/,30199,1618226296.0,0,,False,,,,,,204
1365,,LanguageTechnology,"Suppose you have downloaded the pdf of every Shakespeare play on your computer. Suppose now you want to find the name of a Shakespeare play that you read in high school, but you can't remember it's name - however, you do remember the general plot of the play, e.g. ""a danish prince is visted by his father's ghost and holds a skull in his hand while delivering a speech"". (Btw this is the plot of hamlet)

Suppose you type this sentence in - could the cosine similarity be used to find out which play is most similar to this sentence? Is there a common way to solve this kind of problem?",t2_3f0i9m72,False,,0,False,[D] is this a correct application of the cosine similarity?,[],r/LanguageTechnology,False,6,,0,,False,t3_mp5ipq,False,dark,1.0,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1618225273.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Suppose you have downloaded the pdf of every Shakespeare play on your computer. Suppose now you want to find the name of a Shakespeare play that you read in high school, but you can&amp;#39;t remember it&amp;#39;s name - however, you do remember the general plot of the play, e.g. &amp;quot;a danish prince is visted by his father&amp;#39;s ghost and holds a skull in his hand while delivering a speech&amp;quot;. (Btw this is the plot of hamlet)&lt;/p&gt;

&lt;p&gt;Suppose you type this sentence in - could the cosine similarity be used to find out which play is most similar to this sentence? Is there a common way to solve this kind of problem?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mp5ipq,True,,SQL_beginner,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mp5ipq/d_is_this_a_correct_application_of_the_cosine/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mp5ipq/d_is_this_a_correct_application_of_the_cosine/,30199,1618196473.0,0,,False,,,,,,587
1366,,LanguageTechnology,"Can somebody urgently suggest me the best method to extract a specified section from a .txt webpage.

Example-  [https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-000413.txt](https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-000413.txt)   

Suppose, I want to extract all text from the section ITEM 7. Management's Discussion and Analysis section. Now, the catch is that there are several such webpages and not all of them start with Item 7 \[they have Management's discussion and analysis common though\]. How to do this?",t2_5wcskabr,False,,0,False,Suggestion for web scraping,[],r/LanguageTechnology,False,6,,0,,False,t3_mpdt8m,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1618262787.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can somebody urgently suggest me the best method to extract a specified section from a .txt webpage.&lt;/p&gt;

&lt;p&gt;Example-  &lt;a href=""https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-000413.txt""&gt;https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-000413.txt&lt;/a&gt;   &lt;/p&gt;

&lt;p&gt;Suppose, I want to extract all text from the section ITEM 7. Management&amp;#39;s Discussion and Analysis section. Now, the catch is that there are several such webpages and not all of them start with Item 7 [they have Management&amp;#39;s discussion and analysis common though]. How to do this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mpdt8m,True,,debadri3,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mpdt8m/suggestion_for_web_scraping/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mpdt8m/suggestion_for_web_scraping/,30199,1618233987.0,0,,False,,,,,,540
1367,,LanguageTechnology,"Hello!  For my MSc Artificial Intelligence thesis I would like to work on fine-tuning transformers for NLP. I will have 5 months to complete my thesis. I don't have too much experience in NLP because the course was more focused on Computer Vision. I find transformers very interesting and I want to work on fine-tuning for dialogue systems and text classification. I haven't fully decided on the dataset or what classification I want to do. 

1. Can you suggest a topic for text classification? I am looking for an interesting subject that is also relevant for employers. 
2. There are so many different transformer models that I can work on. Which ones do you think would be the best? 

Thank you in advance!",t2_63f2q1ee,False,,0,False,Can you suggest me a Transformer Model for fine-tuning for my thesis?,[],r/LanguageTechnology,False,6,,0,,False,t3_mpaood,False,dark,0.5,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,False,,[],{},,True,,1618250465.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!  For my MSc Artificial Intelligence thesis I would like to work on fine-tuning transformers for NLP. I will have 5 months to complete my thesis. I don&amp;#39;t have too much experience in NLP because the course was more focused on Computer Vision. I find transformers very interesting and I want to work on fine-tuning for dialogue systems and text classification. I haven&amp;#39;t fully decided on the dataset or what classification I want to do. &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Can you suggest a topic for text classification? I am looking for an interesting subject that is also relevant for employers. &lt;/li&gt;
&lt;li&gt;There are so many different transformer models that I can work on. Which ones do you think would be the best? &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thank you in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mpaood,True,,bayraksc,,5,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mpaood/can_you_suggest_me_a_transformer_model_for/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mpaood/can_you_suggest_me_a_transformer_model_for/,30199,1618221665.0,0,,False,,,,,,709
1368,,LanguageTechnology,,t2_5fsp2x6v,False,,0,False,NER for Social Media Texts with Semantic Augmentation | Research Papers Summary 013,[],r/LanguageTechnology,False,6,,0,,False,t3_mow4oz,False,dark,0.88,,public,16,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/FmnbBL0ems8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'NER for Social Media Texts with Semantic Augmentation | Research Papers Summary 013', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/FmnbBL0ems8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/FmnbBL0ems8/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/FmnbBL0ems8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mow4oz', 'height': 200}",,False,16,,False,False,,False,,[],{},,False,,1618193991.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mow4oz,True,,RyanAI100,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mow4oz/ner_for_social_media_texts_with_semantic/,all_ads,False,https://youtu.be/FmnbBL0ems8,30199,1618165191.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'NER for Social Media Texts with Semantic Augmentation | Research Papers Summary 013', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/FmnbBL0ems8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Ryan Ong', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/FmnbBL0ems8/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/basketball10029508'}}",False,https://youtu.be/FmnbBL0ems8,,,,,0
1369,,LanguageTechnology,"
I have found various topic modelling algorithms, but they seem to depend on the interrelationships among words used in a document to determine the similarity. I want to achieve something similar to keyword clustering based on its meaning and retrieve top 5 most similar words of a query keyword. Any suggestions or path to solve this problem will be very helpful.

This community has been amazing for a fresher like me and thank you to each and everyone who takes out their time to help out people like me.",t2_8n5d56qx,False,,0,False,Given a corpus of about 10k keywords and another keyword x. How do I extract the top 5 most similar words to x.,[],r/LanguageTechnology,False,6,,0,,False,t3_moxlf4,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1618198422.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have found various topic modelling algorithms, but they seem to depend on the interrelationships among words used in a document to determine the similarity. I want to achieve something similar to keyword clustering based on its meaning and retrieve top 5 most similar words of a query keyword. Any suggestions or path to solve this problem will be very helpful.&lt;/p&gt;

&lt;p&gt;This community has been amazing for a fresher like me and thank you to each and everyone who takes out their time to help out people like me.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,moxlf4,True,,ChandlerBingggggggg,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/moxlf4/given_a_corpus_of_about_10k_keywords_and_another/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/moxlf4/given_a_corpus_of_about_10k_keywords_and_another/,30199,1618169622.0,0,,False,,,,,,507
1370,,LanguageTechnology,"Have a product in the works which includes a quiz element. Would like to support fill-in-the-blank for entering text answers, but not require perfect spelling.

If correct answer is “Guatemala”, would like to give 100 pts for exact match, and (for example) 93 for Guatamela, and 45 for Gwattanala, and so on. 

Are there good examples of this in the wild?",t2_ophpfym,False,,0,False,How to award a score (1-100) for closeness to the right answer?,[],r/LanguageTechnology,False,6,,0,,False,t3_moqi8s,False,dark,0.99,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1618174027.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Have a product in the works which includes a quiz element. Would like to support fill-in-the-blank for entering text answers, but not require perfect spelling.&lt;/p&gt;

&lt;p&gt;If correct answer is “Guatemala”, would like to give 100 pts for exact match, and (for example) 93 for Guatamela, and 45 for Gwattanala, and so on. &lt;/p&gt;

&lt;p&gt;Are there good examples of this in the wild?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,moqi8s,True,,TheBraveProtonDuck,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/moqi8s/how_to_award_a_score_1100_for_closeness_to_the/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/moqi8s/how_to_award_a_score_1100_for_closeness_to_the/,30199,1618145227.0,0,,False,,,,,,355
1371,,LanguageTechnology,"I'm a beginner in data science. I would like to have a roadmap to follow and understand the broad extent of this amazing field.

May be it could be difficult to find something specific for NLP. In that case, please share Machine Learning or Data Science roadmaps that you may have.

Beginners of this community would appreciate it!",t2_4df6a1hn,False,,0,False,Any good NLP roadmap?,[],r/LanguageTechnology,False,6,,0,,False,t3_moc365,False,dark,0.95,,public,35,0,{},,False,[],,False,False,,{},,False,35,,False,False,,False,,[],{},,True,,1618114317.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a beginner in data science. I would like to have a roadmap to follow and understand the broad extent of this amazing field.&lt;/p&gt;

&lt;p&gt;May be it could be difficult to find something specific for NLP. In that case, please share Machine Learning or Data Science roadmaps that you may have.&lt;/p&gt;

&lt;p&gt;Beginners of this community would appreciate it!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,moc365,True,,JotaPe-exe,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/moc365/any_good_nlp_roadmap/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/moc365/any_good_nlp_roadmap/,30199,1618085517.0,0,,False,,,,,,331
1372,,LanguageTechnology,"I have a training set of news articles that I wish to condense into triples, and I actually can evaluate how well these triples describe the original text and use it as a training signal, but I'm new to the field of NLP and uncertain about my options.  Which ML algorithms would you consider to take a BERT embedding layer as input and learn to turn/decode it to triples based on the mentioned training signal?",t2_75k22yqr,False,,0,False,"Embedding2triples, which ML algorithm?",[],r/LanguageTechnology,False,6,,0,,False,t3_mopzoq,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1618171955.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a training set of news articles that I wish to condense into triples, and I actually can evaluate how well these triples describe the original text and use it as a training signal, but I&amp;#39;m new to the field of NLP and uncertain about my options.  Which ML algorithms would you consider to take a BERT embedding layer as input and learn to turn/decode it to triples based on the mentioned training signal?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mopzoq,True,,NoRexTreX,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mopzoq/embedding2triples_which_ml_algorithm/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mopzoq/embedding2triples_which_ml_algorithm/,30199,1618143155.0,1,,False,,,,,,410
1373,,LanguageTechnology,,t2_3z8a2x6g,False,,0,False,Event extraction/Highlight detection from transcript,[],r/LanguageTechnology,False,6,,0,,False,t3_moc42e,False,dark,0.86,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,False,,1618114403.0,text,6,,,text,self.MachineLearning,False,,,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,moc42e,True,,i_kurt_i,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/moc42e/event_extractionhighlight_detection_from/,all_ads,False,/r/MachineLearning/comments/moc0zc/r_techniques_for_nlp_event_extraction_from_large/,30199,1618085603.0,0,,False,/r/MachineLearning/comments/moc0zc/r_techniques_for_nlp_event_extraction_from_large/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': '[removed]', 'author_fullname': 't2_3z8a2x6g', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[R] Techniques for NLP event extraction from large text of commentator speech from football match?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'three', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_moc0zc', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Research', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1618114113.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': 'moderator', 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'moc0zc', 'is_robot_indexable': False, 'report_reasons': None, 'author': 'i_kurt_i', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/moc0zc/r_techniques_for_nlp_event_extraction_from_large/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/moc0zc/r_techniques_for_nlp_event_extraction_from_large/', 'subreddit_subscribers': 1930729, 'created_utc': 1618085313.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_moc0zc,,,0
1374,,LanguageTechnology,"when doing NLP, do you usually augment your data with a ""pre defined corpus"" specific to the field your data comes from?

E.g. if you are working with medical data, do nlp procedures require you to use a predefined corpus from the medical domain?",t2_o4xj9,False,,0,False,"when doing NLP, do you usually augment your data with a ""pre defined corpus"" specific to the field your data comes from?",[],r/LanguageTechnology,False,6,,0,,False,t3_mnzami,False,dark,0.91,,public,9,0,{},,False,[],,False,False,,{},,False,9,,False,False,,False,,[],{},,True,,1618063633.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;when doing NLP, do you usually augment your data with a &amp;quot;pre defined corpus&amp;quot; specific to the field your data comes from?&lt;/p&gt;

&lt;p&gt;E.g. if you are working with medical data, do nlp procedures require you to use a predefined corpus from the medical domain?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mnzami,True,,blueest,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mnzami/when_doing_nlp_do_you_usually_augment_your_data/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mnzami/when_doing_nlp_do_you_usually_augment_your_data/,30199,1618034833.0,0,,False,,,,,True,246
1375,,LanguageTechnology,"This tutorial gives a step-by-step guide to implementing an RNN model (encoder-decoder sequence-to-sequence with attention mechanism) for French to English translation using Keras.

Additional topics covered include:

* The Problem With Sequence-to-Sequence Models for Neural Machine Translation
* An Introduction to Attention Mechanisms
* Categories of Attention Mechanisms
* Applications of Attention Mechanisms
* Neural Machine Translation Using an RNN With Attention Mechanism (Keras)

Tutorial link: [https://blog.paperspace.com/seq-to-seq-attention-mechanism-keras/](https://blog.paperspace.com/seq-to-seq-attention-mechanism-keras/)

Run all of the code on a free GPU: [https://ml-showcase.paperspace.com/projects/neural-machine-translation-with-keras](https://ml-showcase.paperspace.com/projects/neural-machine-translation-with-keras)",t2_15en0l,False,,0,False,[Tutorial] Neural Machine Translation With Attention With Keras,[],r/LanguageTechnology,False,6,,0,,False,t3_mnis25,False,dark,0.86,,public,10,0,{},,False,[],,False,False,,{},,False,10,,False,False,,False,,[],{},,True,,1618008397.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This tutorial gives a step-by-step guide to implementing an RNN model (encoder-decoder sequence-to-sequence with attention mechanism) for French to English translation using Keras.&lt;/p&gt;

&lt;p&gt;Additional topics covered include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Problem With Sequence-to-Sequence Models for Neural Machine Translation&lt;/li&gt;
&lt;li&gt;An Introduction to Attention Mechanisms&lt;/li&gt;
&lt;li&gt;Categories of Attention Mechanisms&lt;/li&gt;
&lt;li&gt;Applications of Attention Mechanisms&lt;/li&gt;
&lt;li&gt;Neural Machine Translation Using an RNN With Attention Mechanism (Keras)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tutorial link: &lt;a href=""https://blog.paperspace.com/seq-to-seq-attention-mechanism-keras/""&gt;https://blog.paperspace.com/seq-to-seq-attention-mechanism-keras/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Run all of the code on a free GPU: &lt;a href=""https://ml-showcase.paperspace.com/projects/neural-machine-translation-with-keras""&gt;https://ml-showcase.paperspace.com/projects/neural-machine-translation-with-keras&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mnis25,True,,hellopaperspace,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mnis25/tutorial_neural_machine_translation_with/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mnis25/tutorial_neural_machine_translation_with/,30199,1617979597.0,0,,False,,,,,,842
1376,,LanguageTechnology,"Hi everyone!

I got accepted into the Erasmus master in Language and Communications Technology and also into the University of Uppsala master in Language Technology. I need help deciding between the two.

If you don't know, the Erasmus program implies attending two universities, one year each, out of the seven universities of the consortium. They will tell me what universities i will attend once i say yes. Therefore, i don't know exactly what the course will be.

I feel more inclined to choose Uppsala because:
The logistics of living in Sweden for two years are easier than living in two different countries.
The program is free, as opposed to Erasmus in which i have to pay a fee.
I actually know the course of studies program, as opposed to Erasmus in which i will find out later.
I believe studying in Uppsala will make it easier for me to find a job and a life in Sweden.
I have a boyfriend living four hours away from Uppsala (i try not to let this be a factor but who am I kidding, it is).

On the other hand, i think maybe the Erasmus program has a better reputation and also living in two random countries sounds like fun.

If you have any advice it would be welcome. thanks!",t2_30vynpif,False,,0,False,Erasmus or Uppsala?,[],r/LanguageTechnology,False,6,,0,,False,t3_mnecov,False,dark,0.93,,public,12,0,{},,False,[],,False,False,,{},,False,12,,False,False,,False,,[],{},,True,,1617993283.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;

&lt;p&gt;I got accepted into the Erasmus master in Language and Communications Technology and also into the University of Uppsala master in Language Technology. I need help deciding between the two.&lt;/p&gt;

&lt;p&gt;If you don&amp;#39;t know, the Erasmus program implies attending two universities, one year each, out of the seven universities of the consortium. They will tell me what universities i will attend once i say yes. Therefore, i don&amp;#39;t know exactly what the course will be.&lt;/p&gt;

&lt;p&gt;I feel more inclined to choose Uppsala because:
The logistics of living in Sweden for two years are easier than living in two different countries.
The program is free, as opposed to Erasmus in which i have to pay a fee.
I actually know the course of studies program, as opposed to Erasmus in which i will find out later.
I believe studying in Uppsala will make it easier for me to find a job and a life in Sweden.
I have a boyfriend living four hours away from Uppsala (i try not to let this be a factor but who am I kidding, it is).&lt;/p&gt;

&lt;p&gt;On the other hand, i think maybe the Erasmus program has a better reputation and also living in two random countries sounds like fun.&lt;/p&gt;

&lt;p&gt;If you have any advice it would be welcome. thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mnecov,True,,imnowonderwoman,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mnecov/erasmus_or_uppsala/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mnecov/erasmus_or_uppsala/,30199,1617964483.0,0,,False,,,,,,1189
1377,,LanguageTechnology,"I am completely to new to nlp/text mining and am interested in learning more about it (on an applied side). Suppose I have a dataset with 1000 doctor comments (no ""labels"", i.e. I don't know if they are positive or negative) for different comments. The question is now, what can I do? 

Just by doing some google searches, it seems like the two popular things to do with this data is LDA and sentiment analysis. Just by looking how to do this in R, this looks doable. But what are some other popular algorithms that can done on this kind of data? I assume that through LDA and sentiment analysis, you can ""cluster"" these doctor comments together? If you have a new comment, you can see which from clustering which cluster this comment belongs to? 

In this example, how would the newer BERT algorithm come into play? Is there a main way to extract ""insights"" from these comments? Based on the text in these comments, are there algorithms that can determine if its possible to see which medical conditions  are more lethal, which age groups/demographics of patients are healthier, which medications are people taking, etc. ? Or is this a very advanced problem?

Thanks",t2_o4xj9,False,,0,False,Intro to nlp and text mining,[],r/LanguageTechnology,False,6,,0,,False,t3_mnith5,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1618008505.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am completely to new to nlp/text mining and am interested in learning more about it (on an applied side). Suppose I have a dataset with 1000 doctor comments (no &amp;quot;labels&amp;quot;, i.e. I don&amp;#39;t know if they are positive or negative) for different comments. The question is now, what can I do? &lt;/p&gt;

&lt;p&gt;Just by doing some google searches, it seems like the two popular things to do with this data is LDA and sentiment analysis. Just by looking how to do this in R, this looks doable. But what are some other popular algorithms that can done on this kind of data? I assume that through LDA and sentiment analysis, you can &amp;quot;cluster&amp;quot; these doctor comments together? If you have a new comment, you can see which from clustering which cluster this comment belongs to? &lt;/p&gt;

&lt;p&gt;In this example, how would the newer BERT algorithm come into play? Is there a main way to extract &amp;quot;insights&amp;quot; from these comments? Based on the text in these comments, are there algorithms that can determine if its possible to see which medical conditions  are more lethal, which age groups/demographics of patients are healthier, which medications are people taking, etc. ? Or is this a very advanced problem?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mnith5,True,,blueest,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mnith5/intro_to_nlp_and_text_mining/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mnith5/intro_to_nlp_and_text_mining/,30199,1617979705.0,0,,False,,,,,True,1167
1378,,LanguageTechnology," 

Hi all - I'm working on what I think could be an low-level NLP problem at work, and was wondering if anyone here had any ideas on modules I could/should look into that might provide a solution.

Problem: so, the company I work for deploys networked hardware into the field, and that hardware often requires repairs. Our repair folks take down a lot of unstructured data when they get the hardware back up and running, stored as a note on salesforce. I'm trying to figure out a way to process this information to get some meaning from it (e.g., are repair cases most often networking or electrical issues, or from weather damages...etc.,).

Solution: I've been trying to figure out a way to sort all of this information. One idea was to make a list of all strings in all notes, strip out all articles/conjunctions, and then create some word cloud. My hunch is that someone has probably solved something very similar to this, and I wanted to socialize it in this community to see if anyone had any thoughts!

I would massively appreciate any ideas, or pointers on what modules I should be looking into! Just to be clear, we use Python/Jupyter Notebooks.",t2_b5daqq4l,False,,0,False,NLP Problem,[],r/LanguageTechnology,False,6,,0,,False,t3_mn36ij,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1617949552.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all - I&amp;#39;m working on what I think could be an low-level NLP problem at work, and was wondering if anyone here had any ideas on modules I could/should look into that might provide a solution.&lt;/p&gt;

&lt;p&gt;Problem: so, the company I work for deploys networked hardware into the field, and that hardware often requires repairs. Our repair folks take down a lot of unstructured data when they get the hardware back up and running, stored as a note on salesforce. I&amp;#39;m trying to figure out a way to process this information to get some meaning from it (e.g., are repair cases most often networking or electrical issues, or from weather damages...etc.,).&lt;/p&gt;

&lt;p&gt;Solution: I&amp;#39;ve been trying to figure out a way to sort all of this information. One idea was to make a list of all strings in all notes, strip out all articles/conjunctions, and then create some word cloud. My hunch is that someone has probably solved something very similar to this, and I wanted to socialize it in this community to see if anyone had any thoughts!&lt;/p&gt;

&lt;p&gt;I would massively appreciate any ideas, or pointers on what modules I should be looking into! Just to be clear, we use Python/Jupyter Notebooks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mn36ij,True,,Ok_Caterpillar_7948,,8,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mn36ij/nlp_problem/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mn36ij/nlp_problem/,30199,1617920752.0,0,,False,,,,,,1154
1379,,LanguageTechnology,"Hello guys, 

I came here to ask you for a help. I'm writing my final thesis right now on Detecting plagiarism in text documents. And I have to be honest with you guys, It is over my head. Deadline is in 1 month and I don't know how to make a working piece of code. So I wanted to ask you if you are willing to help me or at least tell me if there is something I can do. 

So the task is to compare documents (preferably bunch of documents to one suspicious document) and find if the suspecious document is plagiarism to any of those documents in reference collection. Currently I have done loading multiple pdf files, making dataframe, preprocess the text data (tokenisation, lemmantisation, stemming, stopwords removal, punctuation removal, lowercased) and vectorization. Then I applied cosine similarity to it and it kinda works.

My question is if there is any possible way to apply Support Vector Machine or Naive Bayes to this task? and if so, how do I set it up? The main goal was to apply some Machine Learning algoritm but I took much bigger piece of pie than I was supposed to.

I am really really desperate and any information will help me. Thank you",t2_1q21txa3,False,,0,False,Means to detect plagiarism in textual documents,[],r/LanguageTechnology,False,6,,0,,False,t3_mn2vpx,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1617948681.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello guys, &lt;/p&gt;

&lt;p&gt;I came here to ask you for a help. I&amp;#39;m writing my final thesis right now on Detecting plagiarism in text documents. And I have to be honest with you guys, It is over my head. Deadline is in 1 month and I don&amp;#39;t know how to make a working piece of code. So I wanted to ask you if you are willing to help me or at least tell me if there is something I can do. &lt;/p&gt;

&lt;p&gt;So the task is to compare documents (preferably bunch of documents to one suspicious document) and find if the suspecious document is plagiarism to any of those documents in reference collection. Currently I have done loading multiple pdf files, making dataframe, preprocess the text data (tokenisation, lemmantisation, stemming, stopwords removal, punctuation removal, lowercased) and vectorization. Then I applied cosine similarity to it and it kinda works.&lt;/p&gt;

&lt;p&gt;My question is if there is any possible way to apply Support Vector Machine or Naive Bayes to this task? and if so, how do I set it up? The main goal was to apply some Machine Learning algoritm but I took much bigger piece of pie than I was supposed to.&lt;/p&gt;

&lt;p&gt;I am really really desperate and any information will help me. Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mn2vpx,True,,EnchLUL,,9,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mn2vpx/means_to_detect_plagiarism_in_textual_documents/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mn2vpx/means_to_detect_plagiarism_in_textual_documents/,30199,1617919881.0,0,,False,,,,,,1161
1380,,LanguageTechnology,"Hi, any leads on upcoming NLP summer schools.",t2_xf374,False,,0,False,Summer schools in NLP 2021,[],r/LanguageTechnology,False,6,,0,,False,t3_mmnuhk,False,dark,0.99,,public,30,0,{},,False,[],,False,False,,{},,False,30,,False,False,,False,,[],{},,True,,1617901990.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, any leads on upcoming NLP summer schools.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mmnuhk,True,,thak123,,16,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mmnuhk/summer_schools_in_nlp_2021/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mmnuhk/summer_schools_in_nlp_2021/,30199,1617873190.0,0,,False,,,,,,45
1381,,LanguageTechnology,,t2_apv7o2y8,False,,0,False,Stanford NLP Stanza NLP package - Biomedical NLP models demo #NLproc #python #clincialNLP,[],r/LanguageTechnology,False,6,,0,,False,t3_mmqvao,False,dark,0.86,,public,5,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/iQ5kP2kOGNA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Stanford NLP  Stanza NLP package -  Biomedical NLP models demo #NLproc #python #clincialNLP', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/iQ5kP2kOGNA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/iQ5kP2kOGNA/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/iQ5kP2kOGNA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mmqvao', 'height': 200}",,False,5,,False,False,,False,,[],{},,False,,1617914337.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mmqvao,True,,rsree123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mmqvao/stanford_nlp_stanza_nlp_package_biomedical_nlp/,all_ads,False,https://youtu.be/iQ5kP2kOGNA,30199,1617885537.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Stanford NLP  Stanza NLP package -  Biomedical NLP models demo #NLproc #python #clincialNLP', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/iQ5kP2kOGNA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/iQ5kP2kOGNA/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/RSREETech'}}",False,https://youtu.be/iQ5kP2kOGNA,,,,,0
1382,,LanguageTechnology,,t2_a2fvpp34,False,,0,False,Question Answering (Seq2Seq) Visualization with Transformer Neural Networks,[],r/LanguageTechnology,False,6,,0,,False,t3_mmh89x,False,dark,0.89,,public,13,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/GTVgJhSlHEk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Animation of Sequence to Sequence Learning (Seq2Seq with Transformer Neural Networks)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/GTVgJhSlHEk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'learningcurve', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/GTVgJhSlHEk/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/learningcurveai'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/GTVgJhSlHEk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mmh89x', 'height': 200}",,False,13,,False,False,,False,,[],{},,False,,1617875085.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mmh89x,True,,No-Guard-5438,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mmh89x/question_answering_seq2seq_visualization_with/,all_ads,False,https://youtu.be/GTVgJhSlHEk,30199,1617846285.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Animation of Sequence to Sequence Learning (Seq2Seq with Transformer Neural Networks)', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/GTVgJhSlHEk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'learningcurve', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/GTVgJhSlHEk/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/learningcurveai'}}",False,https://youtu.be/GTVgJhSlHEk,,,,,0
1383,,LanguageTechnology,"It is a common consensus that words appearing in similar contexts are semantically similar. But this definition breaks drastically when we consider antonymy relationships - &gt; For. eg., the word ""positive"" and ""negative"" generally appear in similar contexts and are assigned spatially close vectors. In the inherent sense, it does not contain meaning. 

Word2vec was trained on contextual information is somehow not commonly referred to as contextualized word embeddings. However. embeddings realized through BERT and Elmo are termed contextualized word-embeddings. Is there a reason for this? Or perhaps I might be wrong here and it was already called contextual word embedding. The embeddings through BERT and ElMO can be called dynamic embeddings at best, but it is not difficult to construct dynamic embeddings through word2vec embeddings. Does BERT embedding inherently have any other semantic information (non-positional, non-context), other than a more complicated interaction layer (and perhaps more data)?",t2_a1hqdli,False,,0,False,"Are word embeddings semantically ""meaningful""?",[],r/LanguageTechnology,False,6,,0,,False,t3_mmknon,False,dark,1.0,,public,5,0,{},,False,[],,False,False,,{},,False,5,,False,False,,False,,[],{},,True,,1617887641.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;It is a common consensus that words appearing in similar contexts are semantically similar. But this definition breaks drastically when we consider antonymy relationships - &amp;gt; For. eg., the word &amp;quot;positive&amp;quot; and &amp;quot;negative&amp;quot; generally appear in similar contexts and are assigned spatially close vectors. In the inherent sense, it does not contain meaning. &lt;/p&gt;

&lt;p&gt;Word2vec was trained on contextual information is somehow not commonly referred to as contextualized word embeddings. However. embeddings realized through BERT and Elmo are termed contextualized word-embeddings. Is there a reason for this? Or perhaps I might be wrong here and it was already called contextual word embedding. The embeddings through BERT and ElMO can be called dynamic embeddings at best, but it is not difficult to construct dynamic embeddings through word2vec embeddings. Does BERT embedding inherently have any other semantic information (non-positional, non-context), other than a more complicated interaction layer (and perhaps more data)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mmknon,True,,flerakml,,5,False,all_ads,False,[],False,,/r/LanguageTechnology/comments/mmknon/are_word_embeddings_semantically_meaningful/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mmknon/are_word_embeddings_semantically_meaningful/,30199,1617858841.0,0,,False,,,,,,1016
1384,,LanguageTechnology,"Hi everyone,

is it possible to use MarianMT transformer models with spacy-transformers? Similar to [https://spacy.io/universe/project/spacy-transformers](https://spacy.io/universe/project/spacy-transformers)  
MarianMT: [https://huggingface.co/transformers/model\_doc/marian.html?highlight=mariantokenizer#multilingual-models](https://huggingface.co/transformers/model_doc/marian.html?highlight=mariantokenizer#multilingual-models)

Thanks!",t2_sdfge43,False,,0,False,MarianMT usage with Spacy possible?,[],r/LanguageTechnology,False,6,,0,,False,t3_mmnrmr,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1617901605.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;is it possible to use MarianMT transformer models with spacy-transformers? Similar to &lt;a href=""https://spacy.io/universe/project/spacy-transformers""&gt;https://spacy.io/universe/project/spacy-transformers&lt;/a&gt;&lt;br/&gt;
MarianMT: &lt;a href=""https://huggingface.co/transformers/model_doc/marian.html?highlight=mariantokenizer#multilingual-models""&gt;https://huggingface.co/transformers/model_doc/marian.html?highlight=mariantokenizer#multilingual-models&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mmnrmr,True,,ProKellaSK,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mmnrmr/marianmt_usage_with_spacy_possible/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mmnrmr/marianmt_usage_with_spacy_possible/,30199,1617872805.0,0,,False,,,,,,441
1385,,LanguageTechnology,,t2_j0zwm,False,,0,False,Multi-Document Summarization: The Wikipedia Current Events Portal Dataset (Dataset and Colab notebook),[],r/LanguageTechnology,False,6,,0,,False,t3_mlynns,False,dark,0.97,,public,21,0,{},,False,[],,False,False,,{},,False,21,,False,False,,False,,[],{},,False,,1617818115.0,text,6,,,text,aylien.com,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mlynns,True,,MikeWally,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mlynns/multidocument_summarization_the_wikipedia_current/,all_ads,False,https://aylien.com/blog/multi-document-summarisation-and-the-wcep-dataset,30199,1617789315.0,0,,False,https://aylien.com/blog/multi-document-summarisation-and-the-wcep-dataset,,,,,0
1386,,LanguageTechnology,"Using scikit learn, I managed to train my model but dont know how to use the model to predict new text passages. I have watched tons of tutorials but none of them go beyond training and testing. Below is the code Im using. Any help is appreciated.

        data_source_url = ""/path/to/file.csv""
        airline_tweets = pd.read_csv(data_source_url)
        
        features = airline_tweets.iloc[:, 10].values
        labels = airline_tweets.iloc[:, 1].values
        
        processed_features = []
        
          # I do some text processing here and then append the text to processed_features
        
                
        vectorizer = CountVectorizer(analyzer = 'word', lowercase = False)
        features = vectorizer.fit_transform(processed_features)
        features_nd = features.toarray() # for easy usage
        
        X_train, X_test, y_train, y_test  = train_test_split(features_nd, labels, train_size=0.80, random_state=1234)
        
        log_model = LogisticRegression()
        log_model = log_model.fit(X=X_train, y=y_train)
            
        predictions = log_model.predict(X_test)",t2_5r4mo7fh,False,,0,False,How do I predict sentiment of unseen text (After training and testing),[],r/LanguageTechnology,False,6,,0,,False,t3_mmfwr6,False,dark,0.43,,public,0,0,{},,False,[],,False,False,,{},,False,0,,False,False,,1617847246.0,,[],{},,True,,1617870546.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Using scikit learn, I managed to train my model but dont know how to use the model to predict new text passages. I have watched tons of tutorials but none of them go beyond training and testing. Below is the code Im using. Any help is appreciated.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    data_source_url = &amp;quot;/path/to/file.csv&amp;quot;
    airline_tweets = pd.read_csv(data_source_url)

    features = airline_tweets.iloc[:, 10].values
    labels = airline_tweets.iloc[:, 1].values

    processed_features = []

      # I do some text processing here and then append the text to processed_features


    vectorizer = CountVectorizer(analyzer = &amp;#39;word&amp;#39;, lowercase = False)
    features = vectorizer.fit_transform(processed_features)
    features_nd = features.toarray() # for easy usage

    X_train, X_test, y_train, y_test  = train_test_split(features_nd, labels, train_size=0.80, random_state=1234)

    log_model = LogisticRegression()
    log_model = log_model.fit(X=X_train, y=y_train)

    predictions = log_model.predict(X_test)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mmfwr6,True,,Epiphany925,,4,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mmfwr6/how_do_i_predict_sentiment_of_unseen_text_after/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mmfwr6/how_do_i_predict_sentiment_of_unseen_text_after/,30199,1617841746.0,0,,False,,,,,,1117
1387,,LanguageTechnology,"Im working on predicting answers given question using seq2seq. So far it is as simple as can be . Encoder and decoder are just an LSTM each.

However, even if train loss decreases w epochs, validation loss is too high.

And the question answer train data has many misspellings and words from another language sometimes.

How can I help this? I already included Dropout layers with learning rate 0.2 to 0.5 for input, and 0.8 for output but validation loss still too high..

Accuracy also is so low. Like 5%",t2_445mnejc,False,,0,False,Overfitting? Input data with too many misspellings,[],r/LanguageTechnology,False,6,,0,,False,t3_mm12uc,False,dark,1.0,,public,3,0,{},,False,[],,False,False,,{},,False,3,,False,False,,False,,[],{},,True,,1617827773.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Im working on predicting answers given question using seq2seq. So far it is as simple as can be . Encoder and decoder are just an LSTM each.&lt;/p&gt;

&lt;p&gt;However, even if train loss decreases w epochs, validation loss is too high.&lt;/p&gt;

&lt;p&gt;And the question answer train data has many misspellings and words from another language sometimes.&lt;/p&gt;

&lt;p&gt;How can I help this? I already included Dropout layers with learning rate 0.2 to 0.5 for input, and 0.8 for output but validation loss still too high..&lt;/p&gt;

&lt;p&gt;Accuracy also is so low. Like 5%&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mm12uc,True,,phresia,,2,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mm12uc/overfitting_input_data_with_too_many_misspellings/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mm12uc/overfitting_input_data_with_too_many_misspellings/,30199,1617798973.0,0,,False,,,,,,506
1388,,LanguageTechnology,"Hi all,
Long time lurker, first time poster here. 

I’m turning to the hivemind because I’m at my wit’s end.

I’ve been tasked with providing a simple corpus service that returns tokens, sentences, etc. from a huge corpus based on certain criteria. These criteria are used to filter the corpus according to the annotations of the corpus entries. For example I should be able to return all nouns in the corpus back to the client. 

My first thought was to parse the corpus and load it into some sort of database which I could then query. But it didn’t work because of the technical limitations at my job. Also it seems like a lot of overhead to just do simple queries on the data. 

Then I thought I would load the corpus into a pandas dataframe, keeping it in memory for the lifecycle of the corpus service and querying it when client requests come in. But I find the solution a bit hacky. For example, it becomes very brittle when I try to map the schema of the corpus to my service’s own internal schema. 

Does anyone have experience with this problem? Is there a more straightforward approach that I haven’t thought of before? 

Thanks for taking the time to read this!",t2_kbj77jj,False,,0,False,Simple corpus service,[],r/LanguageTechnology,False,6,,0,,False,t3_mm48xk,False,dark,1.0,,public,1,0,{},,False,[],,False,False,,{},,False,1,,False,False,,False,,[],{},,True,,1617837037.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,
Long time lurker, first time poster here. &lt;/p&gt;

&lt;p&gt;I’m turning to the hivemind because I’m at my wit’s end.&lt;/p&gt;

&lt;p&gt;I’ve been tasked with providing a simple corpus service that returns tokens, sentences, etc. from a huge corpus based on certain criteria. These criteria are used to filter the corpus according to the annotations of the corpus entries. For example I should be able to return all nouns in the corpus back to the client. &lt;/p&gt;

&lt;p&gt;My first thought was to parse the corpus and load it into some sort of database which I could then query. But it didn’t work because of the technical limitations at my job. Also it seems like a lot of overhead to just do simple queries on the data. &lt;/p&gt;

&lt;p&gt;Then I thought I would load the corpus into a pandas dataframe, keeping it in memory for the lifecycle of the corpus service and querying it when client requests come in. But I find the solution a bit hacky. For example, it becomes very brittle when I try to map the schema of the corpus to my service’s own internal schema. &lt;/p&gt;

&lt;p&gt;Does anyone have experience with this problem? Is there a more straightforward approach that I haven’t thought of before? &lt;/p&gt;

&lt;p&gt;Thanks for taking the time to read this!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mm48xk,True,,L4teralu5,,6,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mm48xk/simple_corpus_service/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mm48xk/simple_corpus_service/,30199,1617808237.0,0,,False,,,,,,1173
1389,,LanguageTechnology,"Hey everyone!

I'm applying for my Master's right now and the threads in this sub have been tremendously helpful! I was wondering if anyone is studying / knows someone who's studying at Uni-Potsdam. I've seen next to nothing about the program in this sub. If I'm being honest, that program looks the most appealing of the European programs I've come across in my research. So, to anyone who can provide some insight, I ask:

* Is it as heavy on the Machine Learning / Deep Learning front as it would appear from the course plan?
* Is it technical and applied (as opposed to theoretical and scholarly)?
* What's the general vibe? How do you and/or the students like it?

You can totally stop reading now but in case anyone's interested:  
I've been accepted to UEF and Edinburgh so far, hearing from Uppsala and/or Gothenburg on Friday. I'm Canadian, so the Swedish schools are either scholarship or $$$$$$. German applications aren't due for a little while so I have this nice little intermission to pause and ponder. Any advice is welcome! Thanks!",t2_14nkh3,False,,0,False,What's the word on Potsdam's MSc Cognitive Systems?,[],r/LanguageTechnology,False,6,,0,,False,t3_mlrmmy,False,dark,0.88,,public,6,0,{},,False,[],,False,False,,{},,False,6,,False,False,,False,,[],{},,True,,1617789585.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m applying for my Master&amp;#39;s right now and the threads in this sub have been tremendously helpful! I was wondering if anyone is studying / knows someone who&amp;#39;s studying at Uni-Potsdam. I&amp;#39;ve seen next to nothing about the program in this sub. If I&amp;#39;m being honest, that program looks the most appealing of the European programs I&amp;#39;ve come across in my research. So, to anyone who can provide some insight, I ask:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Is it as heavy on the Machine Learning / Deep Learning front as it would appear from the course plan?&lt;/li&gt;
&lt;li&gt;Is it technical and applied (as opposed to theoretical and scholarly)?&lt;/li&gt;
&lt;li&gt;What&amp;#39;s the general vibe? How do you and/or the students like it?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can totally stop reading now but in case anyone&amp;#39;s interested:&lt;br/&gt;
I&amp;#39;ve been accepted to UEF and Edinburgh so far, hearing from Uppsala and/or Gothenburg on Friday. I&amp;#39;m Canadian, so the Swedish schools are either scholarship or $$$$$$. German applications aren&amp;#39;t due for a little while so I have this nice little intermission to pause and ponder. Any advice is welcome! Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mlrmmy,True,,NooksnGrannies,,1,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mlrmmy/whats_the_word_on_potsdams_msc_cognitive_systems/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mlrmmy/whats_the_word_on_potsdams_msc_cognitive_systems/,30199,1617760785.0,0,,False,,,,,,1048
1390,,LanguageTechnology,,t2_21qaqh1p,False,,0,False,AI (GPT-2) Text Generation in Python with Kaggle Dataset | FineTuning GPT Model,[],r/LanguageTechnology,False,6,,0,,False,t3_mljed7,False,dark,0.86,,public,15,0,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/6HcGTQKfeXY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'AI (GPT-2) Text Generation in Python with Kaggle Dataset | FineTuning GPT Model', 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/6HcGTQKfeXY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'version': '1.0', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/6HcGTQKfeXY/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/6HcGTQKfeXY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/mljed7', 'height': 200}",,False,15,,False,False,,False,,[],{},,False,,1617765459.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mljed7,True,,dulldata,,3,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mljed7/ai_gpt2_text_generation_in_python_with_kaggle/,all_ads,False,https://youtu.be/6HcGTQKfeXY,30199,1617736659.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'AI (GPT-2) Text Generation in Python with Kaggle Dataset | FineTuning GPT Model', 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/6HcGTQKfeXY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'version': '1.0', 'author_name': '1littlecoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/6HcGTQKfeXY/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/c/1littlecoder'}, 'type': 'youtube.com'}",False,https://youtu.be/6HcGTQKfeXY,,,,,0
1391,,LanguageTechnology,"I see several websites listing top X conferences in NLP (Natural Language Processing), but I am not sure if there is some kind of ranking for these conferences. It will be amazing if anyone has some clue about it. Thank you very much for any hints or pointers or answers. :)",t2_9ptho1m,False,,0,False,What are the top 15 conferences in Natural Language Processing?,[],r/LanguageTechnology,False,6,,0,,False,t3_mlce0h,False,dark,0.91,,public,24,1,{},,False,[],,False,False,,{},,False,24,,False,False,,False,,[],{},,True,,1617746640.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I see several websites listing top X conferences in NLP (Natural Language Processing), but I am not sure if there is some kind of ranking for these conferences. It will be amazing if anyone has some clue about it. Thank you very much for any hints or pointers or answers. :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 150, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Helpful', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mlce0h,True,,freaky_eater,,10,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mlce0h/what_are_the_top_15_conferences_in_natural/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mlce0h/what_are_the_top_15_conferences_in_natural/,30199,1617717840.0,0,,False,,,,,,274
1392,,LanguageTechnology,,t2_dwfcl,False,,0,False,[N] Grammarly releases a grammatical error correction (GEC) dataset for the Ukrainian language,[],r/LanguageTechnology,False,6,,0,,False,t3_mld6v9,False,dark,0.95,,public,14,0,{},,False,[],,False,False,,{},,False,14,,False,False,,False,,[],{},,False,,1617748863.0,text,6,,,text,self.MachineLearning,False,,,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mld6v9,True,,asivokon,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mld6v9/n_grammarly_releases_a_grammatical_error/,all_ads,False,/r/MachineLearning/comments/mlcv28/n_grammarly_releases_a_grammatical_error/,30199,1617720063.0,0,,False,/r/MachineLearning/comments/mlcv28/n_grammarly_releases_a_grammatical_error/,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': 'This dataset contains 20,715 sentences annotated for grammatical errors and fluency correction. The license is CC-BY 4.0.\n\nThis blog post provides some context: [https://www.grammarly.com/blog/engineering/announcing-ua-gec/](https://www.grammarly.com/blog/engineering/announcing-ua-gec/)\n\nThe data and code are on Github: [https://github.com/grammarly/ua-gec](https://github.com/grammarly/ua-gec)\n\nPaper (draft): [https://arxiv.org/abs/2103.16997](https://arxiv.org/abs/2103.16997)\n\nI am one of the authors. Happy to answer your questions :)', 'author_fullname': 't2_dwfcl', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[N] Grammarly releases a grammatical error correction (GEC) dataset for the Ukrainian language', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'two', 'downs': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_mlcv28', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 72, 'total_awards_received': 0, 'media_embed': {}, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'News', 'can_mod_post': False, 'score': 72, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': '', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1617747941.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This dataset contains 20,715 sentences annotated for grammatical errors and fluency correction. The license is CC-BY 4.0.&lt;/p&gt;\n\n&lt;p&gt;This blog post provides some context: &lt;a href=""https://www.grammarly.com/blog/engineering/announcing-ua-gec/""&gt;https://www.grammarly.com/blog/engineering/announcing-ua-gec/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The data and code are on Github: &lt;a href=""https://github.com/grammarly/ua-gec""&gt;https://github.com/grammarly/ua-gec&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Paper (draft): &lt;a href=""https://arxiv.org/abs/2103.16997""&gt;https://arxiv.org/abs/2103.16997&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I am one of the authors. Happy to answer your questions :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'mlcv28', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'asivokon', 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/mlcv28/n_grammarly_releases_a_grammatical_error/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/mlcv28/n_grammarly_releases_a_grammatical_error/', 'subreddit_subscribers': 1930729, 'created_utc': 1617719141.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_mlcv28,,,0
1393,,LanguageTechnology,"This sub has been very helpful in the past so I am hoping I can get some feedback. For my project, I am essentially trying to find a way to detect changes over time to a news narrative. At this stage I have applied an algorithm to successfully group together the stories that follow the development of the same event. So now I need to find a way to track, analyze, and maybe quantify how the events (and their coverage changes) . My current approach is using topic modeling to find important keywords in each of the articles. Then, I use those key words to map how the stories change overtime. So in the most basic of terms, I am identifying key words in the first article in each narrative chain and then comparing how those keywords change and are different from key words identified in subsequent articles about the same event.  Does this approach sound reasonable?  Is there anything else I should be trying instead?  Thanks everyone!",t2_4qae118k,False,,0,False,Requesting feedback on project related to tracking how news events change over time,[],r/LanguageTechnology,False,6,,0,,False,t3_mliti6,False,dark,1.0,,public,2,0,{},,False,[],,False,False,,{},,False,2,,False,False,,False,,[],{},,True,,1617763927.0,text,6,,,text,self.LanguageTechnology,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This sub has been very helpful in the past so I am hoping I can get some feedback. For my project, I am essentially trying to find a way to detect changes over time to a news narrative. At this stage I have applied an algorithm to successfully group together the stories that follow the development of the same event. So now I need to find a way to track, analyze, and maybe quantify how the events (and their coverage changes) . My current approach is using topic modeling to find important keywords in each of the articles. Then, I use those key words to map how the stories change overtime. So in the most basic of terms, I am identifying key words in the first article in each narrative chain and then comparing how those keywords change and are different from key words identified in subsequent articles about the same event.  Does this approach sound reasonable?  Is there anything else I should be trying instead?  Thanks everyone!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,[],[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,mliti6,True,,Massive-Marzipan,,7,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/mliti6/requesting_feedback_on_project_related_to/,all_ads,False,https://www.reddit.com/r/LanguageTechnology/comments/mliti6/requesting_feedback_on_project_related_to/,30199,1617735127.0,0,,False,,,,,,938
1394,,LanguageTechnology,,t2_apv7o2y8,False,,0,False,LDA Topic Modelling Explained with implementation using gensim in Python -#NLPRoc tutorial,[],r/LanguageTechnology,False,6,,0,,False,t3_ml2m65,False,dark,0.96,,public,26,1,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/nNvPvvuPnGs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}",,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'LDA Topic Modelling Explained with implementation using gensim in Python -#NLPRoc tutorial', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/nNvPvvuPnGs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech - NLP/AI/ML simplified', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/nNvPvvuPnGs/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCPAfv1GeraR1WM0608He6ow'}}",False,False,,"{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/nNvPvvuPnGs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ml2m65', 'height': 200}",,False,26,,False,False,,False,,[],{},,False,,1617709023.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 125, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'penny_donate': None, 'award_sub_type': 'GLOBAL', 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'days_of_premium': 0, 'tiers_by_required_awardings': None, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_width': 2048, 'static_icon_width': 2048, 'start_date': None, 'is_enabled': True, 'awardings_required_to_grant_benefits': None, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'static_icon_height': 2048, 'name': 'Wholesome', 'resized_static_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128, 'height': 128}], 'icon_format': None, 'icon_height': 2048, 'penny_price': None, 'award_type': 'global', 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png'}]",[],False,False,False,False,,[],False,,,,t5_2rkr2,,,,ml2m65,True,,rsree123,,0,True,all_ads,False,[],False,,/r/LanguageTechnology/comments/ml2m65/lda_topic_modelling_explained_with_implementation/,all_ads,False,https://youtu.be/nNvPvvuPnGs,30199,1617680223.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'LDA Topic Modelling Explained with implementation using gensim in Python -#NLPRoc tutorial', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/nNvPvvuPnGs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'RSREETech - NLP/AI/ML simplified', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/nNvPvvuPnGs/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCPAfv1GeraR1WM0608He6ow'}}",False,https://youtu.be/nNvPvvuPnGs,,,,,0
